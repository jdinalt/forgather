{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bb9655-0848-448a-a500-28196416c634",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Source\n",
    "[aiws.yamldict](../aiws/yamldict.py)  \n",
    "[tutorial_code.datasets](../tutorial_code/datasets.py)  \n",
    "[tutorial_code.tokenizer](../tutorial_code/tokenizer.py)\n",
    "\n",
    "### See Also\n",
    "[tokenizer.ipynb](tokenizer.ipynb)\n",
    "\n",
    "### Config\n",
    "[config.yaml](config/config.yaml)  \n",
    "[paths.yaml](config/paths.yaml)  \n",
    "[tokenizer.yaml](config/tokenizer.yaml)  \n",
    "[dataset.yaml](config/dataset.yaml)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1725f627-3dd3-4214-b066-11888271525c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assets_dir': '..',\n",
      " 'dataset_id': 'roneneldan/TinyStories',\n",
      " 'datasets_dir': '../datasets',\n",
      " 'model_src_dir': '../model_zoo',\n",
      " 'models_dir': 'forgather_demo/output_models',\n",
      " 'script_dir': '../scripts',\n",
      " 'search_paths': ['forgather_demo', '../templates', '../model_zoo'],\n",
      " 'templates_dir': 'forgather_demo',\n",
      " 'tokenizer_dir': '../tokenizers',\n",
      " 'train_script_path': '../scripts/train_script.py',\n",
      " 'whitelist_path': 'forgather_demo/whitelist.yaml'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if '..' not in sys.path: sys.path.insert(0, '..')\n",
    "from aiws.dotdict import DotDict\n",
    "from forgather.config import load_config\n",
    "from pprint import pp, pformat\n",
    "\n",
    "# Load meta-configuration\n",
    "dirs = DotDict(load_config('forgather_config.yaml').config)\n",
    "print(pformat(dirs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0c23e-2311-4dd4-9102-013e5f777bd1",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will need some data to train our model on. For this tutorial, we will use a dataset named \"TinyStories,\" which is a synthetic dataset generated by ChatGPT designed for training very small language models to produce coherent output. This is made possible by limiting the examples to things which a 4-year-old child would be able to understand, with a total vocabulary of about 1500 words.\n",
    "\n",
    "Huggingface dataset link:  \n",
    "https://huggingface.co/datasets/roneneldan/TinyStories  \n",
    "\n",
    "The paper describing the dataset:  \n",
    "https://arxiv.org/abs/2305.07759\n",
    "\n",
    "The first time this is run, it will download the dataset to your cache, which make take a few minutes. After that, the dataset will be loaded from your cache.\n",
    "\n",
    "source: [tutorial_code.datasets.load_dataset_from_config()](../tutorial_code/datasets.py)\n",
    "\n",
    "The dataset is split into two sections, \"train\" and \"validation.\" The validation set is not present in the training dataset, which allows one to test the model on data is has never seen, thus allowing one to confirm that the model is learning to generalize and not just memorize the data. As such, the model should never be trained on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fa226a9-4047-48c3-97c1-6b8cc3b9b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n",
      "****************************************\n",
      "DatasetInfo(description='',\n",
      "            citation='',\n",
      "            homepage='',\n",
      "            license='',\n",
      "            features={'text': Value(dtype='string', id=None)},\n",
      "            post_processed=None,\n",
      "            supervised_keys=None,\n",
      "            task_templates=None,\n",
      "            builder_name='parquet',\n",
      "            dataset_name='tiny_stories',\n",
      "            config_name='default',\n",
      "            version=0.0.0,\n",
      "            splits={'train': SplitInfo(name='train',\n",
      "                                       num_bytes=1911420483,\n",
      "                                       num_examples=2119719,\n",
      "                                       shard_lengths=[559930,\n",
      "                                                      559930,\n",
      "                                                      559930,\n",
      "                                                      439929],\n",
      "                                       dataset_name='tiny_stories'),\n",
      "                    'validation': SplitInfo(name='validation',\n",
      "                                            num_bytes=19306310,\n",
      "                                            num_examples=21990,\n",
      "                                            shard_lengths=None,\n",
      "                                            dataset_name='tiny_stories')},\n",
      "            download_checksums={'hf://datasets/roneneldan/TinyStories@691b0d9bd48ade766778c940011ca1c549f6359b/data/train-00000-of-00004-2d5a1467fff1081b.parquet': {'num_bytes': 248731111,\n",
      "                                                                                                                                                                     'checksum': None},\n",
      "                                'hf://datasets/roneneldan/TinyStories@691b0d9bd48ade766778c940011ca1c549f6359b/data/train-00001-of-00004-5852b56a2bd28fd9.parquet': {'num_bytes': 248171980,\n",
      "                                                                                                                                                                     'checksum': None},\n",
      "                                'hf://datasets/roneneldan/TinyStories@691b0d9bd48ade766778c940011ca1c549f6359b/data/train-00002-of-00004-a26307300439e943.parquet': {'num_bytes': 245894874,\n",
      "                                                                                                                                                                     'checksum': None},\n",
      "                                'hf://datasets/roneneldan/TinyStories@691b0d9bd48ade766778c940011ca1c549f6359b/data/train-00003-of-00004-d243063613e5a057.parquet': {'num_bytes': 247988350,\n",
      "                                                                                                                                                                     'checksum': None},\n",
      "                                'hf://datasets/roneneldan/TinyStories@691b0d9bd48ade766778c940011ca1c549f6359b/data/validation-00000-of-00001-869c898b519ad725.parquet': {'num_bytes': 9989127,\n",
      "                                                                                                                                                                          'checksum': None}},\n",
      "            download_size=1000775442,\n",
      "            post_processing_size=None,\n",
      "            dataset_size=1930726793,\n",
      "            size_in_bytes=2931502235)\n",
      "****************************************\n",
      "{'text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_dict = load_dataset(\"roneneldan/TinyStories\")\n",
    "print(dataset_dict)\n",
    "train_dataset = dataset_dict['train']\n",
    "print('*' * 40)\n",
    "pp(train_dataset.info)\n",
    "print('*' * 40)\n",
    "pp(train_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2387d-2638-4f91-8e89-8d9001cf4107",
   "metadata": {},
   "source": [
    "We can take a look at a random sampling of examples from the training dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "456273f5-c24d-46a5-95b5-d77676846a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing 3 random records from dataset...\n",
      "============================================================================================\n",
      "\n",
      "Once upon a time, there was a little bird named Tweety. Tweety loved to fly around and explore nature. One day, Tweety saw a big tree with lots of colorful leaves. Tweety wanted to see the leaves up close, so he flew down to the ground.\n",
      "\n",
      "As Tweety landed on the ground, he saw a broken branch. The branch was very sad because it couldn't reach the sky anymore. Tweety felt bad for the broken branch, so he decided to cover it with some leaves.\n",
      "\n",
      "Tweety picked up some leaves and carefully placed them \n",
      "============================================================================================\n",
      "\n",
      "One day, a boy named Tom found a map. The map showed a big park. Tom wanted to go to the park. He asked his friend, Sam, to go with him. Sam was very happy to go.\n",
      "\n",
      "At the park, Tom and Sam played with a ball. They had a lot of fun. But then, the ball went into a tree. Tom and Sam were sad. They tried to get the ball, but it remained in the tree.\n",
      "\n",
      "Tom said, \"I am sorry, Sam. I should not have kicked the ball so hard.\" Sam said, \"It is okay, Tom. We can still have fun at the park.\" They played oth\n",
      "============================================================================================\n",
      "\n",
      "Once upon a time, there was a kind girl named Lily. She had a magic mirror that could talk. Lily loved her mirror and always took good care of it. She knew that the mirror was special and she valued it a lot.\n",
      "\n",
      "One day, Lily asked the mirror, \"Mirror, who is the kindest person in the world?\" The mirror replied, \"Lily, you are the kindest person I know. You are always nice to others and you take good care of me.\"\n",
      "\n",
      "Lily smiled and said, \"Thank you, mirror. I will always be kind and value our friend\n"
     ]
    }
   ],
   "source": [
    "def print_sample_records(dataset, section=\"text\", n_records=3, max_length=500):\n",
    "    print(f\"Showing {n_records} random records from dataset...\")\n",
    "    for record in dataset.shuffle()[:n_records][section]:\n",
    "        print(\"============================================================================================\\n\")\n",
    "        print(record[:max_length])\n",
    "\n",
    "print_sample_records(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c51e0d-8cc4-4bc7-9a71-f3513338d530",
   "metadata": {},
   "source": [
    "## Tokenize dataset\n",
    "Before training the model, we need to convert the text in the dataset to the token-ids used by the model.\n",
    "\n",
    "This function is a fairly simple imlementation of this functionality. It will:\n",
    "- Split the dataset into a subset of the total, if 'select' is less than 1.0.\n",
    "- Take each example from the dataset, in batches, and convert the text to the corresponding tokens.\n",
    "- Truncate sequences longer than the model can process.\n",
    "- Add padding tokens, where the length of sequences in the batch are not identical.\n",
    "- Remove unused columns from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200d7c9-d8e9-4545-a79f-b3f07ddde855",
   "metadata": {},
   "source": [
    "### Load tokenizer\n",
    "We will need a tokenizer to tokenize the dataset.\n",
    "We can load our saved tokenizer -- or the tokenizer from any Huggingface model -- with this interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26098951-3b92-493e-9569-06b95cbb37ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='/home/dinalt/ai_assets/models/tiny', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer from a local path -- or from a Huggingface model name.\n",
    "# Rather than starting from scratch, you could replace 'model_path' with the path of an existing model and use its tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f04ea-d1db-4357-b1aa-e6d358960ef7",
   "metadata": {},
   "source": [
    "### Build Tokenizer\n",
    "If you have not built the tokenizer first, follow the linked tutorial...\n",
    "\n",
    "[Tokenizer Notebook](tokenizer.ipynb)\n",
    "\n",
    "...or just run this cell to build and save it.  \n",
    "Building it can take a moment or three. Be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08e8bbf-8555-4ddf-9302-1a838a978563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Completed training\n",
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/dinalt/ai_assets/models/tiny/tokenizer_config.json',\n",
       " '/home/dinalt/ai_assets/models/tiny/special_tokens_map.json',\n",
       " '/home/dinalt/ai_assets/models/tiny/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = train_bpe_tokenizer(config, dataset['train'])\n",
    "print(tokenizer)\n",
    "tokenizer.save_pretrained(config.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493cc70-6d28-4163-bf60-0951f462672e",
   "metadata": {},
   "source": [
    "### Tokenize dataset\n",
    "Before training the model, we need to convert the text in the dataset to the token-ids used by the model.\n",
    "\n",
    "This function is a fairly simple imlementation of this functionality. It will:\n",
    "- Split the dataset into a subset of the total, if 'select' is less than 1.0.\n",
    "- Take each example from the dataset, in batches, and convert the text to the corresponding tokens.\n",
    "- Truncate sequences longer than the model can process.\n",
    "- Add padding tokens, where the length of sequences in the batch are not identical.\n",
    "- Remove unused columns from the data.\n",
    "\n",
    "[tokenize_datasetdict()](../tutorial_code/tokenizer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4602addb-6d64-4d06-a4c9-1112679cd46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5ade5c0b294139936a77577e528962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebfd3cd75d64326a7b0d05c399e2a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/211971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = tokenize_datasetdict(dataset, tokenizer, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150462b-71f2-45ab-9fc5-d6036375e649",
   "metadata": {},
   "source": [
    "#### Save Tokenized Dataset\n",
    "Optional: You can save the datasets in pre-tokenized form.\n",
    "Note: The Datasets library is fairly good about caching, so this may be redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0993c09f-d2dc-44b1-935f-15e83283e8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed50de17b1c5428db4596c93f0d44ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f085ab92334834a4eeeed151dd15ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/211971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(config.dataset.tokenized_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170091b-89d8-477d-86e8-63d0f4ef9900",
   "metadata": {},
   "source": [
    "#### Load Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cc4b8b-484a-4759-b4ae-b8ec8af2f691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 2199\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 211971\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = datasets.load_from_disk(config.dataset.tokenized_dataset_path)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87298b3d-e3a4-4b68-9faf-7ba5a91562c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
