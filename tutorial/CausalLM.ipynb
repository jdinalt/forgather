{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bb9655-0848-448a-a500-28196416c634",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1725f627-3dd3-4214-b066-11888271525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This is where we will save and load our model and tokenizer.\n",
    "model_path = \"/home/dinalt/ai_assets/models/causallm\"\n",
    "\n",
    "# The Huggingface dataset-id to use\n",
    "dataset_id = \"roneneldan/TinyStories\"\n",
    "\n",
    "# Alternative to above: A path to an on-disk dataset to load.\n",
    "dataset_path = \"/home/dinalt/ai_assets/datasets/roneneldan-TinyStories\"\n",
    "\n",
    "# Where to save/load the pretokenized dataset\n",
    "tokenized_dataset_path = \"/home/dinalt/ai_assets/datasets/causallm_tinystories_tokenized\"\n",
    "\n",
    "## Training parameters\n",
    "\n",
    "# 'cuda', 'cpu', 'cuda:1', etc.\n",
    "device = 'cuda'\n",
    "per_device_train_batch_size = 64\n",
    "per_device_eval_batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "num_train_epochs = 1.0\n",
    "eval_steps = 1000\n",
    "num_warmup_steps = 0\n",
    "\n",
    "# See: https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#schedules\n",
    "# linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup, inverse_sqrt, reduce_lr_on_plateau\n",
    "lr_scheduler_name = \"constant\"\n",
    "\n",
    "# If multiple GPUs, this can be used to select a sub-set of GPUs to use.\n",
    "# Using fewer GPUs can actually be faster for small models.\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0fa63d-6cd9-4a74-b405-a021b2d47a85",
   "metadata": {},
   "source": [
    "## Quick Load\n",
    "If you have already created a saved the tokenizer and tokenized datasets, executing this code\n",
    "provides a shortcut to restoring these assets. You can then skip to model creation and training.\n",
    "\n",
    "Otherwise, skip to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71ce9731-12bc-41c0-ada1-a6fabdd62bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "if dataset_path is not None:\n",
    "    dataset = datasets.load_from_disk(dataset_path)\n",
    "else:\n",
    "    dataset = datasets.load_dataset(dataset_path)\n",
    "train_dataset = dataset['train']\n",
    "sample_text = train_dataset['text'][0][:500]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized_dataset = datasets.load_from_disk(tokenized_dataset_path)\n",
    "tok_train_dataset = tokenized_dataset[\"train\"]\n",
    "tok_val_dataset = tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0c23e-2311-4dd4-9102-013e5f777bd1",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will need some data to train our model on. For this tutorial, we will use a dataset named \"TinyStories,\" which is a synthetic dataset generated by ChatGPT designed for training very small language models to produce coherent output. This is made possible by limiting the examples to things which a 4-year-old child would be able to understand, with a total vocabulary of about 1500 words.\n",
    "\n",
    "Huggingface dataset link:  \n",
    "https://huggingface.co/datasets/roneneldan/TinyStories  \n",
    "\n",
    "The paper describing the dataset:  \n",
    "https://arxiv.org/abs/2305.07759\n",
    "\n",
    "The first time this is run, it will download the dataset to your cache, which make take a few minutes. After that, the dataset will be loaded from your cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa226a9-4047-48c3-97c1-6b8cc3b9b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n",
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# Load dataset from either HF Hub or from disk.\n",
    "# Unfortunately, there is not a unified API which works for both.\n",
    "if dataset_path is not None:\n",
    "    dataset = datasets.load_from_disk(dataset_path)\n",
    "else:\n",
    "    dataset = datasets.load_dataset(dataset_path)\n",
    "\n",
    "print(dataset)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# For experimentation, we will want a bit of sample text to work with. \n",
    "# This will grab the first 500 characters from the first record of the training dataset.\n",
    "sample_text = train_dataset['text'][0][:500]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db473db-3fbd-4dfc-8bae-3baf3e7dcb44",
   "metadata": {},
   "source": [
    "The dataset is split into two sections, \"train\" and \"validation.\" The validation set is not present in the training dataset, which allows one to test the model on data is has never seen, thus allowing one to confirm that the model is learning to generalize and not just memorize the data. As such, the model should never be trained on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfbbc02-2174-44cb-8777-3d2c5420d26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "\n",
      " Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2119719\n",
      "})\n",
      "dataset.info:\n",
      "\n",
      " DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='tiny_stories', config_name='default', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=1911420483, num_examples=2119719, shard_lengths=[559930, 559930, 559930, 439929], dataset_name='tiny_stories'), 'validation': SplitInfo(name='validation', num_bytes=19306310, num_examples=21990, shard_lengths=None, dataset_name='tiny_stories')}, download_checksums={'hf://datasets/roneneldan/TinyStories@691b0d9bd48ade766778c940011ca1c549f6359b/data/train-00000-of-00004-2d5a1467fff1081b.parquet': {'num_bytes': 248731111, 'checksum': None}, 'hf://datasets/roneneldan/TinyStories@691b0d9bd48ade766778c940011ca1c549f6359b/data/train-00001-of-00004-5852b56a2bd28fd9.parquet': {'num_bytes': 248171980, 'checksum': None}, 'hf://datasets/roneneldan/TinyStories@691b0d9bd48ade766778c940011ca1c549f6359b/data/train-00002-of-00004-a26307300439e943.parquet': {'num_bytes': 245894874, 'checksum': None}, 'hf://datasets/roneneldan/TinyStories@691b0d9bd48ade766778c940011ca1c549f6359b/data/train-00003-of-00004-d243063613e5a057.parquet': {'num_bytes': 247988350, 'checksum': None}, 'hf://datasets/roneneldan/TinyStories@691b0d9bd48ade766778c940011ca1c549f6359b/data/validation-00000-of-00001-869c898b519ad725.parquet': {'num_bytes': 9989127, 'checksum': None}}, download_size=1000775442, post_processing_size=None, dataset_size=1930726793, size_in_bytes=2931502235)\n",
      "dataset.features:\n",
      "\n",
      " {'text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_info(dataset):\n",
    "    print(\"dataset:\\n\\n\", dataset)\n",
    "    print(\"dataset.info:\\n\\n\", dataset.info)\n",
    "    print(\"dataset.features:\\n\\n\", dataset.features)\n",
    "\n",
    "print_dataset_info(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2387d-2638-4f91-8e89-8d9001cf4107",
   "metadata": {},
   "source": [
    "We can take a look at a random sampling of examples from the training dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "456273f5-c24d-46a5-95b5-d77676846a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing 3 random records from dataset...\n",
      "============================================================================================\n",
      "\n",
      "Once upon a time, there was a green tiger. He was walking in the jungle, feeling excited. He said to himself, \"I want to find an adventure!\"\n",
      "\n",
      "Suddenly he heard some noise. He looked around and saw a little boy. The boy was crying.\n",
      "\n",
      "The tiger asked, \"Why are you crying?\"\n",
      "\n",
      "The little boy replied, \"I'm lost. I can't find my way home.\"\n",
      "\n",
      "The tiger said, \"Don't worry. I will help you find your way home. Come, follow me.\"\n",
      "\n",
      "The tiger held the little boy's hand and walked through the jungle. After a whil\n",
      "============================================================================================\n",
      "\n",
      "Once upon a time, there was a very long hoop lying in the grass. One day a little boy named Jack saw it and decided to pick it up.\n",
      "\n",
      "Jack called to his dad: \"Look Dad! I found a hoop!\" His dad smiled when he saw the hoop and said: \"That is great! Let's see what we can do with it.\"\n",
      "\n",
      "Jack was very excited and asked: \"What should we do?\" His dad replied: \"Let's try and set the hoop up.\"\n",
      "\n",
      "They both started to set the hoop up by placing it on the ground and pushing both ends into the soil. When they w\n",
      "============================================================================================\n",
      "\n",
      "Once upon a time there was a 3 year old boy named John. He was very thirsty and wanted something to drink. He asked his mom for some juice. \n",
      "\n",
      "Mom said, \"I'm sorry John. We don't have any juice. All we have is fake juice.\" \n",
      "\n",
      "John was thrilled, because he loves fake juice! He asked his mom to give him the fake juice. She carefully opened the carton and let John take a few sips. \n",
      "\n",
      "John dropped the cup of fake juice when he heard a loud noise from the other room. He ran to the room to see what the n\n"
     ]
    }
   ],
   "source": [
    "def print_sample_records(dataset, section=\"text\", n_records=3, max_length=500):\n",
    "    print(f\"Showing {n_records} random records from dataset...\")\n",
    "    for record in dataset.shuffle()[:n_records][section]:\n",
    "        print(\"============================================================================================\\n\")\n",
    "        print(record[:max_length])\n",
    "\n",
    "print_sample_records(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c82d0-3e68-4392-87b1-6a2e6b96137f",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Rather than working with the raw ASCII/Unicode from the dataset, we will be \"tokenizing\" the data. A tokenizer is a statisttical model which aggregates individual characters into sub-word, where the most frequent strings of characters are replaced by unique symbols.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Large_language_model#Probabilistic_tokenization\n",
    "\n",
    "For this tutorial, we will be created a Byte Pair Encoding (BPE) tokenizer, which starts with all of the symbols from the ASCII character set, then creates tokens for the most common pairs of ASCII characters. These pairs are further aggregated into larger symbols and the process repeats until a set of symbols matching the target vocabulary size has been created.\n",
    "\n",
    "By starting with the ASCII character set, it is possible to represent any combination of letters, including those which were not observed when the tokenizer was created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604db36-e5ec-4a7e-b175-e2ed6f4f64d4",
   "metadata": {},
   "source": [
    "### Create a pre-tokenizer\n",
    "\n",
    "The first step will be to specify a \"[pre-tokenizer](https://huggingface.co/docs/tokenizers/api/pre-tokenizers),\" which breaks the input text into sub-strings via a regular expression. For example, a simple pre-tokenizer could split the input on spaces and punctuation.\n",
    "\n",
    "We will be using the \"ByteLevel\" pre-tokenizer, which uses a GPT-2 specfic regex for splitting the words and replaces spaces with the 'Ġ' character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63edf63e-c9f2-4def-8fa5-8c0dd56db69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ĠOne' 'Ġday' ',' 'Ġa' 'Ġlittle' 'Ġgirl' 'Ġnamed' 'ĠLily' 'Ġfound' 'Ġa' 'Ġneedle' 'Ġin' 'Ġher' 'Ġroom' '.' 'ĠShe' 'Ġknew' 'Ġit' 'Ġwas' 'Ġdifficult' 'Ġto' 'Ġplay' 'Ġwith' 'Ġit' 'Ġbecause' 'Ġit' 'Ġwas' 'Ġsharp' '.' 'ĠLily' 'Ġwanted' 'Ġto' 'Ġshare' 'Ġthe' 'Ġneedle' 'Ġwith' 'Ġher' 'Ġmom' ',' 'Ġso' 'Ġshe' 'Ġcould' 'Ġsew' 'Ġa' 'Ġbutton' 'Ġon' 'Ġher' 'Ġshirt' '.' 'Ċ' 'Ċ' 'Lily' 'Ġwent' 'Ġto' 'Ġher' 'Ġmom' 'Ġand' 'Ġsaid' ',' 'Ġ\"' 'Mom' ',' 'ĠI' 'Ġfound' 'Ġthis' 'Ġneedle' '.' 'ĠCan' 'Ġyou' 'Ġshare' 'Ġit' 'Ġwith' 'Ġme' 'Ġand' 'Ġsew' 'Ġmy' 'Ġshirt' '?\"' 'ĠHer' 'Ġmom' 'Ġsmiled' 'Ġand' 'Ġsaid' ',' 'Ġ\"' 'Yes' ',' 'ĠLily' ',' 'Ġwe' 'Ġcan' 'Ġshare' 'Ġthe' 'Ġneedle' 'Ġand' 'Ġfix' 'Ġyour' 'Ġshirt' '.\"' 'Ċ' 'Ċ' 'Together' ',' 'Ġthey' 'Ġshared' 'Ġthe' 'Ġneedle' 'Ġand' 'Ġsewed' 'Ġthe' 'Ġbutton' 'Ġon' 'ĠLily' ''s' 'Ġshirt' '.' 'ĠIt' 'Ġwas' 'Ġnot' 'Ġdifficult' 'Ġfor' 'Ġthem' 'Ġb' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "\n",
    "# A pre-tokenizer is responsible for splitting the input text into words.\n",
    "# The ByteLevel pretokenizer uses a regular expression for splitting on word boundaries and\n",
    "# substitutes the space character with 'Ġ'\n",
    "pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel()\n",
    "\n",
    "def test_pretokenizer(pre_tokenizer, sample_text):\n",
    "    tokens = pre_tokenizer.pre_tokenize_str(sample_text)\n",
    "    for token in tokens:\n",
    "        print(f\"'{token[0]}'\", end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "test_pretokenizer(pre_tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df884e-0ee0-4373-96d2-74730b038a31",
   "metadata": {},
   "source": [
    "### Create a new BPE tokenizer\n",
    "\n",
    "[Tokenizer Models](https://huggingface.co/docs/tokenizers/en/api/models)\n",
    "\n",
    "For extra credit, try the other models at the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c764e33-07a3-4d2b-8639-4382e42dfde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "# We can add tokens with special meanings to the tokenizer\n",
    "special_tokens={\n",
    "    \"pad\": \"<|PAD|>\",   # Used to pad unused positions in a sequence.\n",
    "    \"mask\": \"<|MASK|>\", # Used with masked-language-modeling to mark a position as having been masked.\n",
    "    \"bos\": \"<|BOS|>\",   # Beginning of Sequence\n",
    "    \"eos\": \"<|EOS|>\",   # End of Sequence\n",
    "    \"unk\": \"<|UNK|>\",   # Unknown\n",
    "}\n",
    "\n",
    "# Define the size of the vocabulary.\n",
    "# A small vocabulary will result in many fragmented words, but is useful for\n",
    "# minimizing training time.\n",
    "#\n",
    "# Consider using values in the range of 8K - 50K when not\n",
    "# working with the smallest of toy models.\n",
    "vocab_size = 2000\n",
    "\n",
    "# Create a new BPE tokenizer.\n",
    "pretrained_tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE(\n",
    "    cache_capacity=16,\n",
    "    unk_token=special_tokens['unk'],\n",
    "    byte_fallback=True,\n",
    "))\n",
    "\n",
    "# Attach the pre-tokenizer, created above\n",
    "pretrained_tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# The 'normalizer' can be used to transform the characters. For example, they can convert everything to\n",
    "# lowercase, remove accent marks, and translate Unicode. We will use the NFC Unicode normalizer, with the \n",
    "# detaisl explained here: https://unicode.org/reports/tr15/\n",
    "pretrained_tokenizer.normalizer = tokenizers.normalizers.NFC()\n",
    "\n",
    "# The decoder is applied when coverting tokens back into text and the ByteLevel decoder\n",
    "# is responsible for replacing 'Ġ' character with spaces. \n",
    "pretrained_tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
    "\n",
    "# Automatically add Begin Of Sequence (BOS) token to output when 'add_special_tokens' is True\n",
    "# This has relevance to causal models, which predict the next token in a sequence. As the first real token lacks\n",
    "# a preceeding token, this allows the model to identify where the sequence actually begins.\n",
    "#\n",
    "# Note: A causal model can still function without a BOS token and the need to include it is debatable.\n",
    "pretrained_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<BOS> $A\",\n",
    "    special_tokens=[\n",
    "        (\"<BOS>\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286dacf-7f4a-4a03-b19e-90e6a8ad05cb",
   "metadata": {},
   "source": [
    "### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a9c39c-fff7-463e-a537-8d8e79be85b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a BPE trainer, which is used to build an optimal set of tokens from\n",
    "# a a given dataset.\n",
    "tok_trainer = tokenizers.trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    initial_alphabet=tokenizers.pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=list(special_tokens.values()),\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# This abstraction is needed for the trainer to iterate over our dataset\n",
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size]['text']\n",
    "\n",
    "# Train the tokenizer of the dataset\n",
    "# Be patient! This will take a bit of time to complete...\n",
    "pretrained_tokenizer.train_from_iterator(batch_iterator(train_dataset), trainer=tok_trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985509f-4272-43bf-a3c7-2c7302817f45",
   "metadata": {},
   "source": [
    "### Wrap the tokenizer\n",
    "\n",
    "The BPE tokenizer class can be wrapped in a Huggingface [PreTrainedTokenizerFast](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) class, which makes working with the tokenizer easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f3e415d-7bcd-4d3d-8cbc-fcfcfa86b016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# This wraps the tokenizer in a Huggingface transformer tokenizer, which\n",
    "# is a higher level abstraction\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=pretrained_tokenizer,\n",
    "    # This should match the model's input length limit, which depends upon the archetecture.\n",
    "    # If not limit is specified, the default will be a VERY LARGE value.\n",
    "    model_max_length=2048,\n",
    "    pad_token=special_tokens['eos'],\n",
    "    mask_token=special_tokens['mask'],\n",
    "    bos_token=special_tokens['bos'],\n",
    "    eos_token=special_tokens['eos'],\n",
    "    unk_token=special_tokens['unk'],\n",
    "    return_special_tokens_mask=False,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5ab8c-22a7-40f4-91c8-88aa40945363",
   "metadata": {},
   "source": [
    "### Test the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88eab7f4-07fa-4b0e-af00-b97b19b745d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 491, 360, 16, 263, 403, 450, 505, 362, 598, 263, 792, 311, 320, 313, 763, 18, 317, 709, 308, 286, 1035, 74, 475, 1389, 88, 270, 365, 346, 308, 791, 308, 286, 385, 291, 84, 18, 362, 448, 270, 952, 267, 792, 311, 346, 313, 370, 16, 354, 342, 464, 442, 91, 263, 1842, 307, 349, 313, 385, 316, 88, 18, 203, 203, 601, 473, 270, 313, 370, 269, 331, 16, 332, 781, 16, 339, 598, 747, 792, 311, 18, 1283, 350, 952, 308, 346, 522, 269, 442, 91, 656, 385, 316, 88, 481, 869, 370, 503, 269, 331, 16, 332, 836, 16, 362, 16, 369, 477, 952, 267, 792, 311, 269, 1307, 633, 385, 316, 88, 420, 203, 203, 56, 83, 558, 16, 368, 1659, 267, 792, 311, 269, 442, 91, 268, 267, 1842, 307, 349, 362, 376, 385, 316, 88, 18, 413, 286, 390, 1035, 74, 475, 1389, 88, 372, 452, 271]\n",
      "['<|BOS|>', 'ĠOne', 'Ġday', ',', 'Ġa', 'Ġlittle', 'Ġgirl', 'Ġnamed', 'ĠLily', 'Ġfound', 'Ġa', 'Ġneed', 'le', 'Ġin', 'Ġher', 'Ġroom', '.', 'ĠShe', 'Ġknew', 'Ġit', 'Ġwas', 'Ġdif', 'f', 'ic', 'ul', 't', 'Ġto', 'Ġplay', 'Ġwith', 'Ġit', 'Ġbecause', 'Ġit', 'Ġwas', 'Ġsh', 'ar', 'p', '.', 'ĠLily', 'Ġwanted', 'Ġto', 'Ġshare', 'Ġthe', 'Ġneed', 'le', 'Ġwith', 'Ġher', 'Ġmom', ',', 'Ġso', 'Ġshe', 'Ġcould', 'Ġse', 'w', 'Ġa', 'Ġbutt', 'on', 'Ġon', 'Ġher', 'Ġsh', 'ir', 't', '.', 'Ċ', 'Ċ', 'Lily', 'Ġwent', 'Ġto', 'Ġher', 'Ġmom', 'Ġand', 'Ġsaid', ',', 'Ġ\"', 'Mom', ',', 'ĠI', 'Ġfound', 'Ġthis', 'Ġneed', 'le', '.', 'ĠCan', 'Ġyou', 'Ġshare', 'Ġit', 'Ġwith', 'Ġme', 'Ġand', 'Ġse', 'w', 'Ġmy', 'Ġsh', 'ir', 't', '?\"', 'ĠHer', 'Ġmom', 'Ġsmiled', 'Ġand', 'Ġsaid', ',', 'Ġ\"', 'Yes', ',', 'ĠLily', ',', 'Ġwe', 'Ġcan', 'Ġshare', 'Ġthe', 'Ġneed', 'le', 'Ġand', 'Ġfix', 'Ġyour', 'Ġsh', 'ir', 't', '.\"', 'Ċ', 'Ċ', 'T', 'o', 'gether', ',', 'Ġthey', 'Ġshared', 'Ġthe', 'Ġneed', 'le', 'Ġand', 'Ġse', 'w', 'ed', 'Ġthe', 'Ġbutt', 'on', 'Ġon', 'ĠLily', \"'s\", 'Ġsh', 'ir', 't', '.', 'ĠIt', 'Ġwas', 'Ġnot', 'Ġdif', 'f', 'ic', 'ul', 't', 'Ġfor', 'Ġthem', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "# We can use the new tokenizer to tokenizer text via the object's __call__ method, like this:\n",
    "\n",
    "input_ids = tokenizer(sample_text)['input_ids']\n",
    "print(input_ids)\n",
    "\n",
    "# We can convert these to their symbolic representations like this.\n",
    "# Note the 'Ġ' symbols. The tokenizer has folded spaces into the tokens, where this symbol represents the space.\n",
    "# A consequence of this encoding is that tokens may exist for the same word, both with and without a space.\n",
    "# For example, \"she\" and \" she\" would be represented as seperate tokens.\n",
    "for ids in [input_ids]:\n",
    "    print(tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d727c550-95c6-4089-be9d-76e336ccb6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<|BOS|> One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b\"\n"
     ]
    }
   ],
   "source": [
    "# We can decode token ids with decode() or batch_decode()\n",
    "decoded_tokens = tokenizer.batch_decode([input_ids], skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
    "for s in decoded_tokens:\n",
    "    print(f\"\\\"{s}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcde6ca-ffa3-4061-9c2d-44b135139292",
   "metadata": {},
   "source": [
    "---\n",
    "We can dump the vocabulary of the tokenizer. The first part will contain our special tokens and the ASCII character-set. After this, the number of characters in each tokens grows, with the largest tokens at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719c5403-459b-4e4f-97c0-e5aca5272631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0: <|PAD|>' '1: <|MASK|>' '2: <|BOS|>' '3: <|EOS|>' '4: <|UNK|>' '5: !' '6: \"' '7: #' '8: $' '9: %' '10: &' '11: '' '12: (' '13: )' '14: *' '15: +' '16: ,' '17: -' '18: .' '19: /' '20: 0' '21: 1' '22: 2' '23: 3' '24: 4' '25: 5' '26: 6' '27: 7' '28: 8' '29: 9' '30: :' '31: ;' '32: <' '33: =' '34: >' '35: ?' '36: @' '37: A' '38: B' '39: C' '40: D' '41: E' '42: F' '43: G' '44: H' '45: I' '46: J' '47: K' '48: L' '49: M' '50: N' '51: O' '52: P' '53: Q' '54: R' '55: S' '56: T' '57: U' '58: V' '59: W' '60: X' '61: Y' '62: Z' '63: [' \n",
      "\n",
      "'1936: ek' '1937:  shap' '1938:  shook' '1939:  exploring' '1940:  moved' '1941:  purp' '1942:  year' '1943: aughty' '1944:  nearby' '1945:  naughty' '1946:  star' '1947:  soup' '1948:  shop' '1949:  wise' '1950:  stars' '1951:  owl' '1952:  bring' '1953: fused' '1954:  jar' '1955: bow' '1956: Do' '1957: ocked' '1958:  inv' '1959:  exp' '1960:  whe' '1961: yard' '1962:  caught' '1963:  su' '1964: ward' '1965:  Emma' '1966:  backyard' '1967:  seemed' '1968: ail' '1969:  es' '1970:  relie' '1971:  dropped' '1972:  ph' '1973:  rocks' '1974: aughter' '1975: llo' '1976:  3' '1977: ush' '1978:  drink' '1979:  bucket' '1980:  des' '1981:  ca' '1982: adow' '1983: ces' '1984:  purple' '1985:  swam' '1986:  farm' '1987:  resp' '1988:  taking' '1989:  Mama' '1990:  mail' '1991:  orange' '1992:  though' '1993: ions' '1994:  shapes' '1995:  past' '1996: col' '1997:  cow' '1998:  stu' '1999:  Thank' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump a range of the tokenizer's vocabulary\n",
    "def show_vocabulary(tokenizer, token_range):\n",
    "    for i, token in zip(token_range, tokenizer.batch_decode([i for i in token_range], skip_special_tokens=False)):\n",
    "        print(f\"'{i}: {token}'\", end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Show the first and last 64 tokens.\n",
    "show_vocabulary(tokenizer, range(64))\n",
    "show_vocabulary(tokenizer, range(tokenizer.vocab_size - 64, tokenizer.vocab_size))\n",
    "\n",
    "# Show full vocab.\n",
    "#show_vocabulary(tokenizer, range(tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c4c1b-3503-4989-9e7e-626442ab199e",
   "metadata": {},
   "source": [
    "### Save tokenizer\n",
    "We can save the tokenizer, which will allow us to skip recreating it from scratch next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e8841ea-48a8-4b3d-9790-fef1c37ca501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/dinalt/ai_assets/models/causallm/tokenizer_config.json',\n",
       " '/home/dinalt/ai_assets/models/causallm/special_tokens_map.json',\n",
       " '/home/dinalt/ai_assets/models/causallm/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200d7c9-d8e9-4545-a79f-b3f07ddde855",
   "metadata": {},
   "source": [
    "### Load tokenizer\n",
    "We can load our saved tokenizer -- or the tokenizer from any Huggingface model -- with this interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26098951-3b92-493e-9569-06b95cbb37ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='/home/dinalt/ai_assets/models/causallm', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer from a local path -- or from a Huggingface model name.\n",
    "# Rather than starting from scratch, you could replace 'model_path' with the path of an existing model and use its tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c51e0d-8cc4-4bc7-9a71-f3513338d530",
   "metadata": {},
   "source": [
    "## Tokenize dataset\n",
    "Before training the model, we need to convert the text in the dataset to the token-ids used by the model.\n",
    "\n",
    "This function is a fairly simple imlementation of this functionality. It will:\n",
    "- Split the dataset into a subset of the total, if 'select' is less than 1.0.\n",
    "- Take each example from the dataset, in batches, and convert the text to the corresponding tokens.\n",
    "- Truncate sequences longer than the model can process.\n",
    "- Add padding tokens, where the length of sequences in the batch are not identical.\n",
    "- Remove unused columns from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54dee279-5093-48c8-9851-5c7d2d61c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# select: the ratio of the total to tokenize. e.g. 0.1 = 10%\n",
    "def tokenize_dataset(dataset, tokenizer, select=1.0, shuffle=False):\n",
    "    def map_fn(element, tokenizer):\n",
    "        outputs = tokenizer(\n",
    "            element[\"text\"],\n",
    "            truncation=True,\n",
    "\n",
    "            # Other common arguments, for reference.\n",
    "            #padding=True,\n",
    "            #return_tensors='pt',\n",
    "\n",
    "            # This can be used to limit the block-size to less than what the model can handle.\n",
    "            # max_length=block_size,\n",
    "\n",
    "            # If set to True, if the sequence is truncated, the 'overflowing' tokens will be \n",
    "            # returned on the next call.\n",
    "            # return_overflowing_tokens=True,\n",
    "\n",
    "            # This can we used to get the length of the returned sequences, allowing one to\n",
    "            # discard short sequences, if desired.\n",
    "            # return_length=True,\n",
    "        )\n",
    "        return {\"input_ids\": outputs[\"input_ids\"]}\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle()\n",
    "    \n",
    "    if select < 1.0:\n",
    "        dataset = dataset.select(range(0, int(len(dataset) * select)))\n",
    "    \n",
    "    # https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning\n",
    "    #os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    tokenized_data = dataset.map(\n",
    "        map_fn,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        fn_kwargs=dict(tokenizer=tokenizer)\n",
    "    )\n",
    "    #os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493cc70-6d28-4163-bf60-0951f462672e",
   "metadata": {},
   "source": [
    "#### Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4602addb-6d64-4d06-a4c9-1112679cd46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20997b95fd9642a9bc5875ae8d40a0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note the small \"select\" sizes. We are only using 10% of the validation and train\n",
    "# examples. This is to keep the training time reasonable for the examples.\n",
    "tok_val_dataset = tokenize_dataset(dataset[\"validation\"], tokenizer, select=0.1)\n",
    "tok_train_dataset = tokenize_dataset(dataset[\"train\"], tokenizer, select=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150462b-71f2-45ab-9fc5-d6036375e649",
   "metadata": {},
   "source": [
    "#### Save Tokenized Dataset\n",
    "Optional: You can save the datasets in pre-tokenized form.\n",
    "Note: The Datasets library is fairly good about caching, so this may be redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0993c09f-d2dc-44b1-935f-15e83283e8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 211971\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 2199\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345cf328653d4604aade81343a92b2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/211971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a88c635234430394e47f5ec7b2202e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_tokenized_dataset(path, train, validate):\n",
    "    dataset_dict = datasets.DatasetDict()\n",
    "    dataset_dict[\"train\"] = train\n",
    "    dataset_dict[\"validation\"] = validate\n",
    "    print(dataset_dict)\n",
    "    dataset_dict.save_to_disk(path)\n",
    "\n",
    "save_tokenized_dataset(tokenized_dataset_path, tok_train_dataset, tok_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170091b-89d8-477d-86e8-63d0f4ef9900",
   "metadata": {},
   "source": [
    "#### Load Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43cc4b8b-484a-4759-b4ae-b8ec8af2f691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 211971\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 2199\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def load_tokenized_dataset(dataset_path):\n",
    "    tokenized_dataset = datasets.load_from_disk(dataset_path)\n",
    "    print(tokenized_dataset)\n",
    "    return tokenized_dataset[\"train\"], tokenized_dataset[\"validation\"]\n",
    "\n",
    "tok_train_dataset, tok_val_dataset = load_tokenized_dataset(tokenized_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810aa32-3d44-4769-9322-dcb7d7ceb6f6",
   "metadata": {},
   "source": [
    "## Create a simple causal language model\n",
    "A \"causal\" language model is one which makes predictions about future tokens based upon past tokens. We will start with a simple model which predicts the next token, given only the immediadly preceeding token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be197f64-d973-4b6b-b27e-4b4597b2b518",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e8544d-1685-4c84-87d4-c311d0678dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.init as init\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CausalLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # d_model is the number of features in the model's embeddings, where a feature is a single floating-point scalar and\n",
    "        # the embeddings are vectors, each of size d_model. This parameter is sometimes referred to a the model's\n",
    "        # \"hidden\" dimension.\n",
    "        d_model,\n",
    "\n",
    "        # This is the vocabulary size of the model, which should match the size of the model's tokenizer.\n",
    "        vocab_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # The class contains an array of embeddings, with each token-id in the vocabulary corresponding to the element at that index.\n",
    "        # For example, self.embedding.weight[token_id] would refer to the features at the index 'token_id.'\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "\n",
    "        # We will use a linear layer to convert embeddins into a probability distribution accross all of the token-ids, thus\n",
    "        # it has an input size of d_model and an output size of vocab_size.\n",
    "        self.output_projection = nn.Linear(self.d_model, self.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: Tensor, labels: Tensor=None, attention_mask: Tensor=None):\n",
    "        # input_ids (batch_size, seq_len):\n",
    "        #    This contains batches of sequences of token-ids, representing the input text.\n",
    "        # labels (batch_size, seq_len): If given these are the ground-truth targets the model is striving to predict.\n",
    "        #    For a causal model, these are identical to the input-ids, with a special value of -100\n",
    "        #    reserved for padding, which are not scored.\n",
    "        # attention_mask: The Huggingface APIs pass this in, although we don't use it.\n",
    "        \n",
    "        # Convert input_ids to embeddings.\n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        # Convert embeddings to log-probabilities of next token-id\n",
    "        # We could convert the logits to probabilities (0.0 to 1.0) with\n",
    "        # torch.softmax(logits, dim=-1)\n",
    "        logits = self.output_projection(x)\n",
    "\n",
    "        # If we are passed labels, we will compute loss and return both loss and logits.\n",
    "        if labels is not None:\n",
    "            loss = self.loss(logits, labels)\n",
    "            return (loss, logits)\n",
    "        # Otherwise, we only return the logits.\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def loss(self, logits, labels):\n",
    "        # Shift so that tokens < n predict n\n",
    "        # To achieve this, we slice off the last prediction, as we don't have a label\n",
    "        # corresponding to it and slice off the first label, as nothing preceeds it for which\n",
    "        # a prediction could be made.\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        # The loss meaures the error between the ground-truth (lables) and what the model predicted (logits).\n",
    "        # If the model makes a perfect prediction, the loss will be zero, otherwise, it will be a positive\n",
    "        # log-scaled measure of the error.\n",
    "        #\n",
    "        # For each label, the model makes a prediction for every token in the vocabulary, with the logits being\n",
    "        # a log-probability distribution of the prediction. Cross-entroy-loss compares the model's predicted\n",
    "        # distribution with a \"one-hot\" distribution -- that is, a probability distribution with 1.0 where the label\n",
    "        # is and 0.0 for all other elements in the vocabulary.\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "            # labels with this value are ignored when computing loss\n",
    "            ignore_index=-100,\n",
    "            reduction='mean',\n",
    "        )\n",
    "\n",
    "        # Allowing the model to return NaN can cause problems, so we convert these values to a number.\n",
    "        return loss.nan_to_num()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2f37c-bb0c-4987-a05a-7a46bf2b9365",
   "metadata": {},
   "source": [
    "### Instantiate model\n",
    "This will create an instance of our model with a hidden-dimension (d_model) of 128 and a vocabulary size matching that of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f909ad-7100-4bc3-9382-bfcb74d66c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.5M parameters\n",
      "CausalLM(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def print_model_size(model):\n",
    "    model_size = sum(t.numel() for t in model.parameters())\n",
    "    print(f\"Model size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "# Model hidden dimension\n",
    "d_model = 128\n",
    "\n",
    "model = CausalLM(d_model, tokenizer.vocab_size)\n",
    "print_model_size(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5ffaa-1d93-4861-b0ed-c2fd877d809e",
   "metadata": {},
   "source": [
    "### Enable Torch Compile [optional]\n",
    "\n",
    "https://pytorch.org/docs/stable/torch.compiler.html\n",
    "\n",
    "Optionally compile the model. This may not work with all version of Python and Pytorch.\n",
    "Using \"torch.compile()\" is especially effective at speeding up small models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cdc14-1179-4b56-8c61-cef81d85e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_float32_matmul_precision('high')\n",
    "model.compile()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef483f-00de-473e-a729-5e9a278ba64d",
   "metadata": {},
   "source": [
    "### Test model forward\n",
    "This will tokenizer our sample text and feed it through the forward method of the model to ensure that the code does not \"fall-over.\"\n",
    "\n",
    "If the model has not been trained, the loss is expected to be around 7 - 8; lower, if it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8f463bd-262a-4bc8-b83f-91b321d18b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      " tensor([[   2,  491,  360,   16,  263,  403,  450,  505,  362,  598,  263,  792,\n",
      "          311,  320,  313,  763,   18,  317,  709,  308,  286, 1035,   74,  475,\n",
      "         1389,   88,  270,  365,  346,  308,  791,  308,  286,  385,  291,   84,\n",
      "           18,  362,  448,  270,  952,  267,  792,  311,  346,  313,  370,   16,\n",
      "          354,  342,  464,  442,   91,  263, 1842,  307,  349,  313,  385,  316,\n",
      "           88,   18,  203,  203,  601,  473,  270,  313,  370,  269,  331,   16,\n",
      "          332,  781,   16,  339,  598,  747,  792,  311,   18, 1283,  350,  952,\n",
      "          308,  346,  522,  269,  442,   91,  656,  385,  316,   88,  481,  869,\n",
      "          370,  503,  269,  331,   16,  332,  836,   16,  362,   16,  369,  477,\n",
      "          952,  267,  792,  311,  269, 1307,  633,  385,  316,   88,  420,  203,\n",
      "          203,   56,   83,  558,   16,  368, 1659,  267,  792,  311,  269,  442,\n",
      "           91,  268,  267, 1842,  307,  349,  362,  376,  385,  316,   88,   18,\n",
      "          413,  286,  390, 1035,   74,  475, 1389,   88,  372,  452,  271]])\n",
      "tensor(7.6858, grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "def test_model_forward(model, tokenizer, text, device='cpu'):\n",
    "    model.train()\n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    input_ids = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )['input_ids'].to(device=device)\n",
    "\n",
    "    print(\"input_ids:\\n\", input_ids)\n",
    "    labels = input_ids\n",
    "\n",
    "    loss, logits = model(input_ids=input_ids, labels=labels)\n",
    "    print(loss)\n",
    "    \n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # Reset model gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "test_model_forward(model, tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4473aee-f86d-4c72-a6ea-91f8c4e02f92",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "This is an example training-loop implementation.\n",
    "\n",
    "Example code is based upon examples here:\n",
    "https://huggingface.co/learn/nlp-course/en/chapter3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95593f4-48ff-481c-8118-90e29d023f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "class CausalTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataset,\n",
    "        eval_dataset,\n",
    "        per_device_train_batch_size,\n",
    "        per_device_eval_batch_size,\n",
    "        eval_steps,\n",
    "        lr,\n",
    "        num_train_epochs,\n",
    "        optimizer_factory,\n",
    "        lr_scheduler_factory,\n",
    "        device,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eval_steps = eval_steps\n",
    "        self.device = device\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.model = self.model.to(self.device) \n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=per_device_train_batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "        \n",
    "        self.eval_dataloader = DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=per_device_eval_batch_size,\n",
    "            collate_fn=data_collator\n",
    "        )\n",
    "\n",
    "        self.num_train_steps = int(self.num_train_epochs * len(self.train_dataloader))\n",
    "        self.optimizer = optimizer_factory(model.parameters(), learning_rate)\n",
    "        self.lr_scheduler = lr_scheduler_factory(self.optimizer, self.num_train_steps)\n",
    "\n",
    "    def train(self):\n",
    "        self._train_loop()\n",
    "        self.eval()\n",
    "\n",
    "    def _train_loop(self):\n",
    "        self.model.train()\n",
    "        epoch = 0\n",
    "        global_step = 0\n",
    "        eval_step = 0\n",
    "        \n",
    "        \n",
    "        print(f\"Training for {self.num_train_steps} steps\")\n",
    "        progress_bar = tqdm(range(self.num_train_steps))\n",
    "        while True:\n",
    "            for batch in self.train_dataloader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs[0]\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.lr_scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                if global_step >= self.num_train_steps:\n",
    "                    return\n",
    "                eval_step += 1\n",
    "                # Evaluate every 'eval_steps'\n",
    "                if eval_step == self.eval_steps:\n",
    "                    self.eval()\n",
    "                    print(f\"Global step: {global_step}\")\n",
    "                    self.model.train()\n",
    "                    eval_step = 0\n",
    "            \n",
    "            epoch += 1\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval(self):\n",
    "        eval_steps = len(self.eval_dataloader)\n",
    "        #metric = evaluate.load(\"accuracy\")\n",
    "        self.model.eval()\n",
    "        progress_bar = tqdm(range(eval_steps))\n",
    "        total_loss = 0\n",
    "        for batch in self.eval_dataloader:\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            outputs = self.model(**batch)\n",
    "            logits = outputs[1]\n",
    "            loss = outputs[0]\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.update(1)\n",
    "        print(f\"loss={total_loss / eval_steps}\")\n",
    "\n",
    "# This provides a place to configure the training parameters.\n",
    "def do_train():\n",
    "    CausalTrainer(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataset=tok_train_dataset,\n",
    "        eval_dataset=tok_val_dataset,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        lr=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        eval_steps=eval_steps,\n",
    "        optimizer_factory=lambda params, lr: torch.optim.AdamW(params, lr=lr),\n",
    "        lr_scheduler_factory=lambda opt, steps: transformers.get_scheduler(lr_scheduler_name, opt, num_warmup_steps, steps),\n",
    "        device = 'cuda',\n",
    "    ).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb9feb-41b8-47fe-8493-cbb3d2d958b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7f5db-c057-4e90-a8bc-045b9cc4f263",
   "metadata": {},
   "source": [
    "### Accelerate Training Loop\n",
    "\n",
    "This is the same code, but modified to run on multiple GPU's within a notebook using the [Accelerate](https://huggingface.co/docs/accelerate/v0.11.0/en/index) library.\n",
    "\n",
    "Note: For small models, this may actually be slower than the basic training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1cb65c6-05b9-4b46-afec-5293079341a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "from accelerate import notebook_launcher\n",
    "import transformers\n",
    "\n",
    "class CausalAccelerateTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataset,\n",
    "        eval_dataset,\n",
    "        per_device_train_batch_size,\n",
    "        per_device_eval_batch_size,\n",
    "        eval_steps,\n",
    "        lr,\n",
    "        num_train_epochs,\n",
    "        optimizer_factory,\n",
    "        lr_scheduler_factory,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eval_steps = eval_steps\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.accelerator = Accelerator()\n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=per_device_train_batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "        \n",
    "        self.eval_dataloader = DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=per_device_eval_batch_size,\n",
    "            collate_fn=data_collator\n",
    "        )\n",
    "\n",
    "        self.optimizer = optimizer_factory(model.parameters(), learning_rate)\n",
    "\n",
    "        self.train_dataloader, self.eval_dataloader, self.model, self.optimizer = self.accelerator.prepare(\n",
    "            self.train_dataloader,\n",
    "            self.eval_dataloader,\n",
    "            self.model,\n",
    "            self.optimizer,\n",
    "        )\n",
    "\n",
    "        self.num_train_steps = int(self.num_train_epochs * len(self.train_dataloader))\n",
    "        self.lr_scheduler = lr_scheduler_factory(self.optimizer, self.num_train_steps)\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "\n",
    "    def train(self):\n",
    "        self._train_loop()\n",
    "        self.eval()\n",
    "\n",
    "    def _train_loop(self):\n",
    "        self.model.train()\n",
    "        epoch = 0\n",
    "        global_step = 0\n",
    "        eval_step = 0\n",
    "\n",
    "        # Note: Getting this attribute appears to be expensive, so cache it!\n",
    "        is_main_process = self.accelerator.is_main_process\n",
    "        if is_main_process:\n",
    "            print(f\"Training for {self.num_train_steps} steps\")\n",
    "            progress_bar = tqdm(range(self.num_train_steps))\n",
    "        while True:\n",
    "            for batch in self.train_dataloader:\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs[0]\n",
    "                self.accelerator.backward(loss)\n",
    "                self.optimizer.step()\n",
    "                self.lr_scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                if is_main_process:\n",
    "                    progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                if global_step >= self.num_train_steps:\n",
    "                    return\n",
    "                eval_step += 1\n",
    "                # Evaluate every 'eval_steps'\n",
    "                if eval_step == self.eval_steps:\n",
    "                    self.eval()\n",
    "                    if is_main_process:\n",
    "                        print(f\"Global step: {global_step}\")\n",
    "                    self.model.train()\n",
    "                    eval_step = 0\n",
    "            \n",
    "            epoch += 1\n",
    "            if is_main_process:\n",
    "                print(f\"Epoch: {epoch}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval(self):\n",
    "        eval_steps = len(self.eval_dataloader)\n",
    "        is_main_process = self.accelerator.is_main_process\n",
    "        #metric = evaluate.load(\"accuracy\")\n",
    "        self.model.eval()\n",
    "        if is_main_process:\n",
    "            progress_bar = tqdm(range(eval_steps))\n",
    "        total_loss = 0\n",
    "        for batch in self.eval_dataloader:\n",
    "            outputs = self.model(**batch)\n",
    "            logits = outputs[1]\n",
    "            loss = outputs[0]\n",
    "            total_loss += self.accelerator.reduce(loss.detach(), \"mean\").item()\n",
    "            if is_main_process:\n",
    "                progress_bar.update(1)\n",
    "        if is_main_process:\n",
    "            print(f\"loss={total_loss / eval_steps}\")\n",
    "\n",
    "def train_function():\n",
    "    trainer = CausalAccelerateTrainer(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataset=tok_train_dataset,\n",
    "        eval_dataset=tok_val_dataset,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        lr=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        eval_steps=eval_steps,\n",
    "        optimizer_factory=lambda params, lr: torch.optim.AdamW(params, lr=lr),\n",
    "        lr_scheduler_factory=lambda opt, steps: transformers.get_scheduler(lr_scheduler_name, opt, num_warmup_steps, steps),\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "def do_train():\n",
    "    notebook_launcher(train_function, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd58f89f-7477-41af-aabf-608bdda1eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 6 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 553 steps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a03db776204571893128a6e1c7c5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 140\u001b[0m, in \u001b[0;36mdo_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_train\u001b[39m():\n\u001b[0;32m--> 140\u001b[0m     \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/launchers.py:201\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_processes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlauncher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:202\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:114\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Wait for any process to fail or all of them to succeed.\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m error_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentinel \u001b[38;5;129;01min\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db60ebd9-8d68-45ba-8a67-31f54db37183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Huggingface Trainer Example\n",
    "This illustrates how to use the HF trainer class, with the functionality being similar\n",
    "to the above code.\n",
    "\n",
    "Within the context of a notebook and multiple GPU's, the trainer will train the model using torch [Data Parallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html). This is not ideal for performance.\n",
    "\n",
    "The same code, launched using Accelerate in a script will perform much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6819d356-e4b3-43ae-85e5-6e07acaeccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def train_causal_model(\n",
    "    model,tokenizer,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    training_args,\n",
    "):\n",
    "    def preprocess_logits_for_metrics(logits, labels):\n",
    "        if isinstance(logits, tuple):\n",
    "            # Depending on the model and config, logits may contain extra tensors,\n",
    "            # like past_key_values, but logits always come first\n",
    "            logits = logits[0]\n",
    "        return logits.argmax(dim=-1)\n",
    "    \n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    \n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "        # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "        labels = labels[:, 1:].reshape(-1)\n",
    "        preds = preds[:, :-1].reshape(-1)\n",
    "        return metric.compute(predictions=preds, references=labels)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        #compute_metrics=compute_metrics,\n",
    "        #preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df1896-2f96-4311-94b4-4207d890ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "def do_train():\n",
    "    train_causal_model(\n",
    "        model,tokenizer,\n",
    "        tok_train_dataset,\n",
    "        tok_val_dataset,\n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=64,\n",
    "            per_device_eval_batch_size=128,\n",
    "            output_dir=\"test_trainer\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=500,\n",
    "            num_train_epochs=1,\n",
    "    \n",
    "            # If set too high, your GPU may run out of memory.\n",
    "            #per_device_train_batch_size=8,\n",
    "            #per_device_eval_batch_size=16,\n",
    "            \n",
    "            # The learning rate will need to be reduced as model size grows. If the rate is set too high, the\n",
    "            # loss will become unstable, possibly increasing.\n",
    "            learning_rate=1e-3,\n",
    "            # Set for better diagnostics\n",
    "            #use_cpu=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "#notebook_launcher(do_train, num_processes=torch.cuda.device_count())\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f9b16-b814-4f39-90a5-764ef199ea78",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6527d-eaaa-4858-b02e-b875d5ac0f7f",
   "metadata": {},
   "source": [
    "### Implemenation\n",
    "You don't need to look all that closely at this code -- we use it for evaluating the model's prediction in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d88e1a2-d688-4c15-8cd3-998ed75c41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_predictions(model, tokenizer, device, text):\n",
    "    for i, line in enumerate(text):\n",
    "        print(f\"line: {line}\")\n",
    "        model = model.to(device)\n",
    "        logits, label_ids = predict_text(model, tokenizer, line, device=device)\n",
    "            \n",
    "        show_colorized_tokens(\n",
    "            tokenizer=tokenizer,\n",
    "            logits=logits,\n",
    "            label_ids=label_ids,\n",
    "            \n",
    "            # the metric function to use\n",
    "            metric_fn=causal_loss_metric,\n",
    "        \n",
    "            # how to translate metric to colors\n",
    "            color_encoder=ColorEncoder(\n",
    "                is_relative=True,\n",
    "                #lower_bound=0,\n",
    "                #upper_bound=10,\n",
    "                cmap='plasma',\n",
    "            ),\n",
    "            top_k = 10,\n",
    "            \n",
    "            # Filter threshold on metric\n",
    "            #threshold=0,\n",
    "            pad_lines=15\n",
    "        )\n",
    "        print(\"\\n\")\n",
    "\n",
    "def predict_text(model, tokenizer, input_text, device):\n",
    "    batch_encoding = tokenizer(\n",
    "        input_text,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        verbose=True,\n",
    "    )\n",
    "    input_ids = batch_encoding['input_ids'].to(device)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    logits = model(input_ids=input_ids)\n",
    "    return logits.cpu().detach().float(), input_ids.cpu().detach()\n",
    "\n",
    "def show_colorized_tokens(tokenizer, logits, label_ids, metric_fn, color_encoder, top_k, threshold=None, pad_lines=20):\n",
    "    metrics, metric_min, metric_max, metric_label = metric_fn(logits=logits, label_ids=label_ids)\n",
    "    value_min, value_max = metrics.aminmax()\n",
    "    \n",
    "    print(f\"Metric '{metric_label}': n={metrics.numel()}, min={value_min}, max={value_max}, mean={metrics.mean()}, range=({metric_min}, {metric_max})\")\n",
    "\n",
    "    # if causal only; we need to pad the left with something to alighn the predictions.\n",
    "    metrics = torch.cat((torch.zeros(metrics.size(0), 1, device=metrics.device, dtype=metrics.dtype), metrics), dim=-1)\n",
    "    if metrics.size(-1) > label_ids.size(-1):\n",
    "        metrics = metrics.narrow(-1, 0, label_ids.size(-1))\n",
    "    colors = color_encoder(metrics, value_min, value_max, metric_min=metric_min, metric_max=metric_max)\n",
    "    html_text = tooltip_style\n",
    "\n",
    "    label_ids = restore_pad_ids(tokenizer, label_ids)\n",
    "    for i in range(label_ids.size(0)):\n",
    "        metric_mean = metrics[i].mean()\n",
    "        info = f\"Metric[{i}] '{metric_label}': n={metrics[i].numel()}, min={metrics[i].min()}, max={metrics[i].max()}, mean={metric_mean}<br>\"\n",
    "        html_text += info\n",
    "        if threshold is not None and metric_mean < threshold:\n",
    "            continue\n",
    "        token_seq = tokenizer.batch_decode(label_ids[i], skip_special_tokens=True)\n",
    "        color_seq = colors[i]\n",
    "        token_info_list = generate_token_info_list(tokenizer, token_seq, metric_label, metrics[i], logits[i], top_k)\n",
    "        text = color_encode_html_tokens(token_seq, color_seq, token_info_list)\n",
    "        html_text += text + \"<br>\"\n",
    "\n",
    "    for _ in range(pad_lines):\n",
    "        html_text += \"<br>\"\n",
    "    \n",
    "    display(HTML(html_text))\n",
    "    \n",
    "# Replace '-100' tokens with the tokenizer's 'pad' token.\n",
    "def restore_pad_ids(tokenizer, label_ids):\n",
    "    return torch.where(label_ids == -100, tokenizer.eos_token_id, label_ids) \n",
    "\n",
    "tooltip_style = \"\"\"\n",
    "<style>\n",
    "/* Tooltip container class */\n",
    ".token {\n",
    "  position: relative;\n",
    "  display: inline-block;\n",
    "}\n",
    "\n",
    "/* Tooltip text */\n",
    ".token .tooltip {\n",
    "  visibility: hidden;\n",
    "  width: 300px;\n",
    "  background-color: black;\n",
    "  color: #fff;\n",
    "  text-align: left;\n",
    "  padding: 5px 0;\n",
    "  border-radius: 6px;\n",
    " \n",
    "  /* Position the tooltip text - see examples below! */\n",
    "  position: absolute;\n",
    "  z-index: 1;\n",
    "}\n",
    "\n",
    "/* Show the tooltip text when you mouse over the tooltip container */\n",
    ".token:hover .tooltip {\n",
    "  visibility: visible;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "def escape_html_token(token):\n",
    "    match token:\n",
    "        case '\\n':\n",
    "            return '<br>'\n",
    "        case '<':\n",
    "            return '&lt;'\n",
    "        case '>':\n",
    "            return '&gt;'\n",
    "        case '\"':\n",
    "            return '&quot;'\n",
    "        case \"'\":\n",
    "            return '&#39;'\n",
    "        case '&':\n",
    "            return '&amp;'\n",
    "        case _:\n",
    "            return token\n",
    "\n",
    "def html_color(color):\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(int(255*color[0]), int(255*color[1]), int(255*color[2]))\n",
    "\n",
    "def color_encode_html_tokens(token_seq, color_seq, info_seq):\n",
    "    text = \"\"\n",
    "    for token, color, info in zip(token_seq, color_seq, info_seq):\n",
    "        if token == '\\n':\n",
    "            text += \"<br>\"\n",
    "        else:\n",
    "            # HTML will eat your space tokens if you don't do this!\n",
    "            if len(token) > 0 and token[0] == ' ':\n",
    "                token = \"&nbsp;\" + token[1:]\n",
    "            text += f\"<span class='token' style='color: {html_color(color)}'>{escape_html_token(token)}<span class='tooltip'>{info}</span></span>\"\n",
    "    return text\n",
    "\n",
    "class ColorEncoder:\n",
    "    def __init__(self, is_relative=True, upper_bound=math.inf, lower_bound=-math.inf, cmap='viridis'):\n",
    "        self.is_relative = is_relative\n",
    "        self.upper_bound = upper_bound\n",
    "        self.lower_bound = lower_bound\n",
    "        self.colormap = plt.get_cmap(cmap)\n",
    "\n",
    "    def __call__(self, metrics, value_min, value_max, metric_min=None, metric_max=None):\n",
    "        if self.is_relative or metric_min is None and metric_max is None:\n",
    "            minimum, maximum = value_min, value_max\n",
    "        elif metric_min is None:\n",
    "            minimum, maximum = value_min, metric_max\n",
    "        elif metric_max is None:\n",
    "            minimum, maximum = metric_min, value_max\n",
    "        else:\n",
    "            minimum, maximum = metric_min, metric_max\n",
    "            \n",
    "        minimum = max(minimum, self.lower_bound)\n",
    "        maximum = min(maximum, self.upper_bound)\n",
    "        return self.colormap(self.normalize_metric(metrics, minimum, maximum))\n",
    "        \n",
    "    def normalize_metric(self, metric, minimum, maximum):\n",
    "        return torch.clamp(input=(metric - minimum) / (maximum - minimum), min=0.0, max=1.0)\n",
    "\n",
    "def topk_predicted_tokens(tokenizer, logits, top_k=5):\n",
    "    top_prob, top_indices = torch.topk(torch.softmax(logits, dim=-1), top_k, dim=-1)\n",
    "    top_tokens = tokenizer.batch_decode(top_indices.flatten(), skip_special_tokens=True)\n",
    "        \n",
    "    return top_prob.flatten(), top_tokens\n",
    "        \n",
    "def generate_token_info_list(tokenizer, token_seq, metric_label, metrics, logits, top_k=5):\n",
    "    top_prob, top_tokens = topk_predicted_tokens(tokenizer, logits, top_k)\n",
    "    info_list = []\n",
    "    for j in range(len(metrics)):\n",
    "        text = f\"Token: '{token_seq[j]}'<br>{metric_label}: {'%.5f' % metrics[j]}<br>---------------<br>\"\n",
    "        if j != 0: # And is causal!\n",
    "            start = (j - 1) * top_k\n",
    "            for k in range(start, start + top_k):\n",
    "                top_token = top_tokens[k]\n",
    "                if top_token == '\\n':\n",
    "                    top_token = '\\\\n'\n",
    "                text += f\"{'%.2f' % top_prob[k]} : '{escape_html_token(top_token)}'<br>\"\n",
    "        info_list.append(text)\n",
    "    return info_list\n",
    "\n",
    "#### Logits and Lables Metric Functions\n",
    "def causal_loss_metric(logits, label_ids, reduction='none'):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = label_ids[..., 1:].contiguous()\n",
    "    \n",
    "    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), reduction=reduction)\\\n",
    "        .view(label_ids.size(0), label_ids.size(1) - 1)\n",
    "    \n",
    "    return loss, 0, None, \"Causal Loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc798a0c-3398-4b93-bce1-400a6d7839e9",
   "metadata": {},
   "source": [
    "### Predict\n",
    "This will take the input text and have the model make predictions for the next token for each token in the sequence.\n",
    "\n",
    "The color coding indicates the loss for each individual token, with darker colors being more accurate and brighter colors being less so.\n",
    "\n",
    "If you hover over a token, you can see the top-10 predictions for the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab16902e-9f12-4690-8f41-469036fa25b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b\n",
      "Metric 'Causal Loss': n=154, min=0.004118770360946655, max=9.195035934448242, mean=3.3656723499298096, range=(0, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "/* Tooltip container class */\n",
       ".token {\n",
       "  position: relative;\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       "/* Tooltip text */\n",
       ".token .tooltip {\n",
       "  visibility: hidden;\n",
       "  width: 300px;\n",
       "  background-color: black;\n",
       "  color: #fff;\n",
       "  text-align: left;\n",
       "  padding: 5px 0;\n",
       "  border-radius: 6px;\n",
       " \n",
       "  /* Position the tooltip text - see examples below! */\n",
       "  position: absolute;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       "/* Show the tooltip text when you mouse over the tooltip container */\n",
       ".token:hover .tooltip {\n",
       "  visibility: visible;\n",
       "}\n",
       "</style>\n",
       "Metric[0] 'Causal Loss': n=155, min=0.0, max=9.195035934448242, mean=3.3439583778381348<br><span class='token' style='color: #0c0786'><span class='tooltip'>Token: ''<br>Causal Loss: 0.00000<br>---------------<br></span></span><span class='token' style='color: #9511a1'>&nbsp;One<span class='tooltip'>Token: ' One'<br>Causal Loss: 2.89333<br>---------------<br>0.65 : ' Once'<br>0.07 : ' Lily'<br>0.06 : ' One'<br>0.04 : ' Tom'<br>0.02 : ' Anna'<br>0.02 : ' Ben'<br>0.02 : ' Sara'<br>0.01 : ' Tim'<br>0.01 : ' John'<br>0.01 : ' Sam'<br></span></span><span class='token' style='color: #100787'>&nbsp;day<span class='tooltip'>Token: ' day'<br>Causal Loss: 0.05826<br>---------------<br>0.94 : ' day'<br>0.01 : ' of'<br>0.01 : ' night'<br>0.01 : ' morning'<br>0.01 : ' sun'<br>0.00 : ','<br>0.00 : '.'<br>0.00 : ' was'<br>0.00 : ' time'<br>0.00 : ' is'<br></span></span><span class='token' style='color: #310496'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.57909<br>---------------<br>0.56 : ','<br>0.10 : '.'<br>0.09 : ' on'<br>0.03 : ' she'<br>0.02 : ' he'<br>0.02 : ' long'<br>0.02 : ' and'<br>0.02 : 's'<br>0.01 : ' the'<br>0.01 : ' for'<br></span></span><span class='token' style='color: #ae2791'>&nbsp;a<span class='tooltip'>Token: ' a'<br>Causal Loss: 3.59704<br>---------------<br>0.15 : ' \"'<br>0.07 : ' but'<br>0.06 : ' and'<br>0.06 : ' there'<br>0.05 : ' Lily'<br>0.04 : ' she'<br>0.04 : ' the'<br>0.03 : ' he'<br>0.03 : ' a'<br>0.03 : ' they'<br></span></span><span class='token' style='color: #8104a7'>&nbsp;little<span class='tooltip'>Token: ' little'<br>Causal Loss: 2.40632<br>---------------<br>0.11 : ' time'<br>0.10 : ' big'<br>0.09 : ' little'<br>0.02 : ' lot'<br>0.02 : ' new'<br>0.01 : ' good'<br>0.01 : ' small'<br>0.01 : ' loud'<br>0.01 : ' special'<br>0.01 : ' long'<br></span></span><span class='token' style='color: #340498'>&nbsp;girl<span class='tooltip'>Token: ' girl'<br>Causal Loss: 0.67795<br>---------------<br>0.51 : ' girl'<br>0.17 : ' boy'<br>0.05 : ' bird'<br>0.02 : ' bit'<br>0.01 : ' mouse'<br>0.01 : ' fish'<br>0.01 : ' dog'<br>0.01 : ' brother'<br>0.01 : ' bunny'<br>0.01 : ' rabbit'<br></span></span><span class='token' style='color: #44039e'>&nbsp;named<span class='tooltip'>Token: ' named'<br>Causal Loss: 1.00189<br>---------------<br>0.37 : ' named'<br>0.10 : ' was'<br>0.07 : '.'<br>0.06 : ' who'<br>0.03 : ' called'<br>0.03 : ' and'<br>0.03 : ' smiled'<br>0.02 : ','<br>0.02 : ' said'<br>0.01 : ''s'<br></span></span><span class='token' style='color: #49029f'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 1.10298<br>---------------<br>0.33 : ' Lily'<br>0.14 : ' Timmy'<br>0.06 : ' Tim'<br>0.03 : ' Max'<br>0.03 : ' Lucy'<br>0.03 : ' Tom'<br>0.02 : ' B'<br>0.02 : ' T'<br>0.02 : ' Jack'<br>0.02 : ' M'<br></span></span><span class='token' style='color: #e56c5b'>&nbsp;found<span class='tooltip'>Token: ' found'<br>Causal Loss: 5.76965<br>---------------<br>0.19 : '.'<br>0.11 : ' and'<br>0.08 : ' was'<br>0.07 : ''s'<br>0.06 : ' said'<br>0.03 : ','<br>0.03 : ' felt'<br>0.02 : ' were'<br>0.02 : ' saw'<br>0.02 : ' went'<br></span></span><span class='token' style='color: #380499'>&nbsp;a<span class='tooltip'>Token: ' a'<br>Causal Loss: 0.73792<br>---------------<br>0.48 : ' a'<br>0.11 : ' the'<br>0.06 : ' it'<br>0.04 : ' some'<br>0.03 : ' an'<br>0.03 : ' something'<br>0.02 : '.'<br>0.02 : ' her'<br>0.02 : ' out'<br>0.01 : ' his'<br></span></span><span class='token' style='color: #fdb62d'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 7.74836<br>---------------<br>0.11 : ' time'<br>0.10 : ' big'<br>0.09 : ' little'<br>0.02 : ' lot'<br>0.02 : ' new'<br>0.01 : ' good'<br>0.01 : ' small'<br>0.01 : ' loud'<br>0.01 : ' special'<br>0.01 : ' long'<br></span></span><span class='token' style='color: #8908a5'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 2.61453<br>---------------<br>0.42 : ' to'<br>0.09 : 's'<br>0.07 : 'le'<br>0.06 : ' a'<br>0.05 : ' it'<br>0.04 : '.'<br>0.03 : ' the'<br>0.02 : ' help'<br>0.02 : ' some'<br>0.01 : ' something'<br></span></span><span class='token' style='color: #bd3784'>&nbsp;in<span class='tooltip'>Token: ' in'<br>Causal Loss: 4.12354<br>---------------<br>0.17 : '.'<br>0.08 : ' and'<br>0.06 : 't'<br>0.05 : ' of'<br>0.05 : 'x'<br>0.04 : 'br'<br>0.03 : 'ased'<br>0.03 : ','<br>0.03 : ' was'<br>0.02 : ' with'<br></span></span><span class='token' style='color: #900ea3'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 2.77217<br>---------------<br>0.54 : ' the'<br>0.11 : ' a'<br>0.06 : ' her'<br>0.05 : ' his'<br>0.02 : ' their'<br>0.02 : '.'<br>0.01 : ' it'<br>0.01 : 'c'<br>0.01 : ' fr'<br>0.01 : ' and'<br></span></span><span class='token' style='color: #bc3685'>&nbsp;room<span class='tooltip'>Token: ' room'<br>Causal Loss: 4.06107<br>---------------<br>0.11 : ' mom'<br>0.07 : '.'<br>0.04 : ' mommy'<br>0.03 : ' friends'<br>0.03 : ' to'<br>0.03 : ' and'<br>0.02 : ' a'<br>0.02 : ' toys'<br>0.02 : ' room'<br>0.02 : ' friend'<br></span></span><span class='token' style='color: #380499'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 0.72875<br>---------------<br>0.48 : '.'<br>0.19 : ' and'<br>0.07 : ','<br>0.03 : ' with'<br>0.03 : ' was'<br>0.03 : 's'<br>0.02 : ' to'<br>0.01 : ' for'<br>0.01 : '.\"'<br>0.01 : '!'<br></span></span><span class='token' style='color: #6c00a8'>&nbsp;She<span class='tooltip'>Token: ' She'<br>Causal Loss: 1.89533<br>---------------<br>0.17 : '\\n'<br>0.15 : ' She'<br>0.13 : ' They'<br>0.12 : ' He'<br>0.05 : ' It'<br>0.04 : ' The'<br>0.04 : ' \n",
       "'<br>0.02 : ' \"'<br>0.02 : ' One'<br>0.02 : ' I'<br></span></span><span class='token' style='color: #be3883'>&nbsp;knew<span class='tooltip'>Token: ' knew'<br>Causal Loss: 4.16549<br>---------------<br>0.11 : ' was'<br>0.06 : ' said'<br>0.05 : ' had'<br>0.05 : ' loved'<br>0.04 : ' wanted'<br>0.03 : ' saw'<br>0.02 : ' felt'<br>0.02 : ' did'<br>0.02 : ' looked'<br>0.02 : ' asked'<br></span></span><span class='token' style='color: #7200a8'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 2.02758<br>---------------<br>0.31 : ' that'<br>0.14 : ' he'<br>0.13 : ' it'<br>0.10 : ' she'<br>0.06 : ' they'<br>0.04 : ' the'<br>0.03 : ' how'<br>0.03 : ' what'<br>0.01 : ' her'<br>0.01 : ' this'<br></span></span><span class='token' style='color: #6700a7'>&nbsp;was<span class='tooltip'>Token: ' was'<br>Causal Loss: 1.78904<br>---------------<br>0.17 : '.'<br>0.17 : ' was'<br>0.04 : ' to'<br>0.04 : ','<br>0.03 : ' and'<br>0.03 : ' is'<br>0.03 : ''s'<br>0.02 : ' up'<br>0.02 : ' in'<br>0.02 : ' on'<br></span></span><span class='token' style='color: #fa9f3a'>&nbsp;dif<span class='tooltip'>Token: ' dif'<br>Causal Loss: 7.15985<br>---------------<br>0.20 : ' a'<br>0.11 : ' so'<br>0.08 : ' very'<br>0.03 : ' happy'<br>0.02 : ' not'<br>0.02 : ' sad'<br>0.02 : ' too'<br>0.02 : ' scared'<br>0.01 : ' the'<br>0.01 : ' time'<br></span></span><span class='token' style='color: #0c0786'>f<span class='tooltip'>Token: 'f'<br>Causal Loss: 0.00412<br>---------------<br>1.00 : 'f'<br>0.00 : 'ers'<br>0.00 : 'g'<br>0.00 : 'p'<br>0.00 : 'er'<br>0.00 : 'w'<br>0.00 : 'or'<br>0.00 : ' love'<br>0.00 : 'ir'<br>0.00 : 's'<br></span></span><span class='token' style='color: #9f1a9b'>ic<span class='tooltip'>Token: 'ic'<br>Causal Loss: 3.19499<br>---------------<br>0.10 : 'ish'<br>0.09 : '.'<br>0.07 : 'ra'<br>0.06 : ' course'<br>0.04 : ' you'<br>0.04 : 'ic'<br>0.04 : 'l'<br>0.03 : 'ely'<br>0.03 : 'ast'<br>0.03 : 'a'<br></span></span><span class='token' style='color: #7f03a7'>ul<span class='tooltip'>Token: 'ul'<br>Causal Loss: 2.34326<br>---------------<br>0.11 : '.'<br>0.10 : 'ul'<br>0.08 : 'ine'<br>0.06 : 't'<br>0.05 : 'hes'<br>0.04 : 'ro'<br>0.04 : ' and'<br>0.04 : 'ing'<br>0.04 : 'es'<br>0.03 : 'er'<br></span></span><span class='token' style='color: #4c02a1'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 1.17391<br>---------------<br>0.31 : 't'<br>0.28 : 'ar'<br>0.09 : 'ance'<br>0.07 : 'ie'<br>0.04 : 'u'<br>0.03 : 'ia'<br>0.02 : 'ts'<br>0.01 : 'a'<br>0.01 : 'i'<br>0.01 : 'ated'<br></span></span><span class='token' style='color: #c33e7f'>&nbsp;to<span class='tooltip'>Token: ' to'<br>Causal Loss: 4.31489<br>---------------<br>0.10 : '.'<br>0.05 : 'en'<br>0.05 : 'o'<br>0.04 : ' and'<br>0.04 : 'urn'<br>0.04 : ' of'<br>0.04 : 'op'<br>0.03 : ' the'<br>0.03 : 'e'<br>0.02 : 'i'<br></span></span><span class='token' style='color: #8607a6'>&nbsp;play<span class='tooltip'>Token: ' play'<br>Causal Loss: 2.52341<br>---------------<br>0.11 : ' the'<br>0.08 : ' play'<br>0.05 : ' go'<br>0.05 : ' be'<br>0.02 : ' make'<br>0.02 : ' her'<br>0.02 : ' help'<br>0.02 : ' see'<br>0.02 : ' do'<br>0.02 : ' get'<br></span></span><span class='token' style='color: #3b039a'>&nbsp;with<span class='tooltip'>Token: ' with'<br>Causal Loss: 0.80344<br>---------------<br>0.45 : ' with'<br>0.09 : '.'<br>0.09 : ' in'<br>0.08 : ' outside'<br>0.04 : ' together'<br>0.02 : ' and'<br>0.02 : ' a'<br>0.02 : ' on'<br>0.01 : 'g'<br>0.01 : ' again'<br></span></span><span class='token' style='color: #9b179e'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 3.08862<br>---------------<br>0.16 : ' her'<br>0.13 : ' the'<br>0.11 : ' his'<br>0.10 : ' a'<br>0.06 : ' their'<br>0.05 : ' it'<br>0.02 : ' them'<br>0.02 : ' me'<br>0.02 : ' you'<br>0.02 : ' him'<br></span></span><span class='token' style='color: #f68e44'>&nbsp;because<span class='tooltip'>Token: ' because'<br>Causal Loss: 6.72512<br>---------------<br>0.17 : '.'<br>0.17 : ' was'<br>0.04 : ' to'<br>0.04 : ','<br>0.03 : ' and'<br>0.03 : ' is'<br>0.03 : ''s'<br>0.02 : ' up'<br>0.02 : ' in'<br>0.02 : ' on'<br></span></span><span class='token' style='color: #6400a7'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 1.69265<br>---------------<br>0.28 : ' he'<br>0.25 : ' she'<br>0.18 : ' it'<br>0.07 : ' they'<br>0.04 : ' the'<br>0.03 : ' I'<br>0.02 : ' of'<br>0.01 : ' her'<br>0.01 : ' you'<br>0.01 : ' his'<br></span></span><span class='token' style='color: #6700a7'>&nbsp;was<span class='tooltip'>Token: ' was'<br>Causal Loss: 1.78904<br>---------------<br>0.17 : '.'<br>0.17 : ' was'<br>0.04 : ' to'<br>0.04 : ','<br>0.03 : ' and'<br>0.03 : ' is'<br>0.03 : ''s'<br>0.02 : ' up'<br>0.02 : ' in'<br>0.02 : ' on'<br></span></span><span class='token' style='color: #dd5f65'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 5.37110<br>---------------<br>0.20 : ' a'<br>0.11 : ' so'<br>0.08 : ' very'<br>0.03 : ' happy'<br>0.02 : ' not'<br>0.02 : ' sad'<br>0.02 : ' too'<br>0.02 : ' scared'<br>0.01 : ' the'<br>0.01 : ' time'<br></span></span><span class='token' style='color: #8807a5'>ar<span class='tooltip'>Token: 'ar'<br>Causal Loss: 2.55319<br>---------------<br>0.11 : 'in'<br>0.08 : 'ar'<br>0.07 : 'r'<br>0.07 : 'ip'<br>0.07 : 'aring'<br>0.05 : 'ir'<br>0.05 : 'y'<br>0.04 : 'ore'<br>0.04 : 'one'<br>0.03 : 'ake'<br></span></span><span class='token' style='color: #8f0da3'>p<span class='tooltip'>Token: 'p'<br>Causal Loss: 2.75296<br>---------------<br>0.10 : '.'<br>0.06 : 'p'<br>0.06 : 'f'<br>0.05 : 'ge'<br>0.05 : 'm'<br>0.04 : 'l'<br>0.04 : 'ra'<br>0.03 : 'ly'<br>0.03 : ' and'<br>0.03 : 'i'<br></span></span><span class='token' style='color: #9310a1'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 2.85703<br>---------------<br>0.06 : 'ort'<br>0.06 : '.'<br>0.06 : 'ed'<br>0.05 : 'a'<br>0.05 : 'ack'<br>0.03 : 'ty'<br>0.03 : 'ol'<br>0.03 : 'ot'<br>0.03 : 's'<br>0.03 : 'ill'<br></span></span><span class='token' style='color: #bf3982'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 4.18431<br>---------------<br>0.17 : '\\n'<br>0.15 : ' She'<br>0.13 : ' They'<br>0.12 : ' He'<br>0.05 : ' It'<br>0.04 : ' The'<br>0.04 : ' \n",
       "'<br>0.02 : ' \"'<br>0.02 : ' One'<br>0.02 : ' I'<br></span></span><span class='token' style='color: #c5407d'>&nbsp;wanted<span class='tooltip'>Token: ' wanted'<br>Causal Loss: 4.40559<br>---------------<br>0.19 : '.'<br>0.11 : ' and'<br>0.08 : ' was'<br>0.07 : ''s'<br>0.06 : ' said'<br>0.03 : ','<br>0.03 : ' felt'<br>0.02 : ' were'<br>0.02 : ' saw'<br>0.02 : ' went'<br></span></span><span class='token' style='color: #15068a'>&nbsp;to<span class='tooltip'>Token: ' to'<br>Causal Loss: 0.13658<br>---------------<br>0.87 : ' to'<br>0.02 : ' the'<br>0.02 : '.'<br>0.02 : ' a'<br>0.01 : ' it'<br>0.01 : ' her'<br>0.00 : ' some'<br>0.00 : ' something'<br>0.00 : ' his'<br>0.00 : ' their'<br></span></span><span class='token' style='color: #cc4876'>&nbsp;share<span class='tooltip'>Token: ' share'<br>Causal Loss: 4.64817<br>---------------<br>0.11 : ' the'<br>0.08 : ' play'<br>0.05 : ' go'<br>0.05 : ' be'<br>0.02 : ' make'<br>0.02 : ' her'<br>0.02 : ' help'<br>0.02 : ' see'<br>0.02 : ' do'<br>0.02 : ' get'<br></span></span><span class='token' style='color: #6f00a8'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 1.94397<br>---------------<br>0.17 : '.'<br>0.14 : ' the'<br>0.13 : ' and'<br>0.08 : ' it'<br>0.06 : ' their'<br>0.06 : ' with'<br>0.05 : ' her'<br>0.05 : ' his'<br>0.03 : ' them'<br>0.02 : ' your'<br></span></span><span class='token' style='color: #fdb22f'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 7.62155<br>---------------<br>0.04 : ' park'<br>0.02 : ' bird'<br>0.01 : ' sky'<br>0.01 : ' c'<br>0.01 : ' p'<br>0.01 : ' little'<br>0.01 : ' water'<br>0.01 : ' m'<br>0.01 : ' sun'<br>0.01 : ' t'<br></span></span><span class='token' style='color: #8908a5'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 2.61453<br>---------------<br>0.42 : ' to'<br>0.09 : 's'<br>0.07 : 'le'<br>0.06 : ' a'<br>0.05 : ' it'<br>0.04 : '.'<br>0.03 : ' the'<br>0.02 : ' help'<br>0.02 : ' some'<br>0.01 : ' something'<br></span></span><span class='token' style='color: #b93388'>&nbsp;with<span class='tooltip'>Token: ' with'<br>Causal Loss: 3.98167<br>---------------<br>0.17 : '.'<br>0.08 : ' and'<br>0.06 : 't'<br>0.05 : ' of'<br>0.05 : 'x'<br>0.04 : 'br'<br>0.03 : 'ased'<br>0.03 : ','<br>0.03 : ' was'<br>0.02 : ' with'<br></span></span><span class='token' style='color: #6800a7'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 1.81427<br>---------------<br>0.16 : ' her'<br>0.13 : ' the'<br>0.11 : ' his'<br>0.10 : ' a'<br>0.06 : ' their'<br>0.05 : ' it'<br>0.02 : ' them'<br>0.02 : ' me'<br>0.02 : ' you'<br>0.02 : ' him'<br></span></span><span class='token' style='color: #7901a8'>&nbsp;mom<span class='tooltip'>Token: ' mom'<br>Causal Loss: 2.21672<br>---------------<br>0.11 : ' mom'<br>0.07 : '.'<br>0.04 : ' mommy'<br>0.03 : ' friends'<br>0.03 : ' to'<br>0.03 : ' and'<br>0.02 : ' a'<br>0.02 : ' toys'<br>0.02 : ' room'<br>0.02 : ' friend'<br></span></span><span class='token' style='color: #99149f'>,<span class='tooltip'>Token: ','<br>Causal Loss: 2.99272<br>---------------<br>0.15 : '.'<br>0.12 : ' said'<br>0.11 : ' and'<br>0.05 : ','<br>0.04 : ' was'<br>0.04 : ' smiled'<br>0.02 : ''s'<br>0.02 : ' came'<br>0.02 : ' told'<br>0.02 : ' asked'<br></span></span><span class='token' style='color: #bb3586'>&nbsp;so<span class='tooltip'>Token: ' so'<br>Causal Loss: 4.04338<br>---------------<br>0.15 : ' \"'<br>0.07 : ' but'<br>0.06 : ' and'<br>0.06 : ' there'<br>0.05 : ' Lily'<br>0.04 : ' she'<br>0.04 : ' the'<br>0.03 : ' he'<br>0.03 : ' a'<br>0.03 : ' they'<br></span></span><span class='token' style='color: #9511a1'>&nbsp;she<span class='tooltip'>Token: ' she'<br>Causal Loss: 2.89492<br>---------------<br>0.20 : ' happy'<br>0.12 : ' excited'<br>0.07 : ' much'<br>0.06 : ' he'<br>0.06 : ' she'<br>0.04 : ' proud'<br>0.02 : ' they'<br>0.01 : ' pretty'<br>0.01 : 'l'<br>0.01 : ' sad'<br></span></span><span class='token' style='color: #9511a1'>&nbsp;could<span class='tooltip'>Token: ' could'<br>Causal Loss: 2.90117<br>---------------<br>0.10 : ' was'<br>0.08 : ' had'<br>0.05 : ' could'<br>0.05 : ' saw'<br>0.04 : ' said'<br>0.03 : ' would'<br>0.03 : ' went'<br>0.03 : ' wanted'<br>0.02 : ' found'<br>0.02 : 'll'<br></span></span><span class='token' style='color: #fba337'>&nbsp;se<span class='tooltip'>Token: ' se'<br>Causal Loss: 7.25706<br>---------------<br>0.11 : ' not'<br>0.05 : ' have'<br>0.05 : ' see'<br>0.04 : ' go'<br>0.04 : ' do'<br>0.04 : ' help'<br>0.04 : ' be'<br>0.03 : '.'<br>0.03 : ' make'<br>0.03 : ' play'<br></span></span><span class='token' style='color: #9b179e'>w<span class='tooltip'>Token: 'w'<br>Causal Loss: 3.06890<br>---------------<br>0.21 : 'at'<br>0.20 : 'ed'<br>0.16 : 'll'<br>0.12 : 'p'<br>0.08 : 'al'<br>0.06 : 'as'<br>0.05 : 'w'<br>0.02 : 'ag'<br>0.02 : 'ven'<br>0.01 : 'ver'<br></span></span><span class='token' style='color: #f8963f'>&nbsp;a<span class='tooltip'>Token: ' a'<br>Causal Loss: 6.96906<br>---------------<br>0.11 : 'eet'<br>0.07 : 'el'<br>0.06 : 'e'<br>0.05 : 'or'<br>0.05 : '.'<br>0.05 : 'led'<br>0.05 : 'ich'<br>0.05 : 's'<br>0.03 : 'l'<br>0.03 : 'ards'<br></span></span><span class='token' style='color: #faa039'>&nbsp;butt<span class='tooltip'>Token: ' butt'<br>Causal Loss: 7.19477<br>---------------<br>0.11 : ' time'<br>0.10 : ' big'<br>0.09 : ' little'<br>0.02 : ' lot'<br>0.02 : ' new'<br>0.01 : ' good'<br>0.01 : ' small'<br>0.01 : ' loud'<br>0.01 : ' special'<br>0.01 : ' long'<br></span></span><span class='token' style='color: #230590'>on<span class='tooltip'>Token: 'on'<br>Causal Loss: 0.33759<br>---------------<br>0.71 : 'on'<br>0.27 : 'ons'<br>0.00 : 'y'<br>0.00 : 'ered'<br>0.00 : 'et'<br>0.00 : 'w'<br>0.00 : 'ars'<br>0.00 : 'm'<br>0.00 : 'ch'<br>0.00 : 'ot'<br></span></span><span class='token' style='color: #c43f7e'>&nbsp;on<span class='tooltip'>Token: ' on'<br>Causal Loss: 4.36386<br>---------------<br>0.15 : '.'<br>0.09 : 't'<br>0.08 : 'y'<br>0.07 : ','<br>0.06 : ' and'<br>0.06 : 'es'<br>0.05 : 'est'<br>0.04 : ''t'<br>0.03 : ' was'<br>0.02 : ' in'<br></span></span><span class='token' style='color: #8b09a4'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 2.65020<br>---------------<br>0.37 : ' the'<br>0.14 : ','<br>0.08 : ' a'<br>0.07 : ' her'<br>0.05 : ' it'<br>0.05 : ' his'<br>0.03 : ' their'<br>0.02 : '.'<br>0.02 : 't'<br>0.02 : ' an'<br></span></span><span class='token' style='color: #ef7d4f'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 6.28039<br>---------------<br>0.11 : ' mom'<br>0.07 : '.'<br>0.04 : ' mommy'<br>0.03 : ' friends'<br>0.03 : ' to'<br>0.03 : ' and'<br>0.02 : ' a'<br>0.02 : ' toys'<br>0.02 : ' room'<br>0.02 : ' friend'<br></span></span><span class='token' style='color: #9612a0'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 2.91687<br>---------------<br>0.11 : 'in'<br>0.08 : 'ar'<br>0.07 : 'r'<br>0.07 : 'ip'<br>0.07 : 'aring'<br>0.05 : 'ir'<br>0.05 : 'y'<br>0.04 : 'ore'<br>0.04 : 'one'<br>0.03 : 'ake'<br></span></span><span class='token' style='color: #7c02a7'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 2.30182<br>---------------<br>0.10 : 't'<br>0.10 : ' mom'<br>0.09 : 'r'<br>0.09 : 'p'<br>0.07 : 'c'<br>0.06 : 'd'<br>0.06 : 'a'<br>0.05 : 'cle'<br>0.04 : 'red'<br>0.03 : 'on'<br></span></span><span class='token' style='color: #7e03a7'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 2.32582<br>---------------<br>0.10 : '.'<br>0.05 : 'en'<br>0.05 : 'o'<br>0.04 : ' and'<br>0.04 : 'urn'<br>0.04 : ' of'<br>0.04 : 'op'<br>0.03 : ' the'<br>0.03 : 'e'<br>0.02 : 'i'<br></span></span><br><br><span class='token' style='color: #9d189d'>Lily<span class='tooltip'>Token: 'Lily'<br>Causal Loss: 3.10319<br>---------------<br>0.45 : '\\n'<br>0.10 : '&quot;'<br>0.07 : 'The'<br>0.04 : 'Lily'<br>0.03 : 'They'<br>0.03 : 'One'<br>0.02 : 'But'<br>0.02 : 'When'<br>0.02 : 'He'<br>0.01 : 'Ben'<br></span></span><span class='token' style='color: #c6417c'>&nbsp;went<span class='tooltip'>Token: ' went'<br>Causal Loss: 4.43523<br>---------------<br>0.20 : ' and'<br>0.13 : ' was'<br>0.07 : ','<br>0.06 : ''s'<br>0.05 : ' felt'<br>0.04 : ' smiled'<br>0.03 : ' said'<br>0.03 : ' nodded'<br>0.02 : ' thought'<br>0.02 : ' looked'<br></span></span><span class='token' style='color: #3d039b'>&nbsp;to<span class='tooltip'>Token: ' to'<br>Causal Loss: 0.86432<br>---------------<br>0.42 : ' to'<br>0.11 : ' back'<br>0.07 : ' home'<br>0.05 : ' outside'<br>0.05 : ' on'<br>0.04 : ' for'<br>0.03 : ' inside'<br>0.03 : ' out'<br>0.02 : ' down'<br>0.02 : ' up'<br></span></span><span class='token' style='color: #b22c8e'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 3.76787<br>---------------<br>0.11 : ' the'<br>0.08 : ' play'<br>0.05 : ' go'<br>0.05 : ' be'<br>0.02 : ' make'<br>0.02 : ' her'<br>0.02 : ' help'<br>0.02 : ' see'<br>0.02 : ' do'<br>0.02 : ' get'<br></span></span><span class='token' style='color: #7901a8'>&nbsp;mom<span class='tooltip'>Token: ' mom'<br>Causal Loss: 2.21672<br>---------------<br>0.11 : ' mom'<br>0.07 : '.'<br>0.04 : ' mommy'<br>0.03 : ' friends'<br>0.03 : ' to'<br>0.03 : ' and'<br>0.02 : ' a'<br>0.02 : ' toys'<br>0.02 : ' room'<br>0.02 : ' friend'<br></span></span><span class='token' style='color: #7b02a8'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 2.23334<br>---------------<br>0.15 : '.'<br>0.12 : ' said'<br>0.11 : ' and'<br>0.05 : ','<br>0.04 : ' was'<br>0.04 : ' smiled'<br>0.02 : ''s'<br>0.02 : ' came'<br>0.02 : ' told'<br>0.02 : ' asked'<br></span></span><span class='token' style='color: #9310a1'>&nbsp;said<span class='tooltip'>Token: ' said'<br>Causal Loss: 2.86673<br>---------------<br>0.06 : ' said'<br>0.04 : ' the'<br>0.03 : ' Ben'<br>0.02 : ' he'<br>0.02 : ' saw'<br>0.02 : ' Lily'<br>0.02 : ' a'<br>0.01 : ' they'<br>0.01 : ' it'<br>0.01 : ' she'<br></span></span><span class='token' style='color: #3a049a'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.76656<br>---------------<br>0.46 : ','<br>0.19 : '.'<br>0.03 : ' he'<br>0.03 : ' they'<br>0.03 : ' \"'<br>0.03 : ' it'<br>0.03 : ' goodbye'<br>0.02 : ' to'<br>0.02 : ' she'<br>0.02 : ' yes'<br></span></span><span class='token' style='color: #6c00a8'>&nbsp;\"<span class='tooltip'>Token: ' \"'<br>Causal Loss: 1.88892<br>---------------<br>0.15 : ' \"'<br>0.07 : ' but'<br>0.06 : ' and'<br>0.06 : ' there'<br>0.05 : ' Lily'<br>0.04 : ' she'<br>0.04 : ' the'<br>0.03 : ' he'<br>0.03 : ' a'<br>0.03 : ' they'<br></span></span><span class='token' style='color: #cb4777'>Mom<span class='tooltip'>Token: 'Mom'<br>Causal Loss: 4.63118<br>---------------<br>0.14 : 'I'<br>0.06 : 'It'<br>0.05 : 'Yes'<br>0.04 : 'You'<br>0.04 : 'Thank'<br>0.04 : 'We'<br>0.04 : 'Let'<br>0.03 : 'What'<br>0.03 : 'Don'<br>0.03 : 'No'<br></span></span><span class='token' style='color: #5101a2'>,<span class='tooltip'>Token: ','<br>Causal Loss: 1.29241<br>---------------<br>0.27 : ','<br>0.10 : ' and'<br>0.07 : ' said'<br>0.07 : ' smiled'<br>0.03 : ' says'<br>0.03 : ' heard'<br>0.03 : ' smiles'<br>0.02 : ' hugged'<br>0.02 : ' he'<br>0.02 : ' was'<br></span></span><span class='token' style='color: #b7308a'>&nbsp;I<span class='tooltip'>Token: ' I'<br>Causal Loss: 3.88255<br>---------------<br>0.15 : ' \"'<br>0.07 : ' but'<br>0.06 : ' and'<br>0.06 : ' there'<br>0.05 : ' Lily'<br>0.04 : ' she'<br>0.04 : ' the'<br>0.03 : ' he'<br>0.03 : ' a'<br>0.03 : ' they'<br></span></span><span class='token' style='color: #b93388'>&nbsp;found<span class='tooltip'>Token: ' found'<br>Causal Loss: 3.98422<br>---------------<br>0.11 : ''m'<br>0.09 : ' will'<br>0.07 : ' can'<br>0.06 : ' love'<br>0.06 : ' have'<br>0.06 : ' want'<br>0.05 : ' am'<br>0.03 : ' don'<br>0.03 : ''ll'<br>0.02 : ' like'<br></span></span><span class='token' style='color: #e16560'>&nbsp;this<span class='tooltip'>Token: ' this'<br>Causal Loss: 5.55766<br>---------------<br>0.48 : ' a'<br>0.11 : ' the'<br>0.06 : ' it'<br>0.04 : ' some'<br>0.03 : ' an'<br>0.03 : ' something'<br>0.02 : '.'<br>0.02 : ' her'<br>0.02 : ' out'<br>0.01 : ' his'<br></span></span><span class='token' style='color: #fdbe29'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 7.93658<br>---------------<br>0.09 : ' is'<br>0.06 : ' time'<br>0.05 : '?\"'<br>0.04 : ' was'<br>0.04 : '.'<br>0.03 : ','<br>0.03 : ' and'<br>0.02 : ' story'<br>0.02 : ' one'<br>0.02 : '!\"'<br></span></span><span class='token' style='color: #8908a5'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 2.61453<br>---------------<br>0.42 : ' to'<br>0.09 : 's'<br>0.07 : 'le'<br>0.06 : ' a'<br>0.05 : ' it'<br>0.04 : '.'<br>0.03 : ' the'<br>0.02 : ' help'<br>0.02 : ' some'<br>0.01 : ' something'<br></span></span><span class='token' style='color: #6700a7'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 1.77975<br>---------------<br>0.17 : '.'<br>0.08 : ' and'<br>0.06 : 't'<br>0.05 : ' of'<br>0.05 : 'x'<br>0.04 : 'br'<br>0.03 : 'ased'<br>0.03 : ','<br>0.03 : ' was'<br>0.02 : ' with'<br></span></span><span class='token' style='color: #ea7455'>&nbsp;Can<span class='tooltip'>Token: ' Can'<br>Causal Loss: 6.02020<br>---------------<br>0.17 : '\\n'<br>0.15 : ' She'<br>0.13 : ' They'<br>0.12 : ' He'<br>0.05 : ' It'<br>0.04 : ' The'<br>0.04 : ' \n",
       "'<br>0.02 : ' \"'<br>0.02 : ' One'<br>0.02 : ' I'<br></span></span><span class='token' style='color: #45039e'>&nbsp;you<span class='tooltip'>Token: ' you'<br>Causal Loss: 1.03425<br>---------------<br>0.43 : ' I'<br>0.36 : ' you'<br>0.19 : ' we'<br>0.00 : ' they'<br>0.00 : ' it'<br>0.00 : ' he'<br>0.00 : ' she'<br>0.00 : 'ever'<br>0.00 : ' no'<br>0.00 : ' Lily'<br></span></span><span class='token' style='color: #f0814d'>&nbsp;share<span class='tooltip'>Token: ' share'<br>Causal Loss: 6.38228<br>---------------<br>0.09 : ','<br>0.07 : '.'<br>0.06 : ' want'<br>0.06 : ' can'<br>0.05 : ' have'<br>0.04 : '.\"'<br>0.03 : ' are'<br>0.03 : ' like'<br>0.02 : ' for'<br>0.02 : ' to'<br></span></span><span class='token' style='color: #8506a6'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 2.48745<br>---------------<br>0.17 : '.'<br>0.14 : ' the'<br>0.13 : ' and'<br>0.08 : ' it'<br>0.06 : ' their'<br>0.06 : ' with'<br>0.05 : ' her'<br>0.05 : ' his'<br>0.03 : ' them'<br>0.02 : ' your'<br></span></span><span class='token' style='color: #be3883'>&nbsp;with<span class='tooltip'>Token: ' with'<br>Causal Loss: 4.14895<br>---------------<br>0.17 : '.'<br>0.17 : ' was'<br>0.04 : ' to'<br>0.04 : ','<br>0.03 : ' and'<br>0.03 : ' is'<br>0.03 : ''s'<br>0.02 : ' up'<br>0.02 : ' in'<br>0.02 : ' on'<br></span></span><span class='token' style='color: #b62f8b'>&nbsp;me<span class='tooltip'>Token: ' me'<br>Causal Loss: 3.87701<br>---------------<br>0.16 : ' her'<br>0.13 : ' the'<br>0.11 : ' his'<br>0.10 : ' a'<br>0.06 : ' their'<br>0.05 : ' it'<br>0.02 : ' them'<br>0.02 : ' me'<br>0.02 : ' you'<br>0.02 : ' him'<br></span></span><span class='token' style='color: #bb3586'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 4.03819<br>---------------<br>0.09 : '.'<br>0.07 : '?\"'<br>0.06 : '.\"'<br>0.05 : 'et'<br>0.05 : 'adow'<br>0.05 : 'ant'<br>0.04 : ','<br>0.03 : '!\"'<br>0.03 : 'al'<br>0.03 : 'm'<br></span></span><span class='token' style='color: #fdc427'>&nbsp;se<span class='tooltip'>Token: ' se'<br>Causal Loss: 8.04631<br>---------------<br>0.06 : ' said'<br>0.04 : ' the'<br>0.03 : ' Ben'<br>0.02 : ' he'<br>0.02 : ' saw'<br>0.02 : ' Lily'<br>0.02 : ' a'<br>0.01 : ' they'<br>0.01 : ' it'<br>0.01 : ' she'<br></span></span><span class='token' style='color: #9b179e'>w<span class='tooltip'>Token: 'w'<br>Causal Loss: 3.06890<br>---------------<br>0.21 : 'at'<br>0.20 : 'ed'<br>0.16 : 'll'<br>0.12 : 'p'<br>0.08 : 'al'<br>0.06 : 'as'<br>0.05 : 'w'<br>0.02 : 'ag'<br>0.02 : 'ven'<br>0.01 : 'ver'<br></span></span><span class='token' style='color: #fad524'>&nbsp;my<span class='tooltip'>Token: ' my'<br>Causal Loss: 8.42463<br>---------------<br>0.11 : 'eet'<br>0.07 : 'el'<br>0.06 : 'e'<br>0.05 : 'or'<br>0.05 : '.'<br>0.05 : 'led'<br>0.05 : 'ich'<br>0.05 : 's'<br>0.03 : 'l'<br>0.03 : 'ards'<br></span></span><span class='token' style='color: #df6262'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 5.46619<br>---------------<br>0.06 : 'ster'<br>0.06 : ' friend'<br>0.02 : ' car'<br>0.02 : ' favorite'<br>0.02 : ' friends'<br>0.02 : ' lo'<br>0.02 : ' best'<br>0.02 : ' toy'<br>0.02 : ' toys'<br>0.01 : ' doll'<br></span></span><span class='token' style='color: #9612a0'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 2.91687<br>---------------<br>0.11 : 'in'<br>0.08 : 'ar'<br>0.07 : 'r'<br>0.07 : 'ip'<br>0.07 : 'aring'<br>0.05 : 'ir'<br>0.05 : 'y'<br>0.04 : 'ore'<br>0.04 : 'one'<br>0.03 : 'ake'<br></span></span><span class='token' style='color: #7c02a7'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 2.30182<br>---------------<br>0.10 : 't'<br>0.10 : ' mom'<br>0.09 : 'r'<br>0.09 : 'p'<br>0.07 : 'c'<br>0.06 : 'd'<br>0.06 : 'a'<br>0.05 : 'cle'<br>0.04 : 'red'<br>0.03 : 'on'<br></span></span><span class='token' style='color: #e76e5a'>?\"<span class='tooltip'>Token: '?\"'<br>Causal Loss: 5.82228<br>---------------<br>0.10 : '.'<br>0.05 : 'en'<br>0.05 : 'o'<br>0.04 : ' and'<br>0.04 : 'urn'<br>0.04 : ' of'<br>0.04 : 'op'<br>0.03 : ' the'<br>0.03 : 'e'<br>0.02 : 'i'<br></span></span><span class='token' style='color: #b42d8d'>&nbsp;Her<span class='tooltip'>Token: ' Her'<br>Causal Loss: 3.78251<br>---------------<br>0.43 : '\\n'<br>0.07 : ' Lily'<br>0.04 : ' \n",
       "'<br>0.04 : ' she'<br>0.03 : ' Ben'<br>0.03 : ' Tom'<br>0.03 : ' The'<br>0.03 : ' asked'<br>0.02 : ' he'<br>0.02 : ' Her'<br></span></span><span class='token' style='color: #360498'>&nbsp;mom<span class='tooltip'>Token: ' mom'<br>Causal Loss: 0.71182<br>---------------<br>0.49 : ' mom'<br>0.15 : ' mommy'<br>0.03 : ' friend'<br>0.03 : ' name'<br>0.03 : ' dad'<br>0.03 : ' friends'<br>0.03 : ' mum'<br>0.02 : ' parents'<br>0.01 : ' mother'<br>0.01 : ' brother'<br></span></span><span class='token' style='color: #a51f97'>&nbsp;smiled<span class='tooltip'>Token: ' smiled'<br>Causal Loss: 3.34480<br>---------------<br>0.15 : '.'<br>0.12 : ' said'<br>0.11 : ' and'<br>0.05 : ','<br>0.04 : ' was'<br>0.04 : ' smiled'<br>0.02 : ''s'<br>0.02 : ' came'<br>0.02 : ' told'<br>0.02 : ' asked'<br></span></span><span class='token' style='color: #2b0594'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 0.48180<br>---------------<br>0.62 : ' and'<br>0.22 : '.'<br>0.04 : ','<br>0.04 : ' at'<br>0.02 : ' back'<br>0.01 : ' as'<br>0.01 : ' with'<br>0.01 : ' when'<br>0.00 : ' too'<br>0.00 : ' in'<br></span></span><span class='token' style='color: #9310a1'>&nbsp;said<span class='tooltip'>Token: ' said'<br>Causal Loss: 2.86673<br>---------------<br>0.06 : ' said'<br>0.04 : ' the'<br>0.03 : ' Ben'<br>0.02 : ' he'<br>0.02 : ' saw'<br>0.02 : ' Lily'<br>0.02 : ' a'<br>0.01 : ' they'<br>0.01 : ' it'<br>0.01 : ' she'<br></span></span><span class='token' style='color: #3a049a'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.76656<br>---------------<br>0.46 : ','<br>0.19 : '.'<br>0.03 : ' he'<br>0.03 : ' they'<br>0.03 : ' \"'<br>0.03 : ' it'<br>0.03 : ' goodbye'<br>0.02 : ' to'<br>0.02 : ' she'<br>0.02 : ' yes'<br></span></span><span class='token' style='color: #6c00a8'>&nbsp;\"<span class='tooltip'>Token: ' \"'<br>Causal Loss: 1.88892<br>---------------<br>0.15 : ' \"'<br>0.07 : ' but'<br>0.06 : ' and'<br>0.06 : ' there'<br>0.05 : ' Lily'<br>0.04 : ' she'<br>0.04 : ' the'<br>0.03 : ' he'<br>0.03 : ' a'<br>0.03 : ' they'<br></span></span><span class='token' style='color: #99149f'>Yes<span class='tooltip'>Token: 'Yes'<br>Causal Loss: 3.00601<br>---------------<br>0.14 : 'I'<br>0.06 : 'It'<br>0.05 : 'Yes'<br>0.04 : 'You'<br>0.04 : 'Thank'<br>0.04 : 'We'<br>0.04 : 'Let'<br>0.03 : 'What'<br>0.03 : 'Don'<br>0.03 : 'No'<br></span></span><span class='token' style='color: #18068b'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.15789<br>---------------<br>0.85 : ','<br>0.05 : '!'<br>0.03 : '!\"'<br>0.01 : ' please'<br>0.01 : ',\"'<br>0.01 : '.'<br>0.00 : 'ter'<br>0.00 : ' they'<br>0.00 : '.\"'<br>0.00 : ' and'<br></span></span><span class='token' style='color: #9d189d'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 3.09441<br>---------------<br>0.15 : ' \"'<br>0.07 : ' but'<br>0.06 : ' and'<br>0.06 : ' there'<br>0.05 : ' Lily'<br>0.04 : ' she'<br>0.04 : ' the'<br>0.03 : ' he'<br>0.03 : ' a'<br>0.03 : ' they'<br></span></span><span class='token' style='color: #ac2593'>,<span class='tooltip'>Token: ','<br>Causal Loss: 3.55654<br>---------------<br>0.19 : '.'<br>0.11 : ' and'<br>0.08 : ' was'<br>0.07 : ''s'<br>0.06 : ' said'<br>0.03 : ','<br>0.03 : ' felt'<br>0.02 : ' were'<br>0.02 : ' saw'<br>0.02 : ' went'<br></span></span><span class='token' style='color: #d6556d'>&nbsp;we<span class='tooltip'>Token: ' we'<br>Causal Loss: 5.05633<br>---------------<br>0.15 : ' \"'<br>0.07 : ' but'<br>0.06 : ' and'<br>0.06 : ' there'<br>0.05 : ' Lily'<br>0.04 : ' she'<br>0.04 : ' the'<br>0.03 : ' he'<br>0.03 : ' a'<br>0.03 : ' they'<br></span></span><span class='token' style='color: #6400a7'>&nbsp;can<span class='tooltip'>Token: ' can'<br>Causal Loss: 1.71544<br>---------------<br>0.18 : ' can'<br>0.07 : ' have'<br>0.05 : 'aring'<br>0.05 : 'ek'<br>0.04 : ' are'<br>0.03 : ' go'<br>0.03 : ' will'<br>0.03 : ' need'<br>0.02 : 'al'<br>0.02 : ' do'<br></span></span><span class='token' style='color: #c94579'>&nbsp;share<span class='tooltip'>Token: ' share'<br>Causal Loss: 4.53580<br>---------------<br>0.13 : ''t'<br>0.06 : ' make'<br>0.05 : ' have'<br>0.05 : ' play'<br>0.04 : ' be'<br>0.04 : ' I'<br>0.03 : 'n'<br>0.03 : ' help'<br>0.03 : ' do'<br>0.02 : ' we'<br></span></span><span class='token' style='color: #6f00a8'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 1.94397<br>---------------<br>0.17 : '.'<br>0.14 : ' the'<br>0.13 : ' and'<br>0.08 : ' it'<br>0.06 : ' their'<br>0.06 : ' with'<br>0.05 : ' her'<br>0.05 : ' his'<br>0.03 : ' them'<br>0.02 : ' your'<br></span></span><span class='token' style='color: #fdb22f'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 7.62155<br>---------------<br>0.04 : ' park'<br>0.02 : ' bird'<br>0.01 : ' sky'<br>0.01 : ' c'<br>0.01 : ' p'<br>0.01 : ' little'<br>0.01 : ' water'<br>0.01 : ' m'<br>0.01 : ' sun'<br>0.01 : ' t'<br></span></span><span class='token' style='color: #8908a5'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 2.61453<br>---------------<br>0.42 : ' to'<br>0.09 : 's'<br>0.07 : 'le'<br>0.06 : ' a'<br>0.05 : ' it'<br>0.04 : '.'<br>0.03 : ' the'<br>0.02 : ' help'<br>0.02 : ' some'<br>0.01 : ' something'<br></span></span><span class='token' style='color: #8807a5'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 2.56491<br>---------------<br>0.17 : '.'<br>0.08 : ' and'<br>0.06 : 't'<br>0.05 : ' of'<br>0.05 : 'x'<br>0.04 : 'br'<br>0.03 : 'ased'<br>0.03 : ','<br>0.03 : ' was'<br>0.02 : ' with'<br></span></span><span class='token' style='color: #fdb82c'>&nbsp;fix<span class='tooltip'>Token: ' fix'<br>Causal Loss: 7.76854<br>---------------<br>0.06 : ' said'<br>0.04 : ' the'<br>0.03 : ' Ben'<br>0.02 : ' he'<br>0.02 : ' saw'<br>0.02 : ' Lily'<br>0.02 : ' a'<br>0.01 : ' they'<br>0.01 : ' it'<br>0.01 : ' she'<br></span></span><span class='token' style='color: #b42d8d'>&nbsp;your<span class='tooltip'>Token: ' your'<br>Causal Loss: 3.80254<br>---------------<br>0.32 : 'ed'<br>0.22 : ' it'<br>0.20 : ' the'<br>0.06 : 'ing'<br>0.02 : ' things'<br>0.02 : ' your'<br>0.02 : ' them'<br>0.02 : ' her'<br>0.01 : ' his'<br>0.01 : ' my'<br></span></span><span class='token' style='color: #ec7754'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 6.09862<br>---------------<br>0.05 : 'self'<br>0.04 : 's'<br>0.03 : ' toys'<br>0.03 : ' friends'<br>0.03 : ' room'<br>0.03 : ' name'<br>0.03 : ' friend'<br>0.02 : ' own'<br>0.02 : ' car'<br>0.01 : ' mom'<br></span></span><span class='token' style='color: #9612a0'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 2.91687<br>---------------<br>0.11 : 'in'<br>0.08 : 'ar'<br>0.07 : 'r'<br>0.07 : 'ip'<br>0.07 : 'aring'<br>0.05 : 'ir'<br>0.05 : 'y'<br>0.04 : 'ore'<br>0.04 : 'one'<br>0.03 : 'ake'<br></span></span><span class='token' style='color: #7c02a7'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 2.30182<br>---------------<br>0.10 : 't'<br>0.10 : ' mom'<br>0.09 : 'r'<br>0.09 : 'p'<br>0.07 : 'c'<br>0.06 : 'd'<br>0.06 : 'a'<br>0.05 : 'cle'<br>0.04 : 'red'<br>0.03 : 'on'<br></span></span><span class='token' style='color: #dc5e66'>.\"<span class='tooltip'>Token: '.\"'<br>Causal Loss: 5.33617<br>---------------<br>0.10 : '.'<br>0.05 : 'en'<br>0.05 : 'o'<br>0.04 : ' and'<br>0.04 : 'urn'<br>0.04 : ' of'<br>0.04 : 'op'<br>0.03 : ' the'<br>0.03 : 'e'<br>0.02 : 'i'<br></span></span><br><br><span class='token' style='color: #ef7d4f'>T<span class='tooltip'>Token: 'T'<br>Causal Loss: 6.25211<br>---------------<br>0.45 : '\\n'<br>0.10 : '&quot;'<br>0.07 : 'The'<br>0.04 : 'Lily'<br>0.03 : 'They'<br>0.03 : 'One'<br>0.02 : 'But'<br>0.02 : 'When'<br>0.02 : 'He'<br>0.01 : 'Ben'<br></span></span><span class='token' style='color: #40039c'>o<span class='tooltip'>Token: 'o'<br>Causal Loss: 0.93291<br>---------------<br>0.39 : 'o'<br>0.12 : 'w'<br>0.07 : 'eddy'<br>0.05 : 'ime'<br>0.04 : 'in'<br>0.03 : 'r'<br>0.02 : 'on'<br>0.02 : 'oo'<br>0.02 : 'ake'<br>0.02 : 'illy'<br></span></span><span class='token' style='color: #d14e72'>gether<span class='tooltip'>Token: 'gether'<br>Causal Loss: 4.84719<br>---------------<br>0.07 : '.'<br>0.05 : 'se'<br>0.04 : 'nd'<br>0.04 : 'g'<br>0.04 : 've'<br>0.04 : 'st'<br>0.03 : 'ld'<br>0.03 : 'il'<br>0.03 : 'in'<br>0.02 : 'c'<br></span></span><span class='token' style='color: #270592'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.41121<br>---------------<br>0.66 : ','<br>0.23 : ' they'<br>0.02 : '.'<br>0.02 : ' he'<br>0.01 : ' she'<br>0.01 : ' the'<br>0.00 : ' you'<br>0.00 : '!'<br>0.00 : ' on'<br>0.00 : ' in'<br></span></span><span class='token' style='color: #ae2791'>&nbsp;they<span class='tooltip'>Token: ' they'<br>Causal Loss: 3.62670<br>---------------<br>0.15 : ' \"'<br>0.07 : ' but'<br>0.06 : ' and'<br>0.06 : ' there'<br>0.05 : ' Lily'<br>0.04 : ' she'<br>0.04 : ' the'<br>0.03 : ' he'<br>0.03 : ' a'<br>0.03 : ' they'<br></span></span><span class='token' style='color: #ef7d4f'>&nbsp;shared<span class='tooltip'>Token: ' shared'<br>Causal Loss: 6.26320<br>---------------<br>0.12 : ' were'<br>0.07 : ' had'<br>0.06 : ' could'<br>0.05 : ' saw'<br>0.03 : ' would'<br>0.03 : ' went'<br>0.03 : ' did'<br>0.03 : ' all'<br>0.02 : ' both'<br>0.02 : ' got'<br></span></span><span class='token' style='color: #5701a4'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 1.42784<br>---------------<br>0.24 : ' the'<br>0.15 : ' their'<br>0.10 : ' it'<br>0.09 : ' her'<br>0.08 : ' his'<br>0.05 : ' a'<br>0.04 : '.'<br>0.02 : ' some'<br>0.02 : ' and'<br>0.02 : ' them'<br></span></span><span class='token' style='color: #fdb22f'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 7.62155<br>---------------<br>0.04 : ' park'<br>0.02 : ' bird'<br>0.01 : ' sky'<br>0.01 : ' c'<br>0.01 : ' p'<br>0.01 : ' little'<br>0.01 : ' water'<br>0.01 : ' m'<br>0.01 : ' sun'<br>0.01 : ' t'<br></span></span><span class='token' style='color: #8908a5'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 2.61453<br>---------------<br>0.42 : ' to'<br>0.09 : 's'<br>0.07 : 'le'<br>0.06 : ' a'<br>0.05 : ' it'<br>0.04 : '.'<br>0.03 : ' the'<br>0.02 : ' help'<br>0.02 : ' some'<br>0.01 : ' something'<br></span></span><span class='token' style='color: #8807a5'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 2.56491<br>---------------<br>0.17 : '.'<br>0.08 : ' and'<br>0.06 : 't'<br>0.05 : ' of'<br>0.05 : 'x'<br>0.04 : 'br'<br>0.03 : 'ased'<br>0.03 : ','<br>0.03 : ' was'<br>0.02 : ' with'<br></span></span><span class='token' style='color: #fdc427'>&nbsp;se<span class='tooltip'>Token: ' se'<br>Causal Loss: 8.04631<br>---------------<br>0.06 : ' said'<br>0.04 : ' the'<br>0.03 : ' Ben'<br>0.02 : ' he'<br>0.02 : ' saw'<br>0.02 : ' Lily'<br>0.02 : ' a'<br>0.01 : ' they'<br>0.01 : ' it'<br>0.01 : ' she'<br></span></span><span class='token' style='color: #9b179e'>w<span class='tooltip'>Token: 'w'<br>Causal Loss: 3.06890<br>---------------<br>0.21 : 'at'<br>0.20 : 'ed'<br>0.16 : 'll'<br>0.12 : 'p'<br>0.08 : 'al'<br>0.06 : 'as'<br>0.05 : 'w'<br>0.02 : 'ag'<br>0.02 : 'ven'<br>0.01 : 'ver'<br></span></span><span class='token' style='color: #b83289'>ed<span class='tooltip'>Token: 'ed'<br>Causal Loss: 3.92368<br>---------------<br>0.11 : 'eet'<br>0.07 : 'el'<br>0.06 : 'e'<br>0.05 : 'or'<br>0.05 : '.'<br>0.05 : 'led'<br>0.05 : 'ich'<br>0.05 : 's'<br>0.03 : 'l'<br>0.03 : 'ards'<br></span></span><span class='token' style='color: #7f03a7'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 2.33936<br>---------------<br>0.10 : '.'<br>0.10 : ' the'<br>0.06 : ' and'<br>0.05 : ' to'<br>0.04 : ' it'<br>0.03 : ' her'<br>0.03 : ','<br>0.02 : ' in'<br>0.02 : ' up'<br>0.02 : ' them'<br></span></span><span class='token' style='color: #f48947'>&nbsp;butt<span class='tooltip'>Token: ' butt'<br>Causal Loss: 6.59275<br>---------------<br>0.04 : ' park'<br>0.02 : ' bird'<br>0.01 : ' sky'<br>0.01 : ' c'<br>0.01 : ' p'<br>0.01 : ' little'<br>0.01 : ' water'<br>0.01 : ' m'<br>0.01 : ' sun'<br>0.01 : ' t'<br></span></span><span class='token' style='color: #230590'>on<span class='tooltip'>Token: 'on'<br>Causal Loss: 0.33759<br>---------------<br>0.71 : 'on'<br>0.27 : 'ons'<br>0.00 : 'y'<br>0.00 : 'ered'<br>0.00 : 'et'<br>0.00 : 'w'<br>0.00 : 'ars'<br>0.00 : 'm'<br>0.00 : 'ch'<br>0.00 : 'ot'<br></span></span><span class='token' style='color: #c43f7e'>&nbsp;on<span class='tooltip'>Token: ' on'<br>Causal Loss: 4.36386<br>---------------<br>0.15 : '.'<br>0.09 : 't'<br>0.08 : 'y'<br>0.07 : ','<br>0.06 : ' and'<br>0.06 : 'es'<br>0.05 : 'est'<br>0.04 : ''t'<br>0.03 : ' was'<br>0.02 : ' in'<br></span></span><span class='token' style='color: #ed7b51'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 6.20053<br>---------------<br>0.37 : ' the'<br>0.14 : ','<br>0.08 : ' a'<br>0.07 : ' her'<br>0.05 : ' it'<br>0.05 : ' his'<br>0.03 : ' their'<br>0.02 : '.'<br>0.02 : 't'<br>0.02 : ' an'<br></span></span><span class='token' style='color: #8b09a4'>'s<span class='tooltip'>Token: ''s'<br>Causal Loss: 2.63301<br>---------------<br>0.19 : '.'<br>0.11 : ' and'<br>0.08 : ' was'<br>0.07 : ''s'<br>0.06 : ' said'<br>0.03 : ','<br>0.03 : ' felt'<br>0.02 : ' were'<br>0.02 : ' saw'<br>0.02 : ' went'<br></span></span><span class='token' style='color: #f48947'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 6.59447<br>---------------<br>0.12 : ' mom'<br>0.06 : ' go'<br>0.04 : ' a'<br>0.04 : ' okay'<br>0.02 : ' not'<br>0.02 : ' friend'<br>0.02 : ' mommy'<br>0.02 : ' hand'<br>0.01 : ' house'<br>0.01 : ' play'<br></span></span><span class='token' style='color: #9612a0'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 2.91687<br>---------------<br>0.11 : 'in'<br>0.08 : 'ar'<br>0.07 : 'r'<br>0.07 : 'ip'<br>0.07 : 'aring'<br>0.05 : 'ir'<br>0.05 : 'y'<br>0.04 : 'ore'<br>0.04 : 'one'<br>0.03 : 'ake'<br></span></span><span class='token' style='color: #7c02a7'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 2.30182<br>---------------<br>0.10 : 't'<br>0.10 : ' mom'<br>0.09 : 'r'<br>0.09 : 'p'<br>0.07 : 'c'<br>0.06 : 'd'<br>0.06 : 'a'<br>0.05 : 'cle'<br>0.04 : 'red'<br>0.03 : 'on'<br></span></span><span class='token' style='color: #7e03a7'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 2.32582<br>---------------<br>0.10 : '.'<br>0.05 : 'en'<br>0.05 : 'o'<br>0.04 : ' and'<br>0.04 : 'urn'<br>0.04 : ' of'<br>0.04 : 'op'<br>0.03 : ' the'<br>0.03 : 'e'<br>0.02 : 'i'<br></span></span><span class='token' style='color: #99149f'>&nbsp;It<span class='tooltip'>Token: ' It'<br>Causal Loss: 3.00940<br>---------------<br>0.17 : '\\n'<br>0.15 : ' She'<br>0.13 : ' They'<br>0.12 : ' He'<br>0.05 : ' It'<br>0.04 : ' The'<br>0.04 : ' \n",
       "'<br>0.02 : ' \"'<br>0.02 : ' One'<br>0.02 : ' I'<br></span></span><span class='token' style='color: #40039c'>&nbsp;was<span class='tooltip'>Token: ' was'<br>Causal Loss: 0.92377<br>---------------<br>0.40 : ' was'<br>0.14 : ' is'<br>0.06 : ''s'<br>0.04 : ' had'<br>0.02 : ' has'<br>0.02 : ' looked'<br>0.02 : ' made'<br>0.01 : ' will'<br>0.01 : ' can'<br>0.01 : ' said'<br></span></span><span class='token' style='color: #b7308a'>&nbsp;not<span class='tooltip'>Token: ' not'<br>Causal Loss: 3.91003<br>---------------<br>0.20 : ' a'<br>0.11 : ' so'<br>0.08 : ' very'<br>0.03 : ' happy'<br>0.02 : ' not'<br>0.02 : ' sad'<br>0.02 : ' too'<br>0.02 : ' scared'<br>0.01 : ' the'<br>0.01 : ' time'<br></span></span><span class='token' style='color: #eff821'>&nbsp;dif<span class='tooltip'>Token: ' dif'<br>Causal Loss: 9.19504<br>---------------<br>0.09 : ' want'<br>0.06 : ' know'<br>0.06 : ' like'<br>0.03 : ' a'<br>0.03 : ' listen'<br>0.03 : ' to'<br>0.03 : ' see'<br>0.03 : ' happy'<br>0.02 : 'hing'<br>0.02 : ' nice'<br></span></span><span class='token' style='color: #0c0786'>f<span class='tooltip'>Token: 'f'<br>Causal Loss: 0.00412<br>---------------<br>1.00 : 'f'<br>0.00 : 'ers'<br>0.00 : 'g'<br>0.00 : 'p'<br>0.00 : 'er'<br>0.00 : 'w'<br>0.00 : 'or'<br>0.00 : ' love'<br>0.00 : 'ir'<br>0.00 : 's'<br></span></span><span class='token' style='color: #9f1a9b'>ic<span class='tooltip'>Token: 'ic'<br>Causal Loss: 3.19499<br>---------------<br>0.10 : 'ish'<br>0.09 : '.'<br>0.07 : 'ra'<br>0.06 : ' course'<br>0.04 : ' you'<br>0.04 : 'ic'<br>0.04 : 'l'<br>0.03 : 'ely'<br>0.03 : 'ast'<br>0.03 : 'a'<br></span></span><span class='token' style='color: #7f03a7'>ul<span class='tooltip'>Token: 'ul'<br>Causal Loss: 2.34326<br>---------------<br>0.11 : '.'<br>0.10 : 'ul'<br>0.08 : 'ine'<br>0.06 : 't'<br>0.05 : 'hes'<br>0.04 : 'ro'<br>0.04 : ' and'<br>0.04 : 'ing'<br>0.04 : 'es'<br>0.03 : 'er'<br></span></span><span class='token' style='color: #4c02a1'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 1.17391<br>---------------<br>0.31 : 't'<br>0.28 : 'ar'<br>0.09 : 'ance'<br>0.07 : 'ie'<br>0.04 : 'u'<br>0.03 : 'ia'<br>0.02 : 'ts'<br>0.01 : 'a'<br>0.01 : 'i'<br>0.01 : 'ated'<br></span></span><span class='token' style='color: #c6417c'>&nbsp;for<span class='tooltip'>Token: ' for'<br>Causal Loss: 4.44638<br>---------------<br>0.10 : '.'<br>0.05 : 'en'<br>0.05 : 'o'<br>0.04 : ' and'<br>0.04 : 'urn'<br>0.04 : ' of'<br>0.04 : 'op'<br>0.03 : ' the'<br>0.03 : 'e'<br>0.02 : 'i'<br></span></span><span class='token' style='color: #b12b8f'>&nbsp;them<span class='tooltip'>Token: ' them'<br>Causal Loss: 3.70709<br>---------------<br>0.17 : ' a'<br>0.12 : ' the'<br>0.06 : ' her'<br>0.05 : ' you'<br>0.03 : ' being'<br>0.03 : ' help'<br>0.03 : ' his'<br>0.03 : ' him'<br>0.02 : ' them'<br>0.02 : ' helping'<br></span></span><span class='token' style='color: #fcaa33'>&nbsp;b<span class='tooltip'>Token: ' b'<br>Causal Loss: 7.43725<br>---------------<br>0.31 : '.'<br>0.09 : ' and'<br>0.07 : ' to'<br>0.04 : ' in'<br>0.03 : ','<br>0.02 : ' with'<br>0.02 : ' a'<br>0.02 : ' all'<br>0.02 : ' on'<br>0.02 : ' back'<br></span></span><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_predictions(model, tokenizer, device=\"cuda\", text=[sample_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db6ded-ea4d-4f29-9832-9b357bd3da39",
   "metadata": {},
   "source": [
    "### Simple Text Gen\n",
    "This is a very simple text generator implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752d4264-1cde-4e00-811e-0fcb3ce365d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, model, tokenizer, device, temperature=1.0, do_sample=False, seed=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.do_sample = do_sample\n",
    "        self.rand_generator = torch.Generator(device=device)\n",
    "        self.set_seed(seed)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens=20):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.model(input_ids=input_ids)\n",
    "            logits = logits[:, -1, :] / self.temperature\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            if self.do_sample:\n",
    "                next_token_id = torch.multinomial(\n",
    "                    probabilities, num_samples=1, generator=self.rand_generator)\n",
    "            else:\n",
    "                _, next_token_id = torch.topk(probabilities, k=1, dim=-1)\n",
    "            input_ids = torch.cat((input_ids, next_token_id), dim=1)\n",
    "        return input_ids\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        if seed is None:\n",
    "            self.rand_generator.seed()\n",
    "        else:\n",
    "            self.rand_generator.manual_seed(seed)\n",
    "    \n",
    "    # Lazy generation pipeline for simple inference.\n",
    "    def prompt(self, input_text, max_new_tokens=20):\n",
    "        input_ids = self.tokenizer(input_text, return_tensors='pt')['input_ids']\n",
    "        model_output = self.generate(\n",
    "            input_ids.to(self.device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "        return self.tokenizer.decode(model_output.to('cpu')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e791db3-b315-46f9-900a-3cc770b3717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<|BOS|> One day, a little girl named Timmy was very sad. She put her for a ship, hiam, pointing at home little bird was playing in the park and he didn't want tooth, Lily gave Rosaf It was a cold and had many\"\n"
     ]
    }
   ],
   "source": [
    "# Test text generation.\n",
    "# Don't expect too much from this model, as the only input to each prediction is the previous word. \n",
    "text_gen = TextGenerator(model, tokenizer, 'cuda', do_sample=True, seed=42)\n",
    "text = text_gen.prompt(\"One day, a little girl\", max_new_tokens=50)\n",
    "print(repr(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81673434-b429-4afe-bef0-40c65bf07d16",
   "metadata": {},
   "source": [
    "## Create a Huggingface causal model\n",
    "Many of the Huggingface API's, text-generation for example, requires that the model conforms to the Huggingface model API.\n",
    "\n",
    "Here, we will take the model from the last exercise and \"Huggify\" it, while addind a new \"Feedforward\" layer to improve performance.\n",
    "\n",
    "The Feedforward layer (aka Multilayer Perceptron (MLP)) acts like a key-value store for the model, increasing its capacity and capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba3feb-0f8c-43db-8983-48c68fb509c3",
   "metadata": {},
   "source": [
    "### Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd4100eb-c86d-4f03-a565-afee285a6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.init as init\n",
    "from torch.nn import functional as F\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "# We will abstract out the causal loss function for reuse.\n",
    "def causal_loss(logits, labels):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        # labels with this value are ignored when computing loss\n",
    "        ignore_index=-100,\n",
    "        reduction='mean',\n",
    "    )\n",
    "    \n",
    "    return loss.nan_to_num()\n",
    "\n",
    "# A MLP consists of two or more linear layers, each seperated by\n",
    "# a non-linear activation function. Here, we use ReLU, which changes values\n",
    "# less than zero to zero and passes values greater than zero unchanged.\n",
    "#\n",
    "# This is a 2-layer MLP, common to Transformer models. The rows in the first layer activate\n",
    "# the corresponding columns in the second layer, where the input is matched by dot-product\n",
    "# similarity to the input -- this prduces a normalized value between -1 and 1, with one being\n",
    "# a perfect match and -1 being an exact opposite. This value is then offset by the corresponding bias\n",
    "# parameter, with the ReLU layer blocking all inputs less-than or equal to zero.\n",
    "#\n",
    "# In the case where the resulting value is non-zero, the corresponding column is added to the output, in proportion\n",
    "# to the magnitude of the signal.\n",
    "class FeedforwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_feedforward):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feedforward = d_feedforward\n",
    "\n",
    "        self.linear1 = nn.Linear(self.d_model, self.d_feedforward)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.d_feedforward, self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Huggingface model type string\n",
    "# This is a unique identifier for a model type, which allows the API to find the\n",
    "# implementation for the type.\n",
    "model_type = \"simple-causal2\"\n",
    "\n",
    "# Huggingface config class.\n",
    "#\n",
    "# Huggingface 'PreTrainedModel' objects are passed a derivative of this class\n",
    "# when constructed. This is required, if your model will derive from PreTrainedModel.\n",
    "class CausalLM2Config(PretrainedConfig):\n",
    "    model_type = model_type\n",
    "    \n",
    "    def __init__(\n",
    "        # All of these MUST have defaults, even if unused.\n",
    "        self,\n",
    "        vocab_size=8000,\n",
    "        hidden_size=256,\n",
    "        max_sequence_length=2048,\n",
    "        dim_feedforward = 512,\n",
    "        \n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "# The formward method of this model is designed to be compatible with the HuggingFace Trainer and Tokenizer classes.\n",
    "# This is essentially a wrapper for a Pytorch transformer model, which implements the HF API.\n",
    "class CausalLM2(PreTrainedModel):\n",
    "    config_class = CausalLM2Config\n",
    "    model_type = 'Transformer'\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.d_model = config.hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.feedforward = FeedforwardNet(self.d_model, config.dim_feedforward)\n",
    "        self.output_projection = nn.Linear(self.d_model, self.vocab_size)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,\n",
    "    ) -> (Tensor, dict[str, Tensor]):\n",
    "\n",
    "        # Convert input_ids to embeddings.\n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        # Pass the input through the feedforward network.\n",
    "        x = self.feedforward(x)\n",
    "        \n",
    "        # Convert embeddings to log-probabilities of next token-id\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        # Compute loss.\n",
    "        if labels is not None:\n",
    "            loss = causal_loss(logits, labels)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        if return_dict:\n",
    "            return CausalLMOutput(loss=loss, logits=logits)\n",
    "        elif loss is not None:\n",
    "            return (loss, logits)\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    # This is needed for the Huggingface text generation APIs.\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        model_inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "        return model_inputs\n",
    "\n",
    "AutoConfig.register(model_type, CausalLM2Config)\n",
    "AutoModelForCausalLM.register(CausalLM2Config, CausalLM2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1679c8d-696f-4c49-987c-68b42b460628",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30458c94-f72e-4916-8b2c-83e54de072a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1.0M parameters\n",
      "CausalLM2(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (feedforward): FeedforwardNet(\n",
      "    (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  )\n",
      "  (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create model configuration\n",
    "config = CausalLM2Config(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    hidden_size = 128,\n",
    "    max_sequence_length = 2048,\n",
    "    dim_feedforward = 2048,\n",
    ")\n",
    "\n",
    "# A config can also be instantiated from a json file like this:\n",
    "#config = AutoConfig.from_pretrained(\"path-to-config\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "print_model_size(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7b901-008c-41ee-9a23-08f9608c5ac8",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb6d033-05cf-4b1a-a0ab-9005c341a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3313 steps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a685293d6374cfc9c3d59c0b7bad95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca4015afcc0430099d7d0f3e6bdb4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=3.6502459711498685\n",
      "Global step: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806e2a9aa2c341ba8e2f955a5ff9abec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=3.6118180884255304\n",
      "Global step: 2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe8407d60f74eb4ae65f7bf7f96a0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=3.604002899593777\n",
      "Global step: 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd238cbe2db4b47b87e1625fc721c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=3.6064381731881037\n"
     ]
    }
   ],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0815e-0f1b-424f-b352-24643e94ef0e",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "This is a simple wrapper for the Huggingface tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94d72484-6eb0-495e-96d2-81aab3cfb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/v4.34.1/en/generation_strategies\n",
    "\n",
    "class TextGen():\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def generate(self, prompt, do_sample=True, top_k=50, top_p=0.9, max_new_tokens=500):\n",
    "        self.model.to(self.device)\n",
    "        input_ids = self.tokenizer(prompt, return_tensors='pt')['input_ids'].to(self.device)\n",
    "        outputs = model.generate(input_ids, do_sample=do_sample, top_k=top_k, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14751a31-949d-4ed1-9532-d030bc431f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them balouses of the tree to get some cookies and the villie loved playing a bigging the ped to his friend and had a time, \"I'm not your body to look for Lily asked her. You have to their mom and his head. She said \"Arella.\n",
      "Lily learned to play with excite the mad and said. She looked at the dog was a few of the big box with the stretches of the radder and went home.\n",
      "When they saw her back!\" Lily felt guilence. He took her to play with the day, but her head and a very good,\" said, Ben. It's a big red apple. He looked forward, the little boy had been able to play with the stuffedge was a long as fast as she could find his family decided to a small girl named Lily smiled. We will not nice. It was so happy to come with lots of fun. It was a good at her friends. They wanted to make a biggly, the fle! He wanted to take a while, it. Tim, there was a great time, he found a time. She felt a girl who loved to stay and she couldn't want to his friend.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"Well. The man with their dad smiled back inside. Shood job and a nice!\" Ben and tried to play with it on the sunf. He started to the big book?\" asked. They had a happy that it had a few day, Lily. Lily and make sure what a big, they saw a little boy named Tom and ran, but he would be happy. She started to the kitchen. They are you knows a big hug each other animals in her back home. \n",
      "\n",
      "The next to eat some rocks. You can do. They looked at her mom heard a good idea. The cat would beet was very happy that the day, but he found a time, Tom says \"You should share the gathered the box. One day, \"This is fun at the ball, he saw something shiny cozy boy who was very careful with his friends to play with her friends. It wanted to make her new boy named Pio. The boy said. Then, and the little boy was very bad!\"\n",
      "The boy named Lily smiled and said, but it a bit more. It is very happy. I play with them\n"
     ]
    }
   ],
   "source": [
    "gen = TextGen(model, tokenizer, device='cuda')\n",
    "print(gen.generate(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31589f-175a-429e-a718-2aece193ad38",
   "metadata": {},
   "source": [
    "## Save and Load model\n",
    "Should you want to save and restore a model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca7bbe-35b9-4223-a685-d63e637c9383",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "03aff0cc-417f-40d5-98c6-248fa7e7626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\n",
    "    save_directory=model_path,\n",
    "    safe_serialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792a35a-4fbe-47f1-97cc-6eae834203f0",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44b34faf-67d1-4f55-84b0-89be6438b452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'missing_keys': [], 'unexpected_keys': [], 'mismatched_keys': [], 'error_msgs': []}\n",
      "CausalLM2(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (feedforward): FeedforwardNet(\n",
      "    (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      "  (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, load_info = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    output_loading_info=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "print(load_info)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e20ff9f-ab72-4ffc-9e1a-c9243f21394b",
   "metadata": {},
   "source": [
    "## Vanilla transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e760d6ea-5442-4f46-b761-03c42051124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.init as init\n",
    "from torch.nn import functional as F\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "# We will abstract out the causal loss function for reuse.\n",
    "def causal_loss(logits, labels):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        # labels with this value are ignored when computing loss\n",
    "        ignore_index=-100,\n",
    "        reduction='mean',\n",
    "    )\n",
    "    \n",
    "    return loss.nan_to_num()\n",
    "\n",
    "class FeedforwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_feedforward):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feedforward = d_feedforward\n",
    "\n",
    "        self.linear1 = nn.Linear(self.d_model, self.d_feedforward)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.d_feedforward, self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be evenly divisible by num_heads\"\n",
    "\n",
    "        # The dimension of each head.\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        # We scale the attention scores by the inverse-square-root of the head dimension\n",
    "        # this shifts the temerature of softmax.\n",
    "        self.dot_product_scale = 1.0 / math.sqrt(self.d_head)\n",
    "\n",
    "        # Input projection matricies: K, K, V\n",
    "        self.query_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.key_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.value_linear = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        # Output projection matrix:\n",
    "        # The input and output matrices only make sense with multi-head\n",
    "        # Don't bother with the output matrix, with a single head.\n",
    "        if self.num_heads != 1:\n",
    "            self.output_linear = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        # qkv: (batch_size, seq_len, d_qkv)\n",
    "        batch_size, seq_len, d_qkv = qkv.shape\n",
    "        \n",
    "        # Feed the inputs through the K, Q, V matrices.\n",
    "        query, key, value = self.query_linear(qkv), self.key_linear(qkv), self.value_linear(qkv)\n",
    "\n",
    "        # Split projections into multiple heads and swap position of sequence / heads dimension\n",
    "        query = query.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        key = key.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        value = value.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) * self.dot_product_scale\n",
    "\n",
    "        # Mask future positions from the past\n",
    "        causal_mask = torch.triu(torch.full((seq_len, seq_len), True, device=qkv.device), diagonal=1)\n",
    "        scores.masked_fill_(causal_mask, float('-inf'))\n",
    "        \n",
    "        # Calculate the attention weights; avoid NANs that might emerge from zeros in softmax's denominator\n",
    "        attention_weights = torch.softmax(scores, dim=-1).clamp(min=1e-10)\n",
    "        \n",
    "        # Use the attention weights to get a weighted combination of value vectors\n",
    "        attended_values = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        # Concatenate attention heads and project to original embedding size using the output linear layer\n",
    "        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, d_qkv)\n",
    "\n",
    "        # Project the concatenated output through the output matrix.\n",
    "        if self.num_heads != 1:\n",
    "            output = self.output_linear(attended_values)\n",
    "        else:\n",
    "            output = attended_values\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Standard transformer layer, from original paper.\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        attention,\n",
    "        feedforward,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.attention = attention\n",
    "        self.feedforward = feedforward\n",
    "        self.norm1 = nn.LayerNorm(self.d_model)\n",
    "        self.norm2 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Keep input as residual\n",
    "        residual = x\n",
    "\n",
    "        # Compute attention\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Add attention with residual and normalize.\n",
    "        x = self.norm1(residual + x)\n",
    "\n",
    "        # Keep output as next residual.\n",
    "        residual = x\n",
    "\n",
    "        # Pass through feedforward network.\n",
    "        x = self.feedforward(x)\n",
    "\n",
    "        # Combine residual and ff output, then normalize again.\n",
    "        x = self.norm2(residual + x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# A vanilla positional encoder\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_embed, max_seq):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.max_seq = max_seq\n",
    "        \n",
    "        weight = torch.zeros(max_seq, d_embed)\n",
    "        position = torch.arange(0, max_seq, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_embed, 2).float() * (-math.log(10000.0) / d_embed))\n",
    "        weight[:, 0::2] = torch.sin(position * div_term)\n",
    "        weight[:, 1::2] = torch.cos(position * div_term)\n",
    "        weight = weight.unsqueeze(0)\n",
    "        self.register_buffer('weight', weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(-2)\n",
    "        return x + self.weight[:, :seq_len]\n",
    "\n",
    "# Huggingface model type string\n",
    "model_type = \"simple-causal-transformer\"\n",
    "\n",
    "# Huggingface config class.\n",
    "# Huggingface 'PreTrainedModel' objects are passed a derivative of this class\n",
    "# when constructed. This is required, if your model will derive from PreTrainedModel.\n",
    "class CausalTransformerConfig(PretrainedConfig):\n",
    "    model_type = model_type\n",
    "    \n",
    "    def __init__(\n",
    "        # All of these MUST have defaults, even if unused.\n",
    "        self,\n",
    "        vocab_size=2000,\n",
    "        hidden_size=256,\n",
    "        max_sequence_length=2048,\n",
    "        dim_feedforward=512,\n",
    "        num_attention_heads=1,\n",
    "        num_hidden_layers = 4,\n",
    "        \n",
    "        **kwargs,\n",
    "    ):\n",
    "        # These are the canonical names used by Huggingface\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "# The formward method of this model is designed to be compatible with the HuggingFace Trainer and Tokenizer classes.\n",
    "# This is essentially a wrapper for a Pytorch transformer model, which implements the HF API.\n",
    "class CausalTransformer(PreTrainedModel):\n",
    "    config_class = CausalTransformerConfig\n",
    "    model_type = 'Transformer'\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.d_model = config.hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.positional_encoder = PositionalEncoder(d_embed=config.hidden_size, max_seq=config.max_sequence_length)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(\n",
    "                d_model=config.hidden_size,\n",
    "                attention=MultiheadAttention(\n",
    "                    d_model=config.hidden_size,\n",
    "                    num_heads=config.num_attention_heads,\n",
    "                ),\n",
    "                feedforward=FeedforwardLayer(\n",
    "                    d_model=config.hidden_size,\n",
    "                    d_feedforward=config.dim_feedforward,\n",
    "                ),\n",
    "            ) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.output_projection = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "        self.reset_parameters()\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,\n",
    "    ) -> (Tensor, dict[str, Tensor]):\n",
    "\n",
    "        # Convert input_ids to embeddings and add positional information.\n",
    "        x = self.positional_encoder(self.embedding(input_ids) * self.d_model**0.5)\n",
    "\n",
    "        # Pass the input through each of the layers.\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Convert embeddings to log-probabilities of next token-id\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        # Compute loss.\n",
    "        if labels is not None:\n",
    "            loss = causal_loss(logits, labels)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        if return_dict:\n",
    "            return CausalLMOutput(loss=loss, logits=logits)\n",
    "        elif loss is not None:\n",
    "            return (loss, logits)\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Init the embedding weights as per original design.\n",
    "        init.normal_(self.embedding.weight, std=self.d_model**-0.5)\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        model_inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "        return model_inputs\n",
    "\n",
    "AutoConfig.register(model_type, CausalTransformerConfig)\n",
    "AutoModelForCausalLM.register(CausalTransformerConfig, CausalTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc007b8d-31ba-4ac9-a81d-3c2c1ad1b286",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf1e01b-c477-4cdb-bee2-1037124ac3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 2.6M parameters\n",
      "CausalTransformer(\n",
      "  (embedding): Embedding(2000, 256)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (output_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=256, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def print_model_size(model):\n",
    "    model_size = sum(t.numel() for t in model.parameters())\n",
    "    print(f\"Model size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "# Create model configuration\n",
    "config = CausalTransformerConfig(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    hidden_size = 256,\n",
    "    dim_feedforward = 1024,\n",
    "    max_sequence_length = tokenizer.model_max_length,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers= 2\n",
    ")\n",
    "\n",
    "# A config can also be instantiated from a json file like this:\n",
    "#config = AutoConfig.from_pretrained(\"path-to-config\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "print_model_size(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741585b-5774-43cd-a166-0bfea5168de1",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1422fd1-475f-4d13-b7fb-e168b60847b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3313 steps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29a958e0ccf47a18de0bad4b5ffe17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d83b3bd5cc4c0687c0dd25acadc2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.320766806602478\n",
      "Global step: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bda2d163a0f4ca6b13dfd92c8f8f879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0744397242863974\n",
      "Global step: 2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8abe7baa3841e3b18a29279fdf2214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.9775431354840596\n",
      "Global step: 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48460f5ba0f441b4bc6e52ba98b73858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.9802577826711867\n"
     ]
    }
   ],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "076e9505-1b41-4cbd-9e13-345939f4fdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them bleed. Lily was happy because it was very expensive. She said, \"Thank you, mom. I love you.\" And they went to her room and saw the needle and seweds on their shirt.\n",
      "\n",
      "Lily said, \"Thank you, Mommy.\" Her mom said, \"I feel good.\" She said, \"That is nice and pretty. You are a kind girl who can use your help with the needle. But you have to be careful with how to sew and sew sews. And it is also very pretty with what she wanted.\"\n",
      "\n",
      "Lily nodded and said, \"Thank you, mom. I'm happy I still have an adventure. I like to be creative and sews to sew the needle.\" Her mom hugged her tight and said, \"You're welcome, Lily. I'm glad you like it.\" From that day on, Lily and her mom. They learned that sharing is not real. They played with each other and have fun. Lily was happy that they played with her and her shirt to sew. She played nicely and seweds and seweds and sewing her shirt. She felt proud and proud of her pain. The end. Lily, the best day ever. Lily had shown and Lily was happy too. She had made new friends and loved her.\n",
      "\n",
      "From that day on, Lily and her mom and her mom made her happy again. She said, \"Thank you, mom. I love you, mom.\" She waved goodbye to Lily and went to play with her toys and never bought again. She played with the needle. The end. Lily and her friends became very happy. They were happy that they found her and they both good friends forever. The end. Lily and her special day was not good ass. They played with her and the missing toy. She was happy to have. The end.\n",
      "\n",
      "Lily and Lily were happy and proud of her work. She was happy because she was happy for not needing to sewing things. Lily was happy again. She had a new friend. She was proud of her friend and kind gift. She knew that being gather and respectful. She loves her for being kind and happy. She was very happy that she found the needle. They were happy and proud of her,\n"
     ]
    }
   ],
   "source": [
    "gen = TextGen(model, tokenizer, device='cuda')\n",
    "print(gen.generate(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00887792-a64d-4daa-9a12-1dd245a54ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|BOS|> One day, a little girl named Sue was feeling dreamless. She wanted to take her home, but she wasn't send to take her home. She said to her family with.\n",
      "\n",
      "The family was grown-up and windy creative was always scared. He thought this sound had just books that the garden was delicate! Lucy brought it to her bedroom and lifted it off. Sue felt so brave, she started to flow on the pink cover. She started to shine as if she waits for her. \n",
      "\n",
      "Sue felt an exiving back and hugged the gianting. It went to her little girl and said, \"ning are a special surprise adventure!\" She smiled and hopped out of the play. A new isn't charming. This is mine, in a game!\n",
      "\n",
      "The girl felt so happy and knowing that the gift would visit. The father rare time and never had to be fleking on the car again. The prevent quite last time.k for the day, it reminded her the bow. She wanted to take the tray and if it was a safe of fly. \n",
      "\n",
      "Sure, the second sharpy came and the tray, to his family left the cover. Snow thank you? I'm so glad you used it everything that really cool up or nervous candy. I'll show you we can use my shelves. Come on, to go jump and do it again and stay on your early today.\"\n",
      "\n",
      "SpReadyy said, \"Thank you, hear a lot of fun!\" They were shocked too. Andy was not ready to be the game soon and delicious. This is the best thing in her special giftime.\" \n",
      "\n",
      "The grandma had an amazing meal friend, who was excited to or you'll show your une\". \n",
      "\n",
      "The girl packed the special gift of being so thoughtful. She felt so much on making new and thoughtful. It had you were talking about the for a special value and who went. She enjoyed the happier than the day. \n",
      "\n",
      "Lily learned that playing can be more of things in the world that don't invite to swim together with the best! ever! From that was stood and her family can make sure to say health and share no friends and grandma\n"
     ]
    }
   ],
   "source": [
    "# Compared output to simple generator.\n",
    "text_gen = TextGenerator(model, tokenizer, 'cuda', do_sample=True, seed=42)\n",
    "text = text_gen.prompt(\"One day, a little girl\", max_new_tokens=500)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111757f9-b214-4832-b4f6-2d721a804bf9",
   "metadata": {},
   "source": [
    "## Improved transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c6e85cb-6f60-4559-a637-2d173ccb002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.init as init\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\n",
    "\n",
    "def causal_loss(logits, labels):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        # labels with this value are ignored when computing loss\n",
    "        ignore_index=-100,\n",
    "        reduction='mean',\n",
    "    )\n",
    "    \n",
    "    return loss.nan_to_num().unsqueeze(0)\n",
    "\n",
    "def causal_alpha(n_layers):\n",
    "    return (2.0 * n_layers) ** 0.25\n",
    "\n",
    "def causal_beta(n_layers):\n",
    "    return (8.0 * n_layers) ** -0.25\n",
    "\n",
    "class FeedforwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_feedforward, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feedforward = d_feedforward\n",
    "        self.beta = causal_beta(n_layers)\n",
    "\n",
    "        self.linear1 = nn.Linear(self.d_model, self.d_feedforward)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.linear2 = nn.Linear(self.d_feedforward, self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Deepnet initialization\n",
    "        # https://arxiv.org/pdf/2203.00555.pdf\n",
    "        init.xavier_uniform_(self.linear1.weight, gain=self.beta)\n",
    "        init.constant_(self.linear1.bias, 0.)\n",
    "        init.xavier_uniform_(self.linear2.weight, gain=self.beta)\n",
    "        init.constant_(self.linear2.bias, 0.)\n",
    "\n",
    "def alibi_biases(query_len, key_len, device='cpu'):\n",
    "    x = torch.arange(key_len, device=device)[None, :]\n",
    "    y = torch.arange(query_len, device=device)[:, None]\n",
    "    return x - y\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        n_layers,\n",
    "        dropout=0.1,\n",
    "        # Set to False to disable Flash-Attention-2\n",
    "        flash_attention=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.beta = causal_beta(n_layers)\n",
    "        self.flash_attention = flash_attention\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be evenly divisible by num_heads\"\n",
    "\n",
    "        # The dimension of each head.\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        # We scale the attention scores by the inverse-square-root of the head dimension\n",
    "        # this shifts the temerature of softmax.\n",
    "        self.dot_product_scale = 1.0 / math.sqrt(self.d_head)\n",
    "\n",
    "        self.in_proj = nn.Parameter(torch.zeros(3 * self.d_model, self.d_model))\n",
    "        self.in_proj_bias = nn.Parameter(torch.zeros(3 * self.d_model))\n",
    "        self.output_linear = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Use ALiBi relative positional encoding\n",
    "        # https://arxiv.org/pdf/2108.12409.pdf\n",
    "        # This is the original ALiBi distribution.\n",
    "        alibi_slopes = 1.0 / torch.logspace(1, 8, self.num_heads, base=2, dtype=torch.float)\n",
    "        self.alibi_slopes = nn.Parameter(alibi_slopes)\n",
    "        #self.register_buffer('alibi_slopes', alibi_slopes)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def project_input(self, qkv):\n",
    "        proj = F.linear(qkv, self.in_proj, self.in_proj_bias)\n",
    "        return proj.chunk(chunks=3, dim=-1)\n",
    "    \n",
    "    def forward(self, qkv):\n",
    "        if self.flash_attention:\n",
    "            return self.flash_forward(qkv)\n",
    "        # qkv: (batch_size, seq_len, d_qkv)\n",
    "        batch_size, seq_len, d_qkv = qkv.shape\n",
    "        \n",
    "        # Feed the inputs through the K, Q, V matrices.\n",
    "        query, key, value = self.project_input(qkv)\n",
    "\n",
    "        # Split projections into multiple heads and swap position of sequence / heads dimension\n",
    "        query = query.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        key = key.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        value = value.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) * self.dot_product_scale\n",
    "\n",
    "        # Apply Alibi relative positional weights.\n",
    "        scores += alibi_biases(scores.shape[-2], scores.shape[-1], device=scores.device) * self.alibi_slopes.view(-1, 1, 1)\n",
    "        \n",
    "        # Mask future positions from the past\n",
    "        causal_mask = torch.triu(torch.full((seq_len, seq_len), True, device=qkv.device), diagonal=1)\n",
    "        scores.masked_fill_(causal_mask, float('-inf'))\n",
    "        \n",
    "        # Calculate the attention weights; avoid NANs that might emerge from zeros in softmax's denominator\n",
    "        attention_weights = self.dropout(torch.softmax(scores, dim=-1).clamp(min=1e-10))\n",
    "        \n",
    "        # Use the attention weights to get a weighted combination of value vectors\n",
    "        attended_values = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        # Concatenate attention heads and project to original embedding size using the output linear layer\n",
    "        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, d_qkv)\n",
    "\n",
    "        # Project the concatenated output through the output matrix.\n",
    "        output = self.output_linear(attended_values)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def flash_forward(self, qkv):\n",
    "        batch_size, seq_len, d_embed = qkv.shape\n",
    "        \n",
    "        # Feed the inputs through the K, Q, V matrices.\n",
    "        # query : (batch_size, seq_len, d_model)\n",
    "        # qkv : (batch_size, seq_len, 3, num_heads, d_kq)\n",
    "        qkv = F.linear(\n",
    "            qkv,\n",
    "            self.in_proj,\n",
    "            self.in_proj_bias\n",
    "        ).unflatten(\n",
    "            -1,\n",
    "            (3, self.num_heads, self.d_head)\n",
    "        )\n",
    "\n",
    "        attended_values = flash_attn_qkvpacked_func(\n",
    "            qkv,\n",
    "            dropout_p=self.dropout.p,\n",
    "            softmax_scale=self.dot_product_scale,\n",
    "            causal=True,\n",
    "            alibi_slopes=self.alibi_slopes.float(),\n",
    "        )\n",
    "        del qkv\n",
    "        # attended_values: (batch_size, seqlen, nheads, headdim)\n",
    "\n",
    "        # Concatentate heads back into d_embed\n",
    "        attended_values = attended_values.view(batch_size, seq_len, d_embed)\n",
    "\n",
    "        # Munge the concatenated values\n",
    "        output = self.output_linear(attended_values)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Deepnet initialization\n",
    "        # https://arxiv.org/pdf/2203.00555.pdf\n",
    "        \n",
    "        q, k, v = self.in_proj.chunk(3)\n",
    "        init.xavier_uniform_(q, gain=1.0)\n",
    "        init.xavier_uniform_(k, gain=1.0)\n",
    "        init.xavier_uniform_(v, gain=self.beta)\n",
    "        init.constant_(self.in_proj_bias, 0.)\n",
    "        init.xavier_uniform_(self.output_linear.weight, gain=self.beta)\n",
    "        init.constant_(self.output_linear.bias, 0.)\n",
    "\n",
    "class ScaleAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        n_layers,\n",
    "        dropout=0.1,\n",
    "        # Set to False to disable Flash-Attention-2\n",
    "        flash_attention=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.beta = causal_beta(n_layers)\n",
    "        self.flash_attention = flash_attention\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be evenly divisible by num_heads\"\n",
    "\n",
    "        # The dimension of each head.\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        # We scale the attention scores by the inverse-square-root of the head dimension\n",
    "        # this shifts the temerature of softmax.\n",
    "        self.dot_product_scale = 1.0 / math.sqrt(self.d_head)\n",
    "\n",
    "        self.query = nn.Parameter(torch.empty(d_model))\n",
    "        #self.query_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.key = nn.Parameter(torch.empty(d_model))\n",
    "        self.value = nn.Parameter(torch.empty(d_model))\n",
    "        self.output = nn.Parameter(torch.empty(d_model))\n",
    "        #self.output_linear = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Use ALiBi relative positional encoding\n",
    "        # https://arxiv.org/pdf/2108.12409.pdf\n",
    "        # This is the original ALiBi distribution.\n",
    "        alibi_slopes = 1.0 / torch.logspace(1, 8, self.num_heads, base=2, dtype=torch.float)\n",
    "        self.alibi_slopes = nn.Parameter(alibi_slopes)\n",
    "        #self.register_buffer('alibi_slopes', alibi_slopes)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def project_input(self, qkv):\n",
    "        query = qkv * self.query\n",
    "        #query = self.query_linear(qkv)\n",
    "        key = (qkv * self.key).roll(shifts=self.d_head // 2, dims=-1)\n",
    "        value = qkv * self.value\n",
    "        \n",
    "        return query, key, value\n",
    "    \n",
    "    def forward(self, qkv):\n",
    "        if self.flash_attention:\n",
    "            return self.flash_forward(qkv)\n",
    "        # qkv: (batch_size, seq_len, d_qkv)\n",
    "        batch_size, seq_len, d_qkv = qkv.shape\n",
    "        \n",
    "        # Feed the inputs through the K, Q, V matrices.\n",
    "        query, key, value = self.project_input(qkv)\n",
    "\n",
    "        # Split projections into multiple heads and swap position of sequence / heads dimension\n",
    "        query = query.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        key = key.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        value = value.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) * self.dot_product_scale\n",
    "\n",
    "        # Apply Alibi relative positional weights.\n",
    "        scores += alibi_biases(scores.shape[-2], scores.shape[-1], device=scores.device) * self.alibi_slopes.view(-1, 1, 1)\n",
    "        \n",
    "        # Mask future positions from the past\n",
    "        causal_mask = torch.triu(torch.full((seq_len, seq_len), True, device=qkv.device), diagonal=1)\n",
    "        scores.masked_fill_(causal_mask, float('-inf'))\n",
    "        \n",
    "        # Calculate the attention weights; avoid NANs that might emerge from zeros in softmax's denominator\n",
    "        attention_weights = self.dropout(torch.softmax(scores, dim=-1).clamp(min=1e-10))\n",
    "        \n",
    "        # Use the attention weights to get a weighted combination of value vectors\n",
    "        attended_values = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        # Concatenate attention heads and project to original embedding size using the output linear layer\n",
    "        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, d_qkv)\n",
    "\n",
    "        # Project the concatenated output through the output matrix.\n",
    "        #output = self.output_linear(attended_values)\n",
    "        output = attended_values * self.output\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def flash_forward(self, qkv):\n",
    "        batch_size, seq_len, d_embed = qkv.shape\n",
    "        \n",
    "        query, key, value = self.project_input(qkv)\n",
    "        query = query.view(batch_size, seq_len, self.num_heads, self.d_head)\n",
    "        key = key.view(batch_size, seq_len, self.num_heads, self.d_head)\n",
    "        value = value.view(batch_size, seq_len, self.num_heads, self.d_head)\n",
    "\n",
    "        attended_values = flash_attn_func(\n",
    "            q=query,\n",
    "            k=key,\n",
    "            v=value,\n",
    "            dropout_p=self.dropout.p,\n",
    "            softmax_scale=self.dot_product_scale,\n",
    "            causal=True,\n",
    "            alibi_slopes=self.alibi_slopes.float(),\n",
    "        )\n",
    "        del qkv\n",
    "        # attended_values: (batch_size, seqlen, nheads, headdim)\n",
    "\n",
    "        # Concatentate heads back into d_embed\n",
    "        attended_values = attended_values.view(batch_size, seq_len, d_embed)\n",
    "\n",
    "        # Munge the concatenated values\n",
    "        output = self.output_linear(attended_values)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.normal_(self.query)\n",
    "        #init.xavier_uniform_(self.query_linear.weight, gain=self.beta)\n",
    "        #init.constant_(self.query_linear.bias, 0.)\n",
    "        \n",
    "        init.normal_(self.key)\n",
    "        init.normal_(self.value)\n",
    "        init.normal_(self.output)\n",
    "        \n",
    "        #init.xavier_uniform_(self.output_linear.weight, gain=self.beta)\n",
    "        #init.constant_(self.output_linear.bias, 0.)\n",
    "\n",
    "# Deepnet transformer layer\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        attention,\n",
    "        feedforward,\n",
    "        n_layers,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.attention = attention\n",
    "        self.feedforward = feedforward\n",
    "        self.norm1 = nn.LayerNorm(self.d_model)\n",
    "        self.norm2 = nn.LayerNorm(self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Deepnet alpha https://arxiv.org/pdf/2203.00555.pdf\n",
    "        self.alpha = (n_layers * 2.0) ** 0.25\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Keep input as residual\n",
    "        residual = x * self.alpha\n",
    "\n",
    "        # Compute attention\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Add attention with residual and normalize.\n",
    "        x = self.norm1(residual + self.dropout(x))\n",
    "\n",
    "        # Keep output as next residual.\n",
    "        residual = x * self.alpha\n",
    "\n",
    "        # Pass through feedforward network.\n",
    "        x = self.feedforward(x)\n",
    "\n",
    "        # Combine residual and ff output, then normalize again.\n",
    "        x = self.norm2(residual + self.dropout(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Huggingface model type string\n",
    "model_type = \"dinalt-causal-transformer\"\n",
    "\n",
    "# Huggingface config class.\n",
    "# Huggingface 'PreTrainedModel' objects are passed a derivative of this class\n",
    "# when constructed. This is required, if your model will derive from PreTrainedModel.\n",
    "class CausalTransformerConfig(PretrainedConfig):\n",
    "    model_type = model_type\n",
    "    \n",
    "    def __init__(\n",
    "        # All of these MUST have defaults, even if unused.\n",
    "        self,\n",
    "        vocab_size=2000,\n",
    "        hidden_size=256,\n",
    "        max_sequence_length=2048,\n",
    "        dim_feedforward=512,\n",
    "        num_attention_heads=1,\n",
    "        num_hidden_layers = 4,\n",
    "        \n",
    "        **kwargs,\n",
    "    ):\n",
    "        # These are the canonical names used by Huggingface\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "# The formward method of this model is designed to be compatible with the HuggingFace Trainer and Tokenizer classes.\n",
    "# This is essentially a wrapper for a Pytorch transformer model, which implements the HF API.\n",
    "class CausalTransformer(PreTrainedModel):\n",
    "    config_class = CausalTransformerConfig\n",
    "    model_type = 'Transformer'\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.d_model = config.hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(\n",
    "                d_model=config.hidden_size,\n",
    "                attention=MultiheadAttention(\n",
    "                    d_model=config.hidden_size,\n",
    "                    num_heads=config.num_attention_heads,\n",
    "                    n_layers=config.num_hidden_layers,\n",
    "                    flash_attention=config.flash_attention,\n",
    "                ),\n",
    "                feedforward=FeedforwardLayer(\n",
    "                    d_model=config.hidden_size,\n",
    "                    d_feedforward=config.dim_feedforward,\n",
    "                    n_layers=config.num_hidden_layers,\n",
    "                ),\n",
    "                n_layers=config.num_hidden_layers,\n",
    "            ) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.output_projection = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "        self.reset_parameters()\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,\n",
    "    ) -> (Tensor, dict[str, Tensor]):\n",
    "\n",
    "        # Convert input_ids to embeddings and add positional information.\n",
    "        x = self.embedding(input_ids) * self.d_model**0.5\n",
    "\n",
    "        # Pass the input through each of the layers.\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Convert embeddings to log-probabilities of next token-id\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        # Compute loss.\n",
    "        if labels is not None:\n",
    "            loss = causal_loss(logits, labels)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        if return_dict:\n",
    "            return CausalLMOutput(loss=loss, logits=logits)\n",
    "        elif loss is not None:\n",
    "            return (loss, logits)\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Init the embedding weights as per original design.\n",
    "        init.normal_(self.embedding.weight, std=self.d_model**-0.5)\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        model_inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "        return model_inputs\n",
    "\n",
    "AutoConfig.register(model_type, CausalTransformerConfig)\n",
    "AutoModelForCausalLM.register(CausalTransformerConfig, CausalTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4c99cd5-c24a-498a-a8b8-42e5e2e0d552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.9M parameters\n",
      "CausalTransformer(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (activation): SiLU()\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Make a somewhat bigger model\n",
    "\n",
    "def print_model_size(model):\n",
    "    model_size = sum(t.numel() for t in model.parameters())\n",
    "    print(f\"Model size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "# Create model configuration\n",
    "config = CausalTransformerConfig(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    hidden_size = 128,\n",
    "    dim_feedforward = 512,\n",
    "    max_sequence_length = tokenizer.model_max_length,\n",
    "    num_attention_heads=2,\n",
    "    num_hidden_layers=2,\n",
    "    flash_attention=True,\n",
    ")\n",
    "\n",
    "# A config can also be instantiated from a json file like this:\n",
    "#config = AutoConfig.from_pretrained(\"path-to-config\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "print_model_size(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e99acf3b-a609-4873-a182-913c696a5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to tokenize the complete dataset\n",
    "#tok_train_dataset = tokenize_dataset(dataset[\"train\"], tokenizer, select=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ce52f42-0f48-493a-9abf-d9ecceacf6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3313 steps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa35b09d9c6348a3b8fcb335479afd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d240140989834de08ca5ad2e0ba9a793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.8541666666666665\n",
      "Global step: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85efdbca051d4450a9dd2de8aa2b3501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.6067708333333335\n",
      "Global step: 2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaaad117254645c39c5b715a93507f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.5052083333333335\n",
      "Global step: 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47602f07ecb14613a302498e2f6d536e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.4887152777777777\n"
     ]
    }
   ],
   "source": [
    "model = model.bfloat16()\n",
    "do_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
