{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bb9655-0848-448a-a500-28196416c634",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Source\n",
    "[aiws.yamldict](../aiws/yamldict.py)  \n",
    "[tutorial_code.datasets](../tutorial_code/datasets.py)  \n",
    "[tutorial_code.tokenizer](../tutorial_code/tokenizer.py)\n",
    "[tutorial_code.model_utils](../tutorial_code/model_utils.py)  \n",
    "[tutorial_code.train](../tutorial_code/train.py)  \n",
    "[tutorial_code.bigram_model](../tutorial_code/bigram_model.py)  \n",
    "[tutorial_code.inference](../tutorial_code/inference.py)  \n",
    "\n",
    "### See Also\n",
    "[dataset.ipynb](dataset.ipynb)  \n",
    "[tokenizer.ipynb](tokenizer.ipynb)  \n",
    "[my_first_transformer.ipynb](my_first_transformer.ipynb)  \n",
    "\n",
    "### Config\n",
    "[config.yaml](config/config.yaml)  \n",
    "[paths.yaml](config/paths.yaml)  \n",
    "[dataset.yaml](config/dataset.yaml)  \n",
    "[tokenizer.yaml](config/tokenizer.yaml) \n",
    "[dataset.yaml](config/dataset.yaml)  \n",
    "[training.yaml](config/training.yaml)   \n",
    "[model.yaml](config/model.yaml)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53b418e-44b4-4ffd-90c4-b819e6341998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paths': {'models_dir': '/home/dinalt/ai_assets/models',\n",
      "           'datasets_dir': '/home/dinalt/ai_assets/datasets'},\n",
      " 'tokenizer': {'vocab_size': 2000},\n",
      " 'train': {'per_device_train_batch_size': 64,\n",
      "           'per_device_eval_batch_size': 128,\n",
      "           'learning_rate': 0.001,\n",
      "           'num_train_epochs': 1.0,\n",
      "           'eval_steps': 1000,\n",
      "           'num_warmup_steps': 0,\n",
      "           'lr_scheduler_name': 'constant'},\n",
      " 'dataset': {'dataset_id': 'roneneldan-TinyStories',\n",
      "             'tokenized_dataset_path': './tiny_stories_tokenized',\n",
      "             'train_select': 0.1,\n",
      "             'validate_select': 0.1},\n",
      " 'model': {'model_id': 'tiny',\n",
      "           'max_sequence_len': 2048,\n",
      "           'd_model': 128,\n",
      "           'd_feedforward': 512,\n",
      "           'num_attention_heads': 1,\n",
      "           'num_hidden_layers': 2},\n",
      " 'device': 'cuda'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import pprint\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from aiws.yamldict import load_yaml_dict\n",
    "from tutorial_code.datasets import load_dataset_from_config, tokenize_datasetdict\n",
    "from tutorial_code.tokenizer import train_bpe_tokenizer\n",
    "from tutorial_code.model_utils import print_model_size, test_model_forward\n",
    "from tutorial_code.bigram_model import BigramLM\n",
    "\n",
    "config = load_yaml_dict(\"config/config.yaml\")\n",
    "pprint.pp(config)\n",
    "config.model_path = os.path.join(config.paths.models_dir, config.model.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5055af-161e-4e1e-b1e4-81c1c9784a7f",
   "metadata": {},
   "source": [
    "### Reload Module\n",
    "Useful, if you make changes to a module and don't want to restart the notebook.  \n",
    "Otherwise, skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d11a086-e4f6-42b7-8af1-721803aea05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tutorial_code.textgen' from '/home/dinalt/ai_assets/aiworkspace/tutorial/../tutorial_code/textgen.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# If the module has not been imported, we first import it.\n",
    "import tutorial_code.textgen\n",
    "\n",
    "# Trigger a reload of the module.\n",
    "importlib.reload(tutorial_code.textgen)\n",
    "\n",
    "# If the symbol was imported with 'from,' reimport the symbol by running the cell with 'from' again.\n",
    "\n",
    "# If something new was added and can't be found...\n",
    "#importlib.invalidate_caches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295e6d5-e6d1-4c02-b6c3-25340f11191d",
   "metadata": {},
   "source": [
    "## Quick Load\n",
    "If you have built a tokenizer and tokenized dataset already, you can just load them here and skip to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bdae0a7-e20b-481a-abe2-ab936d4323a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "dataset = load_dataset_from_config(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "tokenized_dataset = datasets.load_from_disk(config.dataset.tokenized_dataset_path)\n",
    "sample_text = dataset['train']['text'][0][:500]\n",
    "model = BigramLM(config.model.d_model, tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0c23e-2311-4dd4-9102-013e5f777bd1",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will need some data to train our model on. For this tutorial, we will use a dataset named \"TinyStories,\" which is a synthetic dataset generated by ChatGPT designed for training very small language models to produce coherent output. This is made possible by limiting the examples to things which a 4-year-old child would be able to understand, with a total vocabulary of about 1500 words.\n",
    "\n",
    "Huggingface dataset link:  \n",
    "https://huggingface.co/datasets/roneneldan/TinyStories  \n",
    "\n",
    "The paper describing the dataset:  \n",
    "https://arxiv.org/abs/2305.07759\n",
    "\n",
    "The first time this is run, it will download the dataset to your cache, which make take a few minutes. After that, the dataset will be loaded from your cache.\n",
    "\n",
    "source: [tutorial_code.datasets.load_dataset_from_config()](../tutorial_code/datasets.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa226a9-4047-48c3-97c1-6b8cc3b9b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n",
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset_from_config(config)\n",
    "\n",
    "print(dataset)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# For experimentation, we will want a bit of sample text to work with. \n",
    "# This will grab the first 500 characters from the first record of the training dataset.\n",
    "sample_text = train_dataset['text'][0][:500]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c82d0-3e68-4392-87b1-6a2e6b96137f",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Rather than working with the raw ASCII/Unicode from the dataset, we will be \"tokenizing\" the data. A tokenizer is a statisttical model which aggregates individual characters into sub-word, where the most frequent strings of characters are replaced by unique symbols.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Large_language_model#Probabilistic_tokenization\n",
    "\n",
    "For this tutorial, we will be created a Byte Pair Encoding (BPE) tokenizer, which starts with all of the symbols from the ASCII character set, then creates tokens for the most common pairs of ASCII characters. These pairs are further aggregated into larger symbols and the process repeats until a set of symbols matching the target vocabulary size has been created.\n",
    "\n",
    "By starting with the ASCII character set, it is possible to represent any combination of letters, including those which were not observed when the tokenizer was created.\n",
    "\n",
    "If you have not created a tokenizer, you can follow the [tokenizer tutorial](./tokenizer.ipynb) or just run the \"Build Tokenizer\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200d7c9-d8e9-4545-a79f-b3f07ddde855",
   "metadata": {},
   "source": [
    "### Load tokenizer\n",
    "We can load our saved tokenizer -- or the tokenizer from any Huggingface model -- with this interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26098951-3b92-493e-9569-06b95cbb37ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='/home/dinalt/ai_assets/models/tiny', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer from a local path -- or from a Huggingface model name.\n",
    "# Rather than starting from scratch, you could replace 'model_path' with the path of an existing model and use its tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f04ea-d1db-4357-b1aa-e6d358960ef7",
   "metadata": {},
   "source": [
    "### Build Tokenizer\n",
    "If you have not built the tokenizer first, follow the linked tutorial...\n",
    "\n",
    "[Tokenizer Notebook](tokenizer.ipynb)\n",
    "\n",
    "...or just run this cell to build and save it.  \n",
    "Building it can take a moment or three. Be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08e8bbf-8555-4ddf-9302-1a838a978563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Completed training\n",
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/dinalt/ai_assets/models/tiny/tokenizer_config.json',\n",
       " '/home/dinalt/ai_assets/models/tiny/special_tokens_map.json',\n",
       " '/home/dinalt/ai_assets/models/tiny/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = train_bpe_tokenizer(config, dataset['train'])\n",
    "print(tokenizer)\n",
    "tokenizer.save_pretrained(config.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c51e0d-8cc4-4bc7-9a71-f3513338d530",
   "metadata": {},
   "source": [
    "## Tokenize dataset\n",
    "Before training the model, we need to convert the text in the dataset to the token-ids used by the model.\n",
    "\n",
    "This function is a fairly simple imlementation of this functionality. It will:\n",
    "- Split the dataset into a subset of the total, if 'select' is less than 1.0.\n",
    "- Take each example from the dataset, in batches, and convert the text to the corresponding tokens.\n",
    "- Truncate sequences longer than the model can process.\n",
    "- Add padding tokens, where the length of sequences in the batch are not identical.\n",
    "- Remove unused columns from the data.\n",
    "\n",
    "See Also: [dataset.ipynb](./dataset.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d4a48-bd91-4ea0-bd14-548cacab3e2e",
   "metadata": {},
   "source": [
    "### Load Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cc4b8b-484a-4759-b4ae-b8ec8af2f691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 211971\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 2199\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "tokenized_dataset = datasets.load_from_disk(config.dataset.tokenized_dataset_path)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493cc70-6d28-4163-bf60-0951f462672e",
   "metadata": {},
   "source": [
    "### Build and Save Tokenized Dataset\n",
    "If you have not built the tokenized dataset, you can to so now.\n",
    "\n",
    "[tokenize_datasetdict()](../tutorial_code/tokenizer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4602addb-6d64-4d06-a4c9-1112679cd46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenize_datasetdict(dataset, tokenizer, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170091b-89d8-477d-86e8-63d0f4ef9900",
   "metadata": {},
   "source": [
    "#### Save Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b3d092-d16e-4cde-935d-c9c3832d5109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca9e175839b4a8184bad5e8c4deca14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/211971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1520f308124972920775af9cda0edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(config.dataset.tokenized_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810aa32-3d44-4769-9322-dcb7d7ceb6f6",
   "metadata": {},
   "source": [
    "## Create a simple causal language model\n",
    "A \"causal\" language model is one which makes predictions about future tokens based upon past tokens. We will start with a simple model which predicts the next token, given only the immediadly preceeding token.\n",
    "\n",
    "Source: [BigramLM](../tutorial_code/bigram_model.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2f37c-bb0c-4987-a05a-7a46bf2b9365",
   "metadata": {},
   "source": [
    "### Instantiate model\n",
    "This will create an instance of our model with a hidden-dimension (d_model) of 128 and a vocabulary size matching that of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f909ad-7100-4bc3-9382-bfcb74d66c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.5M parameters\n",
      "BigramLM(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BigramLM(config.model.d_model, tokenizer.vocab_size)\n",
    "print_model_size(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5ffaa-1d93-4861-b0ed-c2fd877d809e",
   "metadata": {},
   "source": [
    "### Enable Torch Compile [optional]\n",
    "\n",
    "https://pytorch.org/docs/stable/torch.compiler.html\n",
    "\n",
    "Optionally compile the model. This may not work with all version of Python and Pytorch.\n",
    "Using \"torch.compile()\" is especially effective at speeding up small models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cdc14-1179-4b56-8c61-cef81d85e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_float32_matmul_precision('high')\n",
    "model.compile()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef483f-00de-473e-a729-5e9a278ba64d",
   "metadata": {},
   "source": [
    "### Test model forward\n",
    "This will tokenizer our sample text and feed it through the forward method of the model to ensure that the code does not \"fall-over.\"\n",
    "\n",
    "If the model has not been trained, the loss is expected to be around 7 - 8; lower, if it has been trained.\n",
    "\n",
    "[test_model_forward()](../tutorial_code/model_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8f463bd-262a-4bc8-b83f-91b321d18b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      " tensor([[   2,  491,  360,   16,  263,  403,  450,  505,  362,  598,  263,  792,\n",
      "          311,  320,  313,  763,   18,  317,  709,  308,  286, 1035,   74,  475,\n",
      "         1389,   88,  270,  365,  346,  308,  791,  308,  286,  385,  291,   84,\n",
      "           18,  362,  448,  270,  952,  267,  792,  311,  346,  313,  370,   16,\n",
      "          354,  342,  464,  442,   91,  263, 1842,  307,  349,  313,  385,  316,\n",
      "           88,   18,  203,  203,  601,  473,  270,  313,  370,  269,  331,   16,\n",
      "          332,  781,   16,  339,  598,  747,  792,  311,   18, 1283,  350,  952,\n",
      "          308,  346,  522,  269,  442,   91,  656,  385,  316,   88,  481,  869,\n",
      "          370,  503,  269,  331,   16,  332,  836,   16,  362,   16,  369,  477,\n",
      "          952,  267,  792,  311,  269, 1307,  633,  385,  316,   88,  420,  203,\n",
      "          203,   56,   83,  558,   16,  368, 1659,  267,  792,  311,  269,  442,\n",
      "           91,  268,  267, 1842,  307,  349,  362,  376,  385,  316,   88,   18,\n",
      "          413,  286,  390, 1035,   74,  475, 1389,   88,  372,  452,  271]])\n",
      "tensor(7.6683, grad_fn=<NanToNumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_model_forward(model, tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4473aee-f86d-4c72-a6ea-91f8c4e02f92",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "This is an example training-loop implementation.\n",
    "\n",
    "Example code is based upon examples here:\n",
    "https://huggingface.co/learn/nlp-course/en/chapter3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f95593f4-48ff-481c-8118-90e29d023f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_code.train import CausalTrainer\n",
    "\n",
    "# This provides a place to configure the training parameters.\n",
    "def do_train():\n",
    "    CausalTrainer(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['validation'],\n",
    "        per_device_train_batch_size=config.train.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=config.train.per_device_eval_batch_size,\n",
    "        learning_rate=config.train.learning_rate,\n",
    "        num_train_epochs=config.train.num_train_epochs,\n",
    "        eval_steps=config.train.eval_steps,\n",
    "        optimizer_factory=lambda params, lr: torch.optim.AdamW(params, lr=lr),\n",
    "        lr_scheduler_factory=lambda opt, steps: transformers.get_scheduler(\n",
    "            config.train.lr_scheduler_name,\n",
    "            opt,\n",
    "            config.train.num_warmup_steps,\n",
    "            steps,\n",
    "        ),\n",
    "        device = config.device,\n",
    "    ).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bcb9feb-41b8-47fe-8493-cbb3d2d958b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3313 steps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c20b5426464f9b8449c46b5dc5e239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b728880d1ec049e687c8b10c5293b69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=3.7317506869633994\n",
      "Global step: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6a71bb08e345a5babe11fd11ca8353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=3.642031921280755\n",
      "Global step: 2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ba6e80d2d64b05accc112a3a468d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=3.6106786727905273\n",
      "Global step: 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9653142df31e44949cbdd364ad15b21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=3.604231927129957\n"
     ]
    }
   ],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7f5db-c057-4e90-a8bc-045b9cc4f263",
   "metadata": {},
   "source": [
    "### Accelerate Training Loop\n",
    "\n",
    "This is the same code, but modified to run on multiple GPU's within a notebook using the [Accelerate](https://huggingface.co/docs/accelerate/v0.11.0/en/index) library.\n",
    "\n",
    "Note: For small models, this may actually be slower than the basic training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1cb65c6-05b9-4b46-afec-5293079341a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "from tutorial_code.train import CausalAccelerateTrainer\n",
    "\n",
    "def train_function():\n",
    "    trainer = CausalAccelerateTrainer(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['validation'],\n",
    "        per_device_train_batch_size=config.train.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=config.train.per_device_eval_batch_size,\n",
    "        learning_rate=config.train.learning_rate,\n",
    "        num_train_epochs=config.train.num_train_epochs,\n",
    "        eval_steps=config.train.eval_steps,\n",
    "        optimizer_factory=lambda params, lr: torch.optim.AdamW(params, lr=lr),\n",
    "        lr_scheduler_factory=lambda opt, steps: transformers.get_scheduler(\n",
    "            config.train.lr_scheduler_name,\n",
    "            opt,\n",
    "            config.train.num_warmup_steps,\n",
    "            steps,\n",
    "        ),\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "def do_train():\n",
    "    notebook_launcher(train_function, num_processes=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd58f89f-7477-41af-aabf-608bdda1eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 6 GPUs.\n",
      "Training for 553 steps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbc0279cb6343d9a3926503e45a7288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9cef15e33e464ca5ebb9bfe1988e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=3.7510127226511636\n"
     ]
    }
   ],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db60ebd9-8d68-45ba-8a67-31f54db37183",
   "metadata": {},
   "source": [
    "### Huggingface Trainer Example\n",
    "This illustrates how to use the HF trainer class, with the functionality being similar\n",
    "to the above code.\n",
    "\n",
    "Within the context of a notebook and multiple GPU's, the trainer will train the model using torch [Data Parallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html). This is not ideal for performance.\n",
    "\n",
    "The same code, launched using Accelerate in a script will perform much better.\n",
    "\n",
    "UNTESTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df1896-2f96-4311-94b4-4207d890ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "from tutorial_code.train import hf_trainer\n",
    "\n",
    "def do_train():\n",
    "    train_causal_model(\n",
    "        model,tokenizer,\n",
    "        tok_train_dataset,\n",
    "        tok_val_dataset,\n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=config.train.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=config.train.per_device_eval_batch_size,\n",
    "            output_dir=\"test_trainer\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=config.train.eval_steps,\n",
    "            num_train_epochs=config.train.num_train_epochs,\n",
    "    \n",
    "            # If set too high, your GPU may run out of memory.\n",
    "            #per_device_train_batch_size=8,\n",
    "            #per_device_eval_batch_size=16,\n",
    "            \n",
    "            # The learning rate will need to be reduced as model size grows. If the rate is set too high, the\n",
    "            # loss will become unstable, possibly increasing.\n",
    "            learning_rate=config.train.learning_rate,\n",
    "            # Set for better diagnostics\n",
    "            #use_cpu=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "#notebook_launcher(do_train, num_processes=torch.cuda.device_count())\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f9b16-b814-4f39-90a5-764ef199ea78",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc798a0c-3398-4b93-bce1-400a6d7839e9",
   "metadata": {},
   "source": [
    "### Predict Tokens\n",
    "This will take the input text and have the model make predictions for the next token for each token in the sequence.\n",
    "\n",
    "The color coding indicates the loss for each individual token, with darker colors being more accurate and brighter colors being less so.\n",
    "\n",
    "If you hover over a token, you can see the top-10 predictions for the next token in the sequence.\n",
    "\n",
    "[tutorial_code.inference](../tutorial_code/inference.py)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab16902e-9f12-4690-8f41-469036fa25b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b\n",
      "Metric 'Causal Loss': n=154, min=6.398907661437988, max=9.475622177124023, mean=7.742974758148193, range=(0, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "/* Tooltip container class */\n",
       ".token {\n",
       "  position: relative;\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       "/* Tooltip text */\n",
       ".token .tooltip {\n",
       "  visibility: hidden;\n",
       "  width: 300px;\n",
       "  background-color: black;\n",
       "  color: #fff;\n",
       "  text-align: left;\n",
       "  padding: 5px 0;\n",
       "  border-radius: 6px;\n",
       " \n",
       "  /* Position the tooltip text - see examples below! */\n",
       "  position: absolute;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       "/* Show the tooltip text when you mouse over the tooltip container */\n",
       ".token:hover .tooltip {\n",
       "  visibility: visible;\n",
       "}\n",
       "</style>\n",
       "Metric[0] 'Causal Loss': n=155, min=0.0, max=9.475622177124023, mean=7.693019866943359<br><span class='token' style='color: #0c0786'><span class='tooltip'>Token: ''<br>Causal Loss: 0.00000<br>---------------<br></span></span><span class='token' style='color: #a41e98'>&nbsp;One<span class='tooltip'>Token: ' One'<br>Causal Loss: 7.51368<br>---------------<br>0.00 : ' catch'<br>0.00 : ' one'<br>0.00 : 's'<br>0.00 : 'ool'<br>0.00 : 'hn'<br>0.00 : ' relie'<br>0.00 : ' through'<br>0.00 : ' wouldn'<br>0.00 : ' bre'<br>0.00 : 'pping'<br></span></span><span class='token' style='color: #fcc726'>&nbsp;day<span class='tooltip'>Token: ' day'<br>Causal Loss: 9.12041<br>---------------<br>0.00 : ' fight'<br>0.00 : '\u0010'<br>0.00 : 'iting'<br>0.00 : ' hear'<br>0.00 : 'ine'<br>0.00 : ' It'<br>0.00 : '�'<br>0.00 : 'M'<br>0.00 : 'ross'<br>0.00 : 'ture'<br></span></span><span class='token' style='color: #dc5e66'>,<span class='tooltip'>Token: ','<br>Causal Loss: 8.18114<br>---------------<br>0.00 : 'bby'<br>0.00 : ' swing'<br>0.00 : ' call'<br>0.00 : ' co'<br>0.00 : ' smiles'<br>0.00 : 'ins'<br>0.00 : ' fish'<br>0.00 : ' Gra'<br>0.00 : 'vel'<br>0.00 : 'gs'<br></span></span><span class='token' style='color: #9310a1'>&nbsp;a<span class='tooltip'>Token: ' a'<br>Causal Loss: 7.35142<br>---------------<br>0.00 : ' spr'<br>0.00 : ' and'<br>0.00 : ' strong'<br>0.00 : ' bo'<br>0.00 : ' tail'<br>0.00 : 'ay'<br>0.00 : ' off'<br>0.00 : ' things'<br>0.00 : ' had'<br>0.00 : 'king'<br></span></span><span class='token' style='color: #df6163'>&nbsp;little<span class='tooltip'>Token: ' little'<br>Causal Loss: 8.21901<br>---------------<br>0.00 : ' monster'<br>0.00 : ' fall'<br>0.00 : ' feels'<br>0.00 : '�'<br>0.00 : '�'<br>0.00 : '\u0002'<br>0.00 : ' block'<br>0.00 : 'ired'<br>0.00 : 'ind'<br>0.00 : ' th'<br></span></span><span class='token' style='color: #b83289'>&nbsp;girl<span class='tooltip'>Token: ' girl'<br>Causal Loss: 7.71239<br>---------------<br>0.00 : 'side'<br>0.00 : ' when'<br>0.00 : 'Okay'<br>0.00 : '\u000f'<br>0.00 : ' turned'<br>0.00 : 'b'<br>0.00 : ' Grandma'<br>0.00 : ' car'<br>0.00 : 'cy'<br>0.00 : 'zz'<br></span></span><span class='token' style='color: #ec7754'>&nbsp;named<span class='tooltip'>Token: ' named'<br>Causal Loss: 8.43244<br>---------------<br>0.00 : ' unt'<br>0.00 : 'bble'<br>0.00 : ' upset'<br>0.00 : '\u001e",
       "'<br>0.00 : ' okay'<br>0.00 : 'Yes'<br>0.00 : '�'<br>0.00 : ' makes'<br>0.00 : ' beaut'<br>0.00 : ' give'<br></span></span><span class='token' style='color: #9e199c'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 7.45037<br>---------------<br>0.00 : '�'<br>0.00 : ' enough'<br>0.00 : 'om'<br>0.00 : ' se'<br>0.00 : '�'<br>0.00 : ' u'<br>0.00 : ' and'<br>0.00 : ' mine'<br>0.00 : 'H'<br>0.00 : 'itt'<br></span></span><span class='token' style='color: #c94579'>&nbsp;found<span class='tooltip'>Token: ' found'<br>Causal Loss: 7.91549<br>---------------<br>0.00 : ' mess'<br>0.00 : 'p'<br>0.00 : ' din'<br>0.00 : ' scared'<br>0.00 : ' higher'<br>0.00 : ' stand'<br>0.00 : ' an'<br>0.00 : 'avy'<br>0.00 : ' monkey'<br>0.00 : ' g'<br></span></span><span class='token' style='color: #c8447a'>&nbsp;a<span class='tooltip'>Token: ' a'<br>Causal Loss: 7.90421<br>---------------<br>0.00 : 'We'<br>0.00 : 'os'<br>0.00 : 'maz'<br>0.00 : ' bar'<br>0.00 : ' pen'<br>0.00 : ' M'<br>0.00 : ' would'<br>0.00 : 'ter'<br>0.00 : ' making'<br>0.00 : 'ased'<br></span></span><span class='token' style='color: #c94579'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 7.91376<br>---------------<br>0.00 : ' monster'<br>0.00 : ' fall'<br>0.00 : ' feels'<br>0.00 : '�'<br>0.00 : '�'<br>0.00 : '\u0002'<br>0.00 : ' block'<br>0.00 : 'ired'<br>0.00 : 'ind'<br>0.00 : ' th'<br></span></span><span class='token' style='color: #ac2593'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 7.58101<br>---------------<br>0.00 : 's'<br>0.00 : ' light'<br>0.00 : ' un'<br>0.00 : 'ier'<br>0.00 : ' night'<br>0.00 : ' reach'<br>0.00 : 'my'<br>0.00 : 'aisy'<br>0.00 : ' watch'<br>0.00 : 'ou'<br></span></span><span class='token' style='color: #e16560'>&nbsp;in<span class='tooltip'>Token: ' in'<br>Causal Loss: 8.25138<br>---------------<br>0.00 : ' boy'<br>0.00 : ' b'<br>0.00 : 'ier'<br>0.00 : ' part'<br>0.00 : ' strange'<br>0.00 : ' big'<br>0.00 : 'wn'<br>0.00 : ' Sarah'<br>0.00 : ' z'<br>0.00 : ' farm'<br></span></span><span class='token' style='color: #47029f'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 6.75522<br>---------------<br>0.00 : 'own'<br>0.00 : 'orm'<br>0.00 : '�'<br>0.00 : ' strange'<br>0.00 : 'ot'<br>0.00 : 'ient'<br>0.00 : ' through'<br>0.00 : ' home'<br>0.00 : ' cu'<br>0.00 : 'les'<br></span></span><span class='token' style='color: #9f1a9b'>&nbsp;room<span class='tooltip'>Token: ' room'<br>Causal Loss: 7.46085<br>---------------<br>0.00 : ' Spot'<br>0.00 : 've'<br>0.00 : 'om'<br>0.00 : ' that'<br>0.00 : '�'<br>0.00 : ' Everyone'<br>0.00 : 'ear'<br>0.00 : 'J'<br>0.00 : ' cold'<br>0.00 : 'own'<br></span></span><span class='token' style='color: #fcad31'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 8.91486<br>---------------<br>0.00 : ' backyard'<br>0.00 : '�'<br>0.00 : ' neigh'<br>0.00 : 'bbed'<br>0.00 : ' others'<br>0.00 : '�'<br>0.00 : ' side'<br>0.00 : '.\"'<br>0.00 : ' In'<br>0.00 : 'ies'<br></span></span><span class='token' style='color: #b93388'>&nbsp;She<span class='tooltip'>Token: ' She'<br>Causal Loss: 7.72166<br>---------------<br>0.00 : 'orrow'<br>0.00 : ' you'<br>0.00 : ' always'<br>0.00 : ' games'<br>0.00 : ' hungry'<br>0.00 : ' angry'<br>0.00 : 'iced'<br>0.00 : ' is'<br>0.00 : ' dolls'<br>0.00 : '!'<br></span></span><span class='token' style='color: #db5b67'>&nbsp;knew<span class='tooltip'>Token: ' knew'<br>Causal Loss: 8.15418<br>---------------<br>0.00 : ' resp'<br>0.00 : ' somet'<br>0.00 : 'But'<br>0.00 : 'bo'<br>0.00 : '0'<br>0.00 : ' lonely'<br>0.00 : 'Finally'<br>0.00 : '�'<br>0.00 : ' loo'<br>0.00 : ' fi'<br></span></span><span class='token' style='color: #e66d5a'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 8.34281<br>---------------<br>0.00 : ' full'<br>0.00 : 'iny'<br>0.00 : 'bbit'<br>0.00 : 'cket'<br>0.00 : 'or'<br>0.00 : '+'<br>0.00 : 'aring'<br>0.00 : ' pre'<br>0.00 : ' special'<br>0.00 : ' moral'<br></span></span><span class='token' style='color: #d8586a'>&nbsp;was<span class='tooltip'>Token: ' was'<br>Causal Loss: 8.12724<br>---------------<br>0.00 : ' share'<br>0.00 : 'le'<br>0.00 : 't'<br>0.00 : '�'<br>0.00 : ' cr'<br>0.00 : 'Thank'<br>0.00 : ' make'<br>0.00 : ' morning'<br>0.00 : ' bow'<br>0.00 : ' magical'<br></span></span><span class='token' style='color: #eff821'>&nbsp;dif<span class='tooltip'>Token: ' dif'<br>Causal Loss: 9.47562<br>---------------<br>0.00 : ' adventure'<br>0.00 : 'ush'<br>0.00 : ' doing'<br>0.00 : ' l'<br>0.00 : ' car'<br>0.00 : 'er'<br>0.00 : '�'<br>0.00 : 'lease'<br>0.00 : 'Oh'<br>0.00 : ' wor'<br></span></span><span class='token' style='color: #7601a8'>f<span class='tooltip'>Token: 'f'<br>Causal Loss: 7.11330<br>---------------<br>0.00 : 'Then'<br>0.00 : 'Tom'<br>0.00 : ' pass'<br>0.00 : ' mum'<br>0.00 : ' remembered'<br>0.00 : ' pap'<br>0.00 : 'ken'<br>0.00 : 'ained'<br>0.00 : ' move'<br>0.00 : ' another'<br></span></span><span class='token' style='color: #9f1a9b'>ic<span class='tooltip'>Token: 'ic'<br>Causal Loss: 7.45750<br>---------------<br>0.00 : 'ittle'<br>0.00 : 'riend'<br>0.00 : ' stayed'<br>0.00 : ''<br>0.00 : ' played'<br>0.00 : ' hungry'<br>0.00 : 'bow'<br>0.00 : ' money'<br>0.00 : ' bir'<br>0.00 : ' ran'<br></span></span><span class='token' style='color: #ae2791'>ul<span class='tooltip'>Token: 'ul'<br>Causal Loss: 7.60691<br>---------------<br>0.00 : '7'<br>0.00 : 'oud'<br>0.00 : ' beh'<br>0.00 : ' try'<br>0.00 : '\f",
       "'<br>0.00 : 'Sure'<br>0.00 : ' scary'<br>0.00 : ' listen'<br>0.00 : '�'<br>0.00 : ' left'<br></span></span><span class='token' style='color: #8405a6'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 7.22762<br>---------------<br>0.00 : 'ucky'<br>0.00 : ' window'<br>0.00 : ' them'<br>0.00 : 'llow'<br>0.00 : ' shouted'<br>0.00 : '�'<br>0.00 : ' nearby'<br>0.00 : ' having'<br>0.00 : ' re'<br>0.00 : 'phant'<br></span></span><span class='token' style='color: #ba3487'>&nbsp;to<span class='tooltip'>Token: ' to'<br>Causal Loss: 7.74136<br>---------------<br>0.00 : ' gone'<br>0.00 : ' bowl'<br>0.00 : 'Mia'<br>0.00 : ' not'<br>0.00 : ' after'<br>0.00 : 'Her'<br>0.00 : 'ued'<br>0.00 : 'hing'<br>0.00 : ' see'<br>0.00 : 'ious'<br></span></span><span class='token' style='color: #af2890'>&nbsp;play<span class='tooltip'>Token: ' play'<br>Causal Loss: 7.62460<br>---------------<br>0.00 : ' went'<br>0.00 : 'bo'<br>0.00 : ' stories'<br>0.00 : 'Tim'<br>0.00 : 'ank'<br>0.00 : '�'<br>0.00 : ' music'<br>0.00 : ' fav'<br>0.00 : 'ver'<br>0.00 : 'oup'<br></span></span><span class='token' style='color: #250591'>&nbsp;with<span class='tooltip'>Token: ' with'<br>Causal Loss: 6.52493<br>---------------<br>0.00 : '\t'<br>0.00 : 'at'<br>0.00 : 'ect'<br>0.00 : ' make'<br>0.00 : '|'<br>0.00 : ' dif'<br>0.00 : ' ice'<br>0.00 : ' t'<br>0.00 : ' sho'<br>0.00 : 'ia'<br></span></span><span class='token' style='color: #9511a1'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 7.36593<br>---------------<br>0.00 : ' light'<br>0.00 : 'les'<br>0.00 : '('<br>0.00 : ' de'<br>0.00 : ' course'<br>0.00 : ' bag'<br>0.00 : 'ter'<br>0.00 : ' where'<br>0.00 : ' cra'<br>0.00 : ' favor'<br></span></span><span class='token' style='color: #e87158'>&nbsp;because<span class='tooltip'>Token: ' because'<br>Causal Loss: 8.37841<br>---------------<br>0.00 : ' share'<br>0.00 : 'le'<br>0.00 : 't'<br>0.00 : '�'<br>0.00 : ' cr'<br>0.00 : 'Thank'<br>0.00 : ' make'<br>0.00 : ' morning'<br>0.00 : ' bow'<br>0.00 : ' magical'<br></span></span><span class='token' style='color: #ec7853'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 8.44310<br>---------------<br>0.00 : ' arm'<br>0.00 : '�'<br>0.00 : ' happened'<br>0.00 : 'h'<br>0.00 : ' must'<br>0.00 : ' G'<br>0.00 : ' fro'<br>0.00 : 'oun'<br>0.00 : 'o'<br>0.00 : ' Fl'<br></span></span><span class='token' style='color: #d8586a'>&nbsp;was<span class='tooltip'>Token: ' was'<br>Causal Loss: 8.12724<br>---------------<br>0.00 : ' share'<br>0.00 : 'le'<br>0.00 : 't'<br>0.00 : '�'<br>0.00 : ' cr'<br>0.00 : 'Thank'<br>0.00 : ' make'<br>0.00 : ' morning'<br>0.00 : ' bow'<br>0.00 : ' magical'<br></span></span><span class='token' style='color: #7b02a8'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 7.14962<br>---------------<br>0.00 : ' adventure'<br>0.00 : 'ush'<br>0.00 : ' doing'<br>0.00 : ' l'<br>0.00 : ' car'<br>0.00 : 'er'<br>0.00 : '�'<br>0.00 : 'lease'<br>0.00 : 'Oh'<br>0.00 : ' wor'<br></span></span><span class='token' style='color: #be3883'>ar<span class='tooltip'>Token: 'ar'<br>Causal Loss: 7.79170<br>---------------<br>0.00 : 'asure'<br>0.00 : ' tw'<br>0.00 : ' butter'<br>0.00 : 'hy'<br>0.00 : ' pic'<br>0.00 : ' Can'<br>0.00 : ' hot'<br>0.00 : ' star'<br>0.00 : ' hand'<br>0.00 : ' Dad'<br></span></span><span class='token' style='color: #9511a1'>p<span class='tooltip'>Token: 'p'<br>Causal Loss: 7.37039<br>---------------<br>0.00 : ' pretend'<br>0.00 : ' min'<br>0.00 : ' Sp'<br>0.00 : ' It'<br>0.00 : ' strong'<br>0.00 : ' sa'<br>0.00 : 'Ben'<br>0.00 : ' F'<br>0.00 : '='<br>0.00 : ' owl'<br></span></span><span class='token' style='color: #bd3784'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 7.77006<br>---------------<br>0.00 : 'fe'<br>0.00 : ' be'<br>0.00 : ' ag'<br>0.00 : 'Tom'<br>0.00 : 'ared'<br>0.00 : ' said'<br>0.00 : 'iny'<br>0.00 : 'ph'<br>0.00 : 'ouse'<br>0.00 : ' T'<br></span></span><span class='token' style='color: #b52e8c'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 7.67319<br>---------------<br>0.00 : 'orrow'<br>0.00 : ' you'<br>0.00 : ' always'<br>0.00 : ' games'<br>0.00 : ' hungry'<br>0.00 : ' angry'<br>0.00 : 'iced'<br>0.00 : ' is'<br>0.00 : ' dolls'<br>0.00 : '!'<br></span></span><span class='token' style='color: #df6163'>&nbsp;wanted<span class='tooltip'>Token: ' wanted'<br>Causal Loss: 8.21969<br>---------------<br>0.00 : ' mess'<br>0.00 : 'p'<br>0.00 : ' din'<br>0.00 : ' scared'<br>0.00 : ' higher'<br>0.00 : ' stand'<br>0.00 : ' an'<br>0.00 : 'avy'<br>0.00 : ' monkey'<br>0.00 : ' g'<br></span></span><span class='token' style='color: #ee7c50'>&nbsp;to<span class='tooltip'>Token: ' to'<br>Causal Loss: 8.48245<br>---------------<br>0.00 : ' needed'<br>0.00 : ' stars'<br>0.00 : ' say'<br>0.00 : ':'<br>0.00 : ' sad'<br>0.00 : ' This'<br>0.00 : ' butterf'<br>0.00 : '8'<br>0.00 : ' amaz'<br>0.00 : 'ret'<br></span></span><span class='token' style='color: #a82296'>&nbsp;share<span class='tooltip'>Token: ' share'<br>Causal Loss: 7.54634<br>---------------<br>0.00 : ' went'<br>0.00 : 'bo'<br>0.00 : ' stories'<br>0.00 : 'Tim'<br>0.00 : 'ank'<br>0.00 : '�'<br>0.00 : ' music'<br>0.00 : ' fav'<br>0.00 : 'ver'<br>0.00 : 'oup'<br></span></span><span class='token' style='color: #f58b46'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 8.62286<br>---------------<br>0.00 : 'gan'<br>0.00 : '\u0003'<br>0.00 : ' surpr'<br>0.00 : ' They'<br>0.00 : 'aughty'<br>0.00 : ' games'<br>0.00 : 'hy'<br>0.00 : 'Max'<br>0.00 : ' door'<br>0.00 : ' perfect'<br></span></span><span class='token' style='color: #3b039a'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 6.66402<br>---------------<br>0.00 : 'sp'<br>0.00 : ' ball'<br>0.00 : ' climb'<br>0.00 : 'Then'<br>0.00 : 'el'<br>0.00 : ' young'<br>0.00 : '�'<br>0.00 : ' gone'<br>0.00 : ' near'<br>0.00 : ' rock'<br></span></span><span class='token' style='color: #ac2593'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 7.58101<br>---------------<br>0.00 : 's'<br>0.00 : ' light'<br>0.00 : ' un'<br>0.00 : 'ier'<br>0.00 : ' night'<br>0.00 : ' reach'<br>0.00 : 'my'<br>0.00 : 'aisy'<br>0.00 : ' watch'<br>0.00 : 'ou'<br></span></span><span class='token' style='color: #b93388'>&nbsp;with<span class='tooltip'>Token: ' with'<br>Causal Loss: 7.72584<br>---------------<br>0.00 : ' boy'<br>0.00 : ' b'<br>0.00 : 'ier'<br>0.00 : ' part'<br>0.00 : ' strange'<br>0.00 : ' big'<br>0.00 : 'wn'<br>0.00 : ' Sarah'<br>0.00 : ' z'<br>0.00 : ' farm'<br></span></span><span class='token' style='color: #7300a8'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 7.08537<br>---------------<br>0.00 : ' light'<br>0.00 : 'les'<br>0.00 : '('<br>0.00 : ' de'<br>0.00 : ' course'<br>0.00 : ' bag'<br>0.00 : 'ter'<br>0.00 : ' where'<br>0.00 : ' cra'<br>0.00 : ' favor'<br></span></span><span class='token' style='color: #8807a5'>&nbsp;mom<span class='tooltip'>Token: ' mom'<br>Causal Loss: 7.25993<br>---------------<br>0.00 : ' Spot'<br>0.00 : 've'<br>0.00 : 'om'<br>0.00 : ' that'<br>0.00 : '�'<br>0.00 : ' Everyone'<br>0.00 : 'ear'<br>0.00 : 'J'<br>0.00 : ' cold'<br>0.00 : 'own'<br></span></span><span class='token' style='color: #ba3487'>,<span class='tooltip'>Token: ','<br>Causal Loss: 7.73972<br>---------------<br>0.00 : ' yard'<br>0.00 : ' have'<br>0.00 : '�'<br>0.00 : 'zz'<br>0.00 : 'us'<br>0.00 : 'iver'<br>0.00 : ''t'<br>0.00 : 'Hello'<br>0.00 : 'ph'<br>0.00 : ' dis'<br></span></span><span class='token' style='color: #5f00a6'>&nbsp;so<span class='tooltip'>Token: ' so'<br>Causal Loss: 6.93017<br>---------------<br>0.00 : ' spr'<br>0.00 : ' and'<br>0.00 : ' strong'<br>0.00 : ' bo'<br>0.00 : ' tail'<br>0.00 : 'ay'<br>0.00 : ' off'<br>0.00 : ' things'<br>0.00 : ' had'<br>0.00 : 'king'<br></span></span><span class='token' style='color: #f9d924'>&nbsp;she<span class='tooltip'>Token: ' she'<br>Causal Loss: 9.25674<br>---------------<br>0.00 : '�'<br>0.00 : ' through'<br>0.00 : ' exc'<br>0.00 : 'lly'<br>0.00 : ' everyone'<br>0.00 : 'orrow'<br>0.00 : 'oun'<br>0.00 : ' stepped'<br>0.00 : ' Her'<br>0.00 : ' Mama'<br></span></span><span class='token' style='color: #9f1a9b'>&nbsp;could<span class='tooltip'>Token: ' could'<br>Causal Loss: 7.46031<br>---------------<br>0.00 : ' important'<br>0.00 : ' lonely'<br>0.00 : ' and'<br>0.00 : ' walking'<br>0.00 : ' n'<br>0.00 : ' ask'<br>0.00 : ' dis'<br>0.00 : ' play'<br>0.00 : ' hear'<br>0.00 : ' funny'<br></span></span><span class='token' style='color: #ec7853'>&nbsp;se<span class='tooltip'>Token: ' se'<br>Causal Loss: 8.44212<br>---------------<br>0.00 : ' ad'<br>0.00 : ' gi'<br>0.00 : 'ater'<br>0.00 : '/'<br>0.00 : ' part'<br>0.00 : 'les'<br>0.00 : ' hot'<br>0.00 : 'dd'<br>0.00 : 'ather'<br>0.00 : ' when'<br></span></span><span class='token' style='color: #8506a6'>w<span class='tooltip'>Token: 'w'<br>Causal Loss: 7.23301<br>---------------<br>0.00 : ' him'<br>0.00 : ' D'<br>0.00 : ' wr'<br>0.00 : '�'<br>0.00 : ' gr'<br>0.00 : 'ways'<br>0.00 : ' hurt'<br>0.00 : ' sleep'<br>0.00 : ' beaut'<br>0.00 : ' kind'<br></span></span><span class='token' style='color: #fca635'>&nbsp;a<span class='tooltip'>Token: ' a'<br>Causal Loss: 8.86104<br>---------------<br>0.00 : 'F'<br>0.00 : ''re'<br>0.00 : 'nder'<br>0.00 : ' bigger'<br>0.00 : 'one'<br>0.00 : 'ough'<br>0.00 : ' nodded'<br>0.00 : 'As'<br>0.00 : 'ittle'<br>0.00 : 'ber'<br></span></span><span class='token' style='color: #5601a3'>&nbsp;butt<span class='tooltip'>Token: ' butt'<br>Causal Loss: 6.86542<br>---------------<br>0.00 : ' monster'<br>0.00 : ' fall'<br>0.00 : ' feels'<br>0.00 : '�'<br>0.00 : '�'<br>0.00 : '\u0002'<br>0.00 : ' block'<br>0.00 : 'ired'<br>0.00 : 'ind'<br>0.00 : ' th'<br></span></span><span class='token' style='color: #d04d73'>on<span class='tooltip'>Token: 'on'<br>Causal Loss: 8.00245<br>---------------<br>0.01 : 'No'<br>0.00 : ' bad'<br>0.00 : 'But'<br>0.00 : 'oug'<br>0.00 : ' wonder'<br>0.00 : ' do'<br>0.00 : ' hopped'<br>0.00 : ' boy'<br>0.00 : ' is'<br>0.00 : ' land'<br></span></span><span class='token' style='color: #f8963f'>&nbsp;on<span class='tooltip'>Token: ' on'<br>Causal Loss: 8.72623<br>---------------<br>0.00 : ' kitchen'<br>0.00 : '�'<br>0.00 : ''m'<br>0.00 : ' got'<br>0.00 : ' shouted'<br>0.00 : ' asks'<br>0.00 : 'ase'<br>0.00 : ' man'<br>0.00 : ' thought'<br>0.00 : '�'<br></span></span><span class='token' style='color: #e16560'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 8.26149<br>---------------<br>0.00 : ' find'<br>0.00 : 'bb'<br>0.00 : 'kes'<br>0.00 : 'her'<br>0.00 : ' game'<br>0.00 : ' magic'<br>0.00 : '�'<br>0.00 : ' ar'<br>0.00 : ''<br>0.00 : ' grandma'<br></span></span><span class='token' style='color: #9b179e'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 7.42442<br>---------------<br>0.00 : ' Spot'<br>0.00 : 've'<br>0.00 : 'om'<br>0.00 : ' that'<br>0.00 : '�'<br>0.00 : ' Everyone'<br>0.00 : 'ear'<br>0.00 : 'J'<br>0.00 : ' cold'<br>0.00 : 'own'<br></span></span><span class='token' style='color: #d25070'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 8.04395<br>---------------<br>0.00 : 'asure'<br>0.00 : ' tw'<br>0.00 : ' butter'<br>0.00 : 'hy'<br>0.00 : ' pic'<br>0.00 : ' Can'<br>0.00 : ' hot'<br>0.00 : ' star'<br>0.00 : ' hand'<br>0.00 : ' Dad'<br></span></span><span class='token' style='color: #a31d99'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 7.50294<br>---------------<br>0.00 : ' mail'<br>0.00 : ' cars'<br>0.00 : 'orn'<br>0.00 : 'ey'<br>0.00 : ' shap'<br>0.00 : ' really'<br>0.00 : ' bit'<br>0.00 : 'ear'<br>0.00 : 'aughter'<br>0.00 : ' blanket'<br></span></span><span class='token' style='color: #8405a6'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 7.21950<br>---------------<br>0.00 : ' gone'<br>0.00 : ' bowl'<br>0.00 : 'Mia'<br>0.00 : ' not'<br>0.00 : ' after'<br>0.00 : 'Her'<br>0.00 : 'ued'<br>0.00 : 'hing'<br>0.00 : ' see'<br>0.00 : 'ious'<br></span></span><br><br><span class='token' style='color: #af2890'>Lily<span class='tooltip'>Token: 'Lily'<br>Causal Loss: 7.62473<br>---------------<br>0.00 : ' had'<br>0.00 : 'ced'<br>0.00 : ' fish'<br>0.00 : ' But'<br>0.00 : 'urp'<br>0.00 : ' dream'<br>0.00 : '�'<br>0.00 : 'im'<br>0.00 : ' tail'<br>0.00 : ' a'<br></span></span><span class='token' style='color: #8908a5'>&nbsp;went<span class='tooltip'>Token: ' went'<br>Causal Loss: 7.27055<br>---------------<br>0.00 : ' pocket'<br>0.00 : 'ice'<br>0.00 : ' se'<br>0.00 : ' shared'<br>0.00 : ' own'<br>0.00 : ' All'<br>0.00 : 'ect'<br>0.00 : ' There'<br>0.00 : ' didn'<br>0.00 : ' blocks'<br></span></span><span class='token' style='color: #9e199c'>&nbsp;to<span class='tooltip'>Token: ' to'<br>Causal Loss: 7.44679<br>---------------<br>0.00 : ' Every'<br>0.00 : ' closer'<br>0.00 : ' It'<br>0.00 : '\u0014'<br>0.00 : 'ir'<br>0.00 : ' squ'<br>0.00 : 'lew'<br>0.00 : 'ak'<br>0.00 : ' very'<br>0.00 : 'iting'<br></span></span><span class='token' style='color: #df6163'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 8.22466<br>---------------<br>0.00 : ' went'<br>0.00 : 'bo'<br>0.00 : ' stories'<br>0.00 : 'Tim'<br>0.00 : 'ank'<br>0.00 : '�'<br>0.00 : ' music'<br>0.00 : ' fav'<br>0.00 : 'ver'<br>0.00 : 'oup'<br></span></span><span class='token' style='color: #8807a5'>&nbsp;mom<span class='tooltip'>Token: ' mom'<br>Causal Loss: 7.25993<br>---------------<br>0.00 : ' Spot'<br>0.00 : 've'<br>0.00 : 'om'<br>0.00 : ' that'<br>0.00 : '�'<br>0.00 : ' Everyone'<br>0.00 : 'ear'<br>0.00 : 'J'<br>0.00 : ' cold'<br>0.00 : 'own'<br></span></span><span class='token' style='color: #a82296'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 7.54176<br>---------------<br>0.00 : ' yard'<br>0.00 : ' have'<br>0.00 : '�'<br>0.00 : 'zz'<br>0.00 : 'us'<br>0.00 : 'iver'<br>0.00 : ''t'<br>0.00 : 'Hello'<br>0.00 : 'ph'<br>0.00 : ' dis'<br></span></span><span class='token' style='color: #af2890'>&nbsp;said<span class='tooltip'>Token: ' said'<br>Causal Loss: 7.62170<br>---------------<br>0.00 : ' con'<br>0.00 : ' grow'<br>0.00 : '�'<br>0.00 : 'yard'<br>0.00 : ' picture'<br>0.00 : ' ra'<br>0.00 : ' And'<br>0.00 : ' tow'<br>0.00 : 'J'<br>0.00 : 'ht'<br></span></span><span class='token' style='color: #ae2791'>,<span class='tooltip'>Token: ','<br>Causal Loss: 7.60953<br>---------------<br>0.00 : ' att'<br>0.00 : ' hopped'<br>0.00 : 'ved'<br>0.00 : 'Sure'<br>0.00 : ' see'<br>0.00 : ' din'<br>0.00 : ' relie'<br>0.00 : 'W'<br>0.00 : 'ied'<br>0.00 : ' opened'<br></span></span><span class='token' style='color: #be3883'>&nbsp;\"<span class='tooltip'>Token: ' \"'<br>Causal Loss: 7.79137<br>---------------<br>0.00 : ' spr'<br>0.00 : ' and'<br>0.00 : ' strong'<br>0.00 : ' bo'<br>0.00 : ' tail'<br>0.00 : 'ay'<br>0.00 : ' off'<br>0.00 : ' things'<br>0.00 : ' had'<br>0.00 : 'king'<br></span></span><span class='token' style='color: #b02a8f'>Mom<span class='tooltip'>Token: 'Mom'<br>Causal Loss: 7.63493<br>---------------<br>0.00 : ' three'<br>0.00 : ' okay'<br>0.00 : '�'<br>0.00 : ' teddy'<br>0.00 : ' ground'<br>0.00 : 'oth'<br>0.00 : ' learned'<br>0.00 : ' break'<br>0.00 : 'oup'<br>0.00 : ' dragon'<br></span></span><span class='token' style='color: #7f03a7'>,<span class='tooltip'>Token: ','<br>Causal Loss: 7.19147<br>---------------<br>0.00 : ' fi'<br>0.00 : 'OK'<br>0.00 : ' many'<br>0.00 : ' cle'<br>0.00 : 'k'<br>0.00 : ' do'<br>0.00 : 'ecial'<br>0.00 : ' part'<br>0.00 : ' b'<br>0.00 : ' su'<br></span></span><span class='token' style='color: #f38748'>&nbsp;I<span class='tooltip'>Token: ' I'<br>Causal Loss: 8.59272<br>---------------<br>0.00 : ' spr'<br>0.00 : ' and'<br>0.00 : ' strong'<br>0.00 : ' bo'<br>0.00 : ' tail'<br>0.00 : 'ay'<br>0.00 : ' off'<br>0.00 : ' things'<br>0.00 : ' had'<br>0.00 : 'king'<br></span></span><span class='token' style='color: #cd4975'>&nbsp;found<span class='tooltip'>Token: ' found'<br>Causal Loss: 7.96757<br>---------------<br>0.00 : ' wasn'<br>0.00 : 'ings'<br>0.00 : ' wall'<br>0.00 : ' began'<br>0.00 : ' carrot'<br>0.00 : '&gt;'<br>0.00 : 'able'<br>0.00 : 'Finally'<br>0.00 : 'ined'<br>0.00 : ' garden'<br></span></span><span class='token' style='color: #f68f43'>&nbsp;this<span class='tooltip'>Token: ' this'<br>Causal Loss: 8.66191<br>---------------<br>0.00 : 'We'<br>0.00 : 'os'<br>0.00 : 'maz'<br>0.00 : ' bar'<br>0.00 : ' pen'<br>0.00 : ' M'<br>0.00 : ' would'<br>0.00 : 'ter'<br>0.00 : ' making'<br>0.00 : 'ased'<br></span></span><span class='token' style='color: #e97257'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 8.39291<br>---------------<br>0.00 : ' bigger'<br>0.00 : ' surprised'<br>0.00 : ' neck'<br>0.00 : ' parents'<br>0.00 : 'able'<br>0.00 : ' Amy'<br>0.00 : 'very'<br>0.00 : ' shouted'<br>0.00 : 'ug'<br>0.00 : ' count'<br></span></span><span class='token' style='color: #ac2593'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 7.58101<br>---------------<br>0.00 : 's'<br>0.00 : ' light'<br>0.00 : ' un'<br>0.00 : 'ier'<br>0.00 : ' night'<br>0.00 : ' reach'<br>0.00 : 'my'<br>0.00 : 'aisy'<br>0.00 : ' watch'<br>0.00 : 'ou'<br></span></span><span class='token' style='color: #8807a5'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 7.25593<br>---------------<br>0.00 : ' boy'<br>0.00 : ' b'<br>0.00 : 'ier'<br>0.00 : ' part'<br>0.00 : ' strange'<br>0.00 : ' big'<br>0.00 : 'wn'<br>0.00 : ' Sarah'<br>0.00 : ' z'<br>0.00 : ' farm'<br></span></span><span class='token' style='color: #ef7d4f'>&nbsp;Can<span class='tooltip'>Token: ' Can'<br>Causal Loss: 8.49385<br>---------------<br>0.00 : 'orrow'<br>0.00 : ' you'<br>0.00 : ' always'<br>0.00 : ' games'<br>0.00 : ' hungry'<br>0.00 : ' angry'<br>0.00 : 'iced'<br>0.00 : ' is'<br>0.00 : ' dolls'<br>0.00 : '!'<br></span></span><span class='token' style='color: #df6262'>&nbsp;you<span class='tooltip'>Token: ' you'<br>Causal Loss: 8.23186<br>---------------<br>0.00 : ' pie'<br>0.00 : 'her'<br>0.00 : ' gent'<br>0.00 : ' green'<br>0.00 : 'ever'<br>0.00 : ' squ'<br>0.00 : ' okay'<br>0.00 : ' floor'<br>0.00 : 'ld'<br>0.00 : ' know'<br></span></span><span class='token' style='color: #ea7356'>&nbsp;share<span class='tooltip'>Token: ' share'<br>Causal Loss: 8.39486<br>---------------<br>0.00 : ' snack'<br>0.00 : '�'<br>0.00 : ' perfect'<br>0.00 : ' From'<br>0.00 : 'ass'<br>0.00 : ' get'<br>0.00 : ' else'<br>0.00 : 'What'<br>0.00 : ' does'<br>0.00 : ' idea'<br></span></span><span class='token' style='color: #f2844b'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 8.55647<br>---------------<br>0.00 : 'gan'<br>0.00 : '\u0003'<br>0.00 : ' surpr'<br>0.00 : ' They'<br>0.00 : 'aughty'<br>0.00 : ' games'<br>0.00 : 'hy'<br>0.00 : 'Max'<br>0.00 : ' door'<br>0.00 : ' perfect'<br></span></span><span class='token' style='color: #df6163'>&nbsp;with<span class='tooltip'>Token: ' with'<br>Causal Loss: 8.22304<br>---------------<br>0.00 : ' share'<br>0.00 : 'le'<br>0.00 : 't'<br>0.00 : '�'<br>0.00 : ' cr'<br>0.00 : 'Thank'<br>0.00 : ' make'<br>0.00 : ' morning'<br>0.00 : ' bow'<br>0.00 : ' magical'<br></span></span><span class='token' style='color: #e97257'>&nbsp;me<span class='tooltip'>Token: ' me'<br>Causal Loss: 8.38946<br>---------------<br>0.00 : ' light'<br>0.00 : 'les'<br>0.00 : '('<br>0.00 : ' de'<br>0.00 : ' course'<br>0.00 : ' bag'<br>0.00 : 'ter'<br>0.00 : ' where'<br>0.00 : ' cra'<br>0.00 : ' favor'<br></span></span><span class='token' style='color: #7801a8'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 7.13195<br>---------------<br>0.00 : 'irst'<br>0.00 : ' break'<br>0.00 : 'unny'<br>0.00 : ' acc'<br>0.00 : ' treasure'<br>0.00 : ' they'<br>0.00 : 'get'<br>0.00 : ' off'<br>0.00 : ' d'<br>0.00 : '�'<br></span></span><span class='token' style='color: #7200a8'>&nbsp;se<span class='tooltip'>Token: ' se'<br>Causal Loss: 7.07928<br>---------------<br>0.00 : ' con'<br>0.00 : ' grow'<br>0.00 : '�'<br>0.00 : 'yard'<br>0.00 : ' picture'<br>0.00 : ' ra'<br>0.00 : ' And'<br>0.00 : ' tow'<br>0.00 : 'J'<br>0.00 : 'ht'<br></span></span><span class='token' style='color: #8506a6'>w<span class='tooltip'>Token: 'w'<br>Causal Loss: 7.23301<br>---------------<br>0.00 : ' him'<br>0.00 : ' D'<br>0.00 : ' wr'<br>0.00 : '�'<br>0.00 : ' gr'<br>0.00 : 'ways'<br>0.00 : ' hurt'<br>0.00 : ' sleep'<br>0.00 : ' beaut'<br>0.00 : ' kind'<br></span></span><span class='token' style='color: #9b179e'>&nbsp;my<span class='tooltip'>Token: ' my'<br>Causal Loss: 7.43127<br>---------------<br>0.00 : 'F'<br>0.00 : ''re'<br>0.00 : 'nder'<br>0.00 : ' bigger'<br>0.00 : 'one'<br>0.00 : 'ough'<br>0.00 : ' nodded'<br>0.00 : 'As'<br>0.00 : 'ittle'<br>0.00 : 'ber'<br></span></span><span class='token' style='color: #d25070'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 8.03423<br>---------------<br>0.00 : ' su'<br>0.00 : '('<br>0.00 : ' wish'<br>0.00 : ' every'<br>0.00 : 'ts'<br>0.00 : ' became'<br>0.00 : ' loved'<br>0.00 : ' shook'<br>0.00 : 'thing'<br>0.00 : ' eating'<br></span></span><span class='token' style='color: #d25070'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 8.04395<br>---------------<br>0.00 : 'asure'<br>0.00 : ' tw'<br>0.00 : ' butter'<br>0.00 : 'hy'<br>0.00 : ' pic'<br>0.00 : ' Can'<br>0.00 : ' hot'<br>0.00 : ' star'<br>0.00 : ' hand'<br>0.00 : ' Dad'<br></span></span><span class='token' style='color: #a31d99'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 7.50294<br>---------------<br>0.00 : ' mail'<br>0.00 : ' cars'<br>0.00 : 'orn'<br>0.00 : 'ey'<br>0.00 : ' shap'<br>0.00 : ' really'<br>0.00 : ' bit'<br>0.00 : 'ear'<br>0.00 : 'aughter'<br>0.00 : ' blanket'<br></span></span><span class='token' style='color: #b52e8c'>?\"<span class='tooltip'>Token: '?\"'<br>Causal Loss: 7.67420<br>---------------<br>0.00 : ' gone'<br>0.00 : ' bowl'<br>0.00 : 'Mia'<br>0.00 : ' not'<br>0.00 : ' after'<br>0.00 : 'Her'<br>0.00 : 'ued'<br>0.00 : 'hing'<br>0.00 : ' see'<br>0.00 : 'ious'<br></span></span><span class='token' style='color: #e3675f'>&nbsp;Her<span class='tooltip'>Token: ' Her'<br>Causal Loss: 8.28144<br>---------------<br>0.00 : ' moved'<br>0.00 : ' farmer'<br>0.00 : '4'<br>0.00 : 'ner'<br>0.00 : '�'<br>0.00 : 'ble'<br>0.00 : ' squirrel'<br>0.00 : ' fi'<br>0.00 : ' ran'<br>0.00 : ' getting'<br></span></span><span class='token' style='color: #b22c8e'>&nbsp;mom<span class='tooltip'>Token: ' mom'<br>Causal Loss: 7.65623<br>---------------<br>0.00 : '�'<br>0.00 : ' box'<br>0.00 : ' able'<br>0.00 : 'bby'<br>0.00 : ' balloon'<br>0.00 : ' sea'<br>0.00 : 'etter'<br>0.00 : 'Wow'<br>0.00 : 'K'<br>0.00 : 'qu'<br></span></span><span class='token' style='color: #cb4777'>&nbsp;smiled<span class='tooltip'>Token: ' smiled'<br>Causal Loss: 7.94799<br>---------------<br>0.00 : ' yard'<br>0.00 : ' have'<br>0.00 : '�'<br>0.00 : 'zz'<br>0.00 : 'us'<br>0.00 : 'iver'<br>0.00 : ''t'<br>0.00 : 'Hello'<br>0.00 : 'ph'<br>0.00 : ' dis'<br></span></span><span class='token' style='color: #e16560'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 8.25984<br>---------------<br>0.00 : 'ther'<br>0.00 : 'row'<br>0.00 : ' buy'<br>0.00 : ' heal'<br>0.00 : ' hole'<br>0.00 : 'ry'<br>0.00 : ' whe'<br>0.00 : 'ake'<br>0.00 : ' Now'<br>0.00 : ' fav'<br></span></span><span class='token' style='color: #af2890'>&nbsp;said<span class='tooltip'>Token: ' said'<br>Causal Loss: 7.62170<br>---------------<br>0.00 : ' con'<br>0.00 : ' grow'<br>0.00 : '�'<br>0.00 : 'yard'<br>0.00 : ' picture'<br>0.00 : ' ra'<br>0.00 : ' And'<br>0.00 : ' tow'<br>0.00 : 'J'<br>0.00 : 'ht'<br></span></span><span class='token' style='color: #ae2791'>,<span class='tooltip'>Token: ','<br>Causal Loss: 7.60953<br>---------------<br>0.00 : ' att'<br>0.00 : ' hopped'<br>0.00 : 'ved'<br>0.00 : 'Sure'<br>0.00 : ' see'<br>0.00 : ' din'<br>0.00 : ' relie'<br>0.00 : 'W'<br>0.00 : 'ied'<br>0.00 : ' opened'<br></span></span><span class='token' style='color: #be3883'>&nbsp;\"<span class='tooltip'>Token: ' \"'<br>Causal Loss: 7.79137<br>---------------<br>0.00 : ' spr'<br>0.00 : ' and'<br>0.00 : ' strong'<br>0.00 : ' bo'<br>0.00 : ' tail'<br>0.00 : 'ay'<br>0.00 : ' off'<br>0.00 : ' things'<br>0.00 : ' had'<br>0.00 : 'king'<br></span></span><span class='token' style='color: #8204a7'>Yes<span class='tooltip'>Token: 'Yes'<br>Causal Loss: 7.21130<br>---------------<br>0.00 : ' three'<br>0.00 : ' okay'<br>0.00 : '�'<br>0.00 : ' teddy'<br>0.00 : ' ground'<br>0.00 : 'oth'<br>0.00 : ' learned'<br>0.00 : ' break'<br>0.00 : 'oup'<br>0.00 : ' dragon'<br></span></span><span class='token' style='color: #bf3982'>,<span class='tooltip'>Token: ','<br>Causal Loss: 7.80346<br>---------------<br>0.00 : ' That'<br>0.00 : ' really'<br>0.00 : 'ation'<br>0.00 : ' d'<br>0.00 : ' bro'<br>0.00 : '�'<br>0.00 : ' games'<br>0.00 : 'x'<br>0.00 : ' y'<br>0.00 : 'row'<br></span></span><span class='token' style='color: #d3516f'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 8.04695<br>---------------<br>0.00 : ' spr'<br>0.00 : ' and'<br>0.00 : ' strong'<br>0.00 : ' bo'<br>0.00 : ' tail'<br>0.00 : 'ay'<br>0.00 : ' off'<br>0.00 : ' things'<br>0.00 : ' had'<br>0.00 : 'king'<br></span></span><span class='token' style='color: #e26660'>,<span class='tooltip'>Token: ','<br>Causal Loss: 8.26713<br>---------------<br>0.00 : ' mess'<br>0.00 : 'p'<br>0.00 : ' din'<br>0.00 : ' scared'<br>0.00 : ' higher'<br>0.00 : ' stand'<br>0.00 : ' an'<br>0.00 : 'avy'<br>0.00 : ' monkey'<br>0.00 : ' g'<br></span></span><span class='token' style='color: #a82296'>&nbsp;we<span class='tooltip'>Token: ' we'<br>Causal Loss: 7.55029<br>---------------<br>0.00 : ' spr'<br>0.00 : ' and'<br>0.00 : ' strong'<br>0.00 : ' bo'<br>0.00 : ' tail'<br>0.00 : 'ay'<br>0.00 : ' off'<br>0.00 : ' things'<br>0.00 : ' had'<br>0.00 : 'king'<br></span></span><span class='token' style='color: #e46a5d'>&nbsp;can<span class='tooltip'>Token: ' can'<br>Causal Loss: 8.29826<br>---------------<br>0.00 : ' stu'<br>0.00 : '\u0014'<br>0.00 : 'ac'<br>0.00 : ' left'<br>0.00 : '1'<br>0.00 : ' red'<br>0.00 : ''<br>0.00 : ' quiet'<br>0.00 : ' plan'<br>0.00 : ' happily'<br></span></span><span class='token' style='color: #340498'>&nbsp;share<span class='tooltip'>Token: ' share'<br>Causal Loss: 6.61564<br>---------------<br>0.00 : 'ds'<br>0.00 : ' neigh'<br>0.00 : '�'<br>0.00 : 'iny'<br>0.00 : ' no'<br>0.00 : '#'<br>0.00 : ' dance'<br>0.00 : ' clo'<br>0.00 : ' pers'<br>0.00 : 'orn'<br></span></span><span class='token' style='color: #f58b46'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 8.62286<br>---------------<br>0.00 : 'gan'<br>0.00 : '\u0003'<br>0.00 : ' surpr'<br>0.00 : ' They'<br>0.00 : 'aughty'<br>0.00 : ' games'<br>0.00 : 'hy'<br>0.00 : 'Max'<br>0.00 : ' door'<br>0.00 : ' perfect'<br></span></span><span class='token' style='color: #3b039a'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 6.66402<br>---------------<br>0.00 : 'sp'<br>0.00 : ' ball'<br>0.00 : ' climb'<br>0.00 : 'Then'<br>0.00 : 'el'<br>0.00 : ' young'<br>0.00 : '�'<br>0.00 : ' gone'<br>0.00 : ' near'<br>0.00 : ' rock'<br></span></span><span class='token' style='color: #ac2593'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 7.58101<br>---------------<br>0.00 : 's'<br>0.00 : ' light'<br>0.00 : ' un'<br>0.00 : 'ier'<br>0.00 : ' night'<br>0.00 : ' reach'<br>0.00 : 'my'<br>0.00 : 'aisy'<br>0.00 : ' watch'<br>0.00 : 'ou'<br></span></span><span class='token' style='color: #9a159e'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 7.41741<br>---------------<br>0.00 : ' boy'<br>0.00 : ' b'<br>0.00 : 'ier'<br>0.00 : ' part'<br>0.00 : ' strange'<br>0.00 : ' big'<br>0.00 : 'wn'<br>0.00 : ' Sarah'<br>0.00 : ' z'<br>0.00 : ' farm'<br></span></span><span class='token' style='color: #8506a6'>&nbsp;fix<span class='tooltip'>Token: ' fix'<br>Causal Loss: 7.23855<br>---------------<br>0.00 : ' con'<br>0.00 : ' grow'<br>0.00 : '�'<br>0.00 : 'yard'<br>0.00 : ' picture'<br>0.00 : ' ra'<br>0.00 : ' And'<br>0.00 : ' tow'<br>0.00 : 'J'<br>0.00 : 'ht'<br></span></span><span class='token' style='color: #9e199c'>&nbsp;your<span class='tooltip'>Token: ' your'<br>Causal Loss: 7.45490<br>---------------<br>0.00 : '�'<br>0.00 : 'el'<br>0.00 : ' exc'<br>0.00 : 'are'<br>0.00 : ''<br>0.00 : ' home'<br>0.00 : ' sang'<br>0.00 : ' different'<br>0.00 : 'ucky'<br>0.00 : ' gl'<br></span></span><span class='token' style='color: #9310a1'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 7.35851<br>---------------<br>0.00 : ' about'<br>0.00 : ' fair'<br>0.00 : ' grass'<br>0.00 : 'l'<br>0.00 : 'gether'<br>0.00 : ' does'<br>0.00 : ' sees'<br>0.00 : ' wet'<br>0.00 : ' com'<br>0.00 : ' curious'<br></span></span><span class='token' style='color: #d25070'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 8.04395<br>---------------<br>0.00 : 'asure'<br>0.00 : ' tw'<br>0.00 : ' butter'<br>0.00 : 'hy'<br>0.00 : ' pic'<br>0.00 : ' Can'<br>0.00 : ' hot'<br>0.00 : ' star'<br>0.00 : ' hand'<br>0.00 : ' Dad'<br></span></span><span class='token' style='color: #a31d99'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 7.50294<br>---------------<br>0.00 : ' mail'<br>0.00 : ' cars'<br>0.00 : 'orn'<br>0.00 : 'ey'<br>0.00 : ' shap'<br>0.00 : ' really'<br>0.00 : ' bit'<br>0.00 : 'ear'<br>0.00 : 'aughter'<br>0.00 : ' blanket'<br></span></span><span class='token' style='color: #af2890'>.\"<span class='tooltip'>Token: '.\"'<br>Causal Loss: 7.62252<br>---------------<br>0.00 : ' gone'<br>0.00 : ' bowl'<br>0.00 : 'Mia'<br>0.00 : ' not'<br>0.00 : ' after'<br>0.00 : 'Her'<br>0.00 : 'ued'<br>0.00 : 'hing'<br>0.00 : ' see'<br>0.00 : 'ious'<br></span></span><br><br><span class='token' style='color: #920fa2'>T<span class='tooltip'>Token: 'T'<br>Causal Loss: 7.34104<br>---------------<br>0.00 : ' had'<br>0.00 : 'ced'<br>0.00 : ' fish'<br>0.00 : ' But'<br>0.00 : 'urp'<br>0.00 : ' dream'<br>0.00 : '�'<br>0.00 : 'im'<br>0.00 : ' tail'<br>0.00 : ' a'<br></span></span><span class='token' style='color: #ad2692'>o<span class='tooltip'>Token: 'o'<br>Causal Loss: 7.59592<br>---------------<br>0.00 : 'uit'<br>0.00 : ' sleep'<br>0.00 : ' j'<br>0.00 : ' hurt'<br>0.00 : ' magical'<br>0.00 : 'inally'<br>0.00 : ' sit'<br>0.00 : ' sw'<br>0.00 : ' woods'<br>0.00 : 'j'<br></span></span><span class='token' style='color: #8104a7'>gether<span class='tooltip'>Token: 'gether'<br>Causal Loss: 7.19967<br>---------------<br>0.00 : 'ained'<br>0.00 : 'ret'<br>0.00 : ' understand'<br>0.00 : ' door'<br>0.00 : ' repl'<br>0.00 : 'om'<br>0.00 : '&#39;'<br>0.00 : ' hold'<br>0.00 : ' me'<br>0.00 : ' butterf'<br></span></span><span class='token' style='color: #b62f8b'>,<span class='tooltip'>Token: ','<br>Causal Loss: 7.68775<br>---------------<br>0.00 : 'ila'<br>0.00 : '�'<br>0.00 : 'ent'<br>0.00 : ' trees'<br>0.00 : '\u000e'<br>0.00 : ' rock'<br>0.00 : ' moral'<br>0.00 : ' gent'<br>0.00 : '&quot;'<br>0.00 : ' stories'<br></span></span><span class='token' style='color: #da5a68'>&nbsp;they<span class='tooltip'>Token: ' they'<br>Causal Loss: 8.15279<br>---------------<br>0.00 : ' spr'<br>0.00 : ' and'<br>0.00 : ' strong'<br>0.00 : ' bo'<br>0.00 : ' tail'<br>0.00 : 'ay'<br>0.00 : ' off'<br>0.00 : ' things'<br>0.00 : ' had'<br>0.00 : 'king'<br></span></span><span class='token' style='color: #b62f8b'>&nbsp;shared<span class='tooltip'>Token: ' shared'<br>Causal Loss: 7.69060<br>---------------<br>0.00 : ' cake'<br>0.00 : '\u0014'<br>0.00 : 'Timmy'<br>0.00 : ' tree'<br>0.00 : ' sear'<br>0.00 : 'lf'<br>0.00 : ' fight'<br>0.00 : ' answ'<br>0.00 : ' same'<br>0.00 : 'ure'<br></span></span><span class='token' style='color: #ca4678'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 7.92654<br>---------------<br>0.00 : '�'<br>0.00 : 'Wow'<br>0.00 : '&quot;'<br>0.00 : '�'<br>0.00 : ' while'<br>0.00 : ' tow'<br>0.00 : ' G'<br>0.00 : ' tired'<br>0.00 : ' catch'<br>0.00 : 'i'<br></span></span><span class='token' style='color: #3b039a'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 6.66402<br>---------------<br>0.00 : 'sp'<br>0.00 : ' ball'<br>0.00 : ' climb'<br>0.00 : 'Then'<br>0.00 : 'el'<br>0.00 : ' young'<br>0.00 : '�'<br>0.00 : ' gone'<br>0.00 : ' near'<br>0.00 : ' rock'<br></span></span><span class='token' style='color: #ac2593'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 7.58101<br>---------------<br>0.00 : 's'<br>0.00 : ' light'<br>0.00 : ' un'<br>0.00 : 'ier'<br>0.00 : ' night'<br>0.00 : ' reach'<br>0.00 : 'my'<br>0.00 : 'aisy'<br>0.00 : ' watch'<br>0.00 : 'ou'<br></span></span><span class='token' style='color: #9a159e'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 7.41741<br>---------------<br>0.00 : ' boy'<br>0.00 : ' b'<br>0.00 : 'ier'<br>0.00 : ' part'<br>0.00 : ' strange'<br>0.00 : ' big'<br>0.00 : 'wn'<br>0.00 : ' Sarah'<br>0.00 : ' z'<br>0.00 : ' farm'<br></span></span><span class='token' style='color: #7200a8'>&nbsp;se<span class='tooltip'>Token: ' se'<br>Causal Loss: 7.07928<br>---------------<br>0.00 : ' con'<br>0.00 : ' grow'<br>0.00 : '�'<br>0.00 : 'yard'<br>0.00 : ' picture'<br>0.00 : ' ra'<br>0.00 : ' And'<br>0.00 : ' tow'<br>0.00 : 'J'<br>0.00 : 'ht'<br></span></span><span class='token' style='color: #8506a6'>w<span class='tooltip'>Token: 'w'<br>Causal Loss: 7.23301<br>---------------<br>0.00 : ' him'<br>0.00 : ' D'<br>0.00 : ' wr'<br>0.00 : '�'<br>0.00 : ' gr'<br>0.00 : 'ways'<br>0.00 : ' hurt'<br>0.00 : ' sleep'<br>0.00 : ' beaut'<br>0.00 : ' kind'<br></span></span><span class='token' style='color: #f68e44'>ed<span class='tooltip'>Token: 'ed'<br>Causal Loss: 8.64877<br>---------------<br>0.00 : 'F'<br>0.00 : ''re'<br>0.00 : 'nder'<br>0.00 : ' bigger'<br>0.00 : 'one'<br>0.00 : 'ough'<br>0.00 : ' nodded'<br>0.00 : 'As'<br>0.00 : 'ittle'<br>0.00 : 'ber'<br></span></span><span class='token' style='color: #db5b67'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 8.15844<br>---------------<br>0.00 : ' po'<br>0.00 : ' doing'<br>0.00 : 'ies'<br>0.00 : ' swim'<br>0.00 : ' ye'<br>0.00 : ' wings'<br>0.00 : ' yard'<br>0.00 : 'il'<br>0.00 : '�'<br>0.00 : ' taking'<br></span></span><span class='token' style='color: #ad2692'>&nbsp;butt<span class='tooltip'>Token: ' butt'<br>Causal Loss: 7.59577<br>---------------<br>0.00 : 'sp'<br>0.00 : ' ball'<br>0.00 : ' climb'<br>0.00 : 'Then'<br>0.00 : 'el'<br>0.00 : ' young'<br>0.00 : '�'<br>0.00 : ' gone'<br>0.00 : ' near'<br>0.00 : ' rock'<br></span></span><span class='token' style='color: #d04d73'>on<span class='tooltip'>Token: 'on'<br>Causal Loss: 8.00245<br>---------------<br>0.01 : 'No'<br>0.00 : ' bad'<br>0.00 : 'But'<br>0.00 : 'oug'<br>0.00 : ' wonder'<br>0.00 : ' do'<br>0.00 : ' hopped'<br>0.00 : ' boy'<br>0.00 : ' is'<br>0.00 : ' land'<br></span></span><span class='token' style='color: #f8963f'>&nbsp;on<span class='tooltip'>Token: ' on'<br>Causal Loss: 8.72623<br>---------------<br>0.00 : ' kitchen'<br>0.00 : '�'<br>0.00 : ''m'<br>0.00 : ' got'<br>0.00 : ' shouted'<br>0.00 : ' asks'<br>0.00 : 'ase'<br>0.00 : ' man'<br>0.00 : ' thought'<br>0.00 : '�'<br></span></span><span class='token' style='color: #b7308a'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 7.70225<br>---------------<br>0.00 : ' find'<br>0.00 : 'bb'<br>0.00 : 'kes'<br>0.00 : 'her'<br>0.00 : ' game'<br>0.00 : ' magic'<br>0.00 : '�'<br>0.00 : ' ar'<br>0.00 : ''<br>0.00 : ' grandma'<br></span></span><span class='token' style='color: #dc5d66'>'s<span class='tooltip'>Token: ''s'<br>Causal Loss: 8.17286<br>---------------<br>0.00 : ' mess'<br>0.00 : 'p'<br>0.00 : ' din'<br>0.00 : ' scared'<br>0.00 : ' higher'<br>0.00 : ' stand'<br>0.00 : ' an'<br>0.00 : 'avy'<br>0.00 : ' monkey'<br>0.00 : ' g'<br></span></span><span class='token' style='color: #7f03a7'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 7.18169<br>---------------<br>0.00 : 'cess'<br>0.00 : ' just'<br>0.00 : ' frog'<br>0.00 : ' amazing'<br>0.00 : ' yellow'<br>0.00 : ' began'<br>0.00 : ' pol'<br>0.00 : ' rest'<br>0.00 : ' brave'<br>0.00 : ' walking'<br></span></span><span class='token' style='color: #d25070'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 8.04395<br>---------------<br>0.00 : 'asure'<br>0.00 : ' tw'<br>0.00 : ' butter'<br>0.00 : 'hy'<br>0.00 : ' pic'<br>0.00 : ' Can'<br>0.00 : ' hot'<br>0.00 : ' star'<br>0.00 : ' hand'<br>0.00 : ' Dad'<br></span></span><span class='token' style='color: #a31d99'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 7.50294<br>---------------<br>0.00 : ' mail'<br>0.00 : ' cars'<br>0.00 : 'orn'<br>0.00 : 'ey'<br>0.00 : ' shap'<br>0.00 : ' really'<br>0.00 : ' bit'<br>0.00 : 'ear'<br>0.00 : 'aughter'<br>0.00 : ' blanket'<br></span></span><span class='token' style='color: #8405a6'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 7.21950<br>---------------<br>0.00 : ' gone'<br>0.00 : ' bowl'<br>0.00 : 'Mia'<br>0.00 : ' not'<br>0.00 : ' after'<br>0.00 : 'Her'<br>0.00 : 'ued'<br>0.00 : 'hing'<br>0.00 : ' see'<br>0.00 : 'ious'<br></span></span><span class='token' style='color: #6d00a8'>&nbsp;It<span class='tooltip'>Token: ' It'<br>Causal Loss: 7.03645<br>---------------<br>0.00 : 'orrow'<br>0.00 : ' you'<br>0.00 : ' always'<br>0.00 : ' games'<br>0.00 : ' hungry'<br>0.00 : ' angry'<br>0.00 : 'iced'<br>0.00 : ' is'<br>0.00 : ' dolls'<br>0.00 : '!'<br></span></span><span class='token' style='color: #0c0786'>&nbsp;was<span class='tooltip'>Token: ' was'<br>Causal Loss: 6.39891<br>---------------<br>0.00 : 'udden'<br>0.00 : 'room'<br>0.00 : ' did'<br>0.00 : '�'<br>0.00 : ' because'<br>0.00 : ' years'<br>0.00 : 'bby'<br>0.00 : ' Once'<br>0.00 : ' cars'<br>0.00 : ' loved'<br></span></span><span class='token' style='color: #b7308a'>&nbsp;not<span class='tooltip'>Token: ' not'<br>Causal Loss: 7.70462<br>---------------<br>0.00 : ' adventure'<br>0.00 : 'ush'<br>0.00 : ' doing'<br>0.00 : ' l'<br>0.00 : ' car'<br>0.00 : 'er'<br>0.00 : '�'<br>0.00 : 'lease'<br>0.00 : 'Oh'<br>0.00 : ' wor'<br></span></span><span class='token' style='color: #230590'>&nbsp;dif<span class='tooltip'>Token: ' dif'<br>Causal Loss: 6.51901<br>---------------<br>0.00 : ' tree'<br>0.00 : ' color'<br>0.00 : ' bright'<br>0.00 : ' snow'<br>0.00 : '�'<br>0.00 : ' owner'<br>0.00 : 'orm'<br>0.00 : ' moved'<br>0.00 : ' wouldn'<br>0.00 : ' chair'<br></span></span><span class='token' style='color: #7601a8'>f<span class='tooltip'>Token: 'f'<br>Causal Loss: 7.11330<br>---------------<br>0.00 : 'Then'<br>0.00 : 'Tom'<br>0.00 : ' pass'<br>0.00 : ' mum'<br>0.00 : ' remembered'<br>0.00 : ' pap'<br>0.00 : 'ken'<br>0.00 : 'ained'<br>0.00 : ' move'<br>0.00 : ' another'<br></span></span><span class='token' style='color: #9f1a9b'>ic<span class='tooltip'>Token: 'ic'<br>Causal Loss: 7.45750<br>---------------<br>0.00 : 'ittle'<br>0.00 : 'riend'<br>0.00 : ' stayed'<br>0.00 : ''<br>0.00 : ' played'<br>0.00 : ' hungry'<br>0.00 : 'bow'<br>0.00 : ' money'<br>0.00 : ' bir'<br>0.00 : ' ran'<br></span></span><span class='token' style='color: #ae2791'>ul<span class='tooltip'>Token: 'ul'<br>Causal Loss: 7.60691<br>---------------<br>0.00 : '7'<br>0.00 : 'oud'<br>0.00 : ' beh'<br>0.00 : ' try'<br>0.00 : '\f",
       "'<br>0.00 : 'Sure'<br>0.00 : ' scary'<br>0.00 : ' listen'<br>0.00 : '�'<br>0.00 : ' left'<br></span></span><span class='token' style='color: #8405a6'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 7.22762<br>---------------<br>0.00 : 'ucky'<br>0.00 : ' window'<br>0.00 : ' them'<br>0.00 : 'llow'<br>0.00 : ' shouted'<br>0.00 : '�'<br>0.00 : ' nearby'<br>0.00 : ' having'<br>0.00 : ' re'<br>0.00 : 'phant'<br></span></span><span class='token' style='color: #aa2494'>&nbsp;for<span class='tooltip'>Token: ' for'<br>Causal Loss: 7.56812<br>---------------<br>0.00 : ' gone'<br>0.00 : ' bowl'<br>0.00 : 'Mia'<br>0.00 : ' not'<br>0.00 : ' after'<br>0.00 : 'Her'<br>0.00 : 'ued'<br>0.00 : 'hing'<br>0.00 : ' see'<br>0.00 : 'ious'<br></span></span><span class='token' style='color: #8f0da3'>&nbsp;them<span class='tooltip'>Token: ' them'<br>Causal Loss: 7.31930<br>---------------<br>0.00 : ' needed'<br>0.00 : ' threw'<br>0.00 : ' move'<br>0.00 : ' real'<br>0.00 : ' rabbit'<br>0.00 : ' Suddenly'<br>0.00 : ' advent'<br>0.00 : ' field'<br>0.00 : 'oug'<br>0.00 : ' help'<br></span></span><span class='token' style='color: #9b179e'>&nbsp;b<span class='tooltip'>Token: ' b'<br>Causal Loss: 7.42327<br>---------------<br>0.00 : 'are'<br>0.00 : ' hun'<br>0.00 : 'ello'<br>0.00 : ' wings'<br>0.00 : ' took'<br>0.00 : ' shout'<br>0.00 : ' shap'<br>0.00 : ' ju'<br>0.00 : '\u0011'<br>0.00 : ' light'<br></span></span><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tutorial_code.inference import show_predictions\n",
    "\n",
    "show_predictions(model, tokenizer, device=\"cuda\", text=[sample_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db6ded-ea4d-4f29-9832-9b357bd3da39",
   "metadata": {},
   "source": [
    "### Simple Text Gen\n",
    "This is a very simple text generator implementation.\n",
    "\n",
    "[tutorial_code.textgen](../tutorial_code/textgen.py)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e791db3-b315-46f9-900a-3cc770b3717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|BOS|> One day, a little girlark towards M ride rem r makes When outside willn clothesimeavy hcoam played poinend smart thoughtmaher\\x1f get peopleeyock lessee strongudden fillress wavedaringside voice Rorrow roll wind grateoredMia its are N thanked'\n"
     ]
    }
   ],
   "source": [
    "from tutorial_code.textgen import TextGenerator\n",
    "\n",
    "# Test text generation.\n",
    "# Don't expect too much from this model, as the only input to each prediction is the previous word. \n",
    "text_gen = TextGenerator(model, tokenizer, 'cuda', do_sample=True, seed=42)\n",
    "text = text_gen.prompt(\"One day, a little girl\", max_new_tokens=50)\n",
    "print(repr(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
