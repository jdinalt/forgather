{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Forgather\n",
    "\n",
    "[forgather/config.py](../forgather/config.py)  \n",
    "[forgather/latent.py](../forgather/latent.py)  \n",
    "[forgather/dynamic.py](../forgather/dynamic.py)  \n",
    "\n",
    "---\n",
    "What exactly is this \"Forgather\" thing? What is it good for?\n",
    "\n",
    "That's a good question. It's probably easiest to just demonstrate..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5a90f-6386-4b8e-89ce-ec19195f7dff",
   "metadata": {},
   "source": [
    "## forgather.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826aff3d-82f7-43bd-a4c1-13f53f45573d",
   "metadata": {},
   "source": [
    "### A Quick Demo\n",
    "\n",
    "This demontrates what this package is about..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0570e4cd-afe6-4de7-996c-ae0e712c93ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-11 03:45:19.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maiws.base_trainer\u001b[0m:\u001b[36m_validate_dirs\u001b[0m:\u001b[36m184\u001b[0m - \u001b[1mCreating output directory: output_models/quick_demo\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67678d204f294c90b4172f49731d2a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 2,119,712\n",
      "total_train_samples: 2,119,712\n",
      "per_device_train_batch_size: 16\n",
      "actual_per_device_batch_size: 16\n",
      "total_train_batch_size: 16\n",
      "max_steps: 500\n",
      "total_parameters: 0.9M\n",
      "trainable_parameters: 0.9M\n",
      "model:\n",
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (key_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (value_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "2024-07-11 03:45:20           50  0.0   train-loss: 6.11427   learning-rate: 1.00e-03\n",
      "2024-07-11 03:45:20          100  0.0   train-loss: 4.90638   learning-rate: 1.00e-03\n",
      "2024-07-11 03:45:21          150  0.0   train-loss: 4.50479   learning-rate: 1.00e-03\n",
      "2024-07-11 03:45:21          200  0.0   train-loss: 4.12771   learning-rate: 1.00e-03\n",
      "2024-07-11 03:45:22          250  0.0   train-loss: 4.05604   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef63a38bb7d04b0db9f4d7a5f44e1914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 03:45:22          250  0.0   eval-loss:  3.89841   \n",
      "2024-07-11 03:45:23          300  0.0   train-loss: 3.91871   learning-rate: 1.00e-03\n",
      "2024-07-11 03:45:23          350  0.0   train-loss: 3.84269   learning-rate: 1.00e-03\n",
      "2024-07-11 03:45:24          400  0.0   train-loss: 3.73142   learning-rate: 1.00e-03\n",
      "2024-07-11 03:45:24          450  0.0   train-loss: 3.60624   learning-rate: 1.00e-03\n",
      "2024-07-11 03:45:24          500  0.0   train-loss: 3.55654   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6590c5d487204d4c840026945905e96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 03:45:25          500  0.0   eval-loss:  3.56949   \n",
      "train_runtime: 5.358\n",
      "train_samples: 8,000\n",
      "step: 500\n",
      "train_samples_per_second: 1.493e+03\n",
      "train_steps_per_second: 93.32\n",
      "train_loss: 4.152\n",
      "epoch: 0.003774\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=3.556535005569458, metrics={'train_runtime': 5.357971906661987, 'train_samples': 8000, 'step': 500, 'train_samples_per_second': 1493.102, 'train_steps_per_second': 93.319, 'train_loss': 4.151750564575195, 'epoch': 0.0037740976132606694})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "from forgather.config import  materialize_config\n",
    "\n",
    "# Define a training configuration with YAML, including specifying object types.\n",
    "#\n",
    "# We use the Yaml SafeLoader, which disallows the creation of arbitrary Python\n",
    "# objects, but we had added the '!callable' tag. More on that later...\n",
    "#\n",
    "# Note that this is not pure Yaml. Jinja (sandboxed) is used as a pre-processor.\n",
    "trian_config = \"\"\"\n",
    "-- set output_dir = path_join('output_models', 'quick_demo')\n",
    "# Define the tokenizer to use\n",
    ".define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
    "    - \"../tokenizers/tiny_stories_2k\"\n",
    "\n",
    "# Define a model -- note how we can specify the file path to the module...\n",
    ".define: &model !callable:../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\n",
    "    - !callable:../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig\n",
    "        hidden_size: 128\n",
    "        num_hidden_layers: 2\n",
    "\n",
    "# Define the train and eval datasets\n",
    ".define: &dataset !callable:datasets:load_from_disk [ \"../datasets/tiny_stories_tokenized\" ]\n",
    ".define: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
    ".define: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
    "\n",
    "# Define a trainer\n",
    "trainer: !callable:aiws.trainer:Trainer\n",
    "    model: *model\n",
    "    train_dataset: *train_dataset\n",
    "    eval_dataset: *eval_dataset\n",
    "    tokenizer: *tokenizer\n",
    "    args: !callable:aiws.trainer_types:TrainingArguments\n",
    "        output_dir: \"{{ output_dir }}\"\n",
    "        eval_steps: 250\n",
    "        logging_steps: 50\n",
    "        max_steps: 500\n",
    "        eval_strategy: \"steps\"\n",
    "        save_strategy: \"no\"\n",
    "\"\"\"\n",
    "\n",
    "# As you may have guessed, the '!callable' tags call Python code.\n",
    "# Which 'Callables' are allowed is controlled by defining a whitelist.\n",
    "#\n",
    "# Note: Passing a whitelist is optional, but VERY strongly recommended.\n",
    "#\n",
    "# It should go without saying that you should NEVER use an untrusted\n",
    "# config file with an untrusted whitelist without careful examination.\n",
    "#\n",
    "# Care has been taking to try to make this as safe as possible, but I can't promise\n",
    "# that the security is perfect. I'm not aware of any flaws, but that does not mean that\n",
    "# they don't exist.\n",
    "whitelist_yaml = \"\"\"\n",
    "- transformers:AutoTokenizer.from_pretrained\n",
    "- datasets:load_from_disk\n",
    "- forgather.construct:get_item\n",
    "- aiws.trainer:Trainer\n",
    "- aiws.trainer_types:TrainingArguments\n",
    "- ../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\n",
    "- ../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig\n",
    "\"\"\"\n",
    "\n",
    "# Materialize the object definition and use it!\n",
    "trainer = materialize_config(trian_config, whitelist_yaml, load_method=\"from_string\").config[\"trainer\"]\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72dc2c0-32ac-4dc9-941e-cd586d8fa2a1",
   "metadata": {},
   "source": [
    "### Digging a little deeper...\n",
    "That's pretty much what it's for, in a nut-shell.\n",
    "\n",
    "But wait. There's more!\n",
    "\n",
    "One of the things which has bugged me when working on ML projects is the proliferation of training scripts and configurations. Before long you are working with a copy-of-a-copy-of-a-copy of a configuration and they keep getting longer, more complex, and harder to maintain. Each tends to be a subtle variation of a previous version and as your code-base evolves, compatibility of an older config with a newer script tends to break.\n",
    "\n",
    "Ultimately, the whole process is directly at odds with principle of \"[Don't Repeat Yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\"\n",
    "\n",
    "Using Yaml to define the configuration is definitely a step-up from defing long strings of command-line arguments or even JSON, but it still does not solve the DRY problem.\n",
    "\n",
    "We can improve on this by using [Jinja](https://jinja.palletsprojects.com/en/3.1.x/) as a pre-processor and templatizing the configurations.\n",
    "\n",
    "The template library is still a work-in-progress, but it is comming along.\n",
    "\n",
    "#### Project Resources\n",
    "**Experiment Template**  \n",
    "\n",
    "[templates/experiments/example/experiment.yaml](templates/experiments/example/experiment.yaml)  \n",
    "[templates/experiments/example/model_config.yaml](templates/experiments/example/model_config.yaml)  \n",
    "[templates/experiments/example/trainer_config.yaml](templates/experiments/example/trainer_config.yaml)  \n",
    "\n",
    "**Project Templates**  \n",
    "\n",
    "[meta_config.yaml](meta_config.yaml)  \n",
    "[templates/directories.yaml](templates/directories.yaml)  \n",
    "[templates/project.yaml](templates/project.yaml)  \n",
    "\n",
    "**Library Templates**  \n",
    "\n",
    "Config Templates  \n",
    "[../templates/configs/default_train_script.yaml](../templates/configs/default_train_script.yaml)  \n",
    "[../templates/configs/base_train_config.yaml](../templates/configs/base_train_config.yaml)  \n",
    "\n",
    "Model Templates  \n",
    "[../templates/models/tiny_d128_l2.yaml](../templates/models/tiny_d128_l2.yaml)  \n",
    "[../templates/models/vanilla_transformer.yaml](../templates/models/vanilla_transformer.yaml)  \n",
    "[../templates/models/custom_model.yaml](../templates/models/custom_model.yaml)  \n",
    "[../templates/models/load_custom_model.yaml](../templates/models/load_custom_model.yaml)  \n",
    "\n",
    "Dataset Templates  \n",
    "[../templates/datasets/tiny_stories_pretokenized_2k.yaml](../templates/datasets/tiny_stories_pretokenized_2k.yaml)  \n",
    "[../templates/datasets/base_dataset_loader.yaml](../templates/datasets/base_dataset_loader.yaml)  \n",
    "\n",
    "Trainer Templates  \n",
    "[../templates/trainers/base_trainer.yaml](../templates/trainers/base_trainer.yaml)  \n",
    "[../templates/trainers/trainer.yaml](../templates/trainers/trainer.yaml)  \n",
    "[../templates/trainers/accel_trainer.yaml](../templates/trainers/accel_trainer.yaml)  \n",
    "[../templates/trainers/hf_trainer.yaml](../templates/trainers/hf_trainer.yaml)  \n",
    "\n",
    "**Whitelists**  \n",
    "[templates/whitelist.yaml](templates/whitelist.yaml)  \n",
    "[../templates/whitelists/global_whitelist.yaml](../templates/whitelists/global_whitelist.yaml)  \n",
    "[../templates/whitelists/model_zoo_whitelist.yaml](../templates/whitelists/model_zoo_whitelist.yaml)  \n",
    "\n",
    "**Model Code**  \n",
    "\n",
    "[model_zoo/vanilla_transformer/vanilla_transformer.py](../model_zoo/vanilla_transformer/vanilla_transformer.py)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed99de1-26f2-4f77-b1cd-b2eef7b3a1f0",
   "metadata": {},
   "source": [
    "### Loading a Template Config\n",
    "\n",
    "Let's start by loading the project meta-configuration, which defines all of the paths, relative to the current-working-directory.\n",
    "\n",
    "[meta_config.yaml](meta_config.yaml)  \n",
    "[templates/directories.yaml](templates/directories.yaml)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d1681b-c3e8-434b-b0f8-c8731b8e8229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- meta_config.yaml -----------\n",
      "assets_dir: '..'\n",
      "datasets_dir: '../datasets'\n",
      "experiment_dir: './templates/experiments/'\n",
      "model_dir: './output_models'\n",
      "model_src_dir: '../model_zoo'\n",
      "project_dir: '.'\n",
      "project_templates: './templates'\n",
      "scripts_dir: '../scripts'\n",
      "search_paths:\n",
      "  - '../templates'\n",
      "  - './templates'\n",
      "templates: '../templates'\n",
      "tokenizer_dir: '../tokenizers'\n",
      "train_script_path: '../scripts/train_script.py'\n",
      "whitelist_path: './templates/whitelist.yaml'\n",
      "-------------- Experiment --------------\n",
      "./templates/experiments/example/experiment.yaml\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "import shutil\n",
    "\n",
    "from pprint import pformat, pp\n",
    "from transformers import set_seed\n",
    "\n",
    "from forgather.config import (\n",
    "    preprocess_config,\n",
    "    load_config,\n",
    "    load_whitelist_as_set,\n",
    "    materialize_config,\n",
    "    enumerate_whitelist_exceptions,\n",
    "    pconfig,\n",
    ")\n",
    "from forgather import Latent\n",
    "from aiws.dotdict import DotDict\n",
    "\n",
    "# Path to your project meta-config\n",
    "meta_config_path = 'meta_config.yaml'\n",
    "experiment_name = 'example'\n",
    "# Path to an experiment config to run.\n",
    "\n",
    "metacfg = DotDict(load_config(meta_config_path).config)\n",
    "print(f\"{' '+meta_config_path+' ':-^40}\")\n",
    "pconfig(metacfg)\n",
    "\n",
    "# Get path to selected experiment\n",
    "experiment_dir = os.path.join(metacfg.experiment_dir, experiment_name)\n",
    "experiment_path = os.path.join(experiment_dir,'experiment.yaml')\n",
    "print(f\"{' Experiment ':-^40}\\n{experiment_path}\")\n",
    "print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7951ef20-45b9-4c2a-a6ce-ebe4329bee0c",
   "metadata": {},
   "source": [
    "Now, each experiment configuration has been reduced to something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4053a968-33a9-492a-8ca5-0dc57c5810f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- experiment.yaml ------------\n",
      "\n",
      "     1: {%- extends 'project.yaml' %}\n",
      "     2: \n",
      "     3: {%- block experiment_defaults %}\n",
      "     4: {{ super() }}\n",
      "     5: -- set ns.EXPERIMENT_NAME = \"Tiny Transformer\"\n",
      "     6: -- set ns.EXPERIMENT_DESCRIPTION = \"It's not supid, it's advanced!\"\n",
      "     7: -- set ns.CREATE_NEW_MODEL = True\n",
      "     8: -- set ns.SAVE_MODEL = True\n",
      "     9: {% endblock experiment_defaults -%}\n",
      "    10: \n",
      "    11: {%- block construct_new_model %}\n",
      "    12: -- include 'model_config.yaml'\n",
      "    13: {% endblock construct_new_model -%}\n",
      "    14: \n",
      "    15: {%- block trainer_definition %}\n",
      "    16: -- include 'trainer_config.yaml'\n",
      "    17: {% endblock trainer_definition -%}\n",
      "\n",
      "---------- model_config.yaml -----------\n",
      "\n",
      "     1: {%- extends 'models/tiny_d128_l2.yaml' %}\n",
      "     2: {%- block model_meta_config %}\n",
      "     3: {{ super() }}\n",
      "     4: {%- set model_def.name = \"Custom \" + model_def.name %}\n",
      "     5: {% endblock model_meta_config %}\n",
      "     6: \n",
      "     7: {%- block model_config %}\n",
      "     8: {{ super() }}\n",
      "     9:     # Experiment overrides\n",
      "    10:     hidden_size: 256\n",
      "    11:     num_hidden_layers: 2\n",
      "    12: {% endblock model_config %}\n",
      "\n",
      "--------- trainer_config.yaml ----------\n",
      "\n",
      "     1: {%- extends 'trainers/trainer.yaml' %}\n",
      "     2: {%- block trainer_meta_config %}\n",
      "     3: {{ super() }}\n",
      "     4: {%- set trainer_def.name = \"Custom \" + trainer_def.name %}\n",
      "     5: {% endblock trainer_meta_config %}\n",
      "     6: \n",
      "     7: {% block trainer_args %}\n",
      "     8: {{ super() }}\n",
      "     9:     max_steps: 2000\n",
      "    10:     logging_steps: 100\n",
      "    11: {% endblock trainer_args %}\n",
      "    12: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from forgather.config import format_line_numbers\n",
    "def pconfig_dir(path, ext='.yaml'):\n",
    "    \"\"\"\n",
    "    Print all files ending with 'ext' in a directory\n",
    "    \"\"\"\n",
    "    for file_name in sorted(os.listdir(path)):\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        if not os.path.isfile(file_path) or not file_name.endswith(ext) or file_name.startswith('.'):\n",
    "            continue\n",
    "        print(f\"{' '+ file_name +' ':-^40}\\n\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            print(format_line_numbers(f.read()))\n",
    "\n",
    "pconfig_dir(experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0835a75-8ac2-4e9a-9937-d9c260cdc211",
   "metadata": {},
   "source": [
    "### Preprocess Configuration\n",
    "Let's feed this input into the Jinja preprocessor and see what comes out...\n",
    "\n",
    "As configured, this generated file will be saved in the model's 'runs' directory. This should allow for reproducability. Even if you muck about with the template defintions afterwards, the generated configuration will still be available for referemce and/or reuse.\n",
    "\n",
    "Note that this automatically generated a number of comments about the experiment details. Overall, this is much better than hand-crafting a heap of arguments to feed to a CLI script!\n",
    "\n",
    "#### preprocess_config() : Preprocess a configuration file with Jinja2\n",
    "```python\n",
    "def preprocess_config(\n",
    "    config:  os.PathLike | str, *,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod = DEFAULT_LOAD_METHOD,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "```\n",
    "Click this link to open the generated file:\n",
    "[preprocessed_config.yaml](preprocessed_config.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bd1cab-66a4-4784-96ce-8514890291bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1: \n",
      "     2: \n",
      "     3: ############## Experiment ##############\n",
      "     4: \n",
      "     5: # Tiny Transformer\n",
      "     6: # 2024-07-11 16:14:59\n",
      "     7: # Description: It's not supid, it's advanced!\n",
      "     8: # Model: anonymous_model\n",
      "     9: # World Size: 1\n",
      "    10: # Hostname: hal9000\n",
      "    11: # Script Args: N/A\n",
      "    12: \n",
      "    13: ############# Config Vars ##############\n",
      "    14: \n",
      "    15: # ns.TOKENIZERS_DIR: \"../tokenizers\"\n",
      "    16: # ns.MODELS_DIR: \"./output_models\"\n",
      "    17: # ns.DATASETS_DIR: \"../datasets\"\n",
      "    18: # ns.SCRIPTS_DIR: \"../scripts\"\n",
      "    19: # ns.MODEL_SOURCE_DIR: \"../model_zoo\"\n",
      "    20: # ns.OUTPUT_DIR: \"./output_models/anonymous_model\"\n",
      "    21: # ns.LOGGING_DIR: path = \"./output_models/anonymous_model/runs/Tiny Transformer_1720714499278730828\"\n",
      "    22: # ns.CREATE_NEW_MODEL: path = True\n",
      "    23: # ns.SAVE_MODEL: path = True\n",
      "    24: \n",
      "    25: ####### Additional Dependencies ########\n",
      "    26: \n",
      "    27: # Experiment tracking: Tensorboard SummaryWriter\n",
      "    28: .define: &summary_writer !callable:torch.utils.tensorboard:SummaryWriter\n",
      "    29:     - \"./output_models/anonymous_model/runs/Tiny Transformer_1720714499278730828\"\n",
      "    30: \n",
      "    31: ################ Model #################\n",
      "    32: \n",
      "    33: # Name: Custom Tiny-d128-2L\n",
      "    34: # Description: A Plain Vanilla Transformer with 2 layers, d=128, and a 2k tiny_stories tokenizer.\n",
      "    35: \n",
      "    36: # model_def.cls = \"VanillaTransformer\"\n",
      "    37: # model_def.cfg_cls = \"VanillaTransformerConfig\"\n",
      "    38: # model_def.tokenizer_path_or_id = \"../tokenizers/tiny_stories_2k\"\n",
      "    39: # model_def.config_path = \"../model_zoo/vanilla_transformer/vanilla_transformer.py\"\n",
      "    40: # model_def.model_path = \"../model_zoo/vanilla_transformer/vanilla_transformer.py\"\n",
      "    41: \n",
      "    42: #**** Tokenizer ****\n",
      "    43: \n",
      "    44: # Load a tokenizer from a file or hub id\n",
      "    45: .define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
      "    46:     - \"../tokenizers/tiny_stories_2k\"\n",
      "    47: \n",
      "    48: #**** Model Config ****\n",
      "    49: \n",
      "    50: .define: &model_config\n",
      "    51:     vocab_size: !callable:forgather.construct:get_attr [ *tokenizer, vocab_size ]\n",
      "    52:     hidden_size: 128\n",
      "    53:     dim_feedforward: 512\n",
      "    54:     num_attention_heads: 1\n",
      "    55:     num_hidden_layers: 2\n",
      "    56:     # Experiment overrides\n",
      "    57:     hidden_size: 256\n",
      "    58:     num_hidden_layers: 2\n",
      "    59: \n",
      "    60: #**** Model Constructor ****\n",
      "    61: \n",
      "    62: # Custom transformer model; registers for AutoClass and will save code with weights.\n",
      "    63: .define: &model !callable:aiws.construct:register_for_auto_class\n",
      "    64:     - !callable:../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\n",
      "    65:         - !callable:aiws.construct:register_for_auto_class\n",
      "    66:             - !callable:../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig\n",
      "    67:                 <<: *model_config\n",
      "    68: \n",
      "    69: ############### Datasets ###############\n",
      "    70: \n",
      "    71: # Name: Tiny Stories 2K\n",
      "    72: # Description: Tiny Stories dataset, pre-tokenized with the tiny_2k tokenizer.\n",
      "    73: \n",
      "    74: # dataset_def.dataset_name_or_path = \"../datasets/tiny_stories_tokenized\"\n",
      "    75: # dataset_def.dataset_train_split = \"train\"\n",
      "    76: # dataset_def.dataset_validation_split = \"validation\"\n",
      "    77: \n",
      "    78: #**** Load Datasets ****\n",
      "    79: \n",
      "    80: .define: &dataset !callable:datasets:load_from_disk [ \"../datasets/tiny_stories_tokenized\" ]\n",
      "    81: \n",
      "    82: #**** Get Splits ****\n",
      "    83: \n",
      "    84: .define: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
      "    85: .define: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
      "    86: \n",
      "    87: ########## Trainer Callbacks ###########\n",
      "    88: \n",
      "    89: .define: &trainer_callbacks\n",
      "    90:     # Log training metrics to JSON fiie.\n",
      "    91:     - !callable:aiws.default_callbacks:JsonLogger []\n",
      "    92:     # Log configuration and metrics to Tensorboard file\n",
      "    93:     - !callable:aiws.tb_logger:TBLogger\n",
      "    94:         args: [ *summary_writer ]\n",
      "    95:         kwargs:\n",
      "    96:             date: \"2024-07-11 16:14:59\"\n",
      "    97:             name: \"Tiny Transformer\"\n",
      "    98:             description: \"It's not supid, it's advanced!\"\n",
      "    99:             args: N/A\n",
      "   100:             world_size: 1\n",
      "   101:             config: !callable:pp_config\n",
      "   102: \n",
      "   103: ############ Data Collator #############\n",
      "   104: \n",
      "   105: .define: &data_collator !callable:transformers:DataCollatorForLanguageModeling\n",
      "   106:     args:\n",
      "   107:         - *tokenizer\n",
      "   108:     kwargs:\n",
      "   109:         mlm: False\n",
      "   110:         return_tensors: pt\n",
      "   111: \n",
      "   112: ############### Trainer ################\n",
      "   113: \n",
      "   114: # Name: Custom aiws.trainer.Trainer\n",
      "   115: # Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
      "   116: \n",
      "   117: #**** Trainer Args ****\n",
      "   118: \n",
      "   119: .define: &trainer_args\n",
      "   120:     output_dir: \"./output_models/anonymous_model\"\n",
      "   121:     logging_dir: \"./output_models/anonymous_model/runs/Tiny Transformer_1720714499278730828\"\n",
      "   122:     overwrite_output_dir: True\n",
      "   123:     per_device_train_batch_size: 16\n",
      "   124:     per_device_eval_batch_size: 16\n",
      "   125:     learning_rate: 1.0e-3\n",
      "   126:     num_train_epochs: 1\n",
      "   127:     eval_steps: 500\n",
      "   128:     logging_steps: 500\n",
      "   129:     seed: 42\n",
      "   130:     eval_strategy: \"steps\"\n",
      "   131:     save_strategy: \"no\"\n",
      "   132:     logging_strategy: \"steps\"\n",
      "   133:     lr_scheduler_type: \"constant\"\n",
      "   134:     max_steps: 2000\n",
      "   135:     logging_steps: 100\n",
      "   136: \n",
      "   137: #**** Trainer Constructor ****\n",
      "   138: \n",
      "   139: .define: &trainer !callable:aiws.trainer:Trainer\n",
      "   140:     model: *model\n",
      "   141:     args: !callable:aiws.trainer_types:TrainingArguments\n",
      "   142:         <<: *trainer_args\n",
      "   143:     data_collator: *data_collator\n",
      "   144:     train_dataset: *train_dataset\n",
      "   145:     eval_dataset: *eval_dataset\n",
      "   146:     tokenizer: *tokenizer\n",
      "   147:     callbacks: *trainer_callbacks\n",
      "   148: \n",
      "   149: ############## Pre Config ##############\n",
      "   150: \n",
      "   151: # Undefined\n",
      "   152: \n",
      "   153: ############ Configuration #############\n",
      "   154: \n",
      "   155: output_dir: \"./output_models/anonymous_model\"\n",
      "   156: logging_dir: \"./output_models/anonymous_model/runs/Tiny Transformer_1720714499278730828\"\n",
      "   157: experiment_name: \"Tiny Transformer\"\n",
      "   158: experiment_description: \"It's not supid, it's advanced!\"\n",
      "   159: trainer: *trainer\n",
      "   160: do_save: True\n",
      "   161: \n",
      "   162: ############# Post Config ##############\n",
      "   163: \n",
      "   164: # Undefined\n",
      "   165: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp_config = preprocess_config(experiment_path, search_path=metacfg.search_paths)\n",
    "print(pp_config.with_line_numbers())\n",
    "with open('preprocessed_config.yaml', 'w') as f:\n",
    "    f.write(pp_config.with_line_numbers(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbbe707-7d1d-448b-940a-fb64694769fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Configuration Syntax\n",
    "\n",
    "[syntax.md](../syntax.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf0ee7-a55e-4f79-aef5-24e1d189832e",
   "metadata": {},
   "source": [
    "### Parse Configuration\n",
    "\n",
    "We can feed the pre-processed configuration into the YAML parser with load_config().\n",
    "\n",
    "By default, this will pre-process the input; this can be skipped by setting 'preprocess=False'\n",
    "\n",
    "Also note the 'load_method' argument. The default is to assume the input string is a file path, but we can tell it that it's the actual input by setting 'from_string'\n",
    "\n",
    "#### load_config() : Load Jinja/Yaml configuration\n",
    "```python\n",
    "load_config(\n",
    "    config: os.PathLike | str, *,\n",
    "    preprocess: bool = True,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod = DEFAULT_LOAD_METHOD,\n",
    "    **kwargs,\n",
    ") -> LoadConfigOutput\n",
    "```\n",
    "\n",
    "By default, the single argument is a relative path to a config file which will be preprocessed with Jinja2 and parsed with Yaml.\n",
    "\n",
    "Any additional keyword-args are passed to the Jina template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bbec1cf-8f74-4d06-b4fb-df33b26bab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do_save: True\n",
      "experiment_description: 'It's not supid, it's advanced!'\n",
      "experiment_name: 'Tiny Transformer'\n",
      "logging_dir: './output_models/anonymous_model/runs/Tiny Transformer_1720604258488372232'\n",
      "output_dir: './output_models/anonymous_model'\n",
      "trainer:\n",
      "  Latent 'aiws.trainer:Trainer'\n",
      "    args:\n",
      "      Latent 'aiws.trainer_types:TrainingArguments'\n",
      "        eval_steps: 500\n",
      "        eval_strategy: 'steps'\n",
      "        learning_rate: 0.001\n",
      "        logging_dir: './output_models/anonymous_model/runs/Tiny Transformer_1720604258488372232'\n",
      "        logging_steps: 500\n",
      "        logging_strategy: 'steps'\n",
      "        lr_scheduler_type: 'constant'\n",
      "        max_steps: 2000\n",
      "        num_train_epochs: 1\n",
      "        output_dir: './output_models/anonymous_model'\n",
      "        overwrite_output_dir: True\n",
      "        per_device_eval_batch_size: 16\n",
      "        per_device_train_batch_size: 16\n",
      "        save_strategy: 'no'\n",
      "        seed: 42\n",
      "    callbacks:\n",
      "      - Latent 'aiws.default_callbacks:JsonLogger'\n",
      "      - Latent 'aiws.tb_logger:TBLogger'\n",
      "        - Latent 'torch.utils.tensorboard:SummaryWriter'\n",
      "          - './output_models/anonymous_model/runs/Tiny Transformer_1720604258488372232'\n",
      "        args: 'N/A'\n",
      "        config: Latent 'pp_config'\n",
      "        date: '2024-07-10 09:37:38'\n",
      "        description: 'It's not supid, it's advanced!'\n",
      "        name: 'Tiny Transformer'\n",
      "        world_size: 1\n",
      "    data_collator:\n",
      "      Latent 'transformers:DataCollatorForLanguageModeling'\n",
      "        - Latent 'transformers:AutoTokenizer.from_pretrained'\n",
      "          - '../tokenizers/tiny_stories_2k'\n",
      "        mlm: False\n",
      "        return_tensors: 'pt'\n",
      "    eval_dataset:\n",
      "      Latent 'forgather.construct:get_item'\n",
      "        - Latent 'datasets:load_from_disk'\n",
      "          - '../datasets/tiny_stories_tokenized'\n",
      "        - 'validation'\n",
      "    model:\n",
      "      Latent 'aiws.construct:register_for_auto_class'\n",
      "        - Latent '../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer'\n",
      "          - Latent 'aiws.construct:register_for_auto_class'\n",
      "            - Latent '../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig'\n",
      "              dim_feedforward: 512\n",
      "              hidden_size: 256\n",
      "              num_attention_heads: 1\n",
      "              num_hidden_layers: 2\n",
      "              vocab_size:\n",
      "                Latent 'forgather.construct:get_attr'\n",
      "                  - Latent 'transformers:AutoTokenizer.from_pretrained'\n",
      "                    - '../tokenizers/tiny_stories_2k'\n",
      "                  - 'vocab_size'\n",
      "    tokenizer:\n",
      "      Latent 'transformers:AutoTokenizer.from_pretrained'\n",
      "        - '../tokenizers/tiny_stories_2k'\n",
      "    train_dataset:\n",
      "      Latent 'forgather.construct:get_item'\n",
      "        - Latent 'datasets:load_from_disk'\n",
      "          - '../datasets/tiny_stories_tokenized'\n",
      "        - 'train'\n"
     ]
    }
   ],
   "source": [
    "config_out = load_config(pp_config, preprocess=False, load_method=\"from_string\")\n",
    "pconfig(config_out.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd6f07-0d66-41b6-be42-49788427ba81",
   "metadata": {},
   "source": [
    "### Load Whitelist\n",
    "\n",
    "#### load_whitelist_as_set() : Load a whitelist configuration from a file or string\n",
    "```python\n",
    "def load_whitelist_as_set(\n",
    "    config: os.PathLike | str, *,\n",
    "    preprocess: bool = True,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod = DEFAULT_LOAD_METHOD\n",
    ") -> NamedTuple[Set[str], str]:\n",
    "```\n",
    "This is essentially just load_config, but it normalizes the paths in the whitelist and converts the list to a set, to improve search performance.\n",
    "\n",
    "The return value is a named-tuple, containing both the set and the preprocessed text, for diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04e1cd26-9fda-4462-b087-c770b28cf442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      "  - '/home/dinalt/ai_assets/aiworkshop/model_zoo/attention_only/attention_only.py:TransformerConfig'\n",
      "  - '/home/dinalt/ai_assets/aiworkshop/model_zoo/attention_only/attention_only.py:TransformerModel'\n",
      "  - '/home/dinalt/ai_assets/aiworkshop/model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer'\n",
      "  - '/home/dinalt/ai_assets/aiworkshop/model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig'\n",
      "  - 'accelerate:DataLoaderConfiguration'\n",
      "  - 'aiws.accel_trainer:AccelTrainer'\n",
      "  - 'aiws.accel_trainer:AccelTrainingArguments'\n",
      "  - 'aiws.construct:register_for_auto_class'\n",
      "  - 'aiws.default_callbacks:InfoCallback'\n",
      "  - 'aiws.default_callbacks:JsonLogger'\n",
      "  - 'aiws.default_callbacks:ProgressCallback'\n",
      "  - 'aiws.tb_logger:TBLogger'\n",
      "  - 'aiws.trainer:Trainer'\n",
      "  - 'aiws.trainer_types:TrainingArguments'\n",
      "  - 'datasets:load_dataset'\n",
      "  - 'datasets:load_from_disk'\n",
      "  - 'forgather.construct:flatten'\n",
      "  - 'forgather.construct:get_attr'\n",
      "  - 'forgather.construct:get_item'\n",
      "  - 'torch.utils.tensorboard:SummaryWriter'\n",
      "  - 'transformers:AutoModel.from_pretrained'\n",
      "  - 'transformers:AutoTokenizer.from_pretrained'\n",
      "  - 'transformers:DataCollatorForLanguageModeling'\n",
      "  - 'transformers:Trainer'\n",
      "  - 'transformers:TrainingArguments'\n",
      "pp_config:\n",
      "       1: # Dynamic Loader Whitelist\n",
      "       2: # Only constructors listed here are allowed in the configuration file.\n",
      "       3: # This is intended to prevent the arbitrary execution of any Python code by\n",
      "       4: # a configuration script. Be very careful what you add when running\n",
      "       5: # a confiuration from an untrusted source!\n",
      "       6: \n",
      "       7: # Transformers\n",
      "       8: - transformers:AutoTokenizer.from_pretrained\n",
      "       9: # TODO: AutoModel.from_pretrained has a 'trust_remote_code' flag; deal with this.\n",
      "      10: - transformers:AutoModel.from_pretrained\n",
      "      11: - transformers:DataCollatorForLanguageModeling\n",
      "      12: - transformers:Trainer\n",
      "      13: - transformers:TrainingArguments\n",
      "      14: \n",
      "      15: # Accelerate\n",
      "      16: - accelerate:DataLoaderConfiguration\n",
      "      17: \n",
      "      18: # Datasets\n",
      "      19: - datasets:load_from_disk\n",
      "      20: # TODO: load_dataset has a 'trust_remote_code' flag; deal with this.\n",
      "      21: - datasets:load_dataset\n",
      "      22: \n",
      "      23: # Torch\n",
      "      24: - torch.utils.tensorboard:SummaryWriter\n",
      "      25: \n",
      "      26: # AIWS\n",
      "      27: - aiws.trainer:Trainer\n",
      "      28: - aiws.trainer_types:TrainingArguments\n",
      "      29: - aiws.accel_trainer:AccelTrainer\n",
      "      30: - aiws.accel_trainer:AccelTrainingArguments\n",
      "      31: - aiws.default_callbacks:InfoCallback\n",
      "      32: - aiws.default_callbacks:ProgressCallback\n",
      "      33: - aiws.default_callbacks:JsonLogger\n",
      "      34: - aiws.tb_logger:TBLogger\n",
      "      35: - aiws.construct:register_for_auto_class\n",
      "      36: \n",
      "      37: # forgather\n",
      "      38: - forgather.construct:get_item\n",
      "      39: - forgather.construct:flatten\n",
      "      40: - forgather.construct:get_attr\n",
      "      41: - ../model_zoo/attention_only/attention_only.py:TransformerConfig\n",
      "      42: - ../model_zoo/attention_only/attention_only.py:TransformerModel\n",
      "      43: - ../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig\n",
      "      44: - ../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "whitelist_out = load_whitelist_as_set(metacfg.whitelist_path, search_path=metacfg.search_paths)\n",
    "pconfig(whitelist_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735df202-bb24-4e20-be38-ce7bd81eae25",
   "metadata": {},
   "source": [
    "#### Check Whitelist Requirements\n",
    "\n",
    "If you would like to see which import-specs are used in a configuraiton (or which are missing), you can use enumerate_whitelist_exceptions()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ffe56d9-5742-45a2-892b-3e0336dfb3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- '/home/dinalt/ai_assets/aiworkshop/model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer'\n",
      "- '/home/dinalt/ai_assets/aiworkshop/model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig'\n",
      "- 'aiws.construct:register_for_auto_class'\n",
      "- 'aiws.default_callbacks:JsonLogger'\n",
      "- 'aiws.tb_logger:TBLogger'\n",
      "- 'aiws.trainer:Trainer'\n",
      "- 'aiws.trainer_types:TrainingArguments'\n",
      "- 'datasets:load_from_disk'\n",
      "- 'forgather.construct:get_attr'\n",
      "- 'forgather.construct:get_item'\n",
      "- 'torch.utils.tensorboard:SummaryWriter'\n",
      "- 'transformers:AutoTokenizer.from_pretrained'\n",
      "- 'transformers:DataCollatorForLanguageModeling'\n"
     ]
    }
   ],
   "source": [
    "enumerate_whitelist_exceptions(config_out.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4848270-6d49-412f-a3a2-b72222461140",
   "metadata": {},
   "source": [
    "### Materialize the Configuration\n",
    "\n",
    "#### materialize_config() : Materialize the Latent objects in the configuration\n",
    "```python\n",
    "def materialize_config(\n",
    "    config: Any,\n",
    "    whitelist: Container | os.PathLike | str = None,\n",
    "    preprocess: bool = True,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod=DEFAULT_LOAD_METHOD,\n",
    "    pp_kwargs: Dict[str, Any] = {},\n",
    "    kwargs: Dict[str, Callable] = {},\n",
    ") -> MaterializedOutput:\n",
    "```\n",
    "- config: An instantiated, but Latent, configuration; a preprocessed configuration string; or a path to a configuraiton file.  \n",
    "- whitelist: A Container type, which means any object which supports 'str is in container'  \n",
    "- preprocess: Preprocess the string or file. Only applies if input is a path or string.  \n",
    "- load_method: One of \"from_file\", \"from_string\", \"from_file_search\"  \n",
    "- search_path: A str or List\\[str\\] paths to search for templates; also applies to \"from_file_search\" load method.  \n",
    "- pp_kwargs: Arguments to pass to the template, if preprocessing is to be performed.  \n",
    "- kwargs: A mapping str -> Callable to substitute when materializing the final config. This allows passing already instantiated objects into the config.\n",
    "\n",
    "\n",
    "We will just materialize the config we have already loaded, which is to be checked against the whitelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ce4a60c-ce9b-4738-9eb1-4deb39d559ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do_save: True\n",
      "experiment_description: 'It's not supid, it's advanced!'\n",
      "experiment_name: 'Tiny Transformer'\n",
      "logging_dir: './output_models/anonymous_model/runs/Tiny Transformer_1720604258488372232'\n",
      "output_dir: './output_models/anonymous_model'\n",
      "trainer:\n",
      "  Trainer(model=VanillaTransformer(\n",
      "    (embedding): Embedding(2000, 256)\n",
      "    (positional_encoder): PositionalEncoder()\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerLayer(\n",
      "        (attention): MultiheadAttention(\n",
      "          (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (feedforward): FeedforwardLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=256, out_features=2000, bias=True)\n",
      "  ),args=TrainingArguments(per_device_train_batch_size=16, output_dir='./output_models/anonymous_model', overwrite_output_dir=True, per_device_eval_batch_size=16, max_steps=2000, logging_steps=500, eval_steps=500, save_steps=500, learning_rate=0.001, num_train_epochs=1, seed=42, lr_scheduler_type='constant', warmup_steps=0, device=None, logging_dir='./output_models/anonymous_model/runs/Tiny Transformer_1720604258488372232', eval_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_strategy=<IntervalStrategy.STEPS: 'steps'>, save_strategy=<IntervalStrategy.NO: 'no'>, logging_first_step=False, eval_delay=0, save_total_limit=2),data_collator=DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='../tokenizers/tiny_stories_2k', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|PAD|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "  \t0: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t1: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t2: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t3: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  }, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt'),train_dataset=Dataset({\n",
      "      features: ['input_ids'],\n",
      "      num_rows: 2119719\n",
      "  }),eval_dataset=Dataset({\n",
      "      features: ['input_ids'],\n",
      "      num_rows: 2199\n",
      "  }),tokenizer=PreTrainedTokenizerFast(name_or_path='../tokenizers/tiny_stories_2k', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|PAD|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "  \t0: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t1: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t2: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t3: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  },model_init=None,callbacks=[<aiws.default_callbacks.ProgressCallback object at 0x7fd3144142b0>, <aiws.default_callbacks.InfoCallback object at 0x7fd1880cf8e0>, <aiws.default_callbacks.JsonLogger object at 0x7fd1880cc280>, <aiws.tb_logger.TBLogger object at 0x7fd21a8c31c0>],)\n"
     ]
    }
   ],
   "source": [
    "# For reproducible experiments, it's probably best to make sure all of your random-seeds have been initialized\n",
    "# to a consistent value. This is especially important in a multi-process environment.\n",
    "set_seed(42)\n",
    "\n",
    "config_output = materialize_config(config_out.config, whitelist=whitelist_out.config)\n",
    "pconfig(config_output.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181646d1-021a-4046-b392-abf37f841dbb",
   "metadata": {},
   "source": [
    "### Run Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b25302e-7056-4653-b33f-0275d4463ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6181dd6909742ff9d3db832d8bf19c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 2,119,712\n",
      "total_train_samples: 2,119,712\n",
      "per_device_train_batch_size: 16\n",
      "actual_per_device_batch_size: 16\n",
      "total_train_batch_size: 16\n",
      "max_steps: 2,000\n",
      "total_parameters: 1.9M\n",
      "trainable_parameters: 1.9M\n",
      "model:\n",
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 256)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=256, out_features=2000, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "2024-07-10 09:38:01          500  0.0   train-loss: 3.83067   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddaf492af3b94883abbe03312c817a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 09:38:01          500  0.0   eval-loss:  3.24676   \n",
      "2024-07-10 09:38:05        1,000  0.01  train-loss: 2.97355   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec0b2b101da4027af7e2de18a745660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 09:38:06        1,000  0.01  eval-loss:  2.84039   \n",
      "2024-07-10 09:38:10        1,500  0.01  train-loss: 2.68934   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d83ffd6a0e9409d8ec66009b34bbb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 09:38:10        1,500  0.01  eval-loss:  2.63415   \n",
      "2024-07-10 09:38:14        2,000  0.02  train-loss: 2.51999   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c669860b004217a8de05fe39a97ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 09:38:14        2,000  0.02  eval-loss:  2.52777   \n",
      "train_runtime: 17.21\n",
      "train_samples: 32,000\n",
      "step: 2,000\n",
      "train_samples_per_second: 1.86e+03\n",
      "train_steps_per_second: 116.2\n",
      "train_loss: 2.997\n",
      "epoch: 0.0151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wrapping the dict in a DotDict just allows using attribute dot-notation to acceses the values.\n",
    "config = DotDict(config_output.config)\n",
    "config.trainer.train()\n",
    "del config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21d135-1575-4684-a639-ce1c38162063",
   "metadata": {},
   "source": [
    "### Run Experiment 2\n",
    "\n",
    "This time we will skip all of the intermediate steps and go straight to instantiating the config.\n",
    "\n",
    "This tiny model has 4 layers, rather than two. How do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9ccce36-feda-4ba1-8fb9-31a5b3d7871c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7aebe2a9e14db78edb18b397c93d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 2,119,712\n",
      "total_train_samples: 2,119,712\n",
      "per_device_train_batch_size: 16\n",
      "actual_per_device_batch_size: 16\n",
      "total_train_batch_size: 16\n",
      "max_steps: 2,000\n",
      "total_parameters: 2.9M\n",
      "trainable_parameters: 2.9M\n",
      "model:\n",
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 256)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=256, out_features=2000, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "2024-07-10 09:42:16          100  0.0   train-loss: 4.98727   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:17          200  0.0   train-loss: 3.9082    learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:18          300  0.0   train-loss: 3.61767   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:19          400  0.0   train-loss: 3.40619   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:20          500  0.0   train-loss: 3.15748   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe135a5666444e0b4b62c7a09e7150f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 09:42:21          500  0.0   eval-loss:  3.20398   \n",
      "2024-07-10 09:42:22          600  0.0   train-loss: 3.13322   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:23          700  0.01  train-loss: 2.96378   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:24          800  0.01  train-loss: 2.98173   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:25          900  0.01  train-loss: 2.75518   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:26        1,000  0.01  train-loss: 2.80829   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e21bebe7564a188ed1670a3d358e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 09:42:27        1,000  0.01  eval-loss:  2.76603   \n",
      "2024-07-10 09:42:28        1,100  0.01  train-loss: 2.73739   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:29        1,200  0.01  train-loss: 2.62718   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:31        1,300  0.01  train-loss: 2.62232   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:32        1,400  0.01  train-loss: 2.54473   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:33        1,500  0.01  train-loss: 2.60957   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6e1f538de74b29a9085f2163322144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 09:42:33        1,500  0.01  eval-loss:  2.5444    \n",
      "2024-07-10 09:42:35        1,600  0.01  train-loss: 2.58322   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:36        1,700  0.01  train-loss: 2.51453   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:37        1,800  0.01  train-loss: 2.457     learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:38        1,900  0.01  train-loss: 2.25938   learning-rate: 1.00e-03\n",
      "2024-07-10 09:42:39        2,000  0.02  train-loss: 2.46873   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bc8a5f56b04c3da29f0e1b290da841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 09:42:40        2,000  0.02  eval-loss:  2.44456   \n",
      "train_runtime: 24.63\n",
      "train_samples: 32,000\n",
      "step: 2,000\n",
      "train_samples_per_second: 1.299e+03\n",
      "train_steps_per_second: 81.2\n",
      "train_loss: 2.928\n",
      "epoch: 0.0151\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=2.4687306880950928, metrics={'train_runtime': 24.631518363952637, 'train_samples': 32000, 'step': 2000, 'train_samples_per_second': 1299.148, 'train_steps_per_second': 81.197, 'train_loss': 2.9275782108306885, 'epoch': 0.015096390453042677})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "materialize_config(\n",
    "    os.path.join(metacfg.experiment_dir, 'example-2', 'experiment.yaml'), metacfg.whitelist_path, search_path=metacfg.search_paths\n",
    ").config['trainer'].train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3e79b-e146-4df5-923f-f42fc9867163",
   "metadata": {},
   "source": [
    "### View in Tensorboard\n",
    "Assuming that you have Tensorboard installed (you do, right?), you can take a look at the information collected by the logger.\n",
    "\n",
    "Ideally, start Tensorboard from a console, but to take a quick peek, you can launch it from the notebook. If using the Notebook, you will have to stop the command when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fafbb8c8-8f81-4eb2-a574-cd7999ec77ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.16.2 at http://hal9000:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Use this version if you are training on the same machine that your web-browser in running on.\n",
    "#!tensorboard --bind_all --logdir forgather_demo/output_models/test_model/runs/\n",
    "\n",
    "# Use this version if you are not running training on the same machine as your web-browser.\n",
    "!tensorboard --bind_all --logdir output_models/anonymous_model/runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a29692-aab9-44d6-b089-bf97e6f81d90",
   "metadata": {},
   "source": [
    "### Running on Multiple GPUs\n",
    "\n",
    "Running a trainer with multiple GPUs inside of a notebook has a number of issues.\n",
    "\n",
    "In the least complex mode, Torch DataParallel is used. This runs each GPU on a seperate Python thread. Unfortunately, the performance is terrible, thnaks to the Global Interpreter Lock.\n",
    "\n",
    "Running training with Torch Distributed solves this problem by running each node in a seperate process. This makes things difficult for running in a notebook. The Accelerate library attempts to solve this by offering a [notebook launcher](https://huggingface.co/docs/accelerate/en/basic_tutorials/notebook), but there are complications: see below.\n",
    "\n",
    "### Multi-GPU Training in a Notebook\n",
    "\n",
    "First, we will need to switch over to a Trainer implementation which supports the Acclerate library.\n",
    "This is easy enough. We can just override the trainer definition in the experiment to use one which supports Accelerate:\n",
    "\n",
    "[forgather_demo/accel_experiment.yaml](forgather_demo/accel_experiment.yaml)  \n",
    "\n",
    "Running in a notebook entails a few additional commplications:\n",
    "\n",
    "- To run the experiment with the notebook_launcher, we need to perform all of our initialization within the 'training_loop' function passed to the launcher.\n",
    "- An additional complication is that you will likely need to restart your notebook's kernel, should you have already used the GPUs.\n",
    "- It tends to be unstable, crashing, witout an obvious cause -- and the crash can't be reproduced from a training script!\n",
    "- If something goes wrong, you will get a 'SIGTERM' and poor diagnostic info. It's best to run the 'train_loop' on its own for better diagnostics.\n",
    "- If you want to work with the model after training, you will need to save it and load it back into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2565208e-5df9-41f6-afc0-dd9d18d22d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "from accelerate import notebook_launcher\n",
    "from forgather.config import load_config, materialize_config\n",
    "from aiws.dotdict import DotDict\n",
    "from transformers import set_seed\n",
    "\n",
    "# This is the entry-point for the spawned procceses.\n",
    "def training_loop(meta_config, experiment_name):\n",
    "    set_seed(42)\n",
    "    metacfg = DotDict(load_config(meta_config).config)\n",
    "\n",
    "    # Get Torch Distributed parameters from environ.\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "    \n",
    "    config_output = materialize_config(\n",
    "        experiment_name,\n",
    "        metacfg.whitelist_path,\n",
    "        search_path=metacfg.search_paths,\n",
    "        pp_kwargs = dict(\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "            local_rank=local_rank,\n",
    "        )\n",
    "    )\n",
    "    config = DotDict(config_output.config)\n",
    "    is_main_process = config.trainer.accelerator if hasattr(config.trainer, \"accelerator\") else True\n",
    "    # If you don't want all processes to print to the console...\n",
    "    if is_main_process:\n",
    "        print(\"**** Training Started *****\")\n",
    "        print(f\"experiment_name: {config.experiment_name}\")\n",
    "        print(f\"experiment_description: {config.experiment_description}\")\n",
    "        print(f\"output_dir: {config.output_dir}\")\n",
    "        print(f\"logging_dir: {config.logging_dir}\")\n",
    "\n",
    "    # This is where the actual 'loop' is.\n",
    "    metrics = config.trainer.train().metrics\n",
    "    \n",
    "    if is_main_process:\n",
    "        print(\"**** Training Completed *****\")\n",
    "        print(metrics)\n",
    "\n",
    "    metrics = config.trainer.evaluate()\n",
    "\n",
    "    if is_main_process:\n",
    "        print(\"**** Evaluation Completed *****\")\n",
    "        print(metrics)\n",
    "    \n",
    "    if config.do_save:\n",
    "        config.trainer.save_model()\n",
    "        if is_main_process:\n",
    "            print(f\"Model saved to: {config.trainer.args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709fd93-5a27-487a-8aeb-166b8cd2274e",
   "metadata": {},
   "source": [
    "#### Launch Accelerate Trainer Directly\n",
    "\n",
    "This will use Accelerate, but if you have multiple GPUs, this will only use one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf0acd-89c0-423c-bb6f-f2b98b6d82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop('forgather_config.yaml', 'accel_experiment.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fa3351-96d7-4769-8b45-99d103e1b941",
   "metadata": {},
   "source": [
    "#### Launch Accelerate Trainer with Notebook Launcher\n",
    "\n",
    "If you have already trained anything in the noteboo, without using notebook_launcher, this\n",
    "will fail with \"ValueError: To launch a multi-GPU training from your notebook ...\"\n",
    "\n",
    "After restarting your notebook, you can just run the prior cell again to reinitialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87183a-dbec-420e-ad2f-796384eb9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(\n",
    "    training_loop,\n",
    "    args=('forgather_config.yaml', 'accel_experiment.yaml',),\n",
    "    num_processes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ea7a1-82f8-484d-a50e-be94614d2ef7",
   "metadata": {},
   "source": [
    "#### Launch Huggingface Trainer directly in Notebook\n",
    "\n",
    "This will use multiple GPUs, but will be hideously crippled on account of contention for the Global Interpreter Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fa1bc-e2a5-4dc4-818c-e3e546338408",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop('forgather_config.yaml', 'hf_trainer_experiment.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c8567-b2a0-4b95-884e-9676a0e3825f",
   "metadata": {},
   "source": [
    "#### Launch Huggingface Trainer with Notebook Launcher\n",
    "\n",
    "In theory, this should work with the Huggingface Trainer...\n",
    "\n",
    "[forgather_demo/hf_trainer_experiment.yaml](forgather_demo/hf_trainer_experiment.yaml)  \n",
    "\n",
    "At present, it does not appear to detect that it is running in a multi-gpu configuration. The same config works just fine from a regual training script. Cause TBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b131a9-7d4e-4f42-a829-358c29713705",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(\n",
    "    training_loop,\n",
    "    args=('forgather_config.yaml', 'hf_trainer_experiment.yaml',),\n",
    "    num_processes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece555d-aa15-4b5e-a80e-20cb9fa9db52",
   "metadata": {},
   "source": [
    "### Train from a Training Script\n",
    "\n",
    "The preferred way to run non-trivial training tasks is from the command-line.\n",
    "\n",
    "The following code can help you get started. It will take the path configuration and build a command-line, which can either be executed from the notebook or can be exported as a bash-script.\n",
    "\n",
    "This is especially important for long-running training sessions, as various issues with the notebook could interrupt training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a399e7a-6220-426f-affa-f9bfba4b9a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "templates_dir: 'forgather_demo'\n",
      "tokenizer_dir: 'experiment.TOKENIZERS_DIR'\n",
      "datasets_dir: 'experiment.DATASETS_DIR'\n",
      "assets_dir: '..'\n",
      "search_paths: '['forgather_demo', '../templates', '../model_zoo']'\n",
      "whitelist_path: 'forgather_demo/whitelist.yaml'\n",
      "model_src_dir: '../model_zoo'\n",
      "script_dir: '../scripts'\n",
      "train_script_path: '../scripts/train_script.py'\n",
      "models_dir: 'forgather_demo/output_models'\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "from aiws.dotdict import DotDict\n",
    "from forgather.config import load_whitelist_as_set, load_config, pconfig\n",
    "from forgather import Latent\n",
    "import stat\n",
    "import os\n",
    "\n",
    "# Load project directory definitions\n",
    "metacfg = DotDict(load_config('forgather_config.yaml').config)\n",
    "pconfig(metacfg)\n",
    "\n",
    "# Output train-script command line as a string\n",
    "def train_cmdline(metacfg, nproc='gpu'):\n",
    "    includes = ''.join(f\"-I '{inc}' \" for inc in metacfg.search_paths)\n",
    "    return f\"torchrun --standalone --nproc-per-node {nproc} '{metacfg.train_script_path}' -w '{metacfg.whitelist_path}' {includes} -s '{metacfg.assets_dir}'\"\n",
    "\n",
    "# Output train-script as command line as a bash-script\n",
    "# ./train.sh [<other-sli-args] <experiment-config-file>\n",
    "def make_bash_script(metacfg, script_path='train.sh', nproc='gpu'):\n",
    "    with open(script_path, 'w') as f:\n",
    "        f.write('#!/bin/bash\\n' + train_cmdline(metacfg, nproc) + ' \"${@}\"\\n')\n",
    "        os.chmod(f.fileno(), stat.S_IREAD|stat.S_IRUSR|stat.S_IWUSR|stat.S_IXUSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae89f3-11cb-45d9-b4ec-6ac2ae35a5be",
   "metadata": {},
   "source": [
    "#### Generate Bash Script\n",
    "\n",
    "This will output a shell-script which will invoke the training script with the arguments for this project.\n",
    "\n",
    "```bash\n",
    "# Optional: Restrict the GPUs to use to a sub-set of those avialable.\n",
    "export CUDA_VISIBLE_DEVICES=\"0,1\"\n",
    "\n",
    "# Run Accelerate Trainer experiment\n",
    "./train forgather_demo/accel_experiment.yaml\n",
    "\n",
    "# Run Huggingface Trainer experiment\n",
    "./train forgather_demo/hf_trainer_experiment.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e3cdc6-925d-4b28-ae59-778c011dcd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "torchrun --standalone --nproc-per-node gpu '../scripts/train_script.py' -w 'forgather_demo/whitelist.yaml' -I 'forgather_demo' -I '../templates' -I '../model_zoo'  -s '..' \"${@}\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_bash_script(metacfg)\n",
    "\n",
    "# Read back to verify\n",
    "with open('train.sh', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a3131-a09e-46de-8f60-6699f758630c",
   "metadata": {},
   "source": [
    "#### Run Training Script from Notebook\n",
    "\n",
    "This will execute a shell command to run the training script, where the notebook will act as the shell console.  \n",
    "\n",
    "Note: The tqdm progress bars do not render properly in the notebook. It will train, but it's ugly. Running this from a real terminal looks much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a0cf4-1c93-47cf-8888-ffb806df684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, this will run on all available GPUs. To restrict it to a sub-set, you can use this envrionment variable.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "!{train_cmdline(metacfg)} 'forgather_demo/hf_trainer_experiment.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369d483-f90f-484e-aeaa-88fe51a20901",
   "metadata": {},
   "source": [
    "### Injecting Callables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a3fd5f-7ba4-4a6d-89e3-dc03401e1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define the tokenizer to use\n",
      ".define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
      "    - \"../tokenizers/tiny_stories_2k\"\n",
      "\n",
      "# Load a dataset from the hub\n",
      ".define: &raw_dataset !callable:datasets:load_dataset [ \"roneneldan/TinyStories\" ]\n",
      "\n",
      "# Call the injected Callable, 'tokenize_dataset'\n",
      "# Note that we can pass objects created in the config into the callable.\n",
      "dataset: &dataset !callable:tokenize_dataset [ *raw_dataset, *tokenizer, [ 0.01, 1.0 ] ]\n",
      "\n",
      "# Get splits\n",
      "train_dataset: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
      "eval_dataset: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
      "----------------------------------------\n",
      "- transformers:AutoTokenizer.from_pretrained\n",
      "- datasets:load_dataset\n",
      "- forgather.construct:get_item\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "\n",
    "tokenize_config = \"\"\"\n",
    "-- set tokenizer_path = path_join('..', 'tokenizers', 'tiny_stories_2k')\n",
    "-- set dataset_id = \"roneneldan/TinyStories\"\n",
    "\n",
    "# Define the tokenizer to use\n",
    ".define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
    "    - \"{{ tokenizer_path }}\"\n",
    "\n",
    "# Load a dataset from the hub\n",
    ".define: &raw_dataset !callable:datasets:load_dataset [ \"{{ dataset_id }}\" ]\n",
    "\n",
    "# Call the injected Callable, 'tokenize_dataset'\n",
    "# Note that we can pass objects created in the config into the callable.\n",
    "dataset: &dataset !callable:tokenize_dataset [ *raw_dataset, *tokenizer, [ 0.01, 1.0 ] ]\n",
    "\n",
    "# Get splits\n",
    "train_dataset: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
    "eval_dataset: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
    "\"\"\"\n",
    "\n",
    "tokenize_whitelist = \"\"\"\n",
    "- transformers:AutoTokenizer.from_pretrained\n",
    "- datasets:load_dataset\n",
    "- forgather.construct:get_item\n",
    "\"\"\"\n",
    "\n",
    "# Dump a preview of the pre-processed configs\n",
    "print(preprocess_config(tokenize_config, load_method=\"from_string\"))\n",
    "print('-' * 40)\n",
    "print(preprocess_config(tokenize_whitelist, load_method=\"from_string\"))\n",
    "print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852741a-6e6f-4409-8f59-4d0e1c9abc34",
   "metadata": {},
   "source": [
    "Note that the tag '!callable:tokenize_dataset' is not a valid import-spec, as it lacks a ':' in the string.\n",
    "\n",
    "This is a 'stand-in' tag, which needs to be filled in when the config is materialized.\n",
    "\n",
    "Let's define the stand-in and inject it into materialize_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f8535d-12c0-4736-9603-f0ff21676abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      "  dataset:\n",
      "    train:\n",
      "      Dataset({\n",
      "          features: ['input_ids'],\n",
      "          num_rows: 21197\n",
      "      })\n",
      "    validation:\n",
      "      Dataset({\n",
      "          features: ['input_ids'],\n",
      "          num_rows: 21990\n",
      "      })\n",
      "  train_dataset:\n",
      "    Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 21197\n",
      "    })\n",
      "  eval_dataset:\n",
      "    Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "pp_config:\n",
      "       1: # Define the tokenizer to use\n",
      "       2: .define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
      "       3:     - \"../tokenizers/tiny_stories_2k\"\n",
      "       4: \n",
      "       5: # Load a dataset from the hub\n",
      "       6: .define: &raw_dataset !callable:datasets:load_dataset [ \"roneneldan/TinyStories\" ]\n",
      "       7: \n",
      "       8: # Call the injected Callable, 'tokenize_dataset'\n",
      "       9: # Note that we can pass objects created in the config into the callable.\n",
      "      10: dataset: &dataset !callable:tokenize_dataset [ *raw_dataset, *tokenizer, [ 0.01, 1.0 ] ]\n",
      "      11: \n",
      "      12: # Get splits\n",
      "      13: train_dataset: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
      "      14: eval_dataset: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
      "  \n",
      "whitelist:\n",
      "  - 'datasets:load_dataset'\n",
      "  - 'forgather.construct:get_item'\n",
      "  - 'transformers:AutoTokenizer.from_pretrained'\n",
      "pp_whitelist:\n",
      "       1: - transformers:AutoTokenizer.from_pretrained\n",
      "       2: - datasets:load_dataset\n",
      "       3: - forgather.construct:get_item\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Define a Callable to inject.\n",
    "def tokenize_dataset(dataset_dict, tokenizer, select: list[float]):\n",
    "    \"\"\"\n",
    "    Given a DatasetDict and tokenizer, tokenize each split and return it in a new dictionary.\n",
    "\n",
    "    select: A list of floats, each which specifies how much of the dataset to include.\n",
    "        e.g. [0.1, 1.0 ] = 10% and 100%\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    \n",
    "    def map_fn(element, tokenizer):\n",
    "        outputs = tokenizer(\n",
    "            element[\"text\"],\n",
    "            truncation=True,\n",
    "        )\n",
    "        return {\"input_ids\": outputs[\"input_ids\"]}\n",
    "    \n",
    "    for i, (split, dataset) in enumerate(dataset_dict.items()):\n",
    "        if select[i] < 1.0:\n",
    "            dataset = dataset.select(range(0, int(len(dataset) * select[i])))\n",
    "        \n",
    "        tokenized_data = dataset.map(\n",
    "            map_fn,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names,\n",
    "            fn_kwargs=dict(tokenizer=tokenizer)\n",
    "        )\n",
    "        output[split] = tokenized_data\n",
    "    return output\n",
    "\n",
    "# Inject the Callable via kwargs.\n",
    "config_output = materialize_config(\n",
    "    tokenize_config,\n",
    "    tokenize_whitelist,\n",
    "    load_method=\"from_string\",\n",
    "    kwargs=dict(tokenize_dataset=tokenize_dataset),\n",
    ")\n",
    "\n",
    "pconfig(config_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af73f5-401e-4758-b4a2-9a6e855235a3",
   "metadata": {},
   "source": [
    "As instances of 'Latent' are themselves Callables, where they materialize their definition when called, they definitions can be chained.\n",
    "\n",
    "Here, we define a config for a raw dataset and a config for tokenizing an injected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd62e654-3062-41de-92fb-a780f60e371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent('datasets:load_dataset', *['roneneldan/TinyStories'], **{})\n",
      "----------------------------------------\n",
      "dataset:\n",
      "  Latent 'tokenize_dataset'\n",
      "    - Latent 'raw_dataset'\n",
      "    - Latent 'transformers:AutoTokenizer.from_pretrained'\n",
      "      - '../tokenizers/tiny_stories_2k'\n",
      "    - - 0.01\n",
      "    - 1.0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from forgather.config import load_config\n",
    "from forgather import Latent\n",
    "\n",
    "raw_dataet_def = \"\"\"\n",
    "-- set dataset_id = \"roneneldan/TinyStories\"\n",
    "!callable:datasets:load_dataset [ \"{{ dataset_id }}\" ]\n",
    "\"\"\"\n",
    "\n",
    "tokenize_dataset_def = \"\"\"\n",
    "-- set tokenizer_path = path_join('..', 'tokenizers', 'tiny_stories_2k')\n",
    ".define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
    "    - \"{{ tokenizer_path }}\"\n",
    "dataset: &dataset !callable:tokenize_dataset [ !callable:raw_dataset [], *tokenizer, [ 0.01, 1.0 ] ]\n",
    "\"\"\"\n",
    "\n",
    "whitelist = load_config(tokenize_whitelist, load_method=\"from_string\").config\n",
    "raw_dataset = load_config(raw_dataet_def, load_method=\"from_string\").config\n",
    "\n",
    "dataset = load_config(\n",
    "    tokenize_dataset_def,\n",
    "    load_method=\"from_string\",\n",
    "    tokenize_dataset=tokenize_dataset,\n",
    "    raw_dataset=raw_dataset\n",
    ").config\n",
    "\n",
    "print(raw_dataset)\n",
    "print('-' * 40)\n",
    "pconfig(dataset)\n",
    "print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86e666-7891-4619-84ee-ff96eceff1c0",
   "metadata": {},
   "source": [
    "When we materialize the dataset, we pass the raw_dataset definition to the tokenize_dataset definition.\n",
    "\n",
    "We could also have first materialized the raw_dataset and then injected it as a lambda expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac67d4b-06dd-443a-ab01-319fab65189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  train:\n",
      "    Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 21197\n",
      "    })\n",
      "  validation:\n",
      "    Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 21990\n",
      "    })\n"
     ]
    }
   ],
   "source": [
    "output_config = Latent.materialize(dataset, whitelist=whitelist, raw_dataset=raw_dataset, tokenize_dataset=tokenize_dataset)\n",
    "pconfig(output_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3c45a-d962-4316-8146-afd0a1024614",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Delete all of the output models produced by the demo and start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073be4ea-bebc-4f64-8248-3818d8717863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 'forgather_demo/output_models'\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "from aiws.dotdict import DotDict\n",
    "from forgather.config import load_config, pconfig\n",
    "metacfg = DotDict(load_config('forgather_config.yaml').config)\n",
    "\n",
    "print(f\"Removing '{metacfg.models_dir}'\")\n",
    "shutil.rmtree(metacfg.models_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808aab0d-4609-4cfc-937d-dad0ec5dccbc",
   "metadata": {},
   "source": [
    "## forgather.latent\n",
    "\n",
    "A Latent \\[object\\] abstracts what to create from when to create it\n",
    "\n",
    "The primary intended use-case is for safely constructing objects from a\n",
    "configuration file. Consider the case where a configuration file may define objects which\n",
    "can take a considerable amount of time to construct (i.e. processing a dataset).\n",
    "\n",
    "In this case, its useful to allow the complete file to be parsed before attempting a\n",
    "lengthy task, as there may still be errors present which will cause the operation to\n",
    "abort. It's much better to first fully parse the file, validate the safety of \n",
    "the all the types, and only then then, materialize the definiton. This is far less\n",
    "painful than having to fix a single error, wait for the long operation to complete (again)\n",
    "and then hit another error. Fun times...\n",
    "\n",
    "Allowing deferal can also avoid materializing expensive objects which are not needed, as per\n",
    "runtime logic. For example, a definition may define several datasets, where-as only a single\n",
    "one is actually selected, contingent upon 'whatever.'\n",
    "\n",
    "If an object is never materialized, this also avoids loading the associated modules.\n",
    "\n",
    "Finally, this allows one two lazilly construct objects in whatever order makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06100e8b-b0d6-41ee-a784-7aab8c8b62d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent('torch:Tensor', *([1, 2, 3],), **{}, as_callable=True, is_singleton=False)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "from forgather import Latent\n",
    "from forgather.config import pconfig\n",
    "\n",
    "# Define the object to construct\n",
    "latent_tensor = Latent(\"torch:Tensor\", [1 ,2, 3], as_callable=True, is_singleton=False)\n",
    "print(latent_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1c6c4b-e681-4768-b344-fb8b42a51620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# ... and some time later, materialize the object instance.\n",
    "tensor = latent_tensor()\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcab27-b7f2-4d0e-8fc4-ba8d476ec07d",
   "metadata": {},
   "source": [
    "This can also extend to graphs of objects..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2501ad79-4f52-46a8-aed7-043c01a22e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': Latent('torch:sum', *(Latent('torch:Tensor', *([1, 2, 3],), **{}, as_callable=False, is_singleton=False),), **{}, as_callable=False, is_singleton=False)}\n",
      "{'total': tensor(6.)}\n"
     ]
    }
   ],
   "source": [
    "data = dict(\n",
    "    total = Latent(\"torch:sum\", Latent(\"torch:Tensor\", [1 ,2, 3]))\n",
    ")\n",
    "print(data)\n",
    "\n",
    "obj = Latent.materialize(data)\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1cf2f-2d34-4de2-b6f9-775fef76df40",
   "metadata": {},
   "source": [
    "We can also specify modules by path-name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e1e95ed-9195-44f8-a560-7607c4cc7f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent('../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer', *(Latent('../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig', *(), **{'hidden_size': 64, 'num_hidden_layers': 3}, as_callable=False, is_singleton=False),), **{}, as_callable=False, is_singleton=False)\n",
      "******************** or pretty-printed... ********************\n",
      "Latent '../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer'\n",
      "  - Latent '../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig'\n",
      "    hidden_size: 64\n",
      "    num_hidden_layers: 3\n"
     ]
    }
   ],
   "source": [
    "model = Latent(\n",
    "    \"../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\",\n",
    "    Latent(\n",
    "        \"../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig\",\n",
    "        hidden_size=64, num_hidden_layers=3\n",
    "    )\n",
    ")\n",
    "print(model)\n",
    "print('*' * 20 + \" or pretty-printed... \" + \"*\" * 20)\n",
    "pconfig(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3920a9d7-cc6b-4774-b21d-17da424c473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 64)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (key_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (value_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=64, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# And materialize the definition...\n",
    "# The __call__ method is short-hand for Latent.materialize(model)\n",
    "# If called (or materialized) again, the same instance will be returned.\n",
    "materialized_model = model()\n",
    "print(materialized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41181c2-715a-491a-aded-06f731f49f42",
   "metadata": {},
   "source": [
    "You can restrict which types of objects can be materialized by specifying a whitelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b01a44-0979-42e8-a16d-bd755e8a6a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from forgather.dynamic import normalize_import_spec\n",
    "# Note: Any import-specs with paths should be normaized\n",
    "# with forgather.dynamic.normalize_import_spec(). This\n",
    "# ensures that all equivalent paths have the same representation.\n",
    "whitelist = set((\n",
    "    \"torch:Tensor\",\n",
    "    \"torch:add\",\n",
    "    normalize_import_spec(\"../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\"),\n",
    "))\n",
    "\n",
    "allowed_instance = Latent(\"torch:Tensor\", [4, 5, 6])\n",
    "allowed_instance(whitelist=whitelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d38023-1c90-4564-a166-38784b5d8bfd",
   "metadata": {},
   "source": [
    "If something is not in the whitelist, an exception will be raised with a list of all prohibited types listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97af7233-685b-4510-82de-2f10657ecfad",
   "metadata": {},
   "outputs": [
    {
     "ename": "LatentException",
     "evalue": "The following dynamic imports were not found in the whitelist: {'torch:mul', 'torch:sum'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLatentException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prohibited_instance \u001b[38;5;241m=\u001b[39m Latent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch:mul\u001b[39m\u001b[38;5;124m\"\u001b[39m, Latent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch:sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, Latent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch:Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;241m1\u001b[39m ,\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])), \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprohibited_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhitelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhitelist\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_assets/aiworkshop/tutorial/../forgather/latent.py:241\u001b[0m, in \u001b[0;36mLatent.__call__\u001b[0;34m(self, whitelist, **mapping)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, whitelist: Container\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmapping):\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Alias for calling materialize() on self\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLatent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhitelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhitelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_assets/aiworkshop/tutorial/../forgather/latent.py:256\u001b[0m, in \u001b[0;36mLatent.materialize\u001b[0;34m(obj, whitelist, **mapping)\u001b[0m\n\u001b[1;32m    254\u001b[0m     invalid_set \u001b[38;5;241m=\u001b[39m Latent\u001b[38;5;241m.\u001b[39mvalidate_whitelist(obj, whitelist)\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(invalid_set):\n\u001b[0;32m--> 256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LatentException(\n\u001b[1;32m    257\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following dynamic imports were not found in the whitelist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpformat(invalid_set)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    258\u001b[0m Latent\u001b[38;5;241m.\u001b[39m_resolve_standins(obj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmapping)\n\u001b[1;32m    259\u001b[0m Latent\u001b[38;5;241m.\u001b[39m_resolve_dynamic_imports(obj)\n",
      "\u001b[0;31mLatentException\u001b[0m: The following dynamic imports were not found in the whitelist: {'torch:mul', 'torch:sum'}"
     ]
    }
   ],
   "source": [
    "prohibited_instance = Latent(\"torch:mul\", Latent(\"torch:sum\", Latent(\"torch:Tensor\", [1 ,2, 3])), 3)\n",
    "prohibited_instance(whitelist=whitelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41011981-a27f-47d3-aaa4-dba4c15355c9",
   "metadata": {},
   "source": [
    "Alternatively, we can just get the list of disallowed types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bccaa2b-ac89-4e47-b547-d28c80bda2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disallowed: {'torch:mul', 'torch:sum'}\n"
     ]
    }
   ],
   "source": [
    "invalid_set = Latent.validate_whitelist(prohibited_instance, whitelist)\n",
    "if len(invalid_set):\n",
    "    # Show all disallowed types in the graph\n",
    "    print(f\"Disallowed: {invalid_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3c0c1-b6d0-4ea8-8cf6-4117ce95dbea",
   "metadata": {},
   "source": [
    "#### Object Identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e72117-f36f-46e1-a631-6b67cb339158",
   "metadata": {},
   "source": [
    "By default, each call returns the same object instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e252f4cf-1d3d-4628-a948-df46c000686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_tensor = Latent(\"torch:Tensor\", [1 ,2, 3])\n",
    "tensor = latent_tensor()\n",
    "assert(id(tensor) != id(latent_tensor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da14f48-e146-4e0e-9f71-383443a3d222",
   "metadata": {},
   "source": [
    "This can be overridden by setting the \"is_singleton\" flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e488729-5015-44cd-87e4-9fd2cc9cfa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_tensor = Latent(\"torch:Tensor\", [1 ,2, 3], is_singleton=True)\n",
    "tensor = latent_tensor()\n",
    "assert(id(tensor) == id(latent_tensor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9440faf4-0863-4f2b-9924-cb56cf46e4db",
   "metadata": {},
   "source": [
    "Arguments can be injected into the graph at the poin of materialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5a40f-f7af-4a1f-9cee-b1f851300828",
   "metadata": {},
   "source": [
    "#### Passing Arugments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4656ee54-2623-4dbc-b9a5-e9e64989bd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Notice how the second import-spec, \"arg,\" does not have a ':' character.\n",
    "# This is a placeholder for a real value to be specified when the object is materialized.\n",
    "deferred_sum = Latent(\"torch:sum\", Latent(\"arg\"))\n",
    "\n",
    "# Let's create an object to substitue 'sum_input' with.\n",
    "tensor = torch.tensor([1 ,2, 3])\n",
    "\n",
    "# Materialize the value\n",
    "deferred_sum(arg=tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc67fc-f344-4a75-ad03-736473dbbaab",
   "metadata": {},
   "source": [
    "### Corner Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef43c2-aae6-4e02-90b9-16ba8e5d7ed8",
   "metadata": {},
   "source": [
    "#### Tied Parameters\n",
    "As mentioned above, the default is for all instancs of the same object to be singletons; that is, there is really only one instance, no matter how many times you call the object.\n",
    "\n",
    "By setting 'is_singleton' to False, you will get a different instance each time the object is materialized, but what happens when a non-singleton object exists in more than one place in the graph?\n",
    "\n",
    "For example, here we have a simple ML model which takes an input tensor and an output tensor as arguments, which are then used as parameters. If we create a single tensor and pass it as both the input and output arguments, this ties the weights together, as they share the same instance.\n",
    "\n",
    "If the shared parameter is not a singleton, will won't this 'untie' the shared parameter?\n",
    "\n",
    "No. When constructing the object graph, we keep track of which objects have already been instantiated with an object-id map. If an object with the same ID is 'constructed' a second time, the 'cached' object will be returned, rather than a new one.\n",
    "\n",
    "The difference only comes about when the object is materialized more than once, in which case the 'cache' is flushed between calls and a new instance of the object will be constructed. This difference can only be observed when the graph is constructed more than once.\n",
    "\n",
    "In this example, we construct the model described above and print the values of the input and output weights.\n",
    "\n",
    "If the shared_weights are non-singleton, then each call will initialize different weights, but they will still be tied.\n",
    "\n",
    "If configured as a singleton, each call will produce the same weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3666ae25-ca34-4616-a731-f0f5373f9783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent(<class '__main__.Net'>, *(), **{'input': Latent('torch:randn', *(3, 4), **{'requires_grad': True}, as_callable=False, is_singleton=False), 'output': Latent('torch:randn', *(3, 4), **{'requires_grad': True}, as_callable=False, is_singleton=False)}, as_callable=False, is_singleton=False)\n",
      "tensor([[-1.0802, -0.2785, -0.5239,  1.4973],\n",
      "        [-1.2288,  0.7229,  1.9864,  0.4387],\n",
      "        [ 1.3913, -1.0829, -0.9404, -0.3665]])\n",
      "tensor([[-1.0802, -0.2785, -0.5239,  1.4973],\n",
      "        [-1.2288,  0.7229,  1.9864,  0.4387],\n",
      "        [ 1.3913, -1.0829, -0.9404, -0.3665]])\n",
      "\n",
      "\n",
      "\n",
      "tensor([[ 1.1079,  0.3517, -0.6129,  0.7602],\n",
      "        [-0.4242, -0.4379,  0.8860,  0.6022],\n",
      "        [ 0.0201,  0.4126, -1.1070,  0.5717]])\n",
      "tensor([[ 1.1079,  0.3517, -0.6129,  0.7602],\n",
      "        [-0.4242, -0.4379,  0.8860,  0.6022],\n",
      "        [ 0.0201,  0.4126, -1.1070,  0.5717]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a simple model\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super().__init__()\n",
    "        self.input = torch.nn.Parameter(input)\n",
    "        self.output = torch.nn.Parameter(output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  x @ self.input\n",
    "        x = x @ self.output.t()\n",
    "        return x\n",
    "\n",
    "# Create a 'shared' tensor for both input and output networks.\n",
    "# Try changing 'is_singleton'\n",
    "shared_weights = Latent('torch:randn', 3, 4, requires_grad=True, is_singleton=False)\n",
    "latent_model = Latent(Net, input=shared_weights, output=shared_weights, is_singleton=False)\n",
    "print(latent_model)\n",
    "\n",
    "model = latent_model()\n",
    "print(model.input.data)\n",
    "print(model.output.data)\n",
    "print(\"\\n\\n\")\n",
    "model = latent_model()\n",
    "print(model.input.data)\n",
    "print(model.output.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d62de-7c91-421b-948b-c88ee322654b",
   "metadata": {},
   "source": [
    "#### Factory Objects\n",
    "\n",
    "One use-case calls for providing 'factory' agruments to an object, where each call produces a new objects instance.\n",
    "\n",
    "Consider this use-case:\n",
    "\n",
    "The Huggingface Trainer class allows you to pass a \"model intializer,\" rather than a model instance, to the Trainer. The Trainer will then explore various hyper-parameters, initializing a new model instance on each iteration.\n",
    "\n",
    "If the Trainer is part of a configuration and the model is also in the configuration, this makes it rather difficult to pass a \"model initializer\" to the Trainer; when the Latent graph is constructed, the initializer will be a concrete model instance, not a callable constructor.\n",
    "\n",
    "This can be solved by setting the 'as_callable' flag on the model constructor, which result in an unmaterialized callable being passed to the Trainer.\n",
    "\n",
    "Now, when the Trainer calls the model initializer, the model will be materialized.\n",
    "\n",
    "This does not fully solve the problem, as subsequent calls will return the same model. We can solve this by setting 'is_singleton=False,' which will resut in a new model instance each time it is called.\n",
    "\n",
    "If the model has any other Latent objects, these too can be independently configured as singletons or callables.\n",
    "\n",
    "Finally, if the called function is expected to take any arguments, these can be mapped to arguments anywhere in the graph of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eb9abcc-166d-4e14-9a90-ccb68e01ebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Construct Latent; all Latent objects are still latent.\n",
      "\n",
      "Latent(<class '__main__.TensorFactory'>, *(Latent('torch:randn', *(Latent('rows', *(), **{}, as_callable=False, is_singleton=False), Latent('cols', *(), **{}, as_callable=False, is_singleton=False)), **{}, as_callable=True, is_singleton=False),), **{}, as_callable=False, is_singleton=False)\n",
      "----------------------------------------\n",
      "\n",
      "Materialized Latnet; as the 2nd level latent is 'as_callable,' is was passed to the factory untouched.\n",
      "\n",
      "Furthermore, this isolated the 3rd level Latent, so it was also not materialized.\n",
      "\n",
      "TensorFactory(Latent(<built-in method randn of type object at 0x7f262e6a4760>, *(Latent('rows', *(), **{}, as_callable=False, is_singleton=False), Latent('cols', *(), **{}, as_callable=False, is_singleton=False)), **{}, as_callable=True, is_singleton=False))\n",
      "----------------------------------------\n",
      "\n",
      "Factory calls Latent, passing different arguments each time.\n",
      " On each call, the arguments are resovled and a new instance is returned\n",
      "\n",
      "Tensor 1:  tensor([[-1.6665,  1.2161, -0.9514],\n",
      "        [ 0.2967,  0.1698,  0.7825],\n",
      "        [-0.2784,  0.6371,  0.6405],\n",
      "        [ 0.3670, -1.2892, -1.6077],\n",
      "        [-0.7381,  0.7189, -0.8046],\n",
      "        [-1.0673, -0.6421, -0.2898]])\n",
      "Tensor 2:  tensor([[ 2.2907,  1.9049,  1.6259, -0.0329,  0.8835],\n",
      "        [-0.5203,  0.3435, -0.2153, -1.8938, -0.5813],\n",
      "        [ 0.4697,  0.6029,  0.6229,  0.5153, -1.1612]])\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Factory class. Given a callable, when called it uses the provided callable to create new objects.\n",
    "class TensorFactory:\n",
    "    def __init__(self, factory: Callable):\n",
    "        assert isinstance(factory, Callable)\n",
    "        self.factory = factory\n",
    "        self.n_cols = 1\n",
    "\n",
    "    def make_tensor(self, **argv):\n",
    "        # Pass a combination of arguments from the caller and from the factory\n",
    "        # Increase the number of columns by two each time\n",
    "        self.n_cols += 2\n",
    "        return self.factory(cols=self.n_cols, **argv)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TensorFactory({self.factory})\"\n",
    "\n",
    "# Experiment with changing the arguments to see how this works.\n",
    "print(\"\\Construct Latent; all Latent objects are still latent.\\n\")\n",
    "latent_factory = Latent(\n",
    "    TensorFactory,\n",
    "    Latent(\n",
    "        \"torch:randn\", # Initialize a random tensor with the specified dimensions.\n",
    "        Latent(\n",
    "            \"rows\", # This is an argument which can be specified called.\n",
    "            is_singleton=False, # Create a new instance each time.\n",
    "        ),\n",
    "        Latent(\n",
    "            \"cols\", # This is an argument which can be specified called.\n",
    "            is_singleton=False, # Create a new instance each time.\n",
    "        ),\n",
    "        is_singleton=False, # Each call should return a new instance.\n",
    "        as_callable=True, # Pass object as a Callable, rather than immediately materializing it.\n",
    "    )\n",
    ")\n",
    "print(latent_factory)\n",
    "\n",
    "print('-' * 40)\n",
    "print(\"\\nMaterialized Latnet; as the 2nd level latent is 'as_callable,' is was passed to the factory untouched.\")\n",
    "print(\"\\nFurthermore, this isolated the 3rd level Latent, so it was also not materialized.\\n\")\n",
    "factory = latent_factory()\n",
    "print(factory)\n",
    "\n",
    "print('-' * 40)\n",
    "print(\"\\nFactory calls Latent, passing different arguments each time.\")\n",
    "print(\" On each call, the arguments are resovled and a new instance is returned\\n\")\n",
    "print(\"Tensor 1: \", factory.make_tensor(rows=6))\n",
    "print(\"Tensor 2: \", factory.make_tensor(rows=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ea98c-3972-4ff8-a66e-c04d543ab1cb",
   "metadata": {},
   "source": [
    "## forgather.dynamic\n",
    "\n",
    "The 'dynamic' module can dynamically import attributes from Python modules, given either a module and attribute name in the sys.path or directly from a file-path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad9db0ef-6705-478e-9794-028ef0cd4f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from forgather.dynamic import dynamic_import\n",
    "import os\n",
    "\n",
    "# Create a simple namespace to put our dynamic imports in\n",
    "# The global namespace works too, but I want to avoid cluttering it with our demo imports.\n",
    "ns = SimpleNamespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d1859-f8a4-4caf-a2ba-de23eefee98a",
   "metadata": {},
   "source": [
    "We can import an attribute from a module.\n",
    "\n",
    "In this example, we get the torch.tensor class and the torch 'nn' namespace. Once imported, be can use them just like a regular import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "678aea2b-da25-4002-8418-26a3b856bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([1, 2, 3])\n",
      "Module()\n"
     ]
    }
   ],
   "source": [
    "ns.tensor = dynamic_import(\"torch:tensor\")\n",
    "ns.nn = dynamic_import(\"torch:nn\")\n",
    "\n",
    "# Create a tensor\n",
    "tensor = ns.tensor([1, 2, 3])\n",
    "print(type(tensor), tensor)\n",
    "\n",
    "# Create an torch.nn.Module\n",
    "module = ns.nn.Module()\n",
    "print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3a172-11bb-4c6b-aafc-b651dd049763",
   "metadata": {},
   "source": [
    "We can also import modules directly from a Python source file, even if it's not in our sys.path.\n",
    "\n",
    "For example, let's get a transformer config and model definiton and instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c744a22-3a29-4dc6-80d3-d79de5a31434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model_zoo/vanilla_transformer/vanilla_transformer.py\n",
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (key_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (value_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "module_path = os.path.join('..', 'model_zoo', 'vanilla_transformer', 'vanilla_transformer.py')\n",
    "print(module_path)\n",
    "ns.ModelConfig = dynamic_import(module_path + ':VanillaTransformerConfig')\n",
    "ns.TransformerModel = dynamic_import(module_path + ':VanillaTransformer')\n",
    "\n",
    "model = ns.TransformerModel(ns.ModelConfig(hidden_size=128, num_hidden_layers=2))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c60124-c1f7-44dc-8fb4-40090aa40253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
