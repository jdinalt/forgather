{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Forgather\n",
    "\n",
    "[forgather/config.py](../forgather/config.py)  \n",
    "[forgather/latent.py](../forgather/latent.py)  \n",
    "[forgather/dynamic.py](../forgather/dynamic.py)  \n",
    "\n",
    "---\n",
    "What exactly is this \"Forgather\" thing? What is it good for?\n",
    "\n",
    "That's a good question. It's probably easiest to just demonstrate..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5a90f-6386-4b8e-89ce-ec19195f7dff",
   "metadata": {},
   "source": [
    "## forgather.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "\n",
    "from pprint import pformat, pp\n",
    "from transformers import set_seed\n",
    "\n",
    "from forgather.config import (\n",
    "    preprocess_config,\n",
    "    load_config,\n",
    "    load_whitelist_as_set,\n",
    "    materialize_config,\n",
    "    enumerate_whitelist_exceptions,\n",
    "    pconfig,\n",
    ")\n",
    "#from forgather import Latent\n",
    "from aiws.dotdict import DotDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826aff3d-82f7-43bd-a4c1-13f53f45573d",
   "metadata": {},
   "source": [
    "### A Quick Demo\n",
    "\n",
    "This demontrates what this package is about..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0570e4cd-afe6-4de7-996c-ae0e712c93ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a training configuration with YAML, including specifying object types.\n",
    "#\n",
    "# We use the Yaml SafeLoader, which disallows the creation of arbitrary Python\n",
    "# objects, but we had added the '!callable' tag. More on that later...\n",
    "#\n",
    "# Note that this is not pure Yaml. Jinja (sandboxed) is used as a pre-processor.\n",
    "yaml_config = \"\"\"\n",
    "-- set output_dir = path_join('forgather_demo', 'output_models', 'quick_demo')\n",
    "# Define the tokenizer to use\n",
    ".define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
    "    - \"../tokenizers/tiny_stories_2k\"\n",
    "\n",
    "# Define a model -- note how we can specify the file path to the module...\n",
    ".define: &model !callable:../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\n",
    "    - !callable:../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig\n",
    "        kwargs:\n",
    "            hidden_size: 128\n",
    "            num_hidden_layers: 2\n",
    "\n",
    "# Define the train and eval datasets\n",
    ".define: &dataset !callable:datasets:load_from_disk [ \"../datasets/tiny_stories_tokenized\" ]\n",
    ".define: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
    ".define: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
    "\n",
    "# Define a trainer\n",
    "trainer: !callable:aiws.trainer:Trainer\n",
    "    kwargs:\n",
    "        model: *model\n",
    "        train_dataset: *train_dataset\n",
    "        eval_dataset: *eval_dataset\n",
    "        tokenizer: *tokenizer\n",
    "        args: !callable:aiws.trainer_types:TrainingArguments\n",
    "            kwargs:\n",
    "                output_dir: \"{{ output_dir }}\"\n",
    "                eval_steps: 250\n",
    "                logging_steps: 50\n",
    "                max_steps: 500\n",
    "                eval_strategy: \"steps\"\n",
    "                save_strategy: \"no\"\n",
    "\"\"\"\n",
    "\n",
    "# As you may have guessed, the '!callable' tags call Python code.\n",
    "# Which 'Callables' are allowed is controlled by defining a whitelist.\n",
    "#\n",
    "# Note: Passing a whitelist is optional, but VERY strongly recommended.\n",
    "#\n",
    "# It should go without saying that you should NEVER use an untrusted\n",
    "# config file with an untrusted whitelist without careful examination.\n",
    "#\n",
    "# Care has been taking to try to make this as safe as possible, but I can't promise\n",
    "# that the security is perfect. I'm not aware of any flaws, but that does not mean that\n",
    "# they don't exist.\n",
    "whitelist_yaml = \"\"\"\n",
    "- transformers:AutoTokenizer.from_pretrained\n",
    "- datasets:load_from_disk\n",
    "- forgather.construct:get_item\n",
    "- aiws.trainer:Trainer\n",
    "- aiws.trainer_types:TrainingArguments\n",
    "- ../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\n",
    "- ../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b7a02-f9e0-4104-a427-9257f22ee864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the object definition and use it!\n",
    "trainer = materialize_config(yaml_config, whitelist_yaml, load_method=\"from_string\").config[\"trainer\"]\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72dc2c0-32ac-4dc9-941e-cd586d8fa2a1",
   "metadata": {},
   "source": [
    "### Digging a little deeper...\n",
    "That's pretty much what it's for, in a nut-shell.\n",
    "\n",
    "But wait. There's more!\n",
    "\n",
    "One of the things which has bugged me when working on ML projects is the proliferation of training scripts and configurations. Before long you are working with a copy-of-a-copy-of-a-copy of a configuration and they keep getting longer, more complex, and harder to maintain. Each tends to be a subtle variation of a previous version and as your code-base evolves, compatibility of an older config with a newer script tends to break.\n",
    "\n",
    "Ultimately, the whole process is directly at odds with principle of \"[Don't Repeat Yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\"\n",
    "\n",
    "Using Yaml to define the configuration is definitely a step-up from defing long strings of command-line arguments or even JSON, but it still does not solve the DRY problem.\n",
    "\n",
    "We can improve on this by using [Jinja](https://jinja.palletsprojects.com/en/3.1.x/) as a pre-processor and templatizing the configurations.\n",
    "\n",
    "The template library is still a work-in-progress, but it is comming along.\n",
    "\n",
    "**Experiments Definitions**  \n",
    "\n",
    "[forgather_demo/experiment 1.yaml](forgather_demo/experiment%201.yaml)  \n",
    "[forgather_demo/experiment 2.yaml](forgather_demo/experiment%202.yaml)  \n",
    "\n",
    "**Project Definitions**  \n",
    "\n",
    "[forgather_demo/paths.yaml](forgather_demo/paths.yaml)  \n",
    "[forgather_demo/defaults.yaml](forgather_demo/defaults.yaml)  \n",
    "[forgather_demo/whitelist.yaml](forgather_demo/whitelist.yaml)  \n",
    "\n",
    "**Library Definitions**  \n",
    "\n",
    "[templates/common/whitelist.yaml](../templates/common/whitelist.yaml)  \n",
    "[templates/common/defaults.yaml](../templates/common/defaults.yaml)  \n",
    "[templates/common/helpers.yaml](../templates/common/helpers.yaml)  \n",
    "[templates/common/trainer/base_trainer.yaml](../templates/common/trainer/base_trainer.yaml)  \n",
    "[templates/common/causal_lm/base_train.yaml](../templates/common/causal_lm/base_train.yaml)  \n",
    "[model_zoo/models/vanilla_transformer/vanilla_transformer.yaml](../model_zoo/models/vanilla_transformer/vanilla_transformer.yaml)  \n",
    "[model_zoo/models/model_zoo_whitelist.yaml](../model_zoo/models/model_zoo_whitelist.yaml)  \n",
    "\n",
    "**Model Code**  \n",
    "\n",
    "[model_zoo/models/vanilla_transformer/vanilla_transformer.py](../model_zoo/models/vanilla_transformer/vanilla_transformer.py)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed99de1-26f2-4f77-b1cd-b2eef7b3a1f0",
   "metadata": {},
   "source": [
    "### Loading a Template Config\n",
    "\n",
    "Let's start by using a config to get the paths we will need. As we would like to avoid defining the same thing more than once, this config makes use of the project's 'paths.yaml' file.\n",
    "\n",
    "[forgather_config.yaml](forgather_config.yaml)  \n",
    "[forgather_demo/paths.yaml](forgather_demo/paths.yaml)  \n",
    "\n",
    "#### load_config() : Load Jinja/Yaml configuration\n",
    "```python\n",
    "load_config(\n",
    "    config: os.PathLike | str, *,\n",
    "    preprocess: bool = True,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod = DEFAULT_LOAD_METHOD,\n",
    "    **kwargs,\n",
    ") -> LoadConfigOutput\n",
    "```\n",
    "\n",
    "By default, the single argument is a relative path to a config file which will be preprocessed with Jinja2 and parsed with Yaml.\n",
    "\n",
    "Any additional keyword-args are passed to the Jina template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fdb5ca0-34cf-4274-b679-d8c791c953f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_templates: 'forgather_demo'\n",
      "templates: '../templates'\n",
      "tokenizer_dir: '../tokenizers'\n",
      "datasets_dir: '../datasets'\n",
      "assets_dir: '..'\n",
      "search_paths:\n",
      "  - 'forgather_demo'\n",
      "  - '../templates'\n",
      "  - '../model_zoo'\n",
      "whitelist_path: 'forgather_demo/whitelist.yaml'\n",
      "model_src_dir: '../model_zoo'\n",
      "script_dir: '../scripts'\n",
      "train_script_path: '../scripts/train_script.py'\n",
      "models_dir: 'forgather_demo/output_models'\n",
      "dataset_id: 'roneneldan/TinyStories'\n",
      "tokenizer_def: '../templates/common/tokenizers/tiny_2k_bpe.yaml'\n",
      "tokenizer_path: '../tokenizers/tiny_stories_2k'\n",
      "tokenizers_whitelist: '../templates/common/tokenizers/whitelist.yaml'\n"
     ]
    }
   ],
   "source": [
    "# The 'DotDict' just allows access to the dictionary keys using attribute dot-notation.\n",
    "metacfg = DotDict(load_config('forgather_config.yaml').config)\n",
    "\n",
    "# pconfig() is from fortather.config; it just pretty-formats a configuration.\n",
    "pconfig(metacfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4053a968-33a9-492a-8ca5-0dc57c5810f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- set experiment = namespace()\n",
      "-- include 'project_defaults.yaml'\n",
      "-- set experiment.EXPERIMENT_NAME = 'Single Layer'\n",
      "\n",
      "-- set experiment.MODEL_CONFIG\n",
      "{{ experiment.MODEL_CONFIG }}\n",
      "    # The single variable under study.\n",
      "    num_hidden_layers: 1\n",
      "-- endset\n",
      "\n",
      "-- include 'common/causal_lm/base_train.yaml'\n"
     ]
    }
   ],
   "source": [
    "# Now, each experiment configuration has been reduced to something like this.\n",
    "with open(os.path.join(metacfg.project_templates, 'experiment 1.yaml'), 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0835a75-8ac2-4e9a-9937-d9c260cdc211",
   "metadata": {},
   "source": [
    "### Preprocess Configuration\n",
    "First, let's take a closer look at the pre-processed configuraiton file.\n",
    "\n",
    "As configured, this generated file will be saved in the test-run directory for the experiment. This should allow for reproducability; even if you muck about with the template defintions afterwards, the generated configuration will still be available for to use on its own.\n",
    "\n",
    "Also note that this automatically generated a number of comments about the experiment details. Overall, this is much better than hand-crafting a heap of command-line arguments to feed to a script!\n",
    "\n",
    "#### preprocess_config() : Preprocess a configuration file.\n",
    "```python\n",
    "def preprocess_config(\n",
    "    config:  os.PathLike | str, *,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod = DEFAULT_LOAD_METHOD,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46bd1cab-66a4-4784-96ce-8514890291bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1: # Single Layer\n",
      "     2: # 2024-07-08 04:49:42\n",
      "     3: # Description: Compare the impact of different numbers of layers on model performance\n",
      "     4: # World Size: 1\n",
      "     5: # Hostname: hal9000\n",
      "     6: # Script Args:: N/A\n",
      "     7: \n",
      "     8: # experiment.TOKENIZERS_DIR: \"../tokenizers\"\n",
      "     9: # experiment.DATASETS_DIR: \"../datasets\"\n",
      "    10: # experiment.MODEL_SRC_DIR: \"../model_zoo\"\n",
      "    11: # experiment.MODELS_DIR: \"forgather_demo/output_models\"\n",
      "    12: \n",
      "    13: # experiment.DATASET: \"tiny_stories_tokenized\"\n",
      "    14: # experiment.EXPERIMENT_NAME: \"Single Layer\"\n",
      "    15: # experiment.EXPERIMENT_DESCRIPTION: \"Compare the impact of different numbers of layers on model performance\"\n",
      "    16: # experiment.MODEL_NAME: \"test_model\"\n",
      "    17: # experiment.CREATE_NEW_MODEL: True\n",
      "    18: # experiment.SAVE_MODEL: True\n",
      "    19: \n",
      "    20: # config.OUTPUT_DIR: path = \"forgather_demo/output_models/test_model\"\n",
      "    21: # config.DATASET_PATH: path = \"../datasets/tiny_stories_tokenized\"\n",
      "    22: # config.LOGGING_DIR: path = \"forgather_demo/output_models/test_model/runs/Single Layer_1720414182151205578\"\n",
      "    23: \n",
      "    24: # experiment.tokenizer\n",
      "    25: .define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
      "    26:     - \"../tokenizers/tiny_stories_2k\"\n",
      "    27: \n",
      "    28: .define: &model_config\n",
      "    29:     vocab_size: !callable:forgather.construct:get_attr [ *tokenizer, vocab_size ]\n",
      "    30:     hidden_size: 128\n",
      "    31:     dim_feedforward: 512\n",
      "    32:     num_attention_heads: 1\n",
      "    33:     num_hidden_layers: 2\n",
      "    34: \n",
      "    35:     hidden_size: 128\n",
      "    36: \n",
      "    37:     # The single variable under study.\n",
      "    38:     num_hidden_layers: 1\n",
      "    39: \n",
      "    40: # experiment.model\n",
      "    41: .define: &model !callable:aiws.construct:register_for_auto_class\n",
      "    42:     - !callable:../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\n",
      "    43:         - !callable:aiws.construct:register_for_auto_class\n",
      "    44:             - !callable:../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig\n",
      "    45:                 kwargs: *model_config\n",
      "    46: \n",
      "    47: \n",
      "    48: .define: &summary_writer !callable:torch.utils.tensorboard:SummaryWriter\n",
      "    49:     - \"forgather_demo/output_models/test_model/runs/Single Layer_1720414182151205578\"\n",
      "    50: .define: &dataset !callable:datasets:load_from_disk [ \"../datasets/tiny_stories_tokenized\" ]\n",
      "    51: .define: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
      "    52: .define: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
      "    53: .define: &data_collator !callable:transformers:DataCollatorForLanguageModeling\n",
      "    54:     args:\n",
      "    55:         - *tokenizer\n",
      "    56:     kwargs:\n",
      "    57:         mlm: False\n",
      "    58:         return_tensors: pt\n",
      "    59: \n",
      "    60: .define: &training_args\n",
      "    61: \n",
      "    62: # Training args\n",
      "    63:     overwrite_output_dir: False\n",
      "    64:     per_device_train_batch_size: 16\n",
      "    65:     per_device_eval_batch_size: 16\n",
      "    66:     learning_rate: 1.0e-3\n",
      "    67:     num_train_epochs: 1\n",
      "    68:     eval_steps: 500\n",
      "    69:     logging_steps: 500\n",
      "    70:     seed: 42\n",
      "    71:     eval_strategy: \"steps\"\n",
      "    72:     save_strategy: \"no\"\n",
      "    73:     logging_strategy: \"steps\"\n",
      "    74:     lr_scheduler_type: \"constant\"\n",
      "    75: \n",
      "    76:     overwrite_output_dir: True\n",
      "    77:     per_device_train_batch_size: 32\n",
      "    78:     per_device_eval_batch_size: 32\n",
      "    79:     learning_rate: 1.0e-3\n",
      "    80:     num_train_epochs: 1\n",
      "    81:     eval_steps: 200\n",
      "    82:     logging_steps: 100\n",
      "    83:     max_steps: 2000\n",
      "    84: \n",
      "    85:     output_dir: \"forgather_demo/output_models/test_model\"\n",
      "    86:     logging_dir: \"forgather_demo/output_models/test_model/runs/Single Layer_1720414182151205578\"\n",
      "    87: \n",
      "    88: # Callbacks\n",
      "    89: .define: &callbacks\n",
      "    90:     - !callable:aiws.default_callbacks:JsonLogger []\n",
      "    91:     - !callable:aiws.tb_logger:TBLogger\n",
      "    92:         args: [ *summary_writer ]\n",
      "    93:         kwargs:\n",
      "    94:             date: \"2024-07-08 04:49:42\"\n",
      "    95:             name: \"Single Layer\"\n",
      "    96:             description: \"Compare the impact of different numbers of layers on model performance\"\n",
      "    97:             args: N/A\n",
      "    98:             world_size: 1\n",
      "    99:             config: !callable:pp_config\n",
      "   100: \n",
      "   101: \n",
      "   102: # Accel trainer args\n",
      "   103: .define:  &accel_trainer_args\n",
      "   104:     device_placement: True\n",
      "   105:     dataloader_config: !callable:accelerate:DataLoaderConfiguration\n",
      "   106:         kwargs:\n",
      "   107:             dispatch_batches: False\n",
      "   108:             split_batches: False\n",
      "   109: \n",
      "   110: \n",
      "   111: # HF Trainer args\n",
      "   112: .define: &hf_trainer_args \n",
      "   113:     accelerator_config:\n",
      "   114:         dispatch_batches: False\n",
      "   115:         split_batches: False\n",
      "   116:     ddp_find_unused_parameters: False\n",
      "   117:     report_to: \"none\"\n",
      "   118:     logging_nan_inf_filter: False\n",
      "   119: \n",
      "   120: \n",
      "   121: # Trainer\n",
      "   122: .define: &trainer !callable:aiws.trainer:Trainer\n",
      "   123:     kwargs:\n",
      "   124:         model: *model\n",
      "   125:         args: !callable:aiws.trainer_types:TrainingArguments\n",
      "   126:             kwargs: *training_args\n",
      "   127:         data_collator: *data_collator\n",
      "   128:         train_dataset: *train_dataset\n",
      "   129:         eval_dataset: *eval_dataset\n",
      "   130:         tokenizer: *tokenizer\n",
      "   131:         callbacks: *callbacks\n",
      "   132: \n",
      "   133: \n",
      "   134: \n",
      "   135: # Final Configuraiton\n",
      "   136: output_dir: \"forgather_demo/output_models/test_model\"\n",
      "   137: logging_dir: \"forgather_demo/output_models/test_model/runs/Single Layer_1720414182151205578\"\n",
      "   138: experiment_name: \"Single Layer\"\n",
      "   139: experiment_description: \"Compare the impact of different numbers of layers on model performance\"\n",
      "   140: trainer: *trainer\n",
      "   141: do_save: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select an experiment template\n",
    "experiment_path = os.path.join(metacfg.project_templates, 'experiment 1.yaml')\n",
    "\n",
    "# Only preprocess the experiment template\n",
    "pp_config = preprocess_config(experiment_path, search_path=metacfg.search_paths)\n",
    "\n",
    "# You can print a pre-processed config, a sub-class of 'str', '.with_line_numbers()'\n",
    "# This can be helpful when trying to diagnose YAML parse errors.\n",
    "print(pp_config.with_line_numbers(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbbe707-7d1d-448b-940a-fb64694769fc",
   "metadata": {},
   "source": [
    "### Configuration Syntax\n",
    "\n",
    "We use [Jinja2](https://jinja.palletsprojects.com/) for preprocessing and [YAML](https://pyyaml.org/wiki/PyYAMLDocumentation) for the actual configuration. I'll spare going into details, as these are well covered in the links, but it may be helpful to point out a few non-standard and non-obvious things.\n",
    "\n",
    "#### Jinja\n",
    "\n",
    "Jinja is running in a sandboxed environment. This limits what functions and data may be accessed.\n",
    "\n",
    "We have enabled line-statement and line comments.\n",
    "\n",
    "```jinja2\n",
    "## This is a line-comment. The next line is a line-statement.\n",
    "-- set foo = 'bar'\n",
    "\n",
    "## Line comments are shorthand for...\n",
    "{# ...regular comments and line-statements are short for... #}\n",
    "{% macro foobar() %}\n",
    "```\n",
    "While both of these are regular Jinja features, there is not a standard prefix for either. The prefix is set when the environment is created.\n",
    "\n",
    "Line-comments don't show up in the pre-processed configuration, while regular Yaml comments will show up.  \n",
    "Warning: Something appears to be broken in Jina. While line-comments are removed, the associated newline is not and regular whitespace control appears to be unable to remove it. This can result in non-obvious failures.\n",
    "\n",
    "We have injected a number of symbols into the environment.\n",
    "```\n",
    "now : datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    local time\n",
    "utcnow : datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    UTC time\n",
    "time_ns : str(time.time_ns())\n",
    "    Timestamp, integer nanoseconds\n",
    "path_join(...) : os.path.join(...)\n",
    "    Join path names in an os independent way.\n",
    "dirname(dir) : os.path.dirname(dir)\n",
    "    Get the directory part of a path.\n",
    "\n",
    "# Defaults for the following have been set, but can be overridden by the training-script.\n",
    "world_size : 1\n",
    "    The number of concurrent proccesses used in distributed training.\n",
    "rank: 0\n",
    "    The global multiprocess rank. See torch.distributed\n",
    "local_rank:\n",
    "    The local multiprocess rank. See torch.distributed\n",
    "script_args : 'N/A'\n",
    "    The args passed to the configuration script.\n",
    "hostname: platform.node()\n",
    "    The hostname of the machine.\n",
    "```\n",
    "\n",
    "*White-space control*\n",
    "\n",
    "If you need wish to strip leading and/or trailing whitespace surrounding a Jinja statement, add the '-' symbol to the start and end tokens.\n",
    "```jinja2\n",
    "## Strip both left and right sides\n",
    "{{- foo + bar -}}\n",
    "{#- strip only left #}\n",
    "## Strip only right-side.\n",
    "{% if foo > 12 -%}\n",
    "```\n",
    "\n",
    "*Namespaces*\n",
    "\n",
    "By default, 'included' templates inherit the namespace of the caller, while 'imported' templates do not.\n",
    "\n",
    "Despite what you may assume, 'include' is not quite the same as directly substituting text. If root template 'A' includes templates 'B' and 'C', the namespace of A is visible to both B and C and vice-versa. What is not obvious is that B and C will not have access to each others namespaces.\n",
    "\n",
    "```jinja2\n",
    "## Content of A.jinja\n",
    "-- include 'B.jinja'\n",
    "-- include 'C.jinja'\n",
    "## Contents of B.jinja\n",
    "-- set FOO = 1\n",
    "## Contents of C.jinja\n",
    "{{ FOO }}\n",
    "```\n",
    "\n",
    "This will not work, as 'FOO' will not be visible in C. To work around this, you can declare a namespace in A, which will be visible to both B and C. Any changes by one will be visible to the other.\n",
    "\n",
    "```jinja2\n",
    "## Content of A.jinja\n",
    "-- set experiment = namespace()\n",
    "-- include 'B.jinja'\n",
    "-- include 'C.jinja'\n",
    "## Contents of B.jinja\n",
    "-- set experiment.FOO = 1\n",
    "## Contents of C.jinja\n",
    "{{ experiment.FOO }}\n",
    "```\n",
    "This will work.\n",
    "\n",
    "Another namespace oddity is that you can 'set' a variable on a namespace, but you can't directly define a macro in a namespace.\n",
    "\n",
    "```jinja2\n",
    "## Contents of A.jinja\n",
    "-- set experiment = namespace()\n",
    "-- set experiment.foobar = \"foobar\"\n",
    "-- include 'B'jinja'\n",
    "{{ experimment.some_macro() }}\n",
    "## Contents of B.jinja\n",
    "-- macro experimment.some_macro()\n",
    "{{ experiment.foobar }}\n",
    "-- endmacro\n",
    "```\n",
    "This will not work, as you can't declare a macro in a namespace... but you can assign one!\n",
    "\n",
    "```jinja2\n",
    "## Contents of A.jinja\n",
    "-- set experiment = namespace()\n",
    "-- set experiment.foobar = \"foobar\"\n",
    "-- include 'B'jinja'\n",
    "{{ experimment.some_macro() }}\n",
    "## Contents of B.jinja\n",
    "-- macro B__some_macro()\n",
    "{{ experiment.foobar }}\n",
    "-- endmacro\n",
    "-- set experimment.some_macro = B__some_macro\n",
    "```\n",
    "Seems like a bug, but at least it can be worked around.\n",
    "\n",
    "#### YAML\n",
    "\n",
    "We are using the PyYAML, with all of its warts. This follows the Yaml 1.1 specification... mostly.\n",
    "\n",
    "The loader is the 'yaml.SafeLoader,' which prohibits constructing arbitrary Python objects.\n",
    "\n",
    "There is one custom tag present: '!callable'\n",
    "\n",
    "!callable constructs 'Latent' objects. That is to say, a Laent holds the definition for a Python Callable, but does not immediatly load any module code or construct anything.\n",
    "\n",
    "The Latent objects must be explicilty 'materialized,' at which point the safety of the types are checked, the symbols are resolved, and only then, is anything actually constructed.\n",
    "\n",
    "!callable must always be followed by either a list or a mapping. If a list is given, it contains positional args and may be empty. \n",
    "If a mappying is used, there are two defined keys: 'args,' and 'kwargs'; neither is required.\n",
    "\n",
    "```yaml\n",
    "- !callable:datetime:now [] # No arguments\n",
    "- !callable:torch:tensor [1, 2, 3] # Only positional arguments\n",
    "- !callable:torch:tensor { args: [4, 5, 6], kwargs: { requires_grad: True }} # Both positional and keyword\n",
    "```\n",
    "\n",
    "One feature in Yaml, which you may not be familiar with, are 'anchors' and 'aliases.' An anchor defines a symbolic reference which may be used again later in the definition.\n",
    "```yaml\n",
    "point: &my_anchor\n",
    "    x: 1\n",
    "    y: -5\n",
    "\n",
    "line:\n",
    "    start: *my_anchor\n",
    "    end: *my_anchor \n",
    "```\n",
    "Note that using an alias does not create a copy, it refers to the same instance!\n",
    "\n",
    "We also make use of Yaml's esoteric 'merge' operator, '<<:'\n",
    "```yaml\n",
    "defauts: &defaults\n",
    "    x: 1\n",
    "    y: 2\n",
    "\n",
    "point:\n",
    "    <<: *defaults\n",
    "    x: 5\n",
    "    z: 10\n",
    "# The above results in:\n",
    "point:\n",
    "    x: 5\n",
    "    y: 2\n",
    "    z: 10\n",
    "```\n",
    "\n",
    "Yaml does not allow you abstract anchor definitions; an anchor must refer to an actual object in the graph.\n",
    "\n",
    "This is rather annoying when you just want to define something which is only for later use. To address this, any keys at the root level which start with '.' will be pruned before the configuration is returned. By convention, I give name all of these keys '.define,' as Yaml has no issues with using the same key more than once, but anything starting with '.' will do.\n",
    "\n",
    "```yaml\n",
    ".define: &x !callable:torch.tensor [ 2 ]\n",
    ".define: &y !callable:torch.tensor [ 3 ]\n",
    "sum: !callable:torch.add [ *x, *y ]\n",
    "```\n",
    "After loading, only the key for 'sum' will be in the dictionary.\n",
    "\n",
    "*File Extensions*\n",
    "\n",
    "You may use whater extension you like. All of the pre-defined templates end in '.yaml' This produces the best syntax highlighting compromise between Jinja2 and YAML syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf0ee7-a55e-4f79-aef5-24e1d189832e",
   "metadata": {},
   "source": [
    "### Parse Configuration\n",
    "\n",
    "We can feed the pre-processed configuration into the YAML parser with load_config().\n",
    "\n",
    "By default, this will pre-process the input; this can be skipped by setting 'preprocess=False'\n",
    "\n",
    "Also note the 'load_method' argument. The default is to assume the input string is a file path, but we can tell it that it's the actual input by setting 'from_string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbec1cf-8f74-4d06-b4fb-df33b26bab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir: 'forgather_demo/output_models/test_model'\n",
      "logging_dir: 'forgather_demo/output_models/test_model/runs/Single Layer_1720414182151205578'\n",
      "experiment_name: 'Single Layer'\n",
      "experiment_description: 'Compare the impact of different numbers of layers on model performance'\n",
      "trainer:\n",
      "  Latent 'aiws.trainer:Trainer'\n",
      "    model:\n",
      "      Latent 'aiws.construct:register_for_auto_class'\n",
      "        - Latent '../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer'\n",
      "          - Latent 'aiws.construct:register_for_auto_class'\n",
      "            - Latent '../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig'\n",
      "              vocab_size:\n",
      "                Latent 'forgather.construct:get_attr'\n",
      "                  - Latent 'transformers:AutoTokenizer.from_pretrained'\n",
      "                    - '../tokenizers/tiny_stories_2k'\n",
      "                  - 'vocab_size'\n",
      "              hidden_size: 128\n",
      "              dim_feedforward: 512\n",
      "              num_attention_heads: 1\n",
      "              num_hidden_layers: 1\n",
      "    args:\n",
      "      Latent 'aiws.trainer_types:TrainingArguments'\n",
      "        overwrite_output_dir: True\n",
      "        per_device_train_batch_size: 32\n",
      "        per_device_eval_batch_size: 32\n",
      "        learning_rate: 0.001\n",
      "        num_train_epochs: 1\n",
      "        eval_steps: 200\n",
      "        logging_steps: 100\n",
      "        seed: 42\n",
      "        eval_strategy: 'steps'\n",
      "        save_strategy: 'no'\n",
      "        logging_strategy: 'steps'\n",
      "        lr_scheduler_type: 'constant'\n",
      "        max_steps: 2000\n",
      "        output_dir: 'forgather_demo/output_models/test_model'\n",
      "        logging_dir: 'forgather_demo/output_models/test_model/runs/Single Layer_1720414182151205578'\n",
      "    data_collator:\n",
      "      Latent 'transformers:DataCollatorForLanguageModeling'\n",
      "        - Latent 'transformers:AutoTokenizer.from_pretrained'\n",
      "          - '../tokenizers/tiny_stories_2k'\n",
      "        mlm: False\n",
      "        return_tensors: 'pt'\n",
      "    train_dataset:\n",
      "      Latent 'forgather.construct:get_item'\n",
      "        - Latent 'datasets:load_from_disk'\n",
      "          - '../datasets/tiny_stories_tokenized'\n",
      "        - 'train'\n",
      "    eval_dataset:\n",
      "      Latent 'forgather.construct:get_item'\n",
      "        - Latent 'datasets:load_from_disk'\n",
      "          - '../datasets/tiny_stories_tokenized'\n",
      "        - 'validation'\n",
      "    tokenizer:\n",
      "      Latent 'transformers:AutoTokenizer.from_pretrained'\n",
      "        - '../tokenizers/tiny_stories_2k'\n",
      "    callbacks:\n",
      "      - Latent 'aiws.default_callbacks:JsonLogger'\n",
      "      - Latent 'aiws.tb_logger:TBLogger'\n",
      "        - Latent 'torch.utils.tensorboard:SummaryWriter'\n",
      "          - 'forgather_demo/output_models/test_model/runs/Single Layer_1720414182151205578'\n",
      "        date: '2024-07-08 04:49:42'\n",
      "        name: 'Single Layer'\n",
      "        description: 'Compare the impact of different numbers of layers on model performance'\n",
      "        args: 'N/A'\n",
      "        world_size: 1\n",
      "        config: Latent 'pp_config'\n",
      "do_save: True\n"
     ]
    }
   ],
   "source": [
    "config_out = load_config(pp_config, preprocess=False, load_method=\"from_string\")\n",
    "pconfig(config_out.config)\n",
    "# Note: The config_out also has a 'pp_config' member, which would have the preprocessed file, if we had combined parsing and pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd6f07-0d66-41b6-be42-49788427ba81",
   "metadata": {},
   "source": [
    "### Load Whitelist\n",
    "\n",
    "#### load_whitelist_as_set() : Load a whitelist configuration from a file or string\n",
    "```python\n",
    "def load_whitelist_as_set(\n",
    "    config: os.PathLike | str, *,\n",
    "    preprocess: bool = True,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod = DEFAULT_LOAD_METHOD\n",
    ") -> Set[str]:\n",
    "```\n",
    "This is essentially just load_config, but it normalizes the paths in the whitelist and converts the list to a set, to improve search performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e1cd26-9fda-4462-b087-c770b28cf442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 'forgather.construct:flatten'\n",
      "- 'aiws.accel_trainer:AccelTrainingArguments'\n",
      "- 'aiws.trainer:Trainer'\n",
      "- '/home/dinalt/ai_assets/aiworkshop/model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer'\n",
      "- '/home/dinalt/ai_assets/aiworkshop/model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig'\n",
      "- 'forgather.construct:get_item'\n",
      "- 'accelerate:DataLoaderConfiguration'\n",
      "- 'datasets:load_from_disk'\n",
      "- 'aiws.default_callbacks:JsonLogger'\n",
      "- 'aiws.default_callbacks:InfoCallback'\n",
      "- 'transformers:AutoTokenizer.from_pretrained'\n",
      "- 'aiws.construct:register_for_auto_class'\n",
      "- 'aiws.accel_trainer:AccelTrainer'\n",
      "- '/home/dinalt/ai_assets/aiworkshop/model_zoo/attention_only/attnonly.py:TransformerConfig'\n",
      "- '/home/dinalt/ai_assets/aiworkshop/model_zoo/attention_only/attnonly.py:TransformerModel'\n",
      "- 'datasets:load_dataset'\n",
      "- 'aiws.trainer_types:TrainingArguments'\n",
      "- 'aiws.tb_logger:TBLogger'\n",
      "- 'aiws.default_callbacks:ProgressCallback'\n",
      "- 'torch.utils.tensorboard:SummaryWriter'\n",
      "- 'transformers:TrainingArguments'\n",
      "- 'transformers:AutoModel.from_pretrained'\n",
      "- 'transformers:DataCollatorForLanguageModeling'\n",
      "- 'forgather.construct:get_attr'\n",
      "- 'transformers:Trainer'\n"
     ]
    }
   ],
   "source": [
    "whitelist_out = load_whitelist_as_set(metacfg.whitelist_path, search_path=metacfg.search_paths)\n",
    "pconfig(whitelist_out.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735df202-bb24-4e20-be38-ce7bd81eae25",
   "metadata": {},
   "source": [
    "#### Check Whitelist Requirements\n",
    "\n",
    "If you would like to see which import-specs are used in a configuraiton (or which are missing), you can use enumerate_whitelist_exceptions()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ffe56d9-5742-45a2-892b-3e0336dfb3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- '/home/dinalt/ai_assets/aiworkshop/model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer'\n",
      "- 'aiws.trainer:Trainer'\n",
      "- 'datasets:load_from_disk'\n",
      "- 'aiws.default_callbacks:JsonLogger'\n",
      "- 'aiws.construct:register_for_auto_class'\n",
      "- 'transformers:AutoTokenizer.from_pretrained'\n",
      "- 'aiws.trainer_types:TrainingArguments'\n",
      "- 'transformers:DataCollatorForLanguageModeling'\n",
      "- 'aiws.tb_logger:TBLogger'\n",
      "- 'forgather.construct:get_attr'\n",
      "- '/home/dinalt/ai_assets/aiworkshop/model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig'\n",
      "- 'forgather.construct:get_item'\n",
      "- 'torch.utils.tensorboard:SummaryWriter'\n"
     ]
    }
   ],
   "source": [
    "enumerate_whitelist_exceptions(config_out.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4848270-6d49-412f-a3a2-b72222461140",
   "metadata": {},
   "source": [
    "### Materialize the Configuration\n",
    "\n",
    "#### materialize_config() : Materialize the Latent objects in the configuration\n",
    "```python\n",
    "def materialize_config(\n",
    "    config: Any,\n",
    "    whitelist: Container | os.PathLike | str = None,\n",
    "    preprocess: bool = True,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod=DEFAULT_LOAD_METHOD,\n",
    "    pp_kwargs: Dict[str, Any] = {},\n",
    "    kwargs: Dict[str, Callable] = {},\n",
    ") -> MaterializedOutput:\n",
    "```\n",
    "- config: An instantiated, but Latent, configuration; a preprocessed configuration string; or a path to a configuraiton file.  \n",
    "- whitelist: A Container type, which means any object which supports 'str is in container'  \n",
    "- preprocess: Preprocess the string or file. Only applies if input is a path or string.  \n",
    "- load_method: One of \"from_file\", \"from_string\", \"from_file_search\"  \n",
    "- search_path: A str or List\\[str\\] paths to search for templates; also applies to \"from_file_search\" load method.  \n",
    "- pp_kwargs: Arguments to pass to the template, if preprocessing is to be performed.  \n",
    "- kwargs: A mapping str -> Callable to substitute when materializing the final config. This allows passing already instantiated objects into the config.\n",
    "\n",
    "\n",
    "We will just materialize the config we have already loaded, which is to be checked against the whitelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce4a60c-ce9b-4738-9eb1-4deb39d559ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-07 21:06:21.625\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36maiws.base_trainer\u001b[0m:\u001b[36m_validate_dirs\u001b[0m:\u001b[36m182\u001b[0m - \u001b[33m\u001b[1mModel exists in output dir 'forgather_demo/output_models/test_model' and model may be overwritten!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir: 'forgather_demo/output_models/test_model'\n",
      "logging_dir: 'forgather_demo/output_models/test_model/runs/Single Layer_1720386364132348531'\n",
      "experiment_name: 'Single Layer'\n",
      "experiment_description: 'Compare the impact of different numbers of layers on model performance'\n",
      "trainer:\n",
      "  Trainer(model=VanillaTransformer(\n",
      "    (embedding): Embedding(2000, 128)\n",
      "    (positional_encoder): PositionalEncoder()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerLayer(\n",
      "        (attention): MultiheadAttention(\n",
      "          (query_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (key_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (value_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (feedforward): FeedforwardLayer(\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      "  ),args=TrainingArguments(per_device_train_batch_size=32, output_dir='forgather_demo/output_models/test_model', overwrite_output_dir=True, per_device_eval_batch_size=32, max_steps=2000, logging_steps=100, eval_steps=200, save_steps=500, learning_rate=0.001, num_train_epochs=1, seed=42, lr_scheduler_type='constant', warmup_steps=0, device=None, logging_dir='forgather_demo/output_models/test_model/runs/Single Layer_1720386364132348531', eval_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_strategy=<IntervalStrategy.STEPS: 'steps'>, save_strategy=<IntervalStrategy.NO: 'no'>, logging_first_step=False, eval_delay=0, save_total_limit=2),data_collator=DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='../tokenizers/tiny_stories_2k', vocab_size=2000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "  \t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  }, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt'),train_dataset=Dataset({\n",
      "      features: ['input_ids'],\n",
      "      num_rows: 2119719\n",
      "  }),eval_dataset=Dataset({\n",
      "      features: ['input_ids'],\n",
      "      num_rows: 2199\n",
      "  }),tokenizer=PreTrainedTokenizerFast(name_or_path='../tokenizers/tiny_stories_2k', vocab_size=2000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "  \t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  },model_init=None,callbacks=[<aiws.default_callbacks.ProgressCallback object at 0x7fac08a8b5b0>, <aiws.default_callbacks.InfoCallback object at 0x7fac08a89270>, <aiws.default_callbacks.JsonLogger object at 0x7fad57f97be0>, <aiws.tb_logger.TBLogger object at 0x7fac08a8a110>],)\n",
      "do_save: True\n"
     ]
    }
   ],
   "source": [
    "# For reproducible experiments, it's probably best to make sure all of your random-seeds have been initialized\n",
    "# to a consistent value. This is especially important in a multi-process environment.\n",
    "set_seed(42)\n",
    "\n",
    "config_output = materialize_config(config_out.config, whitelist=whitelist_out.config)\n",
    "pconfig(config_output.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181646d1-021a-4046-b392-abf37f841dbb",
   "metadata": {},
   "source": [
    "### Run Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25302e-7056-4653-b33f-0275d4463ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping the dict in a DotDict just allows using attribute dot-notation to acceses the values.\n",
    "config = DotDict(config_output.config)\n",
    "config.trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21d135-1575-4684-a639-ce1c38162063",
   "metadata": {},
   "source": [
    "### Run Experiment 2\n",
    "\n",
    "This time we will skip all of the intermediate steps and go straight to instantiating the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ccce36-feda-4ba1-8fb9-31a5b3d7871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "materialize_config(\n",
    "    os.path.join(metacfg.project_templates, 'experiment 2.yaml'), metacfg.whitelist_path, search_path=metacfg.search_paths\n",
    ").config['trainer'].train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3e79b-e146-4df5-923f-f42fc9867163",
   "metadata": {},
   "source": [
    "### View in Tensorboard\n",
    "Assuming that you have Tensorboard installed (you do, right?), you can take a look at the information collected by the logger.\n",
    "\n",
    "Ideally, start Tensorboard from a console, but to take a quick peek, you can launch it from the notebook. If using the Notebook, you will have to stop the command when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fafbb8c8-8f81-4eb2-a574-cd7999ec77ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.16.2 at http://hal9000:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Use this version if you are training on the same machine that your web-browser in running on.\n",
    "#!tensorboard --bind_all --logdir forgather_demo/output_models/test_model/runs/\n",
    "\n",
    "# Use this version if you are not running training on the same machine as your web-browser.\n",
    "!tensorboard --bind_all --logdir forgather_demo/output_models/test_model/runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a29692-aab9-44d6-b089-bf97e6f81d90",
   "metadata": {},
   "source": [
    "### Running on Multiple GPUs\n",
    "\n",
    "Running a trainer with multiple GPUs inside of a notebook has a number of issues.\n",
    "\n",
    "In the least complex mode, Torch DataParallel is used. This runs each GPU on a seperate Python thread. Unfortunately, the performance is terrible, thnaks to the Global Interpreter Lock.\n",
    "\n",
    "Running training with Torch Distributed solves this problem by running each node in a seperate process. This makes things difficult for running in a notebook. The Accelerate library attempts to solve this by offering a [notebook launcher](https://huggingface.co/docs/accelerate/en/basic_tutorials/notebook), but there are complications: see below.\n",
    "\n",
    "### Multi-GPU Training in a Notebook\n",
    "\n",
    "First, we will need to switch over to a Trainer implementation which supports the Acclerate library.\n",
    "This is easy enough. We can just override the trainer definition in the experiment to use one which supports Accelerate:\n",
    "\n",
    "[forgather_demo/accel_experiment.yaml](forgather_demo/accel_experiment.yaml)  \n",
    "\n",
    "Running in a notebook entails a few additional commplications:\n",
    "\n",
    "- To run the experiment with the notebook_launcher, we need to perform all of our initialization within the 'training_loop' function passed to the launcher.\n",
    "- An additional complication is that you will likely need to restart your notebook's kernel, should you have already used the GPUs.\n",
    "- It tends to be unstable, crashing, witout an obvious cause -- and the crash can't be reproduced from a training script!\n",
    "- If something goes wrong, you will get a 'SIGTERM' and poor diagnostic info. It's best to run the 'train_loop' on its own for better diagnostics.\n",
    "- If you want to work with the model after training, you will need to save it and load it back into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2565208e-5df9-41f6-afc0-dd9d18d22d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "from accelerate import notebook_launcher\n",
    "from forgather.config import load_config, materialize_config\n",
    "from aiws.dotdict import DotDict\n",
    "from transformers import set_seed\n",
    "\n",
    "# This is the entry-point for the spawned procceses.\n",
    "def training_loop(dir_config, experiment_name):\n",
    "    set_seed(42)\n",
    "    metacfg = DotDict(load_config('forgather_config.yaml').config)\n",
    "    config_output = materialize_config(os.path.join(metacfg.project_templates, experiment_name),\n",
    "        metacfg.whitelist_path, search_path=metacfg.search_paths)\n",
    "    config = DotDict(config_output.config)\n",
    "    \n",
    "    # If you don't want all processes to print to the console...\n",
    "    if config.trainer.accelerator.is_main_process:\n",
    "        print(\"**** Training Started *****\")\n",
    "        print(f\"experiment_name: {config.experiment_name}\")\n",
    "        print(f\"experiment_description: {config.experiment_description}\")\n",
    "        print(f\"output_dir: {config.output_dir}\")\n",
    "        print(f\"logging_dir: {config.logging_dir}\")\n",
    "\n",
    "    # This is where the actual 'loop' is.\n",
    "    metrics = config.trainer.train().metrics\n",
    "    \n",
    "    if config.trainer.accelerator.is_main_process:\n",
    "        print(\"**** Training Completed *****\")\n",
    "        print(metrics)\n",
    "\n",
    "    metrics = config.trainer.evaluate()\n",
    "\n",
    "    if config.trainer.accelerator.is_main_process:\n",
    "        print(\"**** Evaluation Completed *****\")\n",
    "        print(metrics)\n",
    "    \n",
    "    if config.save:\n",
    "        config.trainer.save_model()\n",
    "        if config.trainer.accelerator.is_main_process:\n",
    "            print(f\"Model saved to: {config.trainer.args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709fd93-5a27-487a-8aeb-166b8cd2274e",
   "metadata": {},
   "source": [
    "#### Launch Accelerate Trainer Directly\n",
    "\n",
    "This will use Accelerate, but if you have multiple GPUs, this will only use one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf0acd-89c0-423c-bb6f-f2b98b6d82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop('forgather_config.yaml', 'accel_experiment.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fa3351-96d7-4769-8b45-99d103e1b941",
   "metadata": {},
   "source": [
    "#### Launch Accelerate Trainer with Notebook Launcher\n",
    "\n",
    "If you have already trained anything in the noteboo, without using notebook_launcher, this\n",
    "will fail with \"ValueError: To launch a multi-GPU training from your notebook ...\"\n",
    "\n",
    "After restarting your notebook, you can just run the prior cell again to reinitialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87183a-dbec-420e-ad2f-796384eb9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(\n",
    "    training_loop,\n",
    "    args=('forgather_config.yaml', 'accel_experiment.yaml',),\n",
    "    num_processes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ea7a1-82f8-484d-a50e-be94614d2ef7",
   "metadata": {},
   "source": [
    "#### Launch Huggingface Trainer directly in Notebook\n",
    "\n",
    "This will use multiple GPUs, but will be hideously crippled on account of contention for the Global Interpreter Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fa1bc-e2a5-4dc4-818c-e3e546338408",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop('forgather_config.yaml', 'hf_trainer_experiment.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c8567-b2a0-4b95-884e-9676a0e3825f",
   "metadata": {},
   "source": [
    "#### Launch Huggingface Trainer with Notebook Launcher\n",
    "\n",
    "In theory, this should work with the Huggingface Trainer...\n",
    "\n",
    "[forgather_demo/hf_trainer_experiment.yaml](forgather_demo/hf_trainer_experiment.yaml)  \n",
    "\n",
    "At present, it does not appear to detect that it is running in a multi-gpu configuration. The same config works just fine from a regual training script. Cause TBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b131a9-7d4e-4f42-a829-358c29713705",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(\n",
    "    training_loop,\n",
    "    args=('forgather_config.yaml', 'hf_trainer_experiment.yaml',),\n",
    "    num_processes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece555d-aa15-4b5e-a80e-20cb9fa9db52",
   "metadata": {},
   "source": [
    "### Train from a Training Script\n",
    "\n",
    "The preferred way to run non-trivial training tasks is from the command-line.\n",
    "\n",
    "The following code can help you get started. It will take the path configuration and build a command-line, which can either be executed from the notebook or can be exported as a bash-script.\n",
    "\n",
    "This is especially important for long-running training sessions, as various issues with the notebook could interrupt training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a399e7a-6220-426f-affa-f9bfba4b9a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "templates_dir: 'forgather_demo'\n",
      "tokenizer_dir: 'experiment.TOKENIZERS_DIR'\n",
      "datasets_dir: 'experiment.DATASETS_DIR'\n",
      "assets_dir: '..'\n",
      "search_paths: '['forgather_demo', '../templates', '../model_zoo']'\n",
      "whitelist_path: 'forgather_demo/whitelist.yaml'\n",
      "model_src_dir: '../model_zoo'\n",
      "script_dir: '../scripts'\n",
      "train_script_path: '../scripts/train_script.py'\n",
      "models_dir: 'forgather_demo/output_models'\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "from aiws.dotdict import DotDict\n",
    "from forgather.config import load_whitelist_as_set, load_config, pconfig\n",
    "from forgather import Latent\n",
    "import stat\n",
    "import os\n",
    "\n",
    "# Load project directory definitions\n",
    "metacfg = DotDict(load_config('forgather_config.yaml').config)\n",
    "pconfig(metacfg)\n",
    "\n",
    "# Output train-script command line as a string\n",
    "def train_cmdline(metacfg, nproc='gpu'):\n",
    "    includes = ''.join(f\"-I '{inc}' \" for inc in metacfg.search_paths)\n",
    "    return f\"torchrun --standalone --nproc-per-node {nproc} '{metacfg.train_script_path}' -w '{metacfg.whitelist_path}' {includes} -s '{metacfg.assets_dir}'\"\n",
    "\n",
    "# Output train-script as command line as a bash-script\n",
    "# ./train.sh [<other-sli-args] <experiment-config-file>\n",
    "def make_bash_script(metacfg, script_path='train.sh', nproc='gpu'):\n",
    "    with open(script_path, 'w') as f:\n",
    "        f.write('#!/bin/bash\\n' + train_cmdline(metacfg) + ' \"${@}\"\\n')\n",
    "        os.chmod(f.fileno(), stat.S_IREAD|stat.S_IRUSR|stat.S_IWUSR|stat.S_IXUSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae89f3-11cb-45d9-b4ec-6ac2ae35a5be",
   "metadata": {},
   "source": [
    "#### Generate Bash Script\n",
    "\n",
    "This will output a shell-script which will invoke the training script with the arguments for this project.\n",
    "\n",
    "```bash\n",
    "# Optional: Restrict the GPUs to use to a sub-set of those avialable.\n",
    "export CUDA_VISIBLE_DEVICES=\"0,1\"\n",
    "\n",
    "# Run Accelerate Trainer experiment\n",
    "./train forgather_demo/accel_experiment.yaml\n",
    "\n",
    "# Run Huggingface Trainer experiment\n",
    "./train forgather_demo/hf_trainer_experiment.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e3cdc6-925d-4b28-ae59-778c011dcd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "torchrun --standalone --nproc-per-node gpu '../scripts/train_script.py' -w 'forgather_demo/whitelist.yaml' -I 'forgather_demo' -I '../templates' -I '../model_zoo'  -s '..' \"${@}\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_bash_script(metacfg)\n",
    "\n",
    "# Read back to verify\n",
    "with open('train.sh', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a3131-a09e-46de-8f60-6699f758630c",
   "metadata": {},
   "source": [
    "#### Run Training Script from Notebook\n",
    "\n",
    "This will execute a shell command to run the training script, where the notebook will act as the shell console.  \n",
    "\n",
    "Note: The tqdm progress bars do not render properly in the notebook. It will train, but it's ugly. Running this from a real terminal looks much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a0cf4-1c93-47cf-8888-ffb806df684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, this will run on all available GPUs. To restrict it to a sub-set, you can use this envrionment variable.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "!{train_cmdline(metacfg)} 'forgather_demo/hf_trainer_experiment.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369d483-f90f-484e-aeaa-88fe51a20901",
   "metadata": {},
   "source": [
    "### Injecting Callables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a3fd5f-7ba4-4a6d-89e3-dc03401e1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define the tokenizer to use\n",
      ".define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
      "    - \"../tokenizers/tiny_stories_2k\"\n",
      "\n",
      "# Load a dataset from the hub\n",
      ".define: &raw_dataset !callable:datasets:load_dataset [ \"roneneldan/TinyStories\" ]\n",
      "\n",
      "# Call the injected Callable, 'tokenize_dataset'\n",
      "# Note that we can pass objects created in the config into the callable.\n",
      "dataset: &dataset !callable:tokenize_dataset [ *raw_dataset, *tokenizer, [ 0.01, 1.0 ] ]\n",
      "\n",
      "# Get splits\n",
      "train_dataset: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
      "eval_dataset: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
      "----------------------------------------\n",
      "- transformers:AutoTokenizer.from_pretrained\n",
      "- datasets:load_dataset\n",
      "- forgather.construct:get_item\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "\n",
    "tokenize_config = \"\"\"\n",
    "-- set tokenizer_path = path_join('..', 'tokenizers', 'tiny_stories_2k')\n",
    "-- set dataset_id = \"roneneldan/TinyStories\"\n",
    "\n",
    "# Define the tokenizer to use\n",
    ".define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
    "    - \"{{ tokenizer_path }}\"\n",
    "\n",
    "# Load a dataset from the hub\n",
    ".define: &raw_dataset !callable:datasets:load_dataset [ \"{{ dataset_id }}\" ]\n",
    "\n",
    "# Call the injected Callable, 'tokenize_dataset'\n",
    "# Note that we can pass objects created in the config into the callable.\n",
    "dataset: &dataset !callable:tokenize_dataset [ *raw_dataset, *tokenizer, [ 0.01, 1.0 ] ]\n",
    "\n",
    "# Get splits\n",
    "train_dataset: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
    "eval_dataset: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
    "\"\"\"\n",
    "\n",
    "tokenize_whitelist = \"\"\"\n",
    "- transformers:AutoTokenizer.from_pretrained\n",
    "- datasets:load_dataset\n",
    "- forgather.construct:get_item\n",
    "\"\"\"\n",
    "\n",
    "# Dump a preview of the pre-processed configs\n",
    "print(preprocess_config(tokenize_config, load_method=\"from_string\"))\n",
    "print('-' * 40)\n",
    "print(preprocess_config(tokenize_whitelist, load_method=\"from_string\"))\n",
    "print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852741a-6e6f-4409-8f59-4d0e1c9abc34",
   "metadata": {},
   "source": [
    "Note that the tag '!callable:tokenize_dataset' is not a valid import-spec, as it lacks a ':' in the string.\n",
    "\n",
    "This is a 'stand-in' tag, which needs to be filled in when the config is materialized.\n",
    "\n",
    "Let's define the stand-in and inject it into materialize_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f8535d-12c0-4736-9603-f0ff21676abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      "  dataset:\n",
      "    train:\n",
      "      Dataset({\n",
      "          features: ['input_ids'],\n",
      "          num_rows: 21197\n",
      "      })\n",
      "    validation:\n",
      "      Dataset({\n",
      "          features: ['input_ids'],\n",
      "          num_rows: 21990\n",
      "      })\n",
      "  train_dataset:\n",
      "    Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 21197\n",
      "    })\n",
      "  eval_dataset:\n",
      "    Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "pp_config:\n",
      "       1: # Define the tokenizer to use\n",
      "       2: .define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
      "       3:     - \"../tokenizers/tiny_stories_2k\"\n",
      "       4: \n",
      "       5: # Load a dataset from the hub\n",
      "       6: .define: &raw_dataset !callable:datasets:load_dataset [ \"roneneldan/TinyStories\" ]\n",
      "       7: \n",
      "       8: # Call the injected Callable, 'tokenize_dataset'\n",
      "       9: # Note that we can pass objects created in the config into the callable.\n",
      "      10: dataset: &dataset !callable:tokenize_dataset [ *raw_dataset, *tokenizer, [ 0.01, 1.0 ] ]\n",
      "      11: \n",
      "      12: # Get splits\n",
      "      13: train_dataset: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
      "      14: eval_dataset: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
      "  \n",
      "whitelist:\n",
      "  - 'datasets:load_dataset'\n",
      "  - 'forgather.construct:get_item'\n",
      "  - 'transformers:AutoTokenizer.from_pretrained'\n",
      "pp_whitelist:\n",
      "       1: - transformers:AutoTokenizer.from_pretrained\n",
      "       2: - datasets:load_dataset\n",
      "       3: - forgather.construct:get_item\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Define a Callable to inject.\n",
    "def tokenize_dataset(dataset_dict, tokenizer, select: list[float]):\n",
    "    \"\"\"\n",
    "    Given a DatasetDict and tokenizer, tokenize each split and return it in a new dictionary.\n",
    "\n",
    "    select: A list of floats, each which specifies how much of the dataset to include.\n",
    "        e.g. [0.1, 1.0 ] = 10% and 100%\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    \n",
    "    def map_fn(element, tokenizer):\n",
    "        outputs = tokenizer(\n",
    "            element[\"text\"],\n",
    "            truncation=True,\n",
    "        )\n",
    "        return {\"input_ids\": outputs[\"input_ids\"]}\n",
    "    \n",
    "    for i, (split, dataset) in enumerate(dataset_dict.items()):\n",
    "        if select[i] < 1.0:\n",
    "            dataset = dataset.select(range(0, int(len(dataset) * select[i])))\n",
    "        \n",
    "        tokenized_data = dataset.map(\n",
    "            map_fn,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names,\n",
    "            fn_kwargs=dict(tokenizer=tokenizer)\n",
    "        )\n",
    "        output[split] = tokenized_data\n",
    "    return output\n",
    "\n",
    "# Inject the Callable via kwargs.\n",
    "config_output = materialize_config(\n",
    "    tokenize_config,\n",
    "    tokenize_whitelist,\n",
    "    load_method=\"from_string\",\n",
    "    kwargs=dict(tokenize_dataset=tokenize_dataset),\n",
    ")\n",
    "\n",
    "pconfig(config_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af73f5-401e-4758-b4a2-9a6e855235a3",
   "metadata": {},
   "source": [
    "As instances of 'Latent' are themselves Callables, where they materialize their definition when called, they definitions can be chained.\n",
    "\n",
    "Here, we define a config for a raw dataset and a config for tokenizing an injected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd62e654-3062-41de-92fb-a780f60e371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent('datasets:load_dataset', *['roneneldan/TinyStories'], **{})\n",
      "----------------------------------------\n",
      "dataset:\n",
      "  Latent 'tokenize_dataset'\n",
      "    - Latent 'raw_dataset'\n",
      "    - Latent 'transformers:AutoTokenizer.from_pretrained'\n",
      "      - '../tokenizers/tiny_stories_2k'\n",
      "    - - 0.01\n",
      "    - 1.0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from forgather.config import load_config\n",
    "from forgather import Latent\n",
    "\n",
    "raw_dataet_def = \"\"\"\n",
    "-- set dataset_id = \"roneneldan/TinyStories\"\n",
    "!callable:datasets:load_dataset [ \"{{ dataset_id }}\" ]\n",
    "\"\"\"\n",
    "\n",
    "tokenize_dataset_def = \"\"\"\n",
    "-- set tokenizer_path = path_join('..', 'tokenizers', 'tiny_stories_2k')\n",
    ".define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
    "    - \"{{ tokenizer_path }}\"\n",
    "dataset: &dataset !callable:tokenize_dataset [ !callable:raw_dataset [], *tokenizer, [ 0.01, 1.0 ] ]\n",
    "\"\"\"\n",
    "\n",
    "whitelist = load_config(tokenize_whitelist, load_method=\"from_string\").config\n",
    "raw_dataset = load_config(raw_dataet_def, load_method=\"from_string\").config\n",
    "\n",
    "dataset = load_config(\n",
    "    tokenize_dataset_def,\n",
    "    load_method=\"from_string\",\n",
    "    tokenize_dataset=tokenize_dataset,\n",
    "    raw_dataset=raw_dataset\n",
    ").config\n",
    "\n",
    "print(raw_dataset)\n",
    "print('-' * 40)\n",
    "pconfig(dataset)\n",
    "print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86e666-7891-4619-84ee-ff96eceff1c0",
   "metadata": {},
   "source": [
    "When we materialize the dataset, we pass the raw_dataset definition to the tokenize_dataset definition.\n",
    "\n",
    "We could also have first materialized the raw_dataset and then injected it as a lambda expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac67d4b-06dd-443a-ab01-319fab65189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  train:\n",
      "    Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 21197\n",
      "    })\n",
      "  validation:\n",
      "    Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 21990\n",
      "    })\n"
     ]
    }
   ],
   "source": [
    "output_config = Latent.materialize(dataset, whitelist=whitelist, raw_dataset=raw_dataset, tokenize_dataset=tokenize_dataset)\n",
    "pconfig(output_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3c45a-d962-4316-8146-afd0a1024614",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Delete all of the output models produced by the demo and start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073be4ea-bebc-4f64-8248-3818d8717863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 'forgather_demo/output_models'\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "from aiws.dotdict import DotDict\n",
    "from forgather.config import load_config, pconfig\n",
    "metacfg = DotDict(load_config('forgather_config.yaml').config)\n",
    "\n",
    "print(f\"Removing '{metacfg.models_dir}'\")\n",
    "shutil.rmtree(metacfg.models_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808aab0d-4609-4cfc-937d-dad0ec5dccbc",
   "metadata": {},
   "source": [
    "## forgather.latent\n",
    "\n",
    "A Latent \\[object\\] abstracts what to create from when to create it\n",
    "\n",
    "The primary intended use-case is for safely constructing objects from a\n",
    "configuration file. Consider the case where a configuration file may define objects which\n",
    "can take a considerable amount of time to construct (i.e. processing a dataset).\n",
    "\n",
    "In this case, its useful to allow the complete file to be parsed before attempting a\n",
    "lengthy task, as there may still be errors present which will cause the operation to\n",
    "abort. It's much better to first fully parse the file, validate the safety of \n",
    "the all the types, and only then then, materialize the definiton. This is far less\n",
    "painful than having to fix a single error, wait for the long operation to complete (again)\n",
    "and then hit another error. Fun times...\n",
    "\n",
    "Allowing deferal can also avoid materializing expensive objects which are not needed, as per\n",
    "runtime logic. For example, a definition may define several datasets, where-as only a single\n",
    "one is actually selected, contingent upon 'whatever.'\n",
    "\n",
    "If an object is never materialized, this also avoids loading the associated modules.\n",
    "\n",
    "Finally, this allows one two lazilly construct objects in whatever order makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06100e8b-b0d6-41ee-a784-7aab8c8b62d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent('torch:Tensor', *([1, 2, 3],), **{}, as_callable=True, is_singleton=False)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "from forgather import Latent\n",
    "from forgather.config import pconfig\n",
    "\n",
    "# Define the object to construct\n",
    "latent_tensor = Latent(\"torch:Tensor\", [1 ,2, 3], as_callable=True, is_singleton=False)\n",
    "print(latent_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1c6c4b-e681-4768-b344-fb8b42a51620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# ... and some time later, materialize the object instance.\n",
    "tensor = latent_tensor()\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcab27-b7f2-4d0e-8fc4-ba8d476ec07d",
   "metadata": {},
   "source": [
    "This can also extend to graphs of objects..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2501ad79-4f52-46a8-aed7-043c01a22e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': Latent('torch:sum', *(Latent('torch:Tensor', *([1, 2, 3],), **{}, as_callable=False, is_singleton=False),), **{}, as_callable=False, is_singleton=False)}\n",
      "{'total': tensor(6.)}\n"
     ]
    }
   ],
   "source": [
    "data = dict(\n",
    "    total = Latent(\"torch:sum\", Latent(\"torch:Tensor\", [1 ,2, 3]))\n",
    ")\n",
    "print(data)\n",
    "\n",
    "obj = Latent.materialize(data)\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1cf2f-2d34-4de2-b6f9-775fef76df40",
   "metadata": {},
   "source": [
    "We can also specify modules by path-name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e1e95ed-9195-44f8-a560-7607c4cc7f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent('../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer', *(Latent('../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig', *(), **{'hidden_size': 64, 'num_hidden_layers': 3}, as_callable=False, is_singleton=False),), **{}, as_callable=False, is_singleton=False)\n",
      "******************** or pretty-printed... ********************\n",
      "Latent '../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer'\n",
      "  - Latent '../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig'\n",
      "    hidden_size: 64\n",
      "    num_hidden_layers: 3\n"
     ]
    }
   ],
   "source": [
    "model = Latent(\n",
    "    \"../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\",\n",
    "    Latent(\n",
    "        \"../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig\",\n",
    "        hidden_size=64, num_hidden_layers=3\n",
    "    )\n",
    ")\n",
    "print(model)\n",
    "print('*' * 20 + \" or pretty-printed... \" + \"*\" * 20)\n",
    "pconfig(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3920a9d7-cc6b-4774-b21d-17da424c473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 64)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (key_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (value_linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=64, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# And materialize the definition...\n",
    "# The __call__ method is short-hand for Latent.materialize(model)\n",
    "# If called (or materialized) again, the same instance will be returned.\n",
    "materialized_model = model()\n",
    "print(materialized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41181c2-715a-491a-aded-06f731f49f42",
   "metadata": {},
   "source": [
    "You can restrict which types of objects can be materialized by specifying a whitelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b01a44-0979-42e8-a16d-bd755e8a6a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from forgather.dynamic import normalize_import_spec\n",
    "# Note: Any import-specs with paths should be normaized\n",
    "# with forgather.dynamic.normalize_import_spec(). This\n",
    "# ensures that all equivalent paths have the same representation.\n",
    "whitelist = set((\n",
    "    \"torch:Tensor\",\n",
    "    \"torch:add\",\n",
    "    normalize_import_spec(\"../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\"),\n",
    "))\n",
    "\n",
    "allowed_instance = Latent(\"torch:Tensor\", [4, 5, 6])\n",
    "allowed_instance(whitelist=whitelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d38023-1c90-4564-a166-38784b5d8bfd",
   "metadata": {},
   "source": [
    "If something is not in the whitelist, an exception will be raised with a list of all prohibited types listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97af7233-685b-4510-82de-2f10657ecfad",
   "metadata": {},
   "outputs": [
    {
     "ename": "LatentException",
     "evalue": "The following dynamic imports were not found in the whitelist: {'torch:mul', 'torch:sum'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLatentException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prohibited_instance \u001b[38;5;241m=\u001b[39m Latent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch:mul\u001b[39m\u001b[38;5;124m\"\u001b[39m, Latent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch:sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, Latent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch:Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;241m1\u001b[39m ,\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])), \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprohibited_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhitelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhitelist\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_assets/aiworkshop/tutorial/../forgather/latent.py:241\u001b[0m, in \u001b[0;36mLatent.__call__\u001b[0;34m(self, whitelist, **mapping)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, whitelist: Container\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmapping):\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Alias for calling materialize() on self\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLatent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhitelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhitelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_assets/aiworkshop/tutorial/../forgather/latent.py:256\u001b[0m, in \u001b[0;36mLatent.materialize\u001b[0;34m(obj, whitelist, **mapping)\u001b[0m\n\u001b[1;32m    254\u001b[0m     invalid_set \u001b[38;5;241m=\u001b[39m Latent\u001b[38;5;241m.\u001b[39mvalidate_whitelist(obj, whitelist)\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(invalid_set):\n\u001b[0;32m--> 256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LatentException(\n\u001b[1;32m    257\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following dynamic imports were not found in the whitelist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpformat(invalid_set)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    258\u001b[0m Latent\u001b[38;5;241m.\u001b[39m_resolve_standins(obj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmapping)\n\u001b[1;32m    259\u001b[0m Latent\u001b[38;5;241m.\u001b[39m_resolve_dynamic_imports(obj)\n",
      "\u001b[0;31mLatentException\u001b[0m: The following dynamic imports were not found in the whitelist: {'torch:mul', 'torch:sum'}"
     ]
    }
   ],
   "source": [
    "prohibited_instance = Latent(\"torch:mul\", Latent(\"torch:sum\", Latent(\"torch:Tensor\", [1 ,2, 3])), 3)\n",
    "prohibited_instance(whitelist=whitelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41011981-a27f-47d3-aaa4-dba4c15355c9",
   "metadata": {},
   "source": [
    "Alternatively, we can just get the list of disallowed types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bccaa2b-ac89-4e47-b547-d28c80bda2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disallowed: {'torch:mul', 'torch:sum'}\n"
     ]
    }
   ],
   "source": [
    "invalid_set = Latent.validate_whitelist(prohibited_instance, whitelist)\n",
    "if len(invalid_set):\n",
    "    # Show all disallowed types in the graph\n",
    "    print(f\"Disallowed: {invalid_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3c0c1-b6d0-4ea8-8cf6-4117ce95dbea",
   "metadata": {},
   "source": [
    "#### Object Identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e72117-f36f-46e1-a631-6b67cb339158",
   "metadata": {},
   "source": [
    "By default, each call returns the same object instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e252f4cf-1d3d-4628-a948-df46c000686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_tensor = Latent(\"torch:Tensor\", [1 ,2, 3])\n",
    "tensor = latent_tensor()\n",
    "assert(id(tensor) != id(latent_tensor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da14f48-e146-4e0e-9f71-383443a3d222",
   "metadata": {},
   "source": [
    "This can be overridden by setting the \"is_singleton\" flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e488729-5015-44cd-87e4-9fd2cc9cfa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_tensor = Latent(\"torch:Tensor\", [1 ,2, 3], is_singleton=True)\n",
    "tensor = latent_tensor()\n",
    "assert(id(tensor) == id(latent_tensor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9440faf4-0863-4f2b-9924-cb56cf46e4db",
   "metadata": {},
   "source": [
    "Arguments can be injected into the graph at the poin of materialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5a40f-f7af-4a1f-9cee-b1f851300828",
   "metadata": {},
   "source": [
    "#### Passing Arugments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4656ee54-2623-4dbc-b9a5-e9e64989bd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Notice how the second import-spec, \"arg,\" does not have a ':' character.\n",
    "# This is a placeholder for a real value to be specified when the object is materialized.\n",
    "deferred_sum = Latent(\"torch:sum\", Latent(\"arg\"))\n",
    "\n",
    "# Let's create an object to substitue 'sum_input' with.\n",
    "tensor = torch.tensor([1 ,2, 3])\n",
    "\n",
    "# Materialize the value\n",
    "deferred_sum(arg=tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc67fc-f344-4a75-ad03-736473dbbaab",
   "metadata": {},
   "source": [
    "### Corner Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef43c2-aae6-4e02-90b9-16ba8e5d7ed8",
   "metadata": {},
   "source": [
    "#### Tied Parameters\n",
    "As mentioned above, the default is for all instancs of the same object to be singletons; that is, there is really only one instance, no matter how many times you call the object.\n",
    "\n",
    "By setting 'is_singleton' to False, you will get a different instance each time the object is materialized, but what happens when a non-singleton object exists in more than one place in the graph?\n",
    "\n",
    "For example, here we have a simple ML model which takes an input tensor and an output tensor as arguments, which are then used as parameters. If we create a single tensor and pass it as both the input and output arguments, this ties the weights together, as they share the same instance.\n",
    "\n",
    "If the shared parameter is not a singleton, will won't this 'untie' the shared parameter?\n",
    "\n",
    "No. When constructing the object graph, we keep track of which objects have already been instantiated with an object-id map. If an object with the same ID is 'constructed' a second time, the 'cached' object will be returned, rather than a new one.\n",
    "\n",
    "The difference only comes about when the object is materialized more than once, in which case the 'cache' is flushed between calls and a new instance of the object will be constructed. This difference can only be observed when the graph is constructed more than once.\n",
    "\n",
    "In this example, we construct the model described above and print the values of the input and output weights.\n",
    "\n",
    "If the shared_weights are non-singleton, then each call will initialize different weights, but they will still be tied.\n",
    "\n",
    "If configured as a singleton, each call will produce the same weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3666ae25-ca34-4616-a731-f0f5373f9783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent(<class '__main__.Net'>, *(), **{'input': Latent('torch:randn', *(3, 4), **{'requires_grad': True}, as_callable=False, is_singleton=False), 'output': Latent('torch:randn', *(3, 4), **{'requires_grad': True}, as_callable=False, is_singleton=False)}, as_callable=False, is_singleton=False)\n",
      "tensor([[-1.0802, -0.2785, -0.5239,  1.4973],\n",
      "        [-1.2288,  0.7229,  1.9864,  0.4387],\n",
      "        [ 1.3913, -1.0829, -0.9404, -0.3665]])\n",
      "tensor([[-1.0802, -0.2785, -0.5239,  1.4973],\n",
      "        [-1.2288,  0.7229,  1.9864,  0.4387],\n",
      "        [ 1.3913, -1.0829, -0.9404, -0.3665]])\n",
      "\n",
      "\n",
      "\n",
      "tensor([[ 1.1079,  0.3517, -0.6129,  0.7602],\n",
      "        [-0.4242, -0.4379,  0.8860,  0.6022],\n",
      "        [ 0.0201,  0.4126, -1.1070,  0.5717]])\n",
      "tensor([[ 1.1079,  0.3517, -0.6129,  0.7602],\n",
      "        [-0.4242, -0.4379,  0.8860,  0.6022],\n",
      "        [ 0.0201,  0.4126, -1.1070,  0.5717]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a simple model\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super().__init__()\n",
    "        self.input = torch.nn.Parameter(input)\n",
    "        self.output = torch.nn.Parameter(output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  x @ self.input\n",
    "        x = x @ self.output.t()\n",
    "        return x\n",
    "\n",
    "# Create a 'shared' tensor for both input and output networks.\n",
    "# Try changing 'is_singleton'\n",
    "shared_weights = Latent('torch:randn', 3, 4, requires_grad=True, is_singleton=False)\n",
    "latent_model = Latent(Net, input=shared_weights, output=shared_weights, is_singleton=False)\n",
    "print(latent_model)\n",
    "\n",
    "model = latent_model()\n",
    "print(model.input.data)\n",
    "print(model.output.data)\n",
    "print(\"\\n\\n\")\n",
    "model = latent_model()\n",
    "print(model.input.data)\n",
    "print(model.output.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d62de-7c91-421b-948b-c88ee322654b",
   "metadata": {},
   "source": [
    "#### Factory Objects\n",
    "\n",
    "One use-case calls for providing 'factory' agruments to an object, where each call produces a new objects instance.\n",
    "\n",
    "Consider this use-case:\n",
    "\n",
    "The Huggingface Trainer class allows you to pass a \"model intializer,\" rather than a model instance, to the Trainer. The Trainer will then explore various hyper-parameters, initializing a new model instance on each iteration.\n",
    "\n",
    "If the Trainer is part of a configuration and the model is also in the configuration, this makes it rather difficult to pass a \"model initializer\" to the Trainer; when the Latent graph is constructed, the initializer will be a concrete model instance, not a callable constructor.\n",
    "\n",
    "This can be solved by setting the 'as_callable' flag on the model constructor, which result in an unmaterialized callable being passed to the Trainer.\n",
    "\n",
    "Now, when the Trainer calls the model initializer, the model will be materialized.\n",
    "\n",
    "This does not fully solve the problem, as subsequent calls will return the same model. We can solve this by setting 'is_singleton=False,' which will resut in a new model instance each time it is called.\n",
    "\n",
    "If the model has any other Latent objects, these too can be independently configured as singletons or callables.\n",
    "\n",
    "Finally, if the called function is expected to take any arguments, these can be mapped to arguments anywhere in the graph of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eb9abcc-166d-4e14-9a90-ccb68e01ebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Construct Latent; all Latent objects are still latent.\n",
      "\n",
      "Latent(<class '__main__.TensorFactory'>, *(Latent('torch:randn', *(Latent('rows', *(), **{}, as_callable=False, is_singleton=False), Latent('cols', *(), **{}, as_callable=False, is_singleton=False)), **{}, as_callable=True, is_singleton=False),), **{}, as_callable=False, is_singleton=False)\n",
      "----------------------------------------\n",
      "\n",
      "Materialized Latnet; as the 2nd level latent is 'as_callable,' is was passed to the factory untouched.\n",
      "\n",
      "Furthermore, this isolated the 3rd level Latent, so it was also not materialized.\n",
      "\n",
      "TensorFactory(Latent(<built-in method randn of type object at 0x7f262e6a4760>, *(Latent('rows', *(), **{}, as_callable=False, is_singleton=False), Latent('cols', *(), **{}, as_callable=False, is_singleton=False)), **{}, as_callable=True, is_singleton=False))\n",
      "----------------------------------------\n",
      "\n",
      "Factory calls Latent, passing different arguments each time.\n",
      " On each call, the arguments are resovled and a new instance is returned\n",
      "\n",
      "Tensor 1:  tensor([[-1.6665,  1.2161, -0.9514],\n",
      "        [ 0.2967,  0.1698,  0.7825],\n",
      "        [-0.2784,  0.6371,  0.6405],\n",
      "        [ 0.3670, -1.2892, -1.6077],\n",
      "        [-0.7381,  0.7189, -0.8046],\n",
      "        [-1.0673, -0.6421, -0.2898]])\n",
      "Tensor 2:  tensor([[ 2.2907,  1.9049,  1.6259, -0.0329,  0.8835],\n",
      "        [-0.5203,  0.3435, -0.2153, -1.8938, -0.5813],\n",
      "        [ 0.4697,  0.6029,  0.6229,  0.5153, -1.1612]])\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Factory class. Given a callable, when called it uses the provided callable to create new objects.\n",
    "class TensorFactory:\n",
    "    def __init__(self, factory: Callable):\n",
    "        assert isinstance(factory, Callable)\n",
    "        self.factory = factory\n",
    "        self.n_cols = 1\n",
    "\n",
    "    def make_tensor(self, **argv):\n",
    "        # Pass a combination of arguments from the caller and from the factory\n",
    "        # Increase the number of columns by two each time\n",
    "        self.n_cols += 2\n",
    "        return self.factory(cols=self.n_cols, **argv)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TensorFactory({self.factory})\"\n",
    "\n",
    "# Experiment with changing the arguments to see how this works.\n",
    "print(\"\\Construct Latent; all Latent objects are still latent.\\n\")\n",
    "latent_factory = Latent(\n",
    "    TensorFactory,\n",
    "    Latent(\n",
    "        \"torch:randn\", # Initialize a random tensor with the specified dimensions.\n",
    "        Latent(\n",
    "            \"rows\", # This is an argument which can be specified called.\n",
    "            is_singleton=False, # Create a new instance each time.\n",
    "        ),\n",
    "        Latent(\n",
    "            \"cols\", # This is an argument which can be specified called.\n",
    "            is_singleton=False, # Create a new instance each time.\n",
    "        ),\n",
    "        is_singleton=False, # Each call should return a new instance.\n",
    "        as_callable=True, # Pass object as a Callable, rather than immediately materializing it.\n",
    "    )\n",
    ")\n",
    "print(latent_factory)\n",
    "\n",
    "print('-' * 40)\n",
    "print(\"\\nMaterialized Latnet; as the 2nd level latent is 'as_callable,' is was passed to the factory untouched.\")\n",
    "print(\"\\nFurthermore, this isolated the 3rd level Latent, so it was also not materialized.\\n\")\n",
    "factory = latent_factory()\n",
    "print(factory)\n",
    "\n",
    "print('-' * 40)\n",
    "print(\"\\nFactory calls Latent, passing different arguments each time.\")\n",
    "print(\" On each call, the arguments are resovled and a new instance is returned\\n\")\n",
    "print(\"Tensor 1: \", factory.make_tensor(rows=6))\n",
    "print(\"Tensor 2: \", factory.make_tensor(rows=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ea98c-3972-4ff8-a66e-c04d543ab1cb",
   "metadata": {},
   "source": [
    "## forgather.dynamic\n",
    "\n",
    "The 'dynamic' module can dynamically import attributes from Python modules, given either a module and attribute name in the sys.path or directly from a file-path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad9db0ef-6705-478e-9794-028ef0cd4f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from forgather.dynamic import dynamic_import\n",
    "import os\n",
    "\n",
    "# Create a simple namespace to put our dynamic imports in\n",
    "# The global namespace works too, but I want to avoid cluttering it with our demo imports.\n",
    "ns = SimpleNamespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d1859-f8a4-4caf-a2ba-de23eefee98a",
   "metadata": {},
   "source": [
    "We can import an attribute from a module.\n",
    "\n",
    "In this example, we get the torch.tensor class and the torch 'nn' namespace. Once imported, be can use them just like a regular import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "678aea2b-da25-4002-8418-26a3b856bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([1, 2, 3])\n",
      "Module()\n"
     ]
    }
   ],
   "source": [
    "ns.tensor = dynamic_import(\"torch:tensor\")\n",
    "ns.nn = dynamic_import(\"torch:nn\")\n",
    "\n",
    "# Create a tensor\n",
    "tensor = ns.tensor([1, 2, 3])\n",
    "print(type(tensor), tensor)\n",
    "\n",
    "# Create an torch.nn.Module\n",
    "module = ns.nn.Module()\n",
    "print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3a172-11bb-4c6b-aafc-b651dd049763",
   "metadata": {},
   "source": [
    "We can also import modules directly from a Python source file, even if it's not in our sys.path.\n",
    "\n",
    "For example, let's get a transformer config and model definiton and instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c744a22-3a29-4dc6-80d3-d79de5a31434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model_zoo/vanilla_transformer/vanilla_transformer.py\n",
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (key_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (value_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "module_path = os.path.join('..', 'model_zoo', 'vanilla_transformer', 'vanilla_transformer.py')\n",
    "print(module_path)\n",
    "ns.ModelConfig = dynamic_import(module_path + ':VanillaTransformerConfig')\n",
    "ns.TransformerModel = dynamic_import(module_path + ':VanillaTransformer')\n",
    "\n",
    "model = ns.TransformerModel(ns.ModelConfig(hidden_size=128, num_hidden_layers=2))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c60124-c1f7-44dc-8fb4-40090aa40253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
