{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bb9655-0848-448a-a500-28196416c634",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "[forgather_config.yaml](forgather_config.yaml)  \n",
    "[forgather_demo/paths.yaml](forgather_demo/paths.yaml)  \n",
    "[../templates/common/tokenizers/causal_bpe.yaml](../templates/common/tokenizers/causal_bpe.yaml)  \n",
    "[../templates/common/tokenizers/tiny_2k_bpe.yaml](../templates/common/tokenizers/tiny_2k_bpe.yaml)  \n",
    "[../templates/common/tokenizers/whitelist.yaml](../templates/common/tokenizers/whitelist.yaml)  \n",
    "\n",
    "### See Also\n",
    "[forgather.ipynb](forgather.ipynb)  \n",
    "[aiws.tokenizer_trainer.TokenizerTrainer](../aiws/tokenizer_trainer.py)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d53b418e-44b4-4ffd-90c4-b819e6341998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assets_dir': '..',\n",
      " 'dataset_id': 'roneneldan/TinyStories',\n",
      " 'datasets_dir': '../datasets',\n",
      " 'model_src_dir': '../model_zoo',\n",
      " 'models_dir': 'forgather_demo/output_models',\n",
      " 'project_templates': 'forgather_demo',\n",
      " 'script_dir': '../scripts',\n",
      " 'search_paths': ['forgather_demo', '../templates', '../model_zoo'],\n",
      " 'templates': '../templates',\n",
      " 'tokenizer_def': '../templates/common/tokenizers/tiny_2k_bpe.yaml',\n",
      " 'tokenizer_dir': '../tokenizers',\n",
      " 'tokenizer_path': '../tokenizers/tiny_stories_2k',\n",
      " 'tokenizers_whitelist': '../templates/common/tokenizers/whitelist.yaml',\n",
      " 'train_script_path': '../scripts/train_script.py',\n",
      " 'whitelist_path': 'forgather_demo/whitelist.yaml'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if '..' not in sys.path: sys.path.insert(0, '..')\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "import tokenizers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from datasets import load_dataset\n",
    "\n",
    "from aiws.dotdict import DotDict\n",
    "from forgather.config import load_config\n",
    "from pprint import pp, pformat\n",
    "\n",
    "# Load meta-configuration\n",
    "dirs = DotDict(load_config('forgather_config.yaml').config)\n",
    "print(pformat(dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28c0a469-f3d8-4d4a-8bb9-80028690008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1: # BPE Tokenizer Definition for Causal Model\n",
      "     2: # 2024-07-08 05:15:58\n",
      "     3: # dataset_id: 'roneneldan/TinyStories'\n",
      "     4: # dataset_split: 'train\n",
      "     5: # model_max_length: '2048'\n",
      "     6: # vocab_size: '2000'\n",
      "     7: \n",
      "     8: trainer_args: &trainer_args\n",
      "     9:     model: !callable:tokenizers:models.BPE\n",
      "    10:         kwargs:\n",
      "    11:             cache_capacity: 16\n",
      "    12:             unk_token: \"<|UNK|>\"\n",
      "    13:             byte_fallback: True\n",
      "    14:     normalizer: !callable:tokenizers:normalizers.NFC []\n",
      "    15:     pre_tokenizer: !callable:tokenizers:pre_tokenizers.ByteLevel []\n",
      "    16:     decoder: !callable:tokenizers:decoders.ByteLevel []\n",
      "    17: \n",
      "    18:     # Automatically add bos token to sequence start\n",
      "    19:     post_processor: !callable:tokenizers:processors.TemplateProcessing\n",
      "    20:         single: \"<|BOS|> $A\"\n",
      "    21:         special_tokens: [[ \"<|BOS|>\", 0 ]]\n",
      "    22:     trainer: !callable:tokenizers.trainers:BpeTrainer\n",
      "    23:         kwargs:\n",
      "    24:             vocab_size: 2000\n",
      "    25:             initial_alphabet: !callable:tokenizers:pre_tokenizers.ByteLevel.alphabet []\n",
      "    26:             special_tokens:\n",
      "    27:                 - \"<|BOS|>\"\n",
      "    28:                 - \"<|PAD|>\"\n",
      "    29:                 - \"<|EOS|>\"\n",
      "    30:                 - \"<|UNK|>\"\n",
      "    31:             show_progress: False\n",
      "    32:     dataset: !callable:forgather.construct:get_item\n",
      "    33:         - !callable:datasets:load_dataset [ \"roneneldan/TinyStories\" ]\n",
      "    34:         - \"train\"\n",
      "    35: \n",
      "    36: # aiws.tokenizer_trainer.TokenizerTrainer args\n",
      "    37: trainer: !callable:aiws.tokenizer_trainer:TokenizerTrainer\n",
      "    38:     kwargs: *trainer_args \n",
      "    39: \n",
      "    40: # Args to transformers.PreTrainedTokenizerFast()\n",
      "    41: pretrained_tokenizer_fast_args:\n",
      "    42:     bos_token: \"<|BOS|>\"\n",
      "    43:     eos_token: \"<|EOS|>\"\n",
      "    44:     unk_token: \"<|UNK|>\"\n",
      "    45:     pad_token: \"<|PAD|>\"\n",
      "    46:     return_special_tokens_mask: False\n",
      "    47:     model_max_length: 2048\n",
      "    48:     padding_side: \"right\"\n",
      "    49:     truncation_side: \"right\"\n",
      "    50:     # chat_template: str; a Jinja template string\n",
      "    51:     # additional_special_tokens: List[str]\n",
      "    52:     # clean_up_tokenization_spaces: True\n",
      "    53:     # split_special_tokens: Falsetokenizer_name: \"tiny_stories_2k\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from forgather.config import materialize_config, fconfig, preprocess_config\n",
    "\n",
    "config_out = materialize_config(dirs.tokenizer_def, whitelist=dirs.tokenizers_whitelist, search_path=dirs.search_paths)\n",
    "print(fconfig(config_out.pp_config))\n",
    "config = DotDict(config_out.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fd9ade9-bde3-4379-b080-81a16db230d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer_args:\n",
      "  model: <tokenizers.models.BPE object at 0x7f5dcfcbd9d0>\n",
      "  normalizer: <tokenizers.normalizers.NFC object at 0x7f5e7fea06f0>\n",
      "  pre_tokenizer: <tokenizers.pre_tokenizers.ByteLevel object at 0x7f5dcd34e8b0>\n",
      "  decoder: <tokenizers.decoders.ByteLevel object at 0x7f5e80f00ed0>\n",
      "  post_processor: <tokenizers.processors.TemplateProcessing object at 0x7f5dcfcad320>\n",
      "  trainer: <tokenizers.trainers.BpeTrainer object at 0x7f5dcfcbc770>\n",
      "  dataset:\n",
      "    Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "trainer: <aiws.tokenizer_trainer.TokenizerTrainer object at 0x7f5e80e848e0>\n",
      "pretrained_tokenizer_fast_args:\n",
      "  bos_token: '<|BOS|>'\n",
      "  eos_token: '<|EOS|>'\n",
      "  unk_token: '<|UNK|>'\n",
      "  pad_token: '<|PAD|>'\n",
      "  return_special_tokens_mask: False\n",
      "  model_max_length: 2048\n",
      "  padding_side: 'right'\n",
      "  truncation_side: 'right'\n"
     ]
    }
   ],
   "source": [
    "print(fconfig(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d90c3c30-9cd6-45f1-aaa3-b6832db566a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Tokenizer ****\n",
      "total_samples: 2119719\n",
      "batch_size: 1000\n",
      "steps: 2119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30555c1ef4264ea58157ca14ffcdc131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Completed ****\n",
      "runtime: 26.812073469161987\n",
      "samples_per_second: 79058.377\n"
     ]
    }
   ],
   "source": [
    "config.trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff02cd7f-8524-485d-8bf3-9ccb30943468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|PAD|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = trainer.as_pretrained_tokenizer_fast(**config.pretrained_tokenizer_fast_args)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0c23e-2311-4dd4-9102-013e5f777bd1",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Implementation: [datasets.py](../tutorial_code/datasets.py)  \n",
    "See Also: [dataset.ipynb](./dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa226a9-4047-48c3-97c1-6b8cc3b9b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n",
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = load_dataset(dirs.dataset_id)\n",
    "\n",
    "print(dataset_dict)\n",
    "train_dataset = dataset_dict['train']\n",
    "\n",
    "# For experimentation, we will want a bit of sample text to work with. \n",
    "# This will grab the first 500 characters from the first record of the training dataset.\n",
    "sample_text = train_dataset['text'][0][:500]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c82d0-3e68-4392-87b1-6a2e6b96137f",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Rather than working with the raw ASCII/Unicode from the dataset, we will be \"tokenizing\" the data. A tokenizer is a statisttical model which aggregates individual characters into sub-word, where the most frequent strings of characters are replaced by unique symbols.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Large_language_model#Probabilistic_tokenization\n",
    "\n",
    "For this tutorial, we will be created a Byte Pair Encoding (BPE) tokenizer, which starts with all of the symbols from the ASCII character set, then creates tokens for the most common pairs of ASCII characters. These pairs are further aggregated into larger symbols and the process repeats until a set of symbols matching the target vocabulary size has been created.\n",
    "\n",
    "By starting with the ASCII character set, it is possible to represent any combination of letters, including those which were not observed when the tokenizer was created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd060cfe-460e-4500-a93d-44e9d5b3738d",
   "metadata": {},
   "source": [
    "### Special Character\n",
    "\n",
    "Define the special characters map.\n",
    "\n",
    "The map merely maps the name of the character to how it is represented in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ef912-925b-48a5-a3f0-971d1cea6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens={\n",
    "    \"pad\": \"<|PAD|>\",   # Used to pad unused positions in a sequence.\n",
    "    \"mask\": \"<|MASK|>\", # Used with masked-language-modeling to mark a position as having been masked.\n",
    "    \"bos\": \"<|BOS|>\",   # Beginning of Sequence\n",
    "    \"eos\": \"<|EOS|>\",   # End of Sequence\n",
    "    \"unk\": \"<|UNK|>\",   # Unknown\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604db36-e5ec-4a7e-b175-e2ed6f4f64d4",
   "metadata": {},
   "source": [
    "### Pre-tokenizer\n",
    "\n",
    "Creat a [pre-tokenizer](https://huggingface.co/docs/tokenizers/api/pre-tokenizers),\" which breaks the input text into sub-strings via a regular expression. For example, a simple pre-tokenizer could split the input on spaces and punctuation.\n",
    "\n",
    "We will be using the \"ByteLevel\" pre-tokenizer, which uses a GPT-2 specfic regex for splitting the words and replaces spaces with the 'Ä ' character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63edf63e-c9f2-4def-8fa5-8c0dd56db69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ä One' 'Ä day' ',' 'Ä a' 'Ä little' 'Ä girl' 'Ä named' 'Ä Lily' 'Ä found' 'Ä a' 'Ä needle' 'Ä in' 'Ä her' 'Ä room' '.' 'Ä She' 'Ä knew' 'Ä it' 'Ä was' 'Ä difficult' 'Ä to' 'Ä play' 'Ä with' 'Ä it' 'Ä because' 'Ä it' 'Ä was' 'Ä sharp' '.' 'Ä Lily' 'Ä wanted' 'Ä to' 'Ä share' 'Ä the' 'Ä needle' 'Ä with' 'Ä her' 'Ä mom' ',' 'Ä so' 'Ä she' 'Ä could' 'Ä sew' 'Ä a' 'Ä button' 'Ä on' 'Ä her' 'Ä shirt' '.' 'ÄŠ' 'ÄŠ' 'Lily' 'Ä went' 'Ä to' 'Ä her' 'Ä mom' 'Ä and' 'Ä said' ',' 'Ä \"' 'Mom' ',' 'Ä I' 'Ä found' 'Ä this' 'Ä needle' '.' 'Ä Can' 'Ä you' 'Ä share' 'Ä it' 'Ä with' 'Ä me' 'Ä and' 'Ä sew' 'Ä my' 'Ä shirt' '?\"' 'Ä Her' 'Ä mom' 'Ä smiled' 'Ä and' 'Ä said' ',' 'Ä \"' 'Yes' ',' 'Ä Lily' ',' 'Ä we' 'Ä can' 'Ä share' 'Ä the' 'Ä needle' 'Ä and' 'Ä fix' 'Ä your' 'Ä shirt' '.\"' 'ÄŠ' 'ÄŠ' 'Together' ',' 'Ä they' 'Ä shared' 'Ä the' 'Ä needle' 'Ä and' 'Ä sewed' 'Ä the' 'Ä button' 'Ä on' 'Ä Lily' ''s' 'Ä shirt' '.' 'Ä It' 'Ä was' 'Ä not' 'Ä difficult' 'Ä for' 'Ä them' 'Ä b' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel()\n",
    "\n",
    "def test_pretokenizer(pre_tokenizer, sample_text):\n",
    "    tokens = pre_tokenizer.pre_tokenize_str(sample_text)\n",
    "    for token in tokens:\n",
    "        print(f\"'{token[0]}'\", end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "test_pretokenizer(pre_tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df884e-0ee0-4373-96d2-74730b038a31",
   "metadata": {},
   "source": [
    "### Create a BPE Tokenizer Model\n",
    "\n",
    "[Tokenizer Models](https://huggingface.co/docs/tokenizers/en/api/models)\n",
    "\n",
    "For extra credit, try the other models at the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27d91df-3c7a-4aee-83c6-42843d2f84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model = tokenizers.models.BPE(\n",
    "    cache_capacity=16,\n",
    "    unk_token=special_tokens['unk'],\n",
    "    byte_fallback=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "721bc13f-db43-46d9-801c-0ae0d534a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tokenizers.normalizers.NFC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "362fbb51-fa5f-46e0-b8f2-1be87156183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder is applied when coverting tokens back into text and the ByteLevel decoder\n",
    "# is responsible for replacing 'Ä ' character with spaces. \n",
    "decoder = tokenizers.decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "576161a6-a3c8-4b08-83ec-48d0ab7c9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically add Begin Of Sequence (BOS) token to output when 'add_special_tokens' is True\n",
    "# This has relevance to causal models, which predict the next token in a sequence. As the first real token lacks\n",
    "# a preceeding token, this allows the model to identify where the sequence actually begins.\n",
    "#\n",
    "# Note: A causal model can still function without a BOS token and the need to include it is debatable.\n",
    "post_processor = TemplateProcessing(\n",
    "    single=\"<BOS> $A\",\n",
    "    special_tokens=[\n",
    "        (\"<BOS>\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9ee27-ffca-4a84-90a6-ed69e9f27638",
   "metadata": {},
   "source": [
    "### Define a Constructor\n",
    "The constructor is rather awkward to use, as arguments are set as attributes, rather being passed to the constructor. This just makes construction act more like one would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36751d65-6093-4329-8333-1a557dbb2aaf",
   "metadata": {},
   "source": [
    "### Construct the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56bfc019-7d06-45ab-b0c8-3c0d102fe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_tokenizer = construct_tokenizer(\n",
    "    tokenizer_model,\n",
    "    normalizer,\n",
    "    pre_tokenizer,\n",
    "    decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286dacf-7f4a-4a03-b19e-90e6a8ad05cb",
   "metadata": {},
   "source": [
    "### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a9c39c-fff7-463e-a537-8d8e79be85b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a BPE trainer, which is used to build an optimal set of tokens from\n",
    "# a a given dataset.\n",
    "tok_trainer = tokenizers.trainers.BpeTrainer(\n",
    "    vocab_size=config.tokenizer.vocab_size,\n",
    "    initial_alphabet=tokenizers.pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=list(special_tokens.values()),\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# This abstraction is needed for the trainer to iterate over our dataset\n",
    "def batch_iterator(dataset, batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size]['text']\n",
    "\n",
    "# Train the tokenizer of the dataset\n",
    "# Be patient! This will take a bit of time to complete...\n",
    "pretrained_tokenizer.train_from_iterator(batch_iterator(train_dataset), trainer=tok_trainer, length=len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985509f-4272-43bf-a3c7-2c7302817f45",
   "metadata": {},
   "source": [
    "### Wrap the tokenizer\n",
    "\n",
    "The BPE tokenizer class can be wrapped in a Huggingface [PreTrainedTokenizerFast](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) class, which makes working with the tokenizer easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f3e415d-7bcd-4d3d-8cbc-fcfcfa86b016",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerFast\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This wraps the tokenizer in a Huggingface transformer tokenizer, which\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# is a higher level abstraction\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m PreTrainedTokenizerFast(\n\u001b[1;32m      6\u001b[0m     tokenizer_object\u001b[38;5;241m=\u001b[39mpretrained_tokenizer,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# This should match the model's input length limit, which depends upon the archetecture.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# If not limit is specified, the default will be a VERY LARGE value.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     model_max_length\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmax_sequence_len,\n\u001b[1;32m     10\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# This wraps the tokenizer in a Huggingface transformer tokenizer, which\n",
    "# is a higher level abstraction\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=pretrained_tokenizer,\n",
    "    # This should match the model's input length limit, which depends upon the archetecture.\n",
    "    # If not limit is specified, the default will be a VERY LARGE value.\n",
    "    model_max_length=config.model.max_sequence_len,\n",
    "    pad_token=special_tokens['eos'],\n",
    "    mask_token=special_tokens['mask'],\n",
    "    bos_token=special_tokens['bos'],\n",
    "    eos_token=special_tokens['eos'],\n",
    "    unk_token=special_tokens['unk'],\n",
    "    return_special_tokens_mask=False,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5ab8c-22a7-40f4-91c8-88aa40945363",
   "metadata": {},
   "source": [
    "### Test the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88eab7f4-07fa-4b0e-af00-b97b19b745d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 491, 360, 16, 263, 403, 450, 505, 362, 598, 263, 792, 311, 320, 313, 763, 18, 317, 709, 308, 286, 1035, 74, 475, 1389, 88, 270, 365, 346, 308, 791, 308, 286, 385, 291, 84, 18, 362, 448, 270, 952, 267, 792, 311, 346, 313, 370, 16, 354, 342, 464, 442, 91, 263, 1842, 307, 349, 313, 385, 316, 88, 18, 203, 203, 601, 473, 270, 313, 370, 269, 331, 16, 332, 781, 16, 339, 598, 747, 792, 311, 18, 1283, 350, 952, 308, 346, 522, 269, 442, 91, 656, 385, 316, 88, 481, 869, 370, 503, 269, 331, 16, 332, 836, 16, 362, 16, 369, 477, 952, 267, 792, 311, 269, 1307, 633, 385, 316, 88, 420, 203, 203, 56, 83, 558, 16, 368, 1659, 267, 792, 311, 269, 442, 91, 268, 267, 1842, 307, 349, 362, 376, 385, 316, 88, 18, 413, 286, 390, 1035, 74, 475, 1389, 88, 372, 452, 271]\n",
      "['<|BOS|>', 'Ä One', 'Ä day', ',', 'Ä a', 'Ä little', 'Ä girl', 'Ä named', 'Ä Lily', 'Ä found', 'Ä a', 'Ä need', 'le', 'Ä in', 'Ä her', 'Ä room', '.', 'Ä She', 'Ä knew', 'Ä it', 'Ä was', 'Ä dif', 'f', 'ic', 'ul', 't', 'Ä to', 'Ä play', 'Ä with', 'Ä it', 'Ä because', 'Ä it', 'Ä was', 'Ä sh', 'ar', 'p', '.', 'Ä Lily', 'Ä wanted', 'Ä to', 'Ä share', 'Ä the', 'Ä need', 'le', 'Ä with', 'Ä her', 'Ä mom', ',', 'Ä so', 'Ä she', 'Ä could', 'Ä se', 'w', 'Ä a', 'Ä butt', 'on', 'Ä on', 'Ä her', 'Ä sh', 'ir', 't', '.', 'ÄŠ', 'ÄŠ', 'Lily', 'Ä went', 'Ä to', 'Ä her', 'Ä mom', 'Ä and', 'Ä said', ',', 'Ä \"', 'Mom', ',', 'Ä I', 'Ä found', 'Ä this', 'Ä need', 'le', '.', 'Ä Can', 'Ä you', 'Ä share', 'Ä it', 'Ä with', 'Ä me', 'Ä and', 'Ä se', 'w', 'Ä my', 'Ä sh', 'ir', 't', '?\"', 'Ä Her', 'Ä mom', 'Ä smiled', 'Ä and', 'Ä said', ',', 'Ä \"', 'Yes', ',', 'Ä Lily', ',', 'Ä we', 'Ä can', 'Ä share', 'Ä the', 'Ä need', 'le', 'Ä and', 'Ä fix', 'Ä your', 'Ä sh', 'ir', 't', '.\"', 'ÄŠ', 'ÄŠ', 'T', 'o', 'gether', ',', 'Ä they', 'Ä shared', 'Ä the', 'Ä need', 'le', 'Ä and', 'Ä se', 'w', 'ed', 'Ä the', 'Ä butt', 'on', 'Ä on', 'Ä Lily', \"'s\", 'Ä sh', 'ir', 't', '.', 'Ä It', 'Ä was', 'Ä not', 'Ä dif', 'f', 'ic', 'ul', 't', 'Ä for', 'Ä them', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "# We can use the new tokenizer to tokenizer text via the object's __call__ method, like this:\n",
    "\n",
    "input_ids = tokenizer(sample_text)['input_ids']\n",
    "print(input_ids)\n",
    "\n",
    "# We can convert these to their symbolic representations like this.\n",
    "# Note the 'Ä ' symbols. The tokenizer has folded spaces into the tokens, where this symbol represents the space.\n",
    "# A consequence of this encoding is that tokens may exist for the same word, both with and without a space.\n",
    "# For example, \"she\" and \" she\" would be represented as seperate tokens.\n",
    "for ids in [input_ids]:\n",
    "    print(tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d727c550-95c6-4089-be9d-76e336ccb6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<|BOS|> One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b\"\n"
     ]
    }
   ],
   "source": [
    "# We can decode token ids with decode() or batch_decode()\n",
    "decoded_tokens = tokenizer.batch_decode([input_ids], skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
    "for s in decoded_tokens:\n",
    "    print(f\"\\\"{s}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcde6ca-ffa3-4061-9c2d-44b135139292",
   "metadata": {},
   "source": [
    "---\n",
    "We can dump the vocabulary of the tokenizer. The first part will contain our special tokens and the ASCII character-set. After this, the number of characters in each tokens grows, with the largest tokens at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "719c5403-459b-4e4f-97c0-e5aca5272631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0: <|PAD|>' '1: <|MASK|>' '2: <|BOS|>' '3: <|EOS|>' '4: <|UNK|>' '5: !' '6: \"' '7: #' '8: $' '9: %' '10: &' '11: '' '12: (' '13: )' '14: *' '15: +' '16: ,' '17: -' '18: .' '19: /' '20: 0' '21: 1' '22: 2' '23: 3' '24: 4' '25: 5' '26: 6' '27: 7' '28: 8' '29: 9' '30: :' '31: ;' '32: <' '33: =' '34: >' '35: ?' '36: @' '37: A' '38: B' '39: C' '40: D' '41: E' '42: F' '43: G' '44: H' '45: I' '46: J' '47: K' '48: L' '49: M' '50: N' '51: O' '52: P' '53: Q' '54: R' '55: S' '56: T' '57: U' '58: V' '59: W' '60: X' '61: Y' '62: Z' '63: [' \n",
      "\n",
      "'1936: ek' '1937:  shap' '1938:  shook' '1939:  exploring' '1940:  moved' '1941:  purp' '1942:  year' '1943: aughty' '1944:  nearby' '1945:  naughty' '1946:  star' '1947:  soup' '1948:  shop' '1949:  wise' '1950:  stars' '1951:  owl' '1952:  bring' '1953: fused' '1954:  jar' '1955: bow' '1956: Do' '1957: ocked' '1958:  inv' '1959:  exp' '1960:  whe' '1961: yard' '1962:  caught' '1963:  su' '1964: ward' '1965:  Emma' '1966:  backyard' '1967:  seemed' '1968: ail' '1969:  es' '1970:  relie' '1971:  dropped' '1972:  ph' '1973:  rocks' '1974: aughter' '1975: llo' '1976:  3' '1977: ush' '1978:  drink' '1979:  bucket' '1980:  des' '1981:  ca' '1982: adow' '1983: ces' '1984:  purple' '1985:  swam' '1986:  farm' '1987:  resp' '1988:  taking' '1989:  Mama' '1990:  mail' '1991:  orange' '1992:  though' '1993: ions' '1994:  shapes' '1995:  past' '1996: col' '1997:  cow' '1998:  stu' '1999:  Thank' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump a range of the tokenizer's vocabulary\n",
    "def show_vocabulary(tokenizer, token_range):\n",
    "    for i, token in zip(token_range, tokenizer.batch_decode([i for i in token_range], skip_special_tokens=False)):\n",
    "        print(f\"'{i}: {token}'\", end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Show the first and last 64 tokens.\n",
    "show_vocabulary(tokenizer, range(64))\n",
    "show_vocabulary(tokenizer, range(tokenizer.vocab_size - 64, tokenizer.vocab_size))\n",
    "\n",
    "# Show full vocab.\n",
    "#show_vocabulary(tokenizer, range(tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c4c1b-3503-4989-9e7e-626442ab199e",
   "metadata": {},
   "source": [
    "### Save tokenizer\n",
    "We can save the tokenizer, which will allow us to skip recreating it from scratch next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e8841ea-48a8-4b3d-9790-fef1c37ca501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/dinalt/ai_assets/models/tiny/tokenizer_config.json',\n",
       " '/home/dinalt/ai_assets/models/tiny/special_tokens_map.json',\n",
       " '/home/dinalt/ai_assets/models/tiny/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(config.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200d7c9-d8e9-4545-a79f-b3f07ddde855",
   "metadata": {},
   "source": [
    "### Load tokenizer\n",
    "We can load our saved tokenizer -- or the tokenizer from any Huggingface model -- with this interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26098951-3b92-493e-9569-06b95cbb37ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='/home/dinalt/ai_assets/models/tiny', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer from a local path -- or from a Huggingface model name.\n",
    "# Rather than starting from scratch, you could replace 'model_path' with the path of an existing model and use its tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9c404-b814-476c-80a0-8517bb9a9e10",
   "metadata": {},
   "source": [
    "## Quick Build\n",
    "This function is equivalent to the tutorial.  \n",
    "[source](../tutorial_code/tokenizer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d040d30-87f0-4e11-b9db-d654e12b6441",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = train_bpe_tokenizer(config, dataset['train'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
