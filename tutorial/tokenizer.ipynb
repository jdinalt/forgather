{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc054d8-2467-451f-8161-d80ba96fd4bb",
   "metadata": {},
   "source": [
    "# Training a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb9655-0848-448a-a500-28196416c634",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "[forgather_config.yaml](forgather_config.yaml)  \n",
    "[forgather_demo/paths.yaml](forgather_demo/paths.yaml)  \n",
    "[../templates/common/tokenizers/causal_bpe.yaml](../templates/common/tokenizers/causal_bpe.yaml)  \n",
    "[../templates/common/tokenizers/tiny_2k_bpe.yaml](../templates/common/tokenizers/tiny_2k_bpe.yaml)  \n",
    "[../templates/common/tokenizers/whitelist.yaml](../templates/common/tokenizers/whitelist.yaml)  \n",
    "\n",
    "### See Also\n",
    "[Configuration Loader](forgather.ipynb)  \n",
    "[TokenizerTrainer implementation](../aiws/tokenizer_trainer.py)  \n",
    "[forgather.config](../forgather/config.py)  \n",
    "\n",
    "### Dataset\n",
    "\n",
    "dataset-id: 'roneneldan/TinyStories'\n",
    "\n",
    "Huggingface dataset link:  \n",
    "[https://huggingface.co/datasets/roneneldan/TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories)  \n",
    "\n",
    "Dataset Paper:  \n",
    "[https://arxiv.org/abs/2305.07759](https://arxiv.org/abs/2305.07759)\n",
    "\n",
    "### Tokenizers  \n",
    "Rather than working with the raw ASCII/Unicode from the dataset, we will be \"tokenizing\" the data. A tokenizer is a statisttical model which aggregates individual characters into sub-word, where the most frequent strings of characters are replaced by unique symbols.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Large_language_model#Probabilistic_tokenization\n",
    "\n",
    "For this tutorial, we will be created a Byte Pair Encoding (BPE) tokenizer, which starts with all of the symbols from the ASCII character set, then creates tokens for the most common pairs of ASCII characters. These pairs are further aggregated into larger symbols and the process repeats until a set of symbols matching the target vocabulary size has been created.\n",
    "\n",
    "By starting with the ASCII character set, it is possible to represent any combination of letters, including those which were not observed when the tokenizer was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53b418e-44b4-4ffd-90c4-b819e6341998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_templates: 'forgather_demo'\n",
      "templates: '../templates'\n",
      "tokenizer_dir: '../tokenizers'\n",
      "datasets_dir: '../datasets'\n",
      "assets_dir: '..'\n",
      "search_paths:\n",
      "  - 'forgather_demo'\n",
      "  - '../templates'\n",
      "  - '../model_zoo'\n",
      "whitelist_path: 'forgather_demo/whitelist.yaml'\n",
      "model_src_dir: '../model_zoo'\n",
      "script_dir: '../scripts'\n",
      "train_script_path: '../scripts/train_script.py'\n",
      "models_dir: 'forgather_demo/output_models'\n",
      "dataset_id: 'roneneldan/TinyStories'\n",
      "tokenizer_def: '../templates/common/tokenizers/tiny_2k_bpe.yaml'\n",
      "tokenizer_path: '../tokenizers/tiny_stories_2k'\n",
      "tokenizers_whitelist: '../templates/common/tokenizers/whitelist.yaml'\n"
     ]
    }
   ],
   "source": [
    "# Import some things we will need and get the meta-config for the tutorials.\n",
    "import sys\n",
    "if '..' not in sys.path: sys.path.insert(0, '..')\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "import tokenizers\n",
    "\n",
    "from aiws.dotdict import DotDict\n",
    "from aiws.tokenizer_trainer import TokenizerTrainer\n",
    "from forgather.config import(\n",
    "    pconfig,\n",
    "    preprocess_config,\n",
    "    load_config,\n",
    "    materialize_config,\n",
    "    load_whitelist_as_set,\n",
    ")\n",
    "\n",
    "# Load meta-configuration\n",
    "metacfg = DotDict(load_config('forgather_config.yaml').config)\n",
    "pconfig(metacfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3526a-fa96-4b0e-a618-d7934325e6bd",
   "metadata": {},
   "source": [
    "## Tokenizer Definition Template\n",
    "\n",
    "We will use a tokenizer definition template to make things easier.  \n",
    "[../templates/common/tokenizers/causal_bpe.yaml](../templates/common/tokenizers/causal_bpe.yaml)  \n",
    "\n",
    "This templates takes four arguments:\n",
    "- dataset_id: The dataset to train the tokenizer on.\n",
    "- dataset_split: Which split to train on -- datasets are often divided into multiple splits, like 'train' and 'validate'\n",
    "- model_max_length: The maximum sequence length of the model the tokenizer will be used with. This is meta-data, which is stored in the tokenizer.\n",
    "- vocal_size: The number of unique tokens in the tokenizer, excluding 'special tokens'\n",
    "\n",
    "We will first preprocess the template with Jinja2 and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad0605c-0a0b-4682-9c78-9775ec5fe921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1: # BPE Tokenizer Definition for Causal Model\n",
      "     2: # 2024-07-08 08:45:11\n",
      "     3: # dataset_id: 'roneneldan/TinyStories'\n",
      "     4: # dataset_split: 'train\n",
      "     5: # model_max_length: '2048'\n",
      "     6: # vocab_size: '2000'\n",
      "     7: \n",
      "     8: special_tokens_map: &special_tokens_map\n",
      "     9:     bos: \"<|BOS|>\" # Beginning of Sequence; the first token in a sequence\n",
      "    10:     pad: \"<|PAD|>\" # Padding, used to pad out samples in a batch.\n",
      "    11:     eos: \"<|EOS|>\" # End of Sequence; typically is used to stop generation.\n",
      "    12:     unk: \"<|UNK|>\" # Unknown; used when a symbol can't be represented.\n",
      "    13: \n",
      "    14: # TokenizerTrainer args\n",
      "    15: # aiws.tokenizer_trainer.TokenizerTrainer\n",
      "    16: trainer_args: &trainer_args\n",
      "    17:     # https://huggingface.co/docs/tokenizers/api/trainers#tokenizers.trainers.BpeTrainer\n",
      "    18:     model: !callable:tokenizers:models.BPE\n",
      "    19:         cache_capacity: 16\n",
      "    20:         unk_token: \"<|UNK|>\"\n",
      "    21:         byte_fallback: True\n",
      "    22: \n",
      "    23:     # https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFC\n",
      "    24:     normalizer: !callable:tokenizers:normalizers.NFC []\n",
      "    25: \n",
      "    26:     # https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel\n",
      "    27:     pre_tokenizer: !callable:tokenizers:pre_tokenizers.ByteLevel []\n",
      "    28: \n",
      "    29:     # https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel\n",
      "    30:     decoder: !callable:tokenizers:decoders.ByteLevel []\n",
      "    31: \n",
      "    32:     # Automatically add bos token to sequence start\n",
      "    33:     # https://huggingface.co/docs/tokenizers/api/post-processors#tokenizers.processors.TemplateProcessing\n",
      "    34:     post_processor: !callable:tokenizers:processors.TemplateProcessing\n",
      "    35:         single: \"<bos> $A\"\n",
      "    36:         special_tokens: [ !tuple [ \"<bos>\", 0 ] ]\n",
      "    37: \n",
      "    38:     # https://huggingface.co/docs/tokenizers/api/trainers#tokenizers.trainers.BpeTrainer\n",
      "    39:     trainer: !callable:tokenizers.trainers:BpeTrainer\n",
      "    40:         vocab_size: 2000\n",
      "    41:         # Start the vocabulary with tokens for all 8-bit bytes.\n",
      "    42:         initial_alphabet: !callable:tokenizers:pre_tokenizers.ByteLevel.alphabet []\n",
      "    43:         # Convert special tokes map to list\n",
      "    44:         special_tokens: !callable:forgather.construct:values [ *special_tokens_map ]\n",
      "    45:         # The TokenizerTrainer class handles this.\n",
      "    46:         show_progress: False\n",
      "    47: \n",
      "    48:     # The dataset to use 'roneneldan/TinyStories'\n",
      "    49:     dataset: !callable:forgather.construct:get_item\n",
      "    50:         - !callable:datasets:load_dataset [ \"roneneldan/TinyStories\" ]\n",
      "    51:         - \"train\"\n",
      "    52: \n",
      "    53: # Args to transformers.PreTrainedTokenizerFast()\n",
      "    54: # https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerFast\n",
      "    55: pretrained_tokenizer_fast_args:\n",
      "    56:     bos_token: \"<|BOS|>\"\n",
      "    57:     eos_token: \"<|EOS|>\"\n",
      "    58:     unk_token: \"<|UNK|>\"\n",
      "    59:     pad_token: \"<|PAD|>\"\n",
      "    60:     return_special_tokens_mask: False\n",
      "    61:     model_max_length: 2048\n",
      "    62:     padding_side: \"right\"\n",
      "    63:     truncation_side: \"right\"\n",
      "    64:     # chat_template: str; a Jinja template string\n",
      "    65:     # additional_special_tokens: List[str]\n",
      "    66:     # clean_up_tokenization_spaces: True\n",
      "    67:     # split_special_tokens: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer_template_path = os.path.join(metacfg.templates, 'common', 'tokenizers', 'causal_bpe.yaml')\n",
    "pp_config = preprocess_config(\n",
    "    bpe_tokenizer_template_path,\n",
    "    dataset_id = 'roneneldan/TinyStories',\n",
    "    dataset_split = 'train',\n",
    "    model_max_length = 2048,\n",
    "    vocab_size = 2000,\n",
    ")\n",
    "pconfig(pp_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c338a8e-46e3-4cb5-aac3-7b2ac07047b9",
   "metadata": {},
   "source": [
    "We are manually injecting the template arguments above, but there is a pre-defined template for this configuration, as this tokenizer is used for the other tutotials.\n",
    "\n",
    "As for the base template, you can experiment with modifying it -- better yet, sub-class it using [template inheretance](https://jinja.palletsprojects.com/en/3.1.x/templates/#template-inheritance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6415797a-3269-4f82-a78c-e9a5c0a66023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- set dataset_id = 'roneneldan/TinyStories'\n",
      "-- set model_max_length = 2048\n",
      "-- set vocab_size = 2000\n",
      "-- set dataset_split = 'train'\n",
      "\n",
      "-- include 'common/tokenizers/causal_bpe.yaml'\n",
      "tokenizer_name: \"tiny_stories_2k\"\n"
     ]
    }
   ],
   "source": [
    "with open(metacfg.tokenizer_def) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03c32b-892c-4d6e-91d9-685c92e643f4",
   "metadata": {},
   "source": [
    "### Materialize the Configuration\n",
    "\n",
    "This will instantiate concrete objects, corresponding to the config definition.\n",
    "\n",
    "As we have already preprocessed the config, we will use that, but we could have skipped that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c618594f-9b35-40d6-b4b5-d0bbf9a26974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'special_tokens_map': {'bos': '<|BOS|>', 'pad': '<|PAD|>', 'eos': '<|EOS|>', 'unk': '<|UNK|>'}, 'trainer_args': {'model': Latent('tokenizers:models.BPE', *[], **{'cache_capacity': 16, 'unk_token': '<|UNK|>', 'byte_fallback': True}), 'normalizer': Latent('tokenizers:normalizers.NFC', *[], **{}), 'pre_tokenizer': Latent('tokenizers:pre_tokenizers.ByteLevel', *[], **{}), 'decoder': Latent('tokenizers:decoders.ByteLevel', *[], **{}), 'post_processor': Latent('tokenizers:processors.TemplateProcessing', *[], **{'single': '<bos> $A', 'special_tokens': [('<bos>', 0)]}), 'trainer': Latent('tokenizers.trainers:BpeTrainer', *[], **{'vocab_size': 2000, 'initial_alphabet': Latent('tokenizers:pre_tokenizers.ByteLevel.alphabet', *[], **{}), 'special_tokens': Latent('forgather.construct:values', *[{'bos': '<|BOS|>', 'pad': '<|PAD|>', 'eos': '<|EOS|>', 'unk': '<|UNK|>'}], **{}), 'show_progress': False}), 'dataset': Latent('forgather.construct:get_item', *[Latent('datasets:load_dataset', *['roneneldan/TinyStories'], **{}), 'train'], **{})}, 'pretrained_tokenizer_fast_args': {'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|PAD|>', 'return_special_tokens_mask': False, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right'}}\n"
     ]
    }
   ],
   "source": [
    "latent_config = load_config(pp_config, preprocess=False, load_method=\"from_string\")\n",
    "print(latent_config.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28c0a469-f3d8-4d4a-8bb9-80028690008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_tokens_map:\n",
      "  bos: '<|BOS|>'\n",
      "  pad: '<|PAD|>'\n",
      "  eos: '<|EOS|>'\n",
      "  unk: '<|UNK|>'\n",
      "trainer_args:\n",
      "  model: <tokenizers.models.BPE object at 0x7fc6dd4777d0>\n",
      "  normalizer: <tokenizers.normalizers.NFC object at 0x7fc6952eb7b0>\n",
      "  pre_tokenizer: <tokenizers.pre_tokenizers.ByteLevel object at 0x7fc6952eb430>\n",
      "  decoder: <tokenizers.decoders.ByteLevel object at 0x7fc6dd3dfdb0>\n",
      "  post_processor: <tokenizers.processors.TemplateProcessing object at 0x7fc6dd3ddec0>\n",
      "  trainer: <tokenizers.trainers.BpeTrainer object at 0x7fc6953dcc50>\n",
      "  dataset:\n",
      "    Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "pretrained_tokenizer_fast_args:\n",
      "  bos_token: '<|BOS|>'\n",
      "  eos_token: '<|EOS|>'\n",
      "  unk_token: '<|UNK|>'\n",
      "  pad_token: '<|PAD|>'\n",
      "  return_special_tokens_mask: False\n",
      "  model_max_length: 2048\n",
      "  padding_side: 'right'\n",
      "  truncation_side: 'right'\n"
     ]
    }
   ],
   "source": [
    "config_out = materialize_config(\n",
    "    pp_config,\n",
    "    # We already did this\n",
    "    preprocess=False,\n",
    "    whitelist=load_whitelist_as_set(metacfg.tokenizers_whitelist).config,\n",
    "    search_path=metacfg.search_paths,\n",
    "    # The input is text, not a file path.\n",
    "    load_method=\"from_string\"\n",
    ")\n",
    "\n",
    "# Assing to dot-dict for easy access\n",
    "config = DotDict(config_out.config)\n",
    "pconfig(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0c23e-2311-4dd4-9102-013e5f777bd1",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Implementation: [datasets.py](../tutorial_code/datasets.py)  \n",
    "See Also: [dataset.ipynb](./dataset.ipynb)\n",
    "\n",
    "For experimentation, we will want a bit of sample text to work with. \n",
    "Grab the first example in the dataset and show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fa226a9-4047-48c3-97c1-6b8cc3b9b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
     ]
    }
   ],
   "source": [
    "sample_text = config.trainer_args['dataset'][0]['text']\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd060cfe-460e-4500-a93d-44e9d5b3738d",
   "metadata": {},
   "source": [
    "### Special Tokens Map\n",
    "\n",
    "The special characters map assigns consistent names to what are otherwise configurable representations for the special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aea5e40-8301-4e60-b178-1e514bd25757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos: '<|BOS|>'\n",
      "pad: '<|PAD|>'\n",
      "eos: '<|EOS|>'\n",
      "unk: '<|UNK|>'\n"
     ]
    }
   ],
   "source": [
    "pconfig(config.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604db36-e5ec-4a7e-b175-e2ed6f4f64d4",
   "metadata": {},
   "source": [
    "### Pre-tokenizer\n",
    "\n",
    "The pre-toknizer breaks the input text into sub-strings via a regular expression. For example, a simple pre-tokenizer could split the input on spaces and punctuation.\n",
    "\n",
    "We will be using the \"ByteLevel\" pre-tokenizer, which uses a GPT-2 specfic regex for splitting the words and replaces spaces with the 'Ġ' character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63edf63e-c9f2-4def-8fa5-8c0dd56db69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ĠOne' 'Ġday' ',' 'Ġa' 'Ġlittle' 'Ġgirl' 'Ġnamed' 'ĠLily' 'Ġfound' 'Ġa' 'Ġneedle' 'Ġin' 'Ġher' 'Ġroom' '.' 'ĠShe' 'Ġknew' 'Ġit' 'Ġwas' 'Ġdifficult' 'Ġto' 'Ġplay' 'Ġwith' 'Ġit' 'Ġbecause' 'Ġit' 'Ġwas' 'Ġsharp' '.' 'ĠLily' 'Ġwanted' 'Ġto' 'Ġshare' 'Ġthe' 'Ġneedle' 'Ġwith' 'Ġher' 'Ġmom' ',' 'Ġso' 'Ġshe' 'Ġcould' 'Ġsew' 'Ġa' 'Ġbutton' 'Ġon' 'Ġher' 'Ġshirt' '.' 'Ċ' 'Ċ' 'Lily' 'Ġwent' 'Ġto' 'Ġher' 'Ġmom' 'Ġand' 'Ġsaid' ',' 'Ġ\"' 'Mom' ',' 'ĠI' 'Ġfound' 'Ġthis' 'Ġneedle' '.' 'ĠCan' 'Ġyou' 'Ġshare' 'Ġit' 'Ġwith' 'Ġme' 'Ġand' 'Ġsew' 'Ġmy' 'Ġshirt' '?\"' 'ĠHer' 'Ġmom' 'Ġsmiled' 'Ġand' 'Ġsaid' ',' 'Ġ\"' 'Yes' ',' 'ĠLily' ',' 'Ġwe' 'Ġcan' 'Ġshare' 'Ġthe' 'Ġneedle' 'Ġand' 'Ġfix' 'Ġyour' 'Ġshirt' '.\"' 'Ċ' 'Ċ' 'Together' ',' 'Ġthey' 'Ġshared' 'Ġthe' 'Ġneedle' 'Ġand' 'Ġsewed' 'Ġthe' 'Ġbutton' 'Ġon' 'ĠLily' ''s' 'Ġshirt' '.' 'ĠIt' 'Ġwas' 'Ġnot' 'Ġdifficult' 'Ġfor' 'Ġthem' 'Ġbecause' 'Ġthey' 'Ġwere' 'Ġsharing' 'Ġand' 'Ġhelping' 'Ġeach' 'Ġother' '.' 'ĠAfter' 'Ġthey' 'Ġfinished' ',' 'ĠLily' 'Ġthanked' 'Ġher' 'Ġmom' 'Ġfor' 'Ġsharing' 'Ġthe' 'Ġneedle' 'Ġand' 'Ġfixing' 'Ġher' 'Ġshirt' '.' 'ĠThey' 'Ġboth' 'Ġfelt' 'Ġhappy' 'Ġbecause' 'Ġthey' 'Ġhad' 'Ġshared' 'Ġand' 'Ġworked' 'Ġtogether' '.' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_tokenizer = config.trainer_args['pre_tokenizer']\n",
    "\n",
    "def test_pretokenizer(pre_tokenizer, sample_text):\n",
    "    tokens = pre_tokenizer.pre_tokenize_str(sample_text)\n",
    "    for token in tokens:\n",
    "        print(f\"'{token[0]}'\", end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "test_pretokenizer(pre_tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a12f1c-1ed6-4105-8644-ef5b94c277a1",
   "metadata": {},
   "source": [
    "TODO: Explain the other tokenizer modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00bfa14-184d-4b5f-85a6-50a25c2fa304",
   "metadata": {},
   "source": [
    "## Train the Tokenizer\n",
    "\n",
    "When we 'train' the tokenizer, it builds a statistical module of the vocabulary from the dataset. In the case of the BPE tokenizer, it starts with a vocabulary conisting of first 256 ASCII values (plus special tokens) and starts by finding the most common pairs of symbols; these become the next tokens in the set.\n",
    "\n",
    "The original base alphabet and the pairs can then be combined into pairs to create larger strings of characters. The process is repeated until the target vocabulary size is reached.\n",
    "\n",
    "With the same configuration, it will always produce the same set of tokens, but a different dataset, with a different distribution of symbols would produce a different optimal set of tokens.\n",
    "\n",
    "The process is fairly CPU intensive and can take a bit of time.\n",
    "\n",
    "We will be using my [TokenizerTrainer implementation](../aiws/tokenizer_trainer.py), as this makes things easier, but feel free to look at the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2526ab23-a17e-4b7f-bb73-4ed2273f61be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Tokenizer ****\n",
      "total_samples: 2119719\n",
      "batch_size: 1000\n",
      "steps: 2119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c79681cfa1c471faed11cc6d444d134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Completed ****\n",
      "runtime: 26.4\n",
      "samples_per_second: 80292.39\n"
     ]
    }
   ],
   "source": [
    "trainer = TokenizerTrainer(**config.trainer_args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673bb982-4382-4183-98c5-ba63ded013dd",
   "metadata": {},
   "source": [
    "The base \"[Tokenizers](https://huggingface.co/docs/tokenizers/index)\" API is relatively low level. We would like to wrap the tokenizer object in the higher-level [Transformers Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer) class.\n",
    "\n",
    "The TokenizerTrainer can do this for uss, but we will need some additional arguments. The configuration has already generated these, so we can go ahead and wrap the tokenizer.\n",
    "\n",
    "If you need to modify the wrapped tokenizer, it can be found as the 'backend_tokenizer' attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15bf550-e88e-4db9-adc0-2dca8dfffcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token: '<|BOS|>'\n",
      "eos_token: '<|EOS|>'\n",
      "unk_token: '<|UNK|>'\n",
      "pad_token: '<|PAD|>'\n",
      "return_special_tokens_mask: False\n",
      "model_max_length: 2048\n",
      "padding_side: 'right'\n",
      "truncation_side: 'right'\n",
      "****************************************\n",
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|PAD|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pconfig(config.pretrained_tokenizer_fast_args)\n",
    "print('*' * 40)\n",
    "tokenizer = trainer.as_pretrained_tokenizer_fast(**config.pretrained_tokenizer_fast_args)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5ab8c-22a7-40f4-91c8-88aa40945363",
   "metadata": {},
   "source": [
    "### Test the tokenizer\n",
    "\n",
    "We can use the new tokenizer to tokenizer text via the object's __call__ method, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88eab7f4-07fa-4b0e-af00-b97b19b745d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 490, 359, 15, 262, 402, 449, 504, 361, 597, 262, 791, 310, 319, 312, 762, 17, 316, 708, 307, 285, 1034, 73, 474, 1388, 87, 269, 364, 345, 307, 790, 307, 285, 384, 290, 83, 17, 361, 447, 269, 951, 266, 791, 310, 345, 312, 369, 15, 353, 341, 463, 441, 90, 262, 1841, 306, 348, 312, 384, 315, 87, 17, 202, 202, 600, 472, 269, 312, 369, 268, 330, 15, 331, 780, 15, 338, 597, 746, 791, 310, 17, 1282, 349, 951, 307, 345, 521, 268, 441, 90, 655, 384, 315, 87, 480, 868, 369, 502, 268, 330, 15, 331, 835, 15, 361, 15, 368, 476, 951, 266, 791, 310, 268, 1306, 632, 384, 315, 87, 419, 202, 202, 55, 82, 557, 15, 367, 1658, 266, 791, 310, 268, 441, 90, 267, 266, 1841, 306, 348, 361, 375, 384, 315, 87, 17, 412, 285, 389, 1034, 73, 474, 1388, 87, 371, 451, 790, 367, 432, 384, 1397, 268, 1766, 760, 575, 17, 1456, 367, 1446, 15, 361, 861, 312, 369, 371, 384, 1397, 266, 791, 310, 268, 1306, 292, 312, 384, 315, 87, 17, 323, 900, 516, 408, 790, 367, 365, 1658, 268, 1371, 569, 17]\n",
      "['<|BOS|>', 'ĠOne', 'Ġday', ',', 'Ġa', 'Ġlittle', 'Ġgirl', 'Ġnamed', 'ĠLily', 'Ġfound', 'Ġa', 'Ġneed', 'le', 'Ġin', 'Ġher', 'Ġroom', '.', 'ĠShe', 'Ġknew', 'Ġit', 'Ġwas', 'Ġdif', 'f', 'ic', 'ul', 't', 'Ġto', 'Ġplay', 'Ġwith', 'Ġit', 'Ġbecause', 'Ġit', 'Ġwas', 'Ġsh', 'ar', 'p', '.', 'ĠLily', 'Ġwanted', 'Ġto', 'Ġshare', 'Ġthe', 'Ġneed', 'le', 'Ġwith', 'Ġher', 'Ġmom', ',', 'Ġso', 'Ġshe', 'Ġcould', 'Ġse', 'w', 'Ġa', 'Ġbutt', 'on', 'Ġon', 'Ġher', 'Ġsh', 'ir', 't', '.', 'Ċ', 'Ċ', 'Lily', 'Ġwent', 'Ġto', 'Ġher', 'Ġmom', 'Ġand', 'Ġsaid', ',', 'Ġ\"', 'Mom', ',', 'ĠI', 'Ġfound', 'Ġthis', 'Ġneed', 'le', '.', 'ĠCan', 'Ġyou', 'Ġshare', 'Ġit', 'Ġwith', 'Ġme', 'Ġand', 'Ġse', 'w', 'Ġmy', 'Ġsh', 'ir', 't', '?\"', 'ĠHer', 'Ġmom', 'Ġsmiled', 'Ġand', 'Ġsaid', ',', 'Ġ\"', 'Yes', ',', 'ĠLily', ',', 'Ġwe', 'Ġcan', 'Ġshare', 'Ġthe', 'Ġneed', 'le', 'Ġand', 'Ġfix', 'Ġyour', 'Ġsh', 'ir', 't', '.\"', 'Ċ', 'Ċ', 'T', 'o', 'gether', ',', 'Ġthey', 'Ġshared', 'Ġthe', 'Ġneed', 'le', 'Ġand', 'Ġse', 'w', 'ed', 'Ġthe', 'Ġbutt', 'on', 'Ġon', 'ĠLily', \"'s\", 'Ġsh', 'ir', 't', '.', 'ĠIt', 'Ġwas', 'Ġnot', 'Ġdif', 'f', 'ic', 'ul', 't', 'Ġfor', 'Ġthem', 'Ġbecause', 'Ġthey', 'Ġwere', 'Ġsh', 'aring', 'Ġand', 'Ġhelping', 'Ġeach', 'Ġother', '.', 'ĠAfter', 'Ġthey', 'Ġfinished', ',', 'ĠLily', 'Ġthanked', 'Ġher', 'Ġmom', 'Ġfor', 'Ġsh', 'aring', 'Ġthe', 'Ġneed', 'le', 'Ġand', 'Ġfix', 'ing', 'Ġher', 'Ġsh', 'ir', 't', '.', 'ĠThey', 'Ġboth', 'Ġfelt', 'Ġhappy', 'Ġbecause', 'Ġthey', 'Ġhad', 'Ġshared', 'Ġand', 'Ġworked', 'Ġtogether', '.']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(sample_text)['input_ids']\n",
    "print(input_ids)\n",
    "\n",
    "# We can convert these to their symbolic representations like this.\n",
    "# Note the 'Ġ' symbols. The tokenizer has folded spaces into the tokens, where this symbol represents the space.\n",
    "# A consequence of this encoding is that tokens may exist for the same word, both with and without a space.\n",
    "# For example, \"she\" and \" she\" would be represented as seperate tokens.\n",
    "for ids in [input_ids]:\n",
    "    print(tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d727c550-95c6-4089-be9d-76e336ccb6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<|BOS|> One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\"\n"
     ]
    }
   ],
   "source": [
    "# We can decode token ids with decode() or batch_decode()\n",
    "decoded_tokens = tokenizer.batch_decode([input_ids], skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
    "for s in decoded_tokens:\n",
    "    print(f\"\\\"{s}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcde6ca-ffa3-4061-9c2d-44b135139292",
   "metadata": {},
   "source": [
    "---\n",
    "We can dump the vocabulary of the tokenizer. The first part will contain our special tokens and the ASCII character-set. After this, the number of characters in each tokens grows, with the largest tokens at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "719c5403-459b-4e4f-97c0-e5aca5272631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0: <|BOS|>' '1: <|PAD|>' '2: <|EOS|>' '3: <|UNK|>' '4: !' '5: \"' '6: #' '7: $' '8: %' '9: &' '10: '' '11: (' '12: )' '13: *' '14: +' '15: ,' '16: -' '17: .' '18: /' '19: 0' '20: 1' '21: 2' '22: 3' '23: 4' '24: 5' '25: 6' '26: 7' '27: 8' '28: 9' '29: :' '30: ;' '31: <' '32: =' '33: >' '34: ?' '35: @' '36: A' '37: B' '38: C' '39: D' '40: E' '41: F' '42: G' '43: H' '44: I' '45: J' '46: K' '47: L' '48: M' '49: N' '50: O' '51: P' '52: Q' '53: R' '54: S' '55: T' '56: U' '57: V' '58: W' '59: X' '60: Y' '61: Z' '62: [' '63: \\' \n",
      "\n",
      "'1936:  shap' '1937:  shook' '1938:  exploring' '1939:  moved' '1940:  purp' '1941:  year' '1942: aughty' '1943:  nearby' '1944:  naughty' '1945:  star' '1946:  soup' '1947:  shop' '1948:  wise' '1949:  stars' '1950:  owl' '1951:  bring' '1952: fused' '1953:  jar' '1954: bow' '1955: Do' '1956: ocked' '1957:  inv' '1958:  exp' '1959:  whe' '1960: yard' '1961:  caught' '1962:  su' '1963: ward' '1964:  Emma' '1965:  backyard' '1966:  seemed' '1967: ail' '1968:  es' '1969:  relie' '1970:  dropped' '1971:  ph' '1972:  rocks' '1973: aughter' '1974: llo' '1975:  3' '1976: ush' '1977:  drink' '1978:  bucket' '1979:  des' '1980:  ca' '1981: adow' '1982: ces' '1983:  purple' '1984:  swam' '1985:  farm' '1986:  resp' '1987:  taking' '1988:  Mama' '1989:  mail' '1990:  orange' '1991:  though' '1992: ions' '1993:  shapes' '1994:  past' '1995: col' '1996:  cow' '1997:  stu' '1998:  Thank' '1999:  woke' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump a range of the tokenizer's vocabulary\n",
    "def show_vocabulary(tokenizer, token_range):\n",
    "    for i, token in zip(token_range, tokenizer.batch_decode([i for i in token_range], skip_special_tokens=False)):\n",
    "        print(f\"'{i}: {token}'\", end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Show the first and last 64 tokens.\n",
    "show_vocabulary(tokenizer, range(64))\n",
    "show_vocabulary(tokenizer, range(tokenizer.vocab_size - 64, tokenizer.vocab_size))\n",
    "\n",
    "# Show full vocab.\n",
    "#show_vocabulary(tokenizer, range(tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c4c1b-3503-4989-9e7e-626442ab199e",
   "metadata": {},
   "source": [
    "### Save tokenizer\n",
    "We can save the tokenizer, which will allow us to skip recreating it from scratch next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e8841ea-48a8-4b3d-9790-fef1c37ca501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../tokenizers/tiny_stories_2k/tokenizer_config.json',\n",
       " '../tokenizers/tiny_stories_2k/special_tokens_map.json',\n",
       " '../tokenizers/tiny_stories_2k/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(metacfg.tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200d7c9-d8e9-4545-a79f-b3f07ddde855",
   "metadata": {},
   "source": [
    "### Load tokenizer\n",
    "We can load our saved tokenizer -- or the tokenizer from any Huggingface model -- with this interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26098951-3b92-493e-9569-06b95cbb37ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='../tokenizers/tiny_stories_2k', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|PAD|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer from a local path -- or from a Huggingface model name.\n",
    "# Rather than starting from scratch, you could replace 'model_path' with the path of an existing model and use its tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(metacfg.tokenizer_path)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9c404-b814-476c-80a0-8517bb9a9e10",
   "metadata": {},
   "source": [
    "## Quick Build\n",
    "This function is roughlty equivalent to the tutorial.\n",
    "\n",
    "It checks if the tokenizer has been built and skips if it already exists. Override can be set with 'force=True'\n",
    "\n",
    "[source](../tutorial_code/tokenizer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d040d30-87f0-4e11-b9db-d654e12b6441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Tokenizer ****\n",
      "total_samples: 2119719\n",
      "batch_size: 1000\n",
      "steps: 2119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05164e6fa9ef4be7a4a394660e4077e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Completed ****\n",
      "runtime: 26.54\n",
      "samples_per_second: 79868.84\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if '..' not in sys.path: sys.path.insert(0, '..')\n",
    "from tutorial_code.tokenizer import make_project_tokenizer\n",
    "make_project_tokenizer(force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355be67-204f-483a-9e2b-118afbb0b239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
