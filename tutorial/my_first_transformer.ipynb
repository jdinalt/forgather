{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bb9655-0848-448a-a500-28196416c634",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Source\n",
    "[aiws.config](../aiws/config.py)  \n",
    "[tutorial_code.datasets](../tutorial_code/datasets.py)  \n",
    "[tutorial_code.tokenizer](../tutorial_code/tokenizer.py)  \n",
    "[tutorial_code.model_utils](../tutorial_code/model_utils.py)  \n",
    "[tutorial_code.train](../tutorial_code/train.py)  \n",
    "[tutorial_code.vanilla_transformer](../tutorial_code/vanilla_transformer.py)  \n",
    "[tutorial_code.inference](../tutorial_code/inference.py)  \n",
    "\n",
    "### See Also\n",
    "[dataset.ipynb](dataset.ipynb)  \n",
    "[tokenizer.ipynb](tokenizer.ipynb)  \n",
    "[simple_lm.ipynb](simple_lm.ipynb)  \n",
    "[train_script.py](train_script.py)  \n",
    "\n",
    "### Config\n",
    "[config.yaml](config/config.yaml)  \n",
    "[paths.yaml](config/paths.yaml)  \n",
    "[dataset.yaml](config/dataset.yaml)  \n",
    "[tokenizer.yaml](config/tokenizer.yaml)  \n",
    "[training.yaml](config/training.yaml)  \n",
    "[model.yaml](config/model.yaml)  \n",
    "\n",
    "There is also a train-script adaptation of this notebook: [train_script.py](train_script.py)  \n",
    "```\n",
    "accelerate launch ./train_script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53b418e-44b4-4ffd-90c4-b819e6341998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_name': 'My First Transformer',\n",
      " 'experiment_description': 'A basic introduction to training a transformer '\n",
      "                           'from scratch.',\n",
      " 'output_dir': './models/my_first_transformer/',\n",
      " 'tokenizer': {'tokenizer_path': '../assets/tokenizers/tiny_stories_2k',\n",
      "               'vocab_size': 2000,\n",
      "               'max_sequence_length': 2048},\n",
      " 'dataset': {'tokenized_dataset_path': '',\n",
      "             'dataset_id': 'roneneldan/TinyStories',\n",
      "             'tokenized_dataset': '../assets/datasets/tiny_stories_tokenized',\n",
      "             'train_select': 0.1,\n",
      "             'validate_select': 0.1},\n",
      " 'model': {'args': {'vocab_size': 2000,\n",
      "                    'max_sequence_length': 2048,\n",
      "                    'hidden_size': 128,\n",
      "                    'dim_feedforward': 512,\n",
      "                    'num_attention_heads': 1,\n",
      "                    'num_hidden_layers': 2}},\n",
      " 'training_args': {'output_dir': './models/my_first_transformer/',\n",
      "                   'overwrite_output_dir': True,\n",
      "                   'per_device_train_batch_size': 16,\n",
      "                   'per_device_eval_batch_size': 16,\n",
      "                   'learning_rate': 0.001,\n",
      "                   'num_train_epochs': 1,\n",
      "                   'eval_steps': 250,\n",
      "                   'logging_steps': 250}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from pprint import pp\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import set_seed\n",
    "\n",
    "from aiws.config import load_config\n",
    "from aiws.dotdict import DotDict\n",
    "from tutorial_code.datasets import tokenize_datasetdict\n",
    "from tutorial_code.tokenizer import train_bpe_tokenizer\n",
    "from tutorial_code.model_utils import print_model_size, test_model_forward\n",
    "from tutorial_code.models.vanilla_transformer import VanillaTransformerConfig, VanillaTransformer\n",
    "\n",
    "config = DotDict(load_config(\"my_first_transformer.yaml\", search_path=[\"../config\", \"config\"]))['config']\n",
    "pp(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5055af-161e-4e1e-b1e4-81c1c9784a7f",
   "metadata": {},
   "source": [
    "### Reload Module\n",
    "Useful, if you make changes to a module and don't want to restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d11a086-e4f6-42b7-8af1-721803aea05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tutorial_code.train' from '/home/dinalt/ai_assets/aiworkshop/tutorial/../tutorial_code/train.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# If the module has not been imported, we first import it.\n",
    "import tutorial_code.train\n",
    "\n",
    "# Trigger a reload of the module.\n",
    "importlib.reload(tutorial_code.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fd50f-339d-4f5d-83e6-3292861edb4f",
   "metadata": {},
   "source": [
    "## Build Assets\n",
    "If you have not built a tokenizer or tokenized the dataset, this will quickly get you up and running.  \n",
    "This may take a moment or three. Be patient!\n",
    "\n",
    "If you would like to dive deeper into the details, see:\n",
    "\n",
    "[dataset.ipynb](./dataset.ipynb)  \n",
    "[tokenizer.ipynb](./tokenizer.ipynb)  \n",
    "\n",
    "These should also be rebuilt if you have made changes to the tokenizer or dataset parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f42a4c-9a30-4648-af90-91bb95eea909",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f8581a1-c682-466e-94d8-59037a92fe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading Dataset...\")\n",
    "dataset = datasets.load_dataset(config.dataset.dataset_id)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55011b35-91f1-47b6-a4e8-98362b6fa337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \"Hi, I am Fin. Do you want to play?\" asked the little fish. The crab looked at Fin and said, \"No, I don't want to play. I am cold and I don't feel fine.\"\n",
      "\n",
      "Fin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \"Please, sun, help my new friend feel fine and not freeze!\"\n",
      "\n",
      "The sun heard Fin's call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \"Thank you, little fish, for making me feel fine. I don't feel like I will freeze now. Let's play together!\" And so, Fin and the crab played and became good friends.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][2]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45f251-cf82-4278-9849-8c6fed662875",
   "metadata": {},
   "source": [
    "### Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9550c2c-4494-47d5-a052-984a437ad087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tokenizer...\n",
      "\n",
      "\n",
      "\n",
      "Completed training\n",
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=2000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../assets/tokenizers/tiny_stories_2k/tokenizer_config.json',\n",
       " '../assets/tokenizers/tiny_stories_2k/special_tokens_map.json',\n",
       " '../assets/tokenizers/tiny_stories_2k/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training Tokenizer...\")\n",
    "tokenizer = train_bpe_tokenizer(config.tokenizer, dataset['train'])\n",
    "print(tokenizer)\n",
    "tokenizer.save_pretrained(config.tokenizer.tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8b0d1-823b-45a1-bf8b-ccc3ba4da326",
   "metadata": {},
   "source": [
    "### Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2953de28-02cd-45ab-9755-16e08e453298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tokenizer...\n",
      "\n",
      "\n",
      "\n",
      "Completed training\n",
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|EOS|>', 'mask_token': '<|MASK|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|MASK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Tokenizing Dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd89296a53f428d95057536cf490cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/423943 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c26d9869ac4e12bebb1cfcc40fd7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 423943\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7908fb85cfd64bf893e4d80af04a89dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/423943 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cfaa034df94644bbf7cf2528762f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Tokenizing Dataset...\")\n",
    "tokenized_dataset = tokenize_datasetdict(dataset, tokenizer, config)\n",
    "print(tokenized_dataset)\n",
    "tokenized_dataset.save_to_disk(config.dataset.tokenized_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295e6d5-e6d1-4c02-b6c3-25340f11191d",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Dataset\n",
    "If you have already built the tokenizer and dataset, you can load them from this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bdae0a7-e20b-481a-abe2-ab936d4323a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset_from_config(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "tokenized_dataset = datasets.load_from_disk(config.dataset.tokenized_dataset_path)\n",
    "sample_text = dataset['train']['text'][0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e20ff9f-ab72-4ffc-9e1a-c9243f21394b",
   "metadata": {},
   "source": [
    "## Vanilla transformer model\n",
    "\n",
    "[tutorial_code.vanilla_transformer](../tutorial_code/vanilla_transformer.py)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc007b8d-31ba-4ac9-a81d-3c2c1ad1b286",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf1e01b-c477-4cdb-bee2-1037124ac3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.9M parameters\n",
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (key_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (value_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create model configuration\n",
    "model_config = VanillaTransformerConfig(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    hidden_size = config.model.d_model,\n",
    "    dim_feedforward = config.model.d_feedforward,\n",
    "    max_sequence_length = tokenizer.model_max_length,\n",
    "    num_attention_heads=config.model.num_attention_heads,\n",
    "    num_hidden_layers=config.model.num_hidden_layers,\n",
    ")\n",
    "\n",
    "# A config can also be instantiated from a json file like this:\n",
    "#config = AutoConfig.from_pretrained(\"path-to-config\")\n",
    "\n",
    "# Make model weights deterministic\n",
    "set_seed(42)\n",
    "# Instantiate the model\n",
    "model = AutoModelForCausalLM.from_config(model_config)\n",
    "print_model_size(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4473aee-f86d-4c72-a6ea-91f8c4e02f92",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "This is an example training-loop implementation.\n",
    "\n",
    "Example code is based upon examples here:\n",
    "https://huggingface.co/learn/nlp-course/en/chapter3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36a3a986-1c63-4efe-b666-bf82f4e52a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_code.train import Trainer, AccelTrainer, TrainingArguments\n",
    "import importlib\n",
    "from transformers.utils.notebook import NotebookProgressCallback\n",
    "\n",
    "# If the module has not been imported, we first import it.\n",
    "import tutorial_code.train\n",
    "\n",
    "# Trigger a reload of the module.\n",
    "importlib.reload(tutorial_code.train)\n",
    "\n",
    "# This provides a place to configure the training parameters.\n",
    "def do_train():\n",
    "    start_time = time.perf_counter()\n",
    "    AccelTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['validation'],\n",
    "        callbacks=[ NotebookProgressCallback() ],\n",
    "        training_arguments=TrainingArguments(**config.train),\n",
    "    ).train()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Elapsed {datetime.timedelta(seconds=time.perf_counter() - start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc94ad-00af-4f09-9ffe-2ec21e6da18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size 32\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86ebef-f488-45c2-adf9-43812d76f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic accelerate configuration\n",
    "import os\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()  # Write a config file\n",
    "os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c82819b-3853-48b4-bb23-d952ff979a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher, DataLoaderConfiguration\n",
    "from tutorial_code.trainer_callback import ProgressCallback, InfoCallback\n",
    "from tutorial_code.train import Trainer, AccelTrainer, TrainingArguments\n",
    "from transformers.utils.notebook import NotebookProgressCallback\n",
    "\n",
    "#transformers.utils.logging.set_verbosity_debug()\n",
    "\n",
    "def accel_train_function():\n",
    "    # See: https://github.com/huggingface/accelerate/blob/v0.13.2/src/accelerate/accelerator.py\n",
    "    accelerator_args=dict(\n",
    "        #mixed_precision='bf16',\n",
    "        # DataLoaderConfiguration\n",
    "        dataloader_config=DataLoaderConfiguration(\n",
    "            dispatch_batches=False,\n",
    "            # Note: This requires that your batch size be a multiple of the number of GPUs.\n",
    "            split_batches=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    trainer = AccelTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['validation'],\n",
    "        callbacks=[InfoCallback(), NotebookProgressCallback()],\n",
    "        accelerator_args=accelerator_args,\n",
    "        training_arguments=TrainingArguments(**config.train),\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # The model is not moved back to the notebook process automatically.\n",
    "    # Save the model and then reload it to move it.\n",
    "    # Note: Be careful that this does not clobber a saved model.\n",
    "    # If you don't want this to persist, create a temporary path to save the model to.\n",
    "    trainer.accelerator.wait_for_everyone()\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        model.save_pretrained(\n",
    "            save_directory=config.model_path,\n",
    "            safe_serialization=True,\n",
    "        )\n",
    "\n",
    "\n",
    "def do_train():\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    notebook_launcher(accel_train_function, num_processes=torch.cuda.device_count())\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Elapsed {datetime.timedelta(seconds=time.perf_counter() - start_time)}\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "       config.model_path,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f9b16-b814-4f39-90a5-764ef199ea78",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc798a0c-3398-4b93-bce1-400a6d7839e9",
   "metadata": {},
   "source": [
    "### Predict Tokens\n",
    "This will take the input text and have the model make predictions for the next token for each token in the sequence.\n",
    "\n",
    "The color coding indicates the loss for each individual token, with darker colors being more accurate and brighter colors being less so.\n",
    "\n",
    "If you hover over a token, you can see the top-10 predictions for the next token in the sequence.\n",
    "\n",
    "[tutorial_code.inference](../tutorial_code/inference.py)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab16902e-9f12-4690-8f41-469036fa25b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them b\n",
      "Metric 'Causal Loss': n=154, min=0.016235284507274628, max=9.593510627746582, mean=2.398885488510132, range=(0, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "/* Tooltip container class */\n",
       ".token {\n",
       "  position: relative;\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       "/* Tooltip text */\n",
       ".token .tooltip {\n",
       "  visibility: hidden;\n",
       "  width: 300px;\n",
       "  background-color: black;\n",
       "  color: #fff;\n",
       "  text-align: left;\n",
       "  padding: 5px 0;\n",
       "  border-radius: 6px;\n",
       " \n",
       "  /* Position the tooltip text - see examples below! */\n",
       "  position: absolute;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       "/* Show the tooltip text when you mouse over the tooltip container */\n",
       ".token:hover .tooltip {\n",
       "  visibility: visible;\n",
       "}\n",
       "</style>\n",
       "Metric[0] 'Causal Loss': n=155, min=0.0, max=9.593510627746582, mean=2.383408784866333<br><span class='token' style='color: #0c0786'><span class='tooltip'>Token: ''<br>Causal Loss: 0.00000<br>---------------<br></span></span><span class='token' style='color: #9713a0'>&nbsp;One<span class='tooltip'>Token: ' One'<br>Causal Loss: 3.11953<br>---------------<br>0.67 : ' Once'<br>0.07 : ' Lily'<br>0.04 : ' One'<br>0.03 : ' Tom'<br>0.02 : ' Sara'<br>0.02 : ' Anna'<br>0.02 : ' Ben'<br>0.01 : ' Sam'<br>0.01 : ' Tim'<br>0.01 : ' John'<br></span></span><span class='token' style='color: #130689'>&nbsp;day<span class='tooltip'>Token: ' day'<br>Causal Loss: 0.10831<br>---------------<br>0.90 : ' day'<br>0.03 : ' sun'<br>0.01 : ' night'<br>0.01 : ' morning'<br>0.00 : ' of'<br>0.00 : ' S'<br>0.00 : ' was'<br>0.00 : ' M'<br>0.00 : ' time'<br>0.00 : 'a'<br></span></span><span class='token' style='color: #100787'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.08054<br>---------------<br>0.92 : ','<br>0.02 : ' a'<br>0.01 : ' there'<br>0.00 : ' he'<br>0.00 : ' the'<br>0.00 : ' his'<br>0.00 : ' came'<br>0.00 : ' in'<br>0.00 : ' Sam'<br>0.00 : ' they'<br></span></span><span class='token' style='color: #290593'>&nbsp;a<span class='tooltip'>Token: ' a'<br>Causal Loss: 0.49506<br>---------------<br>0.61 : ' a'<br>0.04 : ' there'<br>0.03 : ' Tom'<br>0.03 : ' two'<br>0.02 : ' Jack'<br>0.02 : ' M'<br>0.01 : ' Lily'<br>0.01 : ' Mommy'<br>0.01 : ' P'<br>0.01 : ' the'<br></span></span><span class='token' style='color: #340498'>&nbsp;little<span class='tooltip'>Token: ' little'<br>Causal Loss: 0.69416<br>---------------<br>0.50 : ' little'<br>0.11 : ' boy'<br>0.06 : ' girl'<br>0.03 : ' big'<br>0.02 : ' young'<br>0.01 : ' small'<br>0.01 : ' 3'<br>0.01 : ' mommy'<br>0.01 : ' very'<br>0.01 : ' brave'<br></span></span><span class='token' style='color: #360498'>&nbsp;girl<span class='tooltip'>Token: ' girl'<br>Causal Loss: 0.73113<br>---------------<br>0.48 : ' girl'<br>0.41 : ' boy'<br>0.01 : ' bird'<br>0.01 : ' dog'<br>0.01 : ' bunny'<br>0.00 : ' brother'<br>0.00 : ' bear'<br>0.00 : ' mouse'<br>0.00 : ' little'<br>0.00 : ' squirrel'<br></span></span><span class='token' style='color: #230590'>&nbsp;named<span class='tooltip'>Token: ' named'<br>Causal Loss: 0.36887<br>---------------<br>0.69 : ' named'<br>0.07 : ' was'<br>0.07 : ' called'<br>0.04 : ' went'<br>0.02 : ' came'<br>0.02 : ' and'<br>0.01 : ' who'<br>0.01 : ' had'<br>0.01 : ' wanted'<br>0.01 : ' saw'<br></span></span><span class='token' style='color: #5201a3'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 1.37985<br>---------------<br>0.25 : ' Lily'<br>0.19 : ' Lucy'<br>0.11 : ' Sue'<br>0.06 : ' Mia'<br>0.04 : ' Amy'<br>0.04 : ' Sarah'<br>0.03 : ' Jane'<br>0.02 : ' Mary'<br>0.02 : ' Jen'<br>0.01 : ' Emma'<br></span></span><span class='token' style='color: #9f1a9b'>&nbsp;found<span class='tooltip'>Token: ' found'<br>Causal Loss: 3.34374<br>---------------<br>0.49 : ' went'<br>0.09 : ' was'<br>0.09 : ' and'<br>0.07 : ' saw'<br>0.04 : ' found'<br>0.03 : ' had'<br>0.02 : '.'<br>0.02 : ''s'<br>0.02 : ' wanted'<br>0.02 : ' decided'<br></span></span><span class='token' style='color: #100787'>&nbsp;a<span class='tooltip'>Token: ' a'<br>Causal Loss: 0.08608<br>---------------<br>0.92 : ' a'<br>0.03 : ' an'<br>0.01 : ' some'<br>0.01 : ' the'<br>0.01 : ' something'<br>0.00 : ' two'<br>0.00 : ' her'<br>0.00 : ' many'<br>0.00 : ' out'<br>0.00 : ' in'<br></span></span><span class='token' style='color: #eff821'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 9.59351<br>---------------<br>0.32 : ' big'<br>0.06 : ' pretty'<br>0.05 : ' little'<br>0.05 : ' shiny'<br>0.04 : ' small'<br>0.02 : ' special'<br>0.02 : ' beautiful'<br>0.02 : ' toy'<br>0.02 : ' very'<br>0.01 : ' c'<br></span></span><span class='token' style='color: #3f039c'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 0.92044<br>---------------<br>0.40 : 'le'<br>0.16 : '.'<br>0.09 : 's'<br>0.05 : ' to'<br>0.01 : ' on'<br>0.01 : 'et'<br>0.01 : 'er'<br>0.01 : ' c'<br>0.01 : ' help'<br>0.01 : ' new'<br></span></span><span class='token' style='color: #7901a8'>&nbsp;in<span class='tooltip'>Token: ' in'<br>Causal Loss: 2.31225<br>---------------<br>0.24 : '.'<br>0.10 : ' in'<br>0.06 : ' with'<br>0.05 : ' on'<br>0.05 : ' of'<br>0.04 : 'x'<br>0.03 : ' and'<br>0.02 : ' that'<br>0.02 : ' day'<br>0.02 : ','<br></span></span><span class='token' style='color: #5701a4'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 1.50089<br>---------------<br>0.65 : ' the'<br>0.22 : ' her'<br>0.09 : ' a'<br>0.00 : ' their'<br>0.00 : ' it'<br>0.00 : ' his'<br>0.00 : ' Lily'<br>0.00 : 'c'<br>0.00 : ' and'<br>0.00 : '.'<br></span></span><span class='token' style='color: #5701a4'>&nbsp;room<span class='tooltip'>Token: ' room'<br>Causal Loss: 1.48823<br>---------------<br>0.23 : ' room'<br>0.12 : ' garden'<br>0.12 : ' backyard'<br>0.06 : ' mom'<br>0.06 : ' house'<br>0.03 : ' bed'<br>0.03 : ' pocket'<br>0.02 : ' yard'<br>0.01 : ' favorite'<br>0.01 : ' kitchen'<br></span></span><span class='token' style='color: #130689'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 0.11438<br>---------------<br>0.89 : '.'<br>0.02 : ' with'<br>0.02 : ' and'<br>0.01 : ','<br>0.01 : ' in'<br>0.01 : ' to'<br>0.00 : ' that'<br>0.00 : ' where'<br>0.00 : ' on'<br>0.00 : ' when'<br></span></span><span class='token' style='color: #1b068c'>&nbsp;She<span class='tooltip'>Token: ' She'<br>Causal Loss: 0.23862<br>---------------<br>0.79 : ' She'<br>0.08 : ' Lily'<br>0.05 : ' They'<br>0.02 : ' It'<br>0.01 : ' The'<br>0.01 : ' He'<br>0.01 : ' Her'<br>0.00 : ' \"'<br>0.00 : ' One'<br>0.00 : ' But'<br></span></span><span class='token' style='color: #ce4a75'>&nbsp;knew<span class='tooltip'>Token: ' knew'<br>Causal Loss: 4.94794<br>---------------<br>0.18 : ' was'<br>0.13 : ' loved'<br>0.11 : ' wanted'<br>0.09 : ' had'<br>0.07 : ' went'<br>0.04 : ' liked'<br>0.04 : ' picked'<br>0.03 : ' asked'<br>0.02 : ' decided'<br>0.02 : ' thought'<br></span></span><span class='token' style='color: #8607a6'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 2.64254<br>---------------<br>0.51 : ' she'<br>0.19 : ' that'<br>0.07 : ' her'<br>0.07 : ' it'<br>0.02 : ' the'<br>0.01 : ' they'<br>0.01 : ' what'<br>0.01 : ' where'<br>0.01 : ' how'<br>0.01 : ' Lily'<br></span></span><span class='token' style='color: #21058f'>&nbsp;was<span class='tooltip'>Token: ' was'<br>Causal Loss: 0.34033<br>---------------<br>0.71 : ' was'<br>0.07 : ' would'<br>0.05 : ' had'<br>0.01 : ' looked'<br>0.01 : ' could'<br>0.01 : ','<br>0.01 : ' wanted'<br>0.01 : ' made'<br>0.01 : ' to'<br>0.01 : ' and'<br></span></span><span class='token' style='color: #fba436'>&nbsp;dif<span class='tooltip'>Token: ' dif'<br>Causal Loss: 7.63459<br>---------------<br>0.18 : ' a'<br>0.10 : ' her'<br>0.06 : ' very'<br>0.05 : ' time'<br>0.03 : ' important'<br>0.03 : ' going'<br>0.02 : ' about'<br>0.02 : ' the'<br>0.02 : ' not'<br>0.02 : ' an'<br></span></span><span class='token' style='color: #0c0786'>f<span class='tooltip'>Token: 'f'<br>Causal Loss: 0.04653<br>---------------<br>0.95 : 'f'<br>0.00 : 'ver'<br>0.00 : 'st'<br>0.00 : 'w'<br>0.00 : 'se'<br>0.00 : ' big'<br>0.00 : 'king'<br>0.00 : 'ster'<br>0.00 : ' was'<br>0.00 : ' long'<br></span></span><span class='token' style='color: #18068b'>ic<span class='tooltip'>Token: 'ic'<br>Causal Loss: 0.19487<br>---------------<br>0.82 : 'ic'<br>0.03 : '.'<br>0.02 : 'ul'<br>0.01 : ','<br>0.01 : 'ive'<br>0.01 : '!'<br>0.01 : 'ut'<br>0.01 : ' to'<br>0.00 : ' for'<br>0.00 : 'ish'<br></span></span><span class='token' style='color: #15068a'>ul<span class='tooltip'>Token: 'ul'<br>Causal Loss: 0.13574<br>---------------<br>0.87 : 'ul'<br>0.03 : '.'<br>0.02 : 't'<br>0.01 : ' to'<br>0.01 : ' for'<br>0.00 : 'er'<br>0.00 : ','<br>0.00 : ' and'<br>0.00 : ' about'<br>0.00 : 'ut'<br></span></span><span class='token' style='color: #100787'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 0.08857<br>---------------<br>0.92 : 't'<br>0.01 : 'ts'<br>0.01 : 'ar'<br>0.00 : 'i'<br>0.00 : 'ated'<br>0.00 : 'ie'<br>0.00 : 'u'<br>0.00 : 'ce'<br>0.00 : 'ance'<br>0.00 : 'ic'<br></span></span><span class='token' style='color: #920fa2'>&nbsp;to<span class='tooltip'>Token: ' to'<br>Causal Loss: 2.96730<br>---------------<br>0.17 : '.'<br>0.15 : ','<br>0.10 : ' of'<br>0.07 : ' and'<br>0.06 : ' for'<br>0.05 : ' to'<br>0.04 : ' that'<br>0.03 : '!'<br>0.02 : 'en'<br>0.02 : ' in'<br></span></span><span class='token' style='color: #8f0da3'>&nbsp;play<span class='tooltip'>Token: ' play'<br>Causal Loss: 2.88794<br>---------------<br>0.30 : ' her'<br>0.07 : ' the'<br>0.06 : ' play'<br>0.04 : ' find'<br>0.03 : ' make'<br>0.02 : ' go'<br>0.02 : ' get'<br>0.02 : ' buy'<br>0.02 : ' eat'<br>0.02 : ' see'<br></span></span><span class='token' style='color: #340498'>&nbsp;with<span class='tooltip'>Token: ' with'<br>Causal Loss: 0.71016<br>---------------<br>0.49 : ' with'<br>0.19 : '.'<br>0.07 : ' in'<br>0.05 : ' outside'<br>0.02 : ' and'<br>0.02 : ' on'<br>0.01 : ' together'<br>0.01 : ','<br>0.01 : ' a'<br>0.01 : ' all'<br></span></span><span class='token' style='color: #a92395'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 3.63025<br>---------------<br>0.64 : ' her'<br>0.24 : '.'<br>0.03 : ' it'<br>0.01 : ' the'<br>0.01 : ' all'<br>0.01 : ','<br>0.00 : ' them'<br>0.00 : ' a'<br>0.00 : ' with'<br>0.00 : ' herself'<br></span></span><span class='token' style='color: #dc5e66'>&nbsp;because<span class='tooltip'>Token: ' because'<br>Causal Loss: 5.55588<br>---------------<br>0.86 : '.'<br>0.02 : ','<br>0.01 : ' and'<br>0.01 : ' to'<br>0.01 : ' with'<br>0.01 : ' all'<br>0.01 : '!'<br>0.01 : ' every'<br>0.01 : ' in'<br>0.00 : ' when'<br></span></span><span class='token' style='color: #6c00a8'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 1.96584<br>---------------<br>0.72 : ' she'<br>0.14 : ' it'<br>0.07 : ' her'<br>0.01 : ' they'<br>0.01 : ' the'<br>0.01 : ' Lily'<br>0.01 : ' of'<br>0.00 : ' he'<br>0.00 : ','<br>0.00 : ' Anna'<br></span></span><span class='token' style='color: #2b0594'>&nbsp;was<span class='tooltip'>Token: ' was'<br>Causal Loss: 0.53655<br>---------------<br>0.58 : ' was'<br>0.08 : ' had'<br>0.06 : ' made'<br>0.03 : ' could'<br>0.02 : ' wanted'<br>0.02 : ' didn'<br>0.02 : ' looked'<br>0.01 : ' would'<br>0.01 : ' couldn'<br>0.01 : ' felt'<br></span></span><span class='token' style='color: #dd5f65'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 5.61107<br>---------------<br>0.09 : ' a'<br>0.07 : ' time'<br>0.06 : ' very'<br>0.03 : ' too'<br>0.03 : '.'<br>0.03 : ' so'<br>0.03 : ' important'<br>0.03 : ' fun'<br>0.02 : ' her'<br>0.02 : ' special'<br></span></span><span class='token' style='color: #d14f71'>ar<span class='tooltip'>Token: 'ar'<br>Causal Loss: 5.07767<br>---------------<br>0.36 : 'ut'<br>0.11 : 'iver'<br>0.06 : 'ocked'<br>0.05 : 'r'<br>0.03 : 'aring'<br>0.03 : 'in'<br>0.03 : 'y'<br>0.02 : 'ip'<br>0.02 : 'ore'<br>0.02 : 'ic'<br></span></span><span class='token' style='color: #2b0594'>p<span class='tooltip'>Token: 'p'<br>Causal Loss: 0.53024<br>---------------<br>0.59 : 'p'<br>0.05 : 'ful'<br>0.02 : '.'<br>0.02 : 'm'<br>0.02 : 'b'<br>0.02 : 'l'<br>0.02 : 'f'<br>0.02 : 't'<br>0.01 : ' to'<br>0.01 : 'ate'<br></span></span><span class='token' style='color: #900ea3'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 2.90043<br>---------------<br>0.07 : 'le'<br>0.06 : 'ol'<br>0.05 : '.'<br>0.05 : '!'<br>0.05 : ' to'<br>0.05 : 'ill'<br>0.04 : 'er'<br>0.03 : 'en'<br>0.03 : 'ed'<br>0.03 : 'ing'<br></span></span><span class='token' style='color: #3f039c'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 0.93642<br>---------------<br>0.39 : ' Lily'<br>0.23 : ' She'<br>0.09 : '\\n'<br>0.05 : ' It'<br>0.04 : ' The'<br>0.04 : ' \n",
       "'<br>0.02 : ' They'<br>0.02 : ' But'<br>0.02 : ' Her'<br>0.02 : ' He'<br></span></span><span class='token' style='color: #6400a7'>&nbsp;wanted<span class='tooltip'>Token: ' wanted'<br>Causal Loss: 1.81055<br>---------------<br>0.25 : ' was'<br>0.16 : ' wanted'<br>0.10 : ' loved'<br>0.05 : ' had'<br>0.04 : ' asked'<br>0.04 : ' didn'<br>0.04 : ' thought'<br>0.03 : ' liked'<br>0.03 : ' felt'<br>0.03 : ' and'<br></span></span><span class='token' style='color: #100787'>&nbsp;to<span class='tooltip'>Token: ' to'<br>Causal Loss: 0.05660<br>---------------<br>0.94 : ' to'<br>0.01 : ' her'<br>0.01 : ' it'<br>0.01 : ' the'<br>0.01 : ' a'<br>0.00 : ' some'<br>0.00 : ' something'<br>0.00 : ' his'<br>0.00 : ' one'<br>0.00 : ' that'<br></span></span><span class='token' style='color: #c13c80'>&nbsp;share<span class='tooltip'>Token: ' share'<br>Causal Loss: 4.43869<br>---------------<br>0.08 : ' make'<br>0.07 : ' play'<br>0.06 : ' help'<br>0.05 : ' show'<br>0.04 : ' go'<br>0.04 : ' be'<br>0.04 : ' try'<br>0.04 : ' eat'<br>0.03 : ' have'<br>0.03 : ' see'<br></span></span><span class='token' style='color: #6d00a8'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 2.01417<br>---------------<br>0.43 : ' her'<br>0.16 : ' it'<br>0.13 : ' the'<br>0.06 : ' with'<br>0.04 : '.'<br>0.03 : ' and'<br>0.03 : ','<br>0.01 : ' a'<br>0.01 : ' his'<br>0.01 : ' something'<br></span></span><span class='token' style='color: #b02a8f'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 3.85451<br>---------------<br>0.02 : ' need'<br>0.02 : ' cup'<br>0.02 : ' paint'<br>0.02 : ' pictures'<br>0.01 : ' store'<br>0.01 : ' sp'<br>0.01 : ' draw'<br>0.01 : ' table'<br>0.01 : ' picture'<br>0.01 : ' show'<br></span></span><span class='token' style='color: #2f0495'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 0.60423<br>---------------<br>0.55 : 'le'<br>0.14 : 's'<br>0.08 : '.'<br>0.08 : ' to'<br>0.02 : 'les'<br>0.01 : 'et'<br>0.01 : ' help'<br>0.01 : ' and'<br>0.00 : ' break'<br>0.00 : 'er'<br></span></span><span class='token' style='color: #900ea3'>&nbsp;with<span class='tooltip'>Token: ' with'<br>Causal Loss: 2.93012<br>---------------<br>0.15 : ' and'<br>0.13 : '.'<br>0.08 : ' of'<br>0.07 : 'br'<br>0.05 : ' with'<br>0.04 : ','<br>0.04 : ' to'<br>0.04 : 't'<br>0.02 : ' when'<br>0.02 : ' in'<br></span></span><span class='token' style='color: #45039e'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 1.06944<br>---------------<br>0.34 : ' her'<br>0.15 : ' it'<br>0.11 : '.'<br>0.07 : ' the'<br>0.04 : ' a'<br>0.02 : ' all'<br>0.02 : ' his'<br>0.01 : ' some'<br>0.01 : ' them'<br>0.01 : ' lots'<br></span></span><span class='token' style='color: #8506a6'>&nbsp;mom<span class='tooltip'>Token: ' mom'<br>Causal Loss: 2.62707<br>---------------<br>0.13 : ' toys'<br>0.07 : ' friends'<br>0.07 : ' mom'<br>0.04 : ' new'<br>0.04 : ' family'<br>0.03 : '.'<br>0.02 : ' favorite'<br>0.02 : ' cray'<br>0.02 : ' mommy'<br>0.01 : ' own'<br></span></span><span class='token' style='color: #8c0aa4'>,<span class='tooltip'>Token: ','<br>Causal Loss: 2.80236<br>---------------<br>0.71 : '.'<br>0.14 : ' and'<br>0.06 : ','<br>0.02 : ' for'<br>0.01 : ' to'<br>0.01 : ''s'<br>0.00 : ' in'<br>0.00 : ' when'<br>0.00 : 's'<br>0.00 : ' with'<br></span></span><span class='token' style='color: #5601a3'>&nbsp;so<span class='tooltip'>Token: ' so'<br>Causal Loss: 1.45855<br>---------------<br>0.55 : ' but'<br>0.23 : ' so'<br>0.05 : ' and'<br>0.03 : ' \"'<br>0.01 : ' to'<br>0.01 : ' because'<br>0.01 : ' who'<br>0.00 : ' like'<br>0.00 : ' with'<br>0.00 : ' or'<br></span></span><span class='token' style='color: #18068b'>&nbsp;she<span class='tooltip'>Token: ' she'<br>Causal Loss: 0.20329<br>---------------<br>0.82 : ' she'<br>0.08 : ' they'<br>0.02 : ' Lily'<br>0.02 : ' he'<br>0.01 : ' it'<br>0.01 : ' her'<br>0.00 : ' that'<br>0.00 : ','<br>0.00 : ' the'<br>0.00 : ' many'<br></span></span><span class='token' style='color: #8204a7'>&nbsp;could<span class='tooltip'>Token: ' could'<br>Causal Loss: 2.52612<br>---------------<br>0.13 : ' asked'<br>0.10 : ' went'<br>0.08 : ' could'<br>0.08 : ' decided'<br>0.04 : ' wanted'<br>0.04 : ' said'<br>0.04 : ' took'<br>0.03 : ' started'<br>0.03 : ' ran'<br>0.02 : ' had'<br></span></span><span class='token' style='color: #f38649'>&nbsp;se<span class='tooltip'>Token: ' se'<br>Causal Loss: 6.79097<br>---------------<br>0.07 : ' make'<br>0.06 : ' play'<br>0.06 : ' find'<br>0.05 : ' see'<br>0.05 : ' have'<br>0.04 : ' help'<br>0.04 : ' get'<br>0.03 : ' go'<br>0.03 : ' do'<br>0.02 : ' use'<br></span></span><span class='token' style='color: #cf4b74'>w<span class='tooltip'>Token: 'w'<br>Causal Loss: 4.97702<br>---------------<br>0.56 : 'p'<br>0.15 : 'at'<br>0.14 : 'll'<br>0.02 : 'ed'<br>0.01 : 'ven'<br>0.01 : 'al'<br>0.01 : 'w'<br>0.01 : 'ver'<br>0.01 : 've'<br>0.01 : 'f'<br></span></span><span class='token' style='color: #f0804d'>&nbsp;a<span class='tooltip'>Token: ' a'<br>Causal Loss: 6.62540<br>---------------<br>0.08 : 'ic'<br>0.05 : 'al'<br>0.05 : 'ich'<br>0.04 : 's'<br>0.04 : 'le'<br>0.04 : 'as'<br>0.04 : 'ling'<br>0.03 : 'l'<br>0.03 : 'or'<br>0.03 : '.'<br></span></span><span class='token' style='color: #f1824c'>&nbsp;butt<span class='tooltip'>Token: ' butt'<br>Causal Loss: 6.69309<br>---------------<br>0.07 : ' big'<br>0.04 : ' new'<br>0.03 : ' picture'<br>0.03 : ' special'<br>0.02 : ' game'<br>0.02 : 'ct'<br>0.02 : ' lot'<br>0.01 : ' long'<br>0.01 : 'dd'<br>0.01 : ' cup'<br></span></span><span class='token' style='color: #250591'>on<span class='tooltip'>Token: 'on'<br>Causal Loss: 0.41250<br>---------------<br>0.66 : 'on'<br>0.32 : 'ons'<br>0.00 : 's'<br>0.00 : 'f'<br>0.00 : 'le'<br>0.00 : 'p'<br>0.00 : 'k'<br>0.00 : 'u'<br>0.00 : 'v'<br>0.00 : 'w'<br></span></span><span class='token' style='color: #b93388'>&nbsp;on<span class='tooltip'>Token: ' on'<br>Causal Loss: 4.13597<br>---------------<br>0.53 : '.'<br>0.05 : ','<br>0.04 : ' with'<br>0.03 : ' and'<br>0.03 : ' for'<br>0.03 : 'es'<br>0.03 : 'est'<br>0.03 : 't'<br>0.02 : ' to'<br>0.02 : ' in'<br></span></span><span class='token' style='color: #3b039a'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 0.87108<br>---------------<br>0.42 : ' her'<br>0.35 : ' the'<br>0.06 : ' a'<br>0.06 : ' it'<br>0.01 : '.'<br>0.01 : 't'<br>0.01 : ' his'<br>0.01 : ' an'<br>0.01 : ' and'<br>0.00 : 'ion'<br></span></span><span class='token' style='color: #e16560'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 5.78946<br>---------------<br>0.14 : ' face'<br>0.05 : ' mom'<br>0.04 : ' head'<br>0.02 : ' own'<br>0.02 : ' hair'<br>0.02 : ' bed'<br>0.01 : ' arm'<br>0.01 : ' c'<br>0.01 : ' p'<br>0.01 : ' table'<br></span></span><span class='token' style='color: #6f00a8'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 2.05035<br>---------------<br>0.17 : 'ar'<br>0.13 : 'ir'<br>0.08 : 'ore'<br>0.06 : 'ield'<br>0.05 : 'aring'<br>0.04 : 'ocked'<br>0.04 : 'ape'<br>0.04 : 'r'<br>0.03 : 'a'<br>0.03 : 'ip'<br></span></span><span class='token' style='color: #270592'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 0.45374<br>---------------<br>0.64 : 't'<br>0.13 : 'c'<br>0.10 : 'p'<br>0.03 : 'cle'<br>0.01 : 'ate'<br>0.01 : 'on'<br>0.01 : 'a'<br>0.01 : 'r'<br>0.01 : ' to'<br>0.01 : 'red'<br></span></span><span class='token' style='color: #4e02a1'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 1.27689<br>---------------<br>0.30 : 'o'<br>0.28 : '.'<br>0.09 : ' and'<br>0.05 : ' of'<br>0.04 : ','<br>0.03 : ' with'<br>0.02 : 'en'<br>0.02 : 'ain'<br>0.01 : ' on'<br>0.01 : ' from'<br></span></span><br><br><span class='token' style='color: #290593'>Lily<span class='tooltip'>Token: 'Lily'<br>Causal Loss: 0.48434<br>---------------<br>0.62 : 'Lily'<br>0.12 : '&quot;'<br>0.04 : 'Suddenly'<br>0.03 : 'When'<br>0.03 : 'As'<br>0.02 : 'One'<br>0.02 : 'But'<br>0.01 : 'L'<br>0.01 : 'She'<br>0.01 : 'The'<br></span></span><span class='token' style='color: #9e199c'>&nbsp;went<span class='tooltip'>Token: ' went'<br>Causal Loss: 3.27861<br>---------------<br>0.20 : ' was'<br>0.13 : ''s'<br>0.06 : ' asked'<br>0.05 : ' and'<br>0.04 : ' felt'<br>0.04 : ' went'<br>0.03 : ' thought'<br>0.03 : ' looked'<br>0.03 : ' saw'<br>0.03 : ' wanted'<br></span></span><span class='token' style='color: #1d068d'>&nbsp;to<span class='tooltip'>Token: ' to'<br>Causal Loss: 0.25292<br>---------------<br>0.78 : ' to'<br>0.04 : ' outside'<br>0.04 : ' on'<br>0.02 : ' for'<br>0.01 : ' inside'<br>0.01 : ' out'<br>0.01 : ' home'<br>0.01 : ' up'<br>0.01 : ' back'<br>0.01 : ' over'<br></span></span><span class='token' style='color: #8807a5'>&nbsp;her<span class='tooltip'>Token: ' her'<br>Causal Loss: 2.69741<br>---------------<br>0.64 : ' the'<br>0.07 : ' her'<br>0.04 : ' a'<br>0.04 : ' play'<br>0.03 : ' bed'<br>0.02 : ' his'<br>0.01 : ' visit'<br>0.01 : ' find'<br>0.01 : ' school'<br>0.01 : ' playing'<br></span></span><span class='token' style='color: #3d039b'>&nbsp;mom<span class='tooltip'>Token: ' mom'<br>Causal Loss: 0.88285<br>---------------<br>0.41 : ' mom'<br>0.12 : ' room'<br>0.09 : ' to'<br>0.04 : ' house'<br>0.03 : ' friend'<br>0.02 : ' bed'<br>0.02 : ' toy'<br>0.01 : ' favorite'<br>0.01 : ' dad'<br>0.01 : ' grandma'<br></span></span><span class='token' style='color: #330497'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 0.68846<br>---------------<br>0.50 : ' and'<br>0.10 : '.'<br>0.10 : ' to'<br>0.08 : ','<br>0.06 : ''s'<br>0.02 : ' for'<br>0.01 : ' in'<br>0.01 : ' with'<br>0.01 : ' a'<br>0.01 : ' said'<br></span></span><span class='token' style='color: #6400a7'>&nbsp;said<span class='tooltip'>Token: ' said'<br>Causal Loss: 1.80218<br>---------------<br>0.20 : ' dad'<br>0.16 : ' said'<br>0.10 : ' asked'<br>0.08 : ' told'<br>0.05 : ' saw'<br>0.04 : ' her'<br>0.02 : ' she'<br>0.02 : ' gave'<br>0.02 : ' daddy'<br>0.01 : ' to'<br></span></span><span class='token' style='color: #18068b'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.19188<br>---------------<br>0.83 : ','<br>0.04 : ' to'<br>0.02 : ' she'<br>0.02 : ' that'<br>0.01 : ':'<br>0.01 : ' \"'<br>0.01 : ' it'<br>0.01 : ' they'<br>0.01 : '.'<br>0.01 : ' yes'<br></span></span><span class='token' style='color: #0c0786'>&nbsp;\"<span class='tooltip'>Token: ' \"'<br>Causal Loss: 0.03158<br>---------------<br>0.97 : ' \"'<br>0.02 : ' â'<br>0.00 : ' but'<br>0.00 : '\\n'<br>0.00 : ' so'<br>0.00 : ' and'<br>0.00 : ' hold'<br>0.00 : ' to'<br>0.00 : ' '<br>0.00 : ' poin'<br></span></span><span class='token' style='color: #a41e98'>Mom<span class='tooltip'>Token: 'Mom'<br>Causal Loss: 3.46052<br>---------------<br>0.11 : 'I'<br>0.07 : 'Can'<br>0.07 : 'Let'<br>0.06 : 'Mommy'<br>0.04 : 'What'<br>0.04 : 'That'<br>0.04 : 'Look'<br>0.03 : 'Hi'<br>0.03 : 'Yes'<br>0.03 : 'Mom'<br></span></span><span class='token' style='color: #100787'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.07300<br>---------------<br>0.93 : ','<br>0.01 : '!'<br>0.00 : ' is'<br>0.00 : ' can'<br>0.00 : ' and'<br>0.00 : ':'<br>0.00 : '.'<br>0.00 : ' was'<br>0.00 : '!\"'<br>0.00 : ' to'<br></span></span><span class='token' style='color: #8b09a4'>&nbsp;I<span class='tooltip'>Token: ' I'<br>Causal Loss: 2.75284<br>---------------<br>0.25 : ' can'<br>0.07 : ' let'<br>0.07 : ' please'<br>0.06 : ' I'<br>0.06 : ' you'<br>0.04 : ' look'<br>0.03 : ' why'<br>0.03 : ' don'<br>0.03 : ' we'<br>0.02 : ' what'<br></span></span><span class='token' style='color: #b62f8b'>&nbsp;found<span class='tooltip'>Token: ' found'<br>Causal Loss: 4.05476<br>---------------<br>0.20 : ' can'<br>0.14 : ' want'<br>0.12 : ' will'<br>0.07 : ' have'<br>0.07 : ' don'<br>0.05 : ''m'<br>0.04 : ''ll'<br>0.03 : ' am'<br>0.02 : ' love'<br>0.02 : ' do'<br></span></span><span class='token' style='color: #9d189d'>&nbsp;this<span class='tooltip'>Token: ' this'<br>Causal Loss: 3.23961<br>---------------<br>0.25 : ' a'<br>0.16 : ' it'<br>0.10 : ' the'<br>0.10 : ' some'<br>0.04 : ' this'<br>0.04 : ' you'<br>0.04 : ' something'<br>0.03 : ' your'<br>0.03 : ' my'<br>0.02 : ' that'<br></span></span><span class='token' style='color: #c7427b'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 4.67387<br>---------------<br>0.05 : ' is'<br>0.03 : ' for'<br>0.03 : ' so'<br>0.02 : ' can'<br>0.02 : ' special'<br>0.02 : ' to'<br>0.01 : '.'<br>0.01 : ','<br>0.01 : ' box'<br>0.01 : ' mess'<br></span></span><span class='token' style='color: #9511a1'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 3.02882<br>---------------<br>0.56 : ' to'<br>0.05 : 'le'<br>0.05 : ' a'<br>0.04 : 's'<br>0.04 : ' it'<br>0.03 : ' you'<br>0.02 : '?\"'<br>0.02 : ' some'<br>0.02 : '.'<br>0.01 : ' the'<br></span></span><span class='token' style='color: #8c0aa4'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 2.81701<br>---------------<br>0.12 : '?\"'<br>0.08 : '.\"'<br>0.07 : ' to'<br>0.06 : '.'<br>0.05 : ' and'<br>0.04 : '!\"'<br>0.04 : ' for'<br>0.04 : ' in'<br>0.03 : ','<br>0.02 : 'br'<br></span></span><span class='token' style='color: #9310a1'>&nbsp;Can<span class='tooltip'>Token: ' Can'<br>Causal Loss: 3.00760<br>---------------<br>0.14 : ' I'<br>0.13 : ' It'<br>0.12 : ' But'<br>0.07 : ' She'<br>0.07 : ' You'<br>0.05 : ' Can'<br>0.03 : ' Let'<br>0.03 : ' We'<br>0.03 : ' Her'<br>0.03 : ' \"'<br></span></span><span class='token' style='color: #49029f'>&nbsp;you<span class='tooltip'>Token: ' you'<br>Causal Loss: 1.17193<br>---------------<br>0.35 : ' I'<br>0.31 : ' we'<br>0.31 : ' you'<br>0.00 : ' be'<br>0.00 : ' my'<br>0.00 : ' Lily'<br>0.00 : ' she'<br>0.00 : ''t'<br>0.00 : ' only'<br>0.00 : ' it'<br></span></span><span class='token' style='color: #d7576b'>&nbsp;share<span class='tooltip'>Token: ' share'<br>Causal Loss: 5.34863<br>---------------<br>0.28 : ' help'<br>0.03 : ' make'<br>0.03 : ' take'<br>0.03 : ' find'<br>0.02 : ' can'<br>0.02 : ' have'<br>0.02 : ' get'<br>0.02 : ' show'<br>0.02 : ' play'<br>0.02 : '?\"'<br></span></span><span class='token' style='color: #5901a4'>&nbsp;it<span class='tooltip'>Token: ' it'<br>Causal Loss: 1.53089<br>---------------<br>0.22 : ' it'<br>0.20 : ' your'<br>0.13 : ' the'<br>0.06 : '?\"'<br>0.05 : ' her'<br>0.05 : ' my'<br>0.03 : ' with'<br>0.02 : ' me'<br>0.02 : ' this'<br>0.02 : ' some'<br></span></span><span class='token' style='color: #a51f97'>&nbsp;with<span class='tooltip'>Token: ' with'<br>Causal Loss: 3.50864<br>---------------<br>0.64 : '?\"'<br>0.03 : ' with'<br>0.03 : '?'<br>0.02 : ','<br>0.02 : ' to'<br>0.02 : ' in'<br>0.01 : ' and'<br>0.01 : ''s'<br>0.01 : '.'<br>0.01 : '.\"'<br></span></span><span class='token' style='color: #5601a3'>&nbsp;me<span class='tooltip'>Token: ' me'<br>Causal Loss: 1.44081<br>---------------<br>0.24 : ' me'<br>0.14 : ' your'<br>0.12 : ' it'<br>0.07 : ' my'<br>0.06 : ' the'<br>0.03 : '?\"'<br>0.03 : ' a'<br>0.02 : ' you'<br>0.02 : ' her'<br>0.02 : ' some'<br></span></span><span class='token' style='color: #cd4975'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 4.89667<br>---------------<br>0.38 : '?\"'<br>0.04 : ' help'<br>0.03 : ' to'<br>0.02 : 'asure'<br>0.02 : '?'<br>0.02 : ' a'<br>0.02 : ' the'<br>0.02 : ' too'<br>0.02 : ' your'<br>0.02 : ','<br></span></span><span class='token' style='color: #ef7e4e'>&nbsp;se<span class='tooltip'>Token: ' se'<br>Causal Loss: 6.56818<br>---------------<br>0.05 : ' it'<br>0.04 : ' make'<br>0.03 : ' the'<br>0.03 : ' be'<br>0.03 : ' put'<br>0.03 : ' you'<br>0.02 : ' some'<br>0.02 : ' then'<br>0.02 : ' get'<br>0.02 : ' help'<br></span></span><span class='token' style='color: #8104a7'>w<span class='tooltip'>Token: 'w'<br>Causal Loss: 2.51338<br>---------------<br>0.22 : 'p'<br>0.18 : 'll'<br>0.16 : 'ed'<br>0.08 : 'w'<br>0.08 : 'at'<br>0.04 : 'ag'<br>0.04 : 'al'<br>0.02 : 'as'<br>0.02 : 'f'<br>0.02 : 'ven'<br></span></span><span class='token' style='color: #fccb25'>&nbsp;my<span class='tooltip'>Token: ' my'<br>Causal Loss: 8.55108<br>---------------<br>0.23 : 's'<br>0.09 : 'led'<br>0.07 : 'ich'<br>0.06 : 'eet'<br>0.06 : 'ed'<br>0.05 : 'as'<br>0.03 : 'el'<br>0.03 : 'or'<br>0.03 : 'es'<br>0.03 : 'ling'<br></span></span><span class='token' style='color: #d7576b'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 5.36578<br>---------------<br>0.04 : ' toys'<br>0.04 : ' favorite'<br>0.03 : ' work'<br>0.02 : ' mom'<br>0.02 : ' new'<br>0.02 : ' cray'<br>0.02 : 'ster'<br>0.02 : ' friend'<br>0.02 : 'self'<br>0.01 : ' friends'<br></span></span><span class='token' style='color: #49029f'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 1.16482<br>---------------<br>0.31 : 'ir'<br>0.13 : 'ar'<br>0.07 : 'ore'<br>0.05 : 'ape'<br>0.03 : 'ield'<br>0.03 : 'aring'<br>0.03 : 'a'<br>0.03 : 'ut'<br>0.03 : 'ocked'<br>0.03 : 'ip'<br></span></span><span class='token' style='color: #1d068d'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 0.25713<br>---------------<br>0.77 : 't'<br>0.08 : 'c'<br>0.03 : 'p'<br>0.02 : 'cle'<br>0.01 : 's'<br>0.01 : 'a'<br>0.00 : 'on'<br>0.00 : 'red'<br>0.00 : '.'<br>0.00 : 'r'<br></span></span><span class='token' style='color: #5c00a5'>?\"<span class='tooltip'>Token: '?\"'<br>Causal Loss: 1.61943<br>---------------<br>0.20 : '?\"'<br>0.16 : '.'<br>0.11 : '.\"'<br>0.04 : ' and'<br>0.04 : ','<br>0.03 : 'ain'<br>0.03 : ' for'<br>0.02 : ' is'<br>0.02 : 'o'<br>0.02 : 'on'<br></span></span><span class='token' style='color: #8908a5'>&nbsp;Her<span class='tooltip'>Token: ' Her'<br>Causal Loss: 2.71307<br>---------------<br>0.42 : '\\n'<br>0.20 : ' Lily'<br>0.09 : ' \n",
       "'<br>0.07 : ' Her'<br>0.06 : ' she'<br>0.03 : ' The'<br>0.01 : ' asked'<br>0.01 : ' her'<br>0.01 : ' She'<br>0.01 : ' They'<br></span></span><span class='token' style='color: #0c0786'>&nbsp;mom<span class='tooltip'>Token: ' mom'<br>Causal Loss: 0.04688<br>---------------<br>0.95 : ' mom'<br>0.02 : ' dad'<br>0.00 : ' mommy'<br>0.00 : ' grandma'<br>0.00 : ' name'<br>0.00 : ' Mom'<br>0.00 : ' parents'<br>0.00 : ' friend'<br>0.00 : ' favorite'<br>0.00 : ' mother'<br></span></span><span class='token' style='color: #920fa2'>&nbsp;smiled<span class='tooltip'>Token: ' smiled'<br>Causal Loss: 2.95965<br>---------------<br>0.43 : ' asked'<br>0.26 : ' said'<br>0.12 : ' replied'<br>0.05 : ' smiled'<br>0.01 : ' told'<br>0.01 : ' explained'<br>0.01 : ' thought'<br>0.01 : ' looked'<br>0.00 : ' nodded'<br>0.00 : ' and'<br></span></span><span class='token' style='color: #1f058e'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 0.29213<br>---------------<br>0.75 : ' and'<br>0.12 : '.'<br>0.03 : ','<br>0.03 : ' at'<br>0.02 : ' as'<br>0.01 : ' too'<br>0.01 : ' with'<br>0.01 : ' back'<br>0.00 : ' from'<br>0.00 : ' in'<br></span></span><span class='token' style='color: #230590'>&nbsp;said<span class='tooltip'>Token: ' said'<br>Causal Loss: 0.35682<br>---------------<br>0.70 : ' said'<br>0.03 : ' replied'<br>0.03 : ' asked'<br>0.02 : ' told'<br>0.02 : ' gave'<br>0.01 : ' hugged'<br>0.01 : ' went'<br>0.01 : ' smiled'<br>0.01 : ' nodded'<br>0.01 : ' took'<br></span></span><span class='token' style='color: #100787'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.06804<br>---------------<br>0.93 : ','<br>0.03 : '.'<br>0.01 : ' \"'<br>0.01 : ':'<br>0.00 : ' to'<br>0.00 : ' and'<br>0.00 : ' yes'<br>0.00 : ' with'<br>0.00 : ' for'<br>0.00 : ' goodbye'<br></span></span><span class='token' style='color: #0c0786'>&nbsp;\"<span class='tooltip'>Token: ' \"'<br>Causal Loss: 0.02385<br>---------------<br>0.98 : ' \"'<br>0.00 : ' â'<br>0.00 : ' but'<br>0.00 : ' poin'<br>0.00 : ' and'<br>0.00 : ' so'<br>0.00 : ' hold'<br>0.00 : ' feeling'<br>0.00 : 'hed'<br>0.00 : ' happy'<br></span></span><span class='token' style='color: #47029f'>Yes<span class='tooltip'>Token: 'Yes'<br>Causal Loss: 1.11220<br>---------------<br>0.33 : 'Yes'<br>0.09 : 'Sure'<br>0.08 : 'I'<br>0.08 : 'That'<br>0.05 : 'O'<br>0.04 : 'Thank'<br>0.04 : 'Okay'<br>0.03 : 'No'<br>0.03 : 'You'<br>0.02 : 'It'<br></span></span><span class='token' style='color: #100787'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.08854<br>---------------<br>0.92 : ','<br>0.02 : '!'<br>0.02 : '!\"'<br>0.01 : ',\"'<br>0.01 : ' please'<br>0.00 : '.'<br>0.00 : '?\"'<br>0.00 : '.\"'<br>0.00 : '\".'<br>0.00 : '?'<br></span></span><span class='token' style='color: #7300a8'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 2.18231<br>---------------<br>0.11 : ' Lily'<br>0.10 : ' can'<br>0.08 : ' we'<br>0.07 : ' I'<br>0.06 : ' you'<br>0.06 : ' please'<br>0.05 : ' but'<br>0.05 : ' let'<br>0.05 : ' it'<br>0.03 : ' sweet'<br></span></span><span class='token' style='color: #5601a3'>,<span class='tooltip'>Token: ','<br>Causal Loss: 1.46145<br>---------------<br>0.56 : '.'<br>0.23 : ','<br>0.07 : '!'<br>0.04 : '!\"'<br>0.03 : ' and'<br>0.03 : '.\"'<br>0.01 : ',\"'<br>0.01 : '?\"'<br>0.00 : ' to'<br>0.00 : ' is'<br></span></span><span class='token' style='color: #8e0ca4'>&nbsp;we<span class='tooltip'>Token: ' we'<br>Causal Loss: 2.82349<br>---------------<br>0.12 : ' can'<br>0.10 : ' let'<br>0.09 : ' you'<br>0.09 : ' I'<br>0.07 : ' please'<br>0.07 : ' but'<br>0.06 : ' we'<br>0.04 : ' Lily'<br>0.03 : ' don'<br>0.02 : ' sweet'<br></span></span><span class='token' style='color: #2b0594'>&nbsp;can<span class='tooltip'>Token: ' can'<br>Causal Loss: 0.53917<br>---------------<br>0.58 : ' can'<br>0.08 : ' have'<br>0.07 : ' will'<br>0.06 : ' need'<br>0.03 : ' are'<br>0.02 : ' want'<br>0.01 : ' love'<br>0.01 : ' don'<br>0.01 : ''ll'<br>0.01 : ' do'<br></span></span><span class='token' style='color: #c43f7e'>&nbsp;share<span class='tooltip'>Token: ' share'<br>Causal Loss: 4.57321<br>---------------<br>0.11 : ' play'<br>0.08 : ' make'<br>0.07 : ''t'<br>0.07 : ' have'<br>0.04 : ' help'<br>0.03 : ' be'<br>0.03 : ' go'<br>0.02 : ' do'<br>0.02 : ' use'<br>0.02 : ' we'<br></span></span><span class='token' style='color: #7200a8'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 2.14364<br>---------------<br>0.16 : ' it'<br>0.14 : ' your'<br>0.12 : ' the'<br>0.09 : ' and'<br>0.06 : ' with'<br>0.06 : ' my'<br>0.05 : ' our'<br>0.05 : '.\"'<br>0.02 : ','<br>0.02 : ' a'<br></span></span><span class='token' style='color: #b93388'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 4.14985<br>---------------<br>0.02 : ' sp'<br>0.02 : ' new'<br>0.02 : ' need'<br>0.02 : ' show'<br>0.01 : ' cl'<br>0.01 : ' sh'<br>0.01 : ' b'<br>0.01 : ' so'<br>0.01 : ' cup'<br>0.01 : ' ju'<br></span></span><span class='token' style='color: #5901a4'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 1.51592<br>---------------<br>0.22 : 'le'<br>0.19 : ' to'<br>0.06 : 's'<br>0.06 : '.'<br>0.04 : '?\"'<br>0.03 : '.\"'<br>0.02 : ' help'<br>0.01 : 'les'<br>0.01 : ' some'<br>0.01 : ' break'<br></span></span><span class='token' style='color: #99149f'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 3.15017<br>---------------<br>0.11 : 'br'<br>0.10 : '.\"'<br>0.08 : '.'<br>0.04 : ' and'<br>0.03 : ','<br>0.03 : ' to'<br>0.03 : '?\"'<br>0.03 : ' of'<br>0.02 : ' with'<br>0.02 : ' together'<br></span></span><span class='token' style='color: #d4526e'>&nbsp;fix<span class='tooltip'>Token: ' fix'<br>Causal Loss: 5.20584<br>---------------<br>0.10 : ' make'<br>0.05 : ' play'<br>0.05 : ' get'<br>0.03 : ' find'<br>0.03 : ' have'<br>0.03 : ' see'<br>0.03 : ' put'<br>0.02 : ' eat'<br>0.02 : ' go'<br>0.02 : ' can'<br></span></span><span class='token' style='color: #bf3982'>&nbsp;your<span class='tooltip'>Token: ' your'<br>Causal Loss: 4.36787<br>---------------<br>0.52 : ' it'<br>0.22 : ' the'<br>0.04 : ' things'<br>0.03 : 'ed'<br>0.02 : ' a'<br>0.01 : ' your'<br>0.01 : ' them'<br>0.01 : 'ing'<br>0.01 : ' my'<br>0.01 : ' her'<br></span></span><span class='token' style='color: #d04d73'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 5.02578<br>---------------<br>0.04 : ' toys'<br>0.03 : ' own'<br>0.02 : ' food'<br>0.02 : ' new'<br>0.02 : ' friends'<br>0.02 : 's'<br>0.02 : ' work'<br>0.02 : 'self'<br>0.02 : ' te'<br>0.02 : ' hands'<br></span></span><span class='token' style='color: #3a049a'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 0.81640<br>---------------<br>0.44 : 'ir'<br>0.08 : 'ore'<br>0.06 : 'r'<br>0.04 : 'ar'<br>0.04 : 'aring'<br>0.03 : 'ut'<br>0.03 : 'ip'<br>0.03 : 'ape'<br>0.02 : 'ocked'<br>0.02 : 'y'<br></span></span><span class='token' style='color: #1b068c'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 0.22886<br>---------------<br>0.80 : 't'<br>0.06 : 'c'<br>0.04 : 'p'<br>0.01 : 'cle'<br>0.01 : 'a'<br>0.00 : ' to'<br>0.00 : 'ate'<br>0.00 : 'on'<br>0.00 : 'red'<br>0.00 : 'd'<br></span></span><span class='token' style='color: #5101a2'>.\"<span class='tooltip'>Token: '.\"'<br>Causal Loss: 1.33320<br>---------------<br>0.26 : '.\"'<br>0.20 : '?\"'<br>0.07 : '.'<br>0.04 : '!\"'<br>0.03 : ' is'<br>0.02 : 'ain'<br>0.02 : ','<br>0.02 : ' and'<br>0.02 : ',\"'<br>0.01 : ' for'<br></span></span><br><br><span class='token' style='color: #f68f43'>T<span class='tooltip'>Token: 'T'<br>Causal Loss: 7.05436<br>---------------<br>0.76 : 'Lily'<br>0.03 : 'Her'<br>0.03 : 'L'<br>0.02 : 'After'<br>0.02 : 'The'<br>0.02 : 'When'<br>0.02 : '&quot;'<br>0.01 : 'As'<br>0.01 : 'But'<br>0.01 : 'They'<br></span></span><span class='token' style='color: #340498'>o<span class='tooltip'>Token: 'o'<br>Causal Loss: 0.69200<br>---------------<br>0.50 : 'o'<br>0.14 : 'r'<br>0.04 : 'ed'<br>0.04 : 'on'<br>0.03 : 'ime'<br>0.02 : 'eddy'<br>0.02 : 'in'<br>0.02 : 'w'<br>0.01 : 'y'<br>0.01 : 'ions'<br></span></span><span class='token' style='color: #4e02a1'>gether<span class='tooltip'>Token: 'gether'<br>Causal Loss: 1.27031<br>---------------<br>0.28 : 'gether'<br>0.05 : ' the'<br>0.04 : ' her'<br>0.04 : 've'<br>0.04 : ' was'<br>0.03 : 'ke'<br>0.02 : 'bby'<br>0.02 : 'nd'<br>0.01 : 'd'<br>0.01 : ''s'<br></span></span><span class='token' style='color: #130689'>,<span class='tooltip'>Token: ','<br>Causal Loss: 0.10176<br>---------------<br>0.90 : ','<br>0.07 : ' they'<br>0.00 : ' she'<br>0.00 : ' and'<br>0.00 : ' Lily'<br>0.00 : '.'<br>0.00 : ' the'<br>0.00 : ' when'<br>0.00 : ' playing'<br>0.00 : ' that'<br></span></span><span class='token' style='color: #18068b'>&nbsp;they<span class='tooltip'>Token: ' they'<br>Causal Loss: 0.18258<br>---------------<br>0.83 : ' they'<br>0.04 : ' Lily'<br>0.02 : ' the'<br>0.01 : ' she'<br>0.01 : ' Mom'<br>0.01 : ' Lucy'<br>0.00 : ' mom'<br>0.00 : ' mommy'<br>0.00 : ' when'<br>0.00 : ' their'<br></span></span><span class='token' style='color: #e56b5c'>&nbsp;shared<span class='tooltip'>Token: ' shared'<br>Causal Loss: 5.96823<br>---------------<br>0.11 : ' went'<br>0.07 : ' played'<br>0.06 : ' put'<br>0.05 : ' found'<br>0.04 : ' all'<br>0.03 : ' saw'<br>0.02 : ' started'<br>0.02 : ' got'<br>0.02 : ' b'<br>0.02 : ' made'<br></span></span><span class='token' style='color: #49029f'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 1.17595<br>---------------<br>0.31 : ' the'<br>0.24 : ' a'<br>0.09 : ' some'<br>0.09 : ' their'<br>0.03 : ' her'<br>0.03 : ' it'<br>0.02 : ' many'<br>0.02 : ' together'<br>0.01 : ' and'<br>0.01 : ' lots'<br></span></span><span class='token' style='color: #df6262'>&nbsp;need<span class='tooltip'>Token: ' need'<br>Causal Loss: 5.70296<br>---------------<br>0.03 : ' new'<br>0.02 : ' food'<br>0.02 : ' sh'<br>0.02 : ' sp'<br>0.01 : ' big'<br>0.01 : ' bath'<br>0.01 : ' cl'<br>0.01 : ' sand'<br>0.01 : ' b'<br>0.01 : ' t'<br></span></span><span class='token' style='color: #130689'>le<span class='tooltip'>Token: 'le'<br>Causal Loss: 0.09588<br>---------------<br>0.91 : 'le'<br>0.02 : 's'<br>0.02 : 'les'<br>0.01 : ' to'<br>0.01 : 'sh'<br>0.00 : '.'<br>0.00 : 'er'<br>0.00 : ' and'<br>0.00 : 'ed'<br>0.00 : ' of'<br></span></span><span class='token' style='color: #5f00a6'>&nbsp;and<span class='tooltip'>Token: ' and'<br>Causal Loss: 1.67131<br>---------------<br>0.19 : ' and'<br>0.08 : 'br'<br>0.07 : ','<br>0.07 : ' of'<br>0.04 : ' together'<br>0.03 : '.'<br>0.03 : ' with'<br>0.03 : 't'<br>0.02 : ' to'<br>0.02 : ' was'<br></span></span><span class='token' style='color: #fdb32e'>&nbsp;se<span class='tooltip'>Token: ' se'<br>Causal Loss: 8.01531<br>---------------<br>0.10 : ' the'<br>0.09 : ' went'<br>0.05 : ' they'<br>0.05 : ' played'<br>0.03 : ' started'<br>0.03 : ' found'<br>0.03 : ' her'<br>0.02 : ' Lily'<br>0.02 : ' put'<br>0.02 : ' it'<br></span></span><span class='token' style='color: #8506a6'>w<span class='tooltip'>Token: 'w'<br>Causal Loss: 2.60008<br>---------------<br>0.16 : 'ed'<br>0.14 : 'll'<br>0.14 : 'at'<br>0.12 : 'p'<br>0.07 : 'w'<br>0.06 : 'as'<br>0.06 : 'ag'<br>0.05 : 'al'<br>0.03 : 'ven'<br>0.01 : 'f'<br></span></span><span class='token' style='color: #7b02a8'>ed<span class='tooltip'>Token: 'ed'<br>Causal Loss: 2.36822<br>---------------<br>0.27 : 's'<br>0.13 : 'as'<br>0.09 : 'ed'<br>0.06 : 'el'<br>0.04 : 'or'<br>0.04 : 'al'<br>0.03 : 'led'<br>0.03 : 'ich'<br>0.03 : 'eet'<br>0.02 : 'ards'<br></span></span><span class='token' style='color: #8908a5'>&nbsp;the<span class='tooltip'>Token: ' the'<br>Causal Loss: 2.72239<br>---------------<br>0.34 : 's'<br>0.07 : ' the'<br>0.07 : ' it'<br>0.04 : ' them'<br>0.03 : '.'<br>0.03 : 'al'<br>0.02 : ' to'<br>0.02 : 'ic'<br>0.02 : 'ient'<br>0.02 : ' and'<br></span></span><span class='token' style='color: #ef7e4e'>&nbsp;butt<span class='tooltip'>Token: ' butt'<br>Causal Loss: 6.59554<br>---------------<br>0.02 : ' sh'<br>0.02 : ' sp'<br>0.02 : ' b'<br>0.02 : ' table'<br>0.02 : ' food'<br>0.01 : ' so'<br>0.01 : ' big'<br>0.01 : ' cl'<br>0.01 : ' show'<br>0.01 : ' bath'<br></span></span><span class='token' style='color: #18068b'>on<span class='tooltip'>Token: 'on'<br>Causal Loss: 0.19187<br>---------------<br>0.83 : 'on'<br>0.17 : 'ons'<br>0.00 : 'f'<br>0.00 : 's'<br>0.00 : 'ing'<br>0.00 : 'le'<br>0.00 : 'ers'<br>0.00 : 'k'<br>0.00 : 'p'<br>0.00 : 'iss'<br></span></span><span class='token' style='color: #cc4876'>&nbsp;on<span class='tooltip'>Token: ' on'<br>Causal Loss: 4.85936<br>---------------<br>0.29 : '.'<br>0.17 : ' and'<br>0.09 : ','<br>0.03 : ' to'<br>0.03 : 'es'<br>0.03 : ' with'<br>0.02 : 't'<br>0.02 : ' was'<br>0.02 : ' together'<br>0.02 : ' for'<br></span></span><span class='token' style='color: #c6417c'>&nbsp;Lily<span class='tooltip'>Token: ' Lily'<br>Causal Loss: 4.63271<br>---------------<br>0.51 : ' the'<br>0.12 : ' her'<br>0.07 : ' it'<br>0.06 : ' a'<br>0.02 : ' their'<br>0.02 : 't'<br>0.02 : ','<br>0.01 : '.'<br>0.01 : ' top'<br>0.01 : ' Lily'<br></span></span><span class='token' style='color: #42039d'>'s<span class='tooltip'>Token: ''s'<br>Causal Loss: 0.99527<br>---------------<br>0.37 : ''s'<br>0.25 : '.'<br>0.09 : ' and'<br>0.02 : ' felt'<br>0.01 : ','<br>0.01 : ' to'<br>0.01 : ' was'<br>0.01 : ' with'<br>0.01 : ' started'<br>0.01 : ' in'<br></span></span><span class='token' style='color: #d7566c'>&nbsp;sh<span class='tooltip'>Token: ' sh'<br>Causal Loss: 5.32037<br>---------------<br>0.14 : ' face'<br>0.03 : ' le'<br>0.03 : ' mom'<br>0.02 : ' head'<br>0.02 : ' te'<br>0.02 : ' hand'<br>0.02 : ' arm'<br>0.02 : ' eyes'<br>0.02 : ' c'<br>0.02 : ' hair'<br></span></span><span class='token' style='color: #290593'>ir<span class='tooltip'>Token: 'ir'<br>Causal Loss: 0.46835<br>---------------<br>0.63 : 'ir'<br>0.05 : 'ore'<br>0.04 : 'a'<br>0.03 : 'aring'<br>0.02 : 'ar'<br>0.02 : 'r'<br>0.02 : 'ip'<br>0.02 : 'ocked'<br>0.01 : 'ield'<br>0.01 : 'ape'<br></span></span><span class='token' style='color: #1b068c'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 0.22291<br>---------------<br>0.80 : 't'<br>0.10 : 'p'<br>0.05 : 'c'<br>0.01 : 'cle'<br>0.00 : 'a'<br>0.00 : 'on'<br>0.00 : 'ate'<br>0.00 : 's'<br>0.00 : 'r'<br>0.00 : ' to'<br></span></span><span class='token' style='color: #6400a7'>.<span class='tooltip'>Token: '.'<br>Causal Loss: 1.79054<br>---------------<br>0.29 : 'o'<br>0.17 : '.'<br>0.12 : ' and'<br>0.05 : ' of'<br>0.04 : ' with'<br>0.03 : 'ain'<br>0.02 : ','<br>0.01 : 'ion'<br>0.01 : 'ime'<br>0.01 : 'on'<br></span></span><span class='token' style='color: #cb4777'>&nbsp;It<span class='tooltip'>Token: ' It'<br>Causal Loss: 4.83915<br>---------------<br>0.50 : ' Lily'<br>0.23 : ' They'<br>0.09 : ' She'<br>0.03 : ' The'<br>0.02 : ' \"'<br>0.01 : ' When'<br>0.01 : '\\n'<br>0.01 : ' It'<br>0.01 : ' But'<br>0.01 : ' Her'<br></span></span><span class='token' style='color: #330497'>&nbsp;was<span class='tooltip'>Token: ' was'<br>Causal Loss: 0.68715<br>---------------<br>0.50 : ' was'<br>0.09 : ' made'<br>0.03 : ' had'<br>0.03 : ' looked'<br>0.02 : ' felt'<br>0.02 : ' said'<br>0.01 : ''s'<br>0.01 : ' started'<br>0.01 : ' went'<br>0.01 : ' broke'<br></span></span><span class='token' style='color: #b83289'>&nbsp;not<span class='tooltip'>Token: ' not'<br>Causal Loss: 4.10292<br>---------------<br>0.20 : ' a'<br>0.13 : ' so'<br>0.07 : ' very'<br>0.06 : ' fun'<br>0.03 : ' the'<br>0.02 : ' big'<br>0.02 : ' shiny'<br>0.02 : ' yummy'<br>0.02 : ' happy'<br>0.02 : ' not'<br></span></span><span class='token' style='color: #fca934'>&nbsp;dif<span class='tooltip'>Token: ' dif'<br>Causal Loss: 7.74907<br>---------------<br>0.18 : ' happy'<br>0.10 : ' a'<br>0.05 : ' so'<br>0.04 : ' to'<br>0.04 : ' very'<br>0.03 : ' nice'<br>0.03 : ' good'<br>0.02 : ' fun'<br>0.02 : 'hing'<br>0.02 : ' scared'<br></span></span><span class='token' style='color: #100787'>f<span class='tooltip'>Token: 'f'<br>Causal Loss: 0.06103<br>---------------<br>0.94 : 'f'<br>0.02 : 'w'<br>0.01 : 'st'<br>0.00 : ' long'<br>0.00 : 's'<br>0.00 : 'b'<br>0.00 : ' big'<br>0.00 : 't'<br>0.00 : ' many'<br>0.00 : ' to'<br></span></span><span class='token' style='color: #100787'>ic<span class='tooltip'>Token: 'ic'<br>Causal Loss: 0.06784<br>---------------<br>0.93 : 'ic'<br>0.01 : 'ul'<br>0.00 : 'ut'<br>0.00 : 'ive'<br>0.00 : 'ie'<br>0.00 : '.'<br>0.00 : 'fe'<br>0.00 : 'ish'<br>0.00 : '.\"'<br>0.00 : 'er'<br></span></span><span class='token' style='color: #15068a'>ul<span class='tooltip'>Token: 'ul'<br>Causal Loss: 0.13553<br>---------------<br>0.87 : 'ul'<br>0.06 : 't'<br>0.01 : '.'<br>0.01 : 'ut'<br>0.00 : ' for'<br>0.00 : 'er'<br>0.00 : ' to'<br>0.00 : 'ic'<br>0.00 : ','<br>0.00 : 'hes'<br></span></span><span class='token' style='color: #0c0786'>t<span class='tooltip'>Token: 't'<br>Causal Loss: 0.01673<br>---------------<br>0.98 : 't'<br>0.01 : 'ts'<br>0.00 : 'ce'<br>0.00 : 'ie'<br>0.00 : 'ic'<br>0.00 : 'i'<br>0.00 : 'ut'<br>0.00 : 's'<br>0.00 : 'v'<br>0.00 : 'ad'<br></span></span><span class='token' style='color: #6800a7'>&nbsp;for<span class='tooltip'>Token: ' for'<br>Causal Loss: 1.88940<br>---------------<br>0.15 : ' for'<br>0.13 : '.'<br>0.09 : ' to'<br>0.07 : '.\"'<br>0.07 : ' and'<br>0.07 : ','<br>0.04 : ' of'<br>0.03 : '!\"'<br>0.02 : ' at'<br>0.02 : 't'<br></span></span><span class='token' style='color: #b93388'>&nbsp;them<span class='tooltip'>Token: ' them'<br>Causal Loss: 4.16717<br>---------------<br>0.24 : ' Lily'<br>0.16 : ' her'<br>0.07 : ' the'<br>0.05 : ' being'<br>0.02 : ' you'<br>0.02 : 't'<br>0.02 : ' it'<br>0.02 : ' dinner'<br>0.02 : ' them'<br>0.01 : ' making'<br></span></span><span class='token' style='color: #fcc726'>&nbsp;b<span class='tooltip'>Token: ' b'<br>Causal Loss: 8.49897<br>---------------<br>0.39 : '.'<br>0.13 : ' to'<br>0.10 : ' and'<br>0.05 : ','<br>0.03 : ' for'<br>0.03 : '.\"'<br>0.02 : ' with'<br>0.02 : ' all'<br>0.01 : ' together'<br>0.01 : ' that'<br></span></span><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tutorial_code.inference import show_predictions\n",
    "\n",
    "show_predictions(model, tokenizer, device=\"cuda\", text=[sample_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db6ded-ea4d-4f29-9832-9b357bd3da39",
   "metadata": {},
   "source": [
    "### Simple Text Gen\n",
    "This is a very simple text generator implementation.\n",
    "\n",
    "[tutorial_code.textgen](../tutorial_code/textgen.py)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e791db3-b315-46f9-900a-3cc770b3717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|BOS|> One day, a little girl named Sue was feeling sad. She Yesternessi, Sue! She played together in the sunglass She learned to take her friends. She always remembered how she got to go home and play together.\\n\\nOne day,'\n"
     ]
    }
   ],
   "source": [
    "from tutorial_code.textgen import TextGenerator\n",
    "\n",
    "# Test text generation.\n",
    "# Don't expect too much from this model, as the only input to each prediction is the previous word. \n",
    "text_gen = TextGenerator(model, tokenizer, config.device, do_sample=True, seed=42)\n",
    "text = text_gen.prompt(\"One day, a little girl\", max_new_tokens=50)\n",
    "print(repr(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0815e-0f1b-424f-b352-24643e94ef0e",
   "metadata": {},
   "source": [
    "## Huggingface Text Gen\n",
    "This is a simple wrapper for the Huggingface tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d72484-6eb0-495e-96d2-81aab3cfb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/v4.34.1/en/generation_strategies\n",
    "\n",
    "class TextGen():\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def generate(self, prompt, do_sample=True, top_k=50, top_p=0.9, max_new_tokens=500):\n",
    "        self.model.to(self.device)\n",
    "        input_ids = self.tokenizer(prompt, return_tensors='pt')['input_ids'].to(self.device)\n",
    "        outputs = model.generate(input_ids, do_sample=do_sample, top_k=top_k, top_p=top_p, max_new_tokens=max_new_tokens)\n",
    "        return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14751a31-949d-4ed1-9532-d030bc431f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them burn. Lily was so proud of Lily and thanked her for their help. From that day on, she was careful not to try new things she had to play together. Lily learned ach and that it's important to share and that they needed to make it better.\"\n",
      "\n",
      "From that day on, Lily played with the next day. She loved to play with the on her friends on her bed. It was very pretty and pretty. And always made sure to share her new shirt, she loved to enjoy. The end. Lily's face felt warm and happy to have fun in the world. And she always wanted to be nice and share it with her mom. They shared her food with a big, and the two of the friendshood and the fun she would always help when we can all the fun together. And they learned that sharing is a good friend. They made a new friend. They are the best of friends and the fun together. The end. Lily and the best of friends were happy again. They learned that sharing is good and pretty things can be together. From that day on, they learned that sharing is more important. The end.tain that when you are always best friends, Lily and the best time ever! They were all the fun they learned that sharing is pretty and the best. They learned that being happy sometimes, even when it is good and not spicy. And they are very happy when the day, they are happy, they can be fun and always try to remember kind to always be kind to share, organizzy and be kind to be kind and kind for her. And sometimes, we can be happy to be kind and warm. From that day on, and the best of friends ever afternes, and bellis again. They learned that it is better to share, and be kind of friends than awray with happiness and make them feel less as new. And that don't think can always be kind, if you like them.\" Lily agreed and her friendship more. They had something new. And they also learned that even what you like to listen to yours. They never to make the again, always remember that new worse of others. They all lived happily together and friends. From that day, the best of the end of them and enjoyed the apeth and shared the right decisonations. And from her kindn\n"
     ]
    }
   ],
   "source": [
    "gen = TextGen(model, tokenizer, device=config.device)\n",
    "print(gen.generate(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31589f-175a-429e-a718-2aece193ad38",
   "metadata": {},
   "source": [
    "## Save and Load model\n",
    "Should you want to save and restore a model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca7bbe-35b9-4223-a685-d63e637c9383",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03aff0cc-417f-40d5-98c6-248fa7e7626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\n",
    "    save_directory=config.model_path,\n",
    "    safe_serialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792a35a-4fbe-47f1-97cc-6eae834203f0",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b34faf-67d1-4f55-84b0-89be6438b452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'missing_keys': [], 'unexpected_keys': [], 'mismatched_keys': [], 'error_msgs': []}\n",
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 128)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (key_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (value_linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=128, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, load_info = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_path,\n",
    "    output_loading_info=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "print(load_info)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e7716-caf4-40a9-8786-fd45b9e1caf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
