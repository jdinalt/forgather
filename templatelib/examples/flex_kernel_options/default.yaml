    # https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.FlexKernelOptions
    # https://github.com/pytorch/pytorch/issues/133254
.define: &flex_attn_kernel_options !dict
    BLOCK_M: 32
    BLOCK_N: 32
    BLOCK_M1: 16
    BLOCK_N1: 32
    BLOCK_M2: 32
    BLOCK_N2: 16

