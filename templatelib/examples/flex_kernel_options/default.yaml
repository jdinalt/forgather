    # On PyTorch < 2.9, the default Flex Attention kernel options do not work with
    # the RTX30xx and RTX40xx. Manually setting these options is a work-around.
    # https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.FlexKernelOptions
    # https://github.com/pytorch/pytorch/issues/133254
.define: &flex_attn_kernel_options !dict
    BLOCK_M: 32
    BLOCK_N: 32
    BLOCK_M1: 16
    BLOCK_N1: 32
    BLOCK_M2: 32
    BLOCK_N2: 16

