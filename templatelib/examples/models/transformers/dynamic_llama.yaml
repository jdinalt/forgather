-- extends 'models/causal_lm/custom_dynamic.yaml'

[model_meta_config]
    == super()
    -- set ns.model_name = "Dynamic Llama"
    -- set ns.model_description = "A Llama compatible dynamic model."
    -- set ns.model_short_name = "dynllama"

[model_tokenizer]
    ## Default tokenizer

[model_bits]
    [llama_bits]
## Override to add more bits to the base model
    [loss_fn]
.define: &loss_fn !call:.causal_loss:CausalLoss@loss_fn

    [layer_norm_factory]
.define: &layer_norm_factory !partial:torch.nn:RMSNorm@layer_norm_factory
    normalized_shape: !var "hidden_size"
    eps: !var "rms_norm_eps"

    [feedforward_factory]
.define: &feedforward_factory !partial:.glu_feedforward:GLUFeedforwardLayer@feedforward_factory
    d_model: !var "hidden_size"
    d_feedforward: !var "intermediate_size"
    activation_factory: !partial:torch.nn.SiLU
    dropout: !var "activation_dropout"

    [rel_positional_encoder]
.define: &relative_pe !call:.rotary_embeddings:RotaryPE@relative_pe
    hidden_size: !var "hidden_size"
    num_attention_heads: !var "num_attention_heads"
    max_sequence_length: !var "max_position_embeddings"
    rope_parameters: !var "rope_parameters"

    [qk_norm_factory]
.define: &qk_norm_factory null

    [flex_attn_kernel_options]
    # The default kernel options don't work properly with some GPUs at present
    -- include "flex_kernel_options/default.yaml"

    [attn_functions]
.define: &attn_functions !dict
    # Interfaces conforming to https://huggingface.co/docs/transformers/main/attention_interface
    # Fallback to using ALL_ATTENTION_FUNCTIONS, if not in dict
    eager: !partial:.attention_interface:eager_attention_forward
    flex_attention: !partial:.attention_interface:flex_attention_forward
        kernel_options: !var "flex_attn_kernel_options"

    [attention_factory]
.define: &attention_factory !partial:.causal_multihead_attn:CausalMultiheadAttn@attention_factory
    d_model: !var "hidden_size"
    num_heads: !var "num_attention_heads"
    num_kv_heads: !var "num_key_value_heads"
    dropout: !var "attention_dropout"
    pos_encoder: *relative_pe
    attn_implementation: !var "attn_implementation" # Value from &model_constructor_args
    attn_functions: *attn_functions
    qk_norm_factory: *qk_norm_factory
    sliding_window: !var "sliding_window"
    # Needed for vLLM attention interface
    config: !var "config"

    [layer_factory]
.define: &layer_factory !partial:.pre_ln_layer:PreLNLayer@layer_factory
    feedforward_factory: *feedforward_factory
    attention_factory: *attention_factory
    norm_factory: *layer_norm_factory
    dropout: !var "layer_dropout"
    residual_dropout: !var "residual_dropout"

    [layer_stack]
.define: &layer_stack !call:.checkpoint_layer_stack:LayerStack@layer_stack
    layer_factory: *layer_factory
    num_hidden_layers: !var "num_hidden_layers"
    post_norm_factory: *layer_norm_factory
    enable_checkpoint: !var "enable_activation_checkpoint"
    checkpoint_stride: !var "checkpoint_stride"

    [output_decoder]
.define: &output_decoder !partial:torch.nn:Linear@output_decoder
    in_features: !var "hidden_size"
    out_features: !var "vocab_size"
    bias: False

    [abs_positional_encoder]
.define: &absolute_pe null

    [embedding]
.define: &embedding !call:torch.nn.Embedding
    num_embeddings: !var "vocab_size"
    embedding_dim: !var "hidden_size"
    padding_idx: !var "pad_token_id"

    [input_encoder]
.define: &input_encoder !call:.input_encoder:InputEncoder@input_encoder
    d_model: !var "hidden_size"
    embedding: *embedding
    dropout: !var "embedding_dropout"
    positional_encoder: *absolute_pe

    [init_weights]
        [init_regex_list]
.define: &init_regex_list !dlist
    # Note: Yaml treats single and double quotes differently WRT escapes. Use single
    # quotes for regex expressions, which prevents Yaml from interpreting escapes.
    # For a literal ' use ''
    zeros: 
        - 'bias'
        - !partial:torch.nn.init:zeros_
    trunc_normal_magic:
        - 'ff.up_proj.weight|attn.query.weight|attn.key.weight|attn.value.weight'
        - !partial:.llama_init:trunc_normal_magic
    trunc_normal:
        - 'ff.gate_proj.weight|ff.down_proj.weight|attn.output.weight'
        - !partial:.llama_init:trunc_normal
            std: !call:.llama_init:llama_std [ !var "num_hidden_layers" ]
    lm_head:
        - 'lm_head.weight'
        - !partial:.llama_init:init_output_layer
            d_model: !var "hidden_size"

        [init_function]
# Init method based upon https://github.com/pytorch/torchtitan/blob/main/torchtitan/models/llama3/model/model.py
.define: &init_weights !partial:.init_weights:init_weights_by_regex@init_weights
    regex_list: *init_regex_list
    
    # Print how each param is being initialized.
    debug: False

    [attn_mask_fn]
.define: &attn_mask_fn !partial:.causal_mask:causal_mask@attn_mask_fn

    [model_factory]
model_factory: &model_factory !dict@model_factory
    causal_model: !partial:.causal_lm:CasualLM
        config: !var "config"
        input_encoder: *input_encoder
        layer_stack: *layer_stack
        init_weights: *init_weights
        attn_mask_fn: *attn_mask_fn
    lm_head: *output_decoder
    loss_fn: *loss_fn

[model_config]
    == super()
    hidden_size: {{ hidden_size | toyaml(4096) }}
    num_attention_heads: {{ num_attention_heads | toyaml(32) }}
    # Default to MHA when null
    num_key_value_heads: {{ num_key_value_heads | toyaml(None) }}
    num_hidden_layers: {{ num_hidden_layers | toyaml(32) }}
    max_position_embeddings: !call:getattr
        - *tokenizer
        - "model_max_length"
    intermediate_size: {{ intermediate_size | toyaml(11008) }}
    rope_parameters: {{ rope_parameters | toyaml({'rope_theta': 10000.0}) }}
    embedding_dropout: {{ embedding_dropout | toyaml(0.0) }}
    rms_norm_eps: {{ rms_norm_eps | toyaml(1.0e-05) }}
    layer_dropout: {{ layer_dropout | toyaml(0.0) }}
    residual_dropout: {{ residual_dropout | toyaml(0.0) }}
    attention_dropout: {{ attention_dropout | toyaml(0.0) }}
    activation_dropout: {{ activation_dropout | toyaml(0.0) }}
    enable_activation_checkpoint: False
    checkpoint_stride: {{ checkpoint_stride | toyaml(1) }}
    tie_word_embeddings: {{ tie_word_embeddings | toyaml(False) }}
    sliding_window: {{ sliding_window | toyaml(None) }}
    flex_attn_kernel_options: *flex_attn_kernel_options
    
[model_code_generator]
    == super()
    # Dynamic Llama
    supports_gradient_checkpointing: True
    supports_sdpa: True
    supports_flex_attn: True
    supports_flash_attn: True
    supports_attention_backend: True
    tied_weights_keys: {causal_lm.input_encoder.embedding.weight: "lm_head.weight"}
    no_split_modules: ["PreLNLayer"]
    
    # vLLM Support
    base_model_tp_plan:
        "causal_lm.layer_stack.layers.*.attention.query_linear": "colwise"
        "causal_lm.layer_stack.layers.*.attention.key_linear": "colwise"
        "causal_lm.layer_stack.layers.*.attention.value_linear": "colwise"
        "causal_lm.layer_stack.layers.*.attention.output_linear": "rowwise"
        "causal_lm.layer_stack.layers.*.feedforward.gate_proj": "colwise"
        "causal_lm.layer_stack.layers.*.feedforward.up_proj": "colwise"
        "causal_lm.layer_stack.layers.*.feedforward.down_proj": "rowwise"
    
    base_model_pp_plan:
        "causal_lm.input_encoder":
            - ["input_ids"]
            - ["hidden_states"]
        "causal_lm.layer_stack.layers":
            - ["hidden_states", "attention_mask"]
            - ["hidden_states"]
        "causal_lm.layer_stack.layer_norm":
            - ["hidden_states"]
            - ["hidden_states"]