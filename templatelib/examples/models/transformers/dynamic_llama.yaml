-- extends 'models/causal_lm/custom_dynamic.yaml'

[model_meta_config]
    == super()
    -- set ns.model_name = "Dynamic Llama"
    -- set ns.model_description = "A Llama compatible dynamic model."
    -- set ns.model_short_name = "dynllama"

[model_tokenizer]
    ## Default tokenizer

[model_bits]
    [loss_fn]
loss_fn: &loss_fn !singleton:.causal_loss:CausalLoss@loss_fn []

    [layer_norm_factory]
layer_norm_factory: &layer_norm_factory !partial:torch.nn:RMSNorm@layer_norm_factory
    normalized_shape: !var "hidden_size"
    eps: !var "rms_norm_eps"

    [feedforward_factory]
feedforward_factory: &feedforward_factory !partial:.glu_feedforward:GLUFeedforwardLayer@feedforward_factory
    d_model: !var "hidden_size"
    d_feedforward: !var "dim_feedforward"
    activation_factory: !partial:torch.nn.SiLU []
    dropout: !var "activation_dropout"

    [rel_positional_encoder]
relative_pe: &relative_pe !singleton:.real_rotary_embeddings:RealRotaryPE@relative_pe
    d_head: !var "d_head"
    max_sequence_length: !var "max_sequence_length"
    rope_theta: !var "rope_theta"

    [attn_functions]
.define: &attn_functions !dict
    ## Interfaces conforming to https://huggingface.co/docs/transformers/main/attention_interface
    ## Fallback to using ALL_ATTENTION_FUNCTIONS, if not in dict
    eager: !partial:.attention_interface:eager_attention_forward
    flex_attention: !partial:.attention_interface:flex_attention_forward
        kernel_options: !var "flex_attn_kernel_options"
        compile_flex: !var "compile_flex"
    sdpa: !partial:.attention_interface:sdpa_attention_forward
        is_causal: True

    [flex_attn_kernel_options]
    # https://docs.pytorch.org/docs/stable/nn.attention.flex_attention.html#torch.nn.attention.flex_attention.FlexKernelOptions
    # https://github.com/pytorch/pytorch/issues/133254
.define: &flex_attn_kernel_options !dict
    BLOCK_M: 32
    BLOCK_N: 32
    BLOCK_M1: 16
    BLOCK_N1: 32
    BLOCK_M2: 32
    BLOCK_N2: 16

    [attention_factory]
attention_factory: &attention_factory !partial:.causal_rpe_attn:CausalRpeAttn@attention_factory
    d_model: !var "hidden_size"
    num_heads: !var "num_attention_heads"
    num_kv_heads: !var "num_kv_heads"
    dropout: !var "attention_dropout"
    bias: False
    pos_encoder: *relative_pe
    attn_implementation: !var "attn_implementation" # Value from &model_constructor_args
    attn_functions: *attn_functions

    [layer_factory]
layer_factory: &layer_factory !partial:.pre_ln_layer:PreLNLayer@layer_factory
    feedforward_factory: *feedforward_factory
    attention_factory: *attention_factory
    norm_factory: *layer_norm_factory
    dropout: !var "layer_dropout"
    residual_dropout: !var "residual_dropout"

    [layer_stack]
layer_stack: &layer_stack !factory:.checkpoint_layer_stack:LayerStack@layer_stack
    layer_factory: *layer_factory
    num_hidden_layers: !var "num_hidden_layers"
    post_norm_factory: *layer_norm_factory
    enable_checkpoint: !var "enable_activation_checkpoint"
    checkpoint_stride: !var "checkpoint_stride"

    [output_decoder]
output_decoder: &output_decoder !factory:torch.nn:Linear@output_decoder
    in_features: !var "hidden_size"
    out_features: !var "vocab_size"
    bias: False

    [abs_positional_encoder]
absolute_pe: &absolute_pe null

    [input_encoder]
input_encoder: &input_encoder !factory:.input_encoder:InputEncoder@input_encoder
    d_model: !var "hidden_size"
    vocab_size: !var "vocab_size"
    dropout: !var "embedding_dropout"
    positional_encoder: *absolute_pe
    scale_sqrt_d_model: False

    [init_weights]
# Init method based upon https://github.com/pytorch/torchtitan/blob/main/torchtitan/models/llama3/model/model.py
init_weights: &init_weights !partial:.init_weights:init_weights_by_regex@init_weights
    # Note: Yaml treats single and double quotes differently WRT escapes. Use single
    # quotes for regex expressions, wihc prevents Yaml from interpreting escapes.
    # For a literal ' use ''
    regex_list:
        - [ 'norm', "pass" ]
        - [ 'bias', "zeros" ]
        - [ 'embedding\.weight', "init_embeddings" ]
        - [ 'up_proj|query_linear|key_linear|value_linear', "trunc_normal_magic" ]
        - [ 'gate_proj|down_proj|output_linear', "trunc_normal" ]
        - [ 'output_decoder', "init_output_layer" ]
    init_f_map:
        pass: !partial:.init_weights:init_pass
        zeros: !partial:torch.nn.init:zeros_ []
        init_embeddings: !partial:.llama_init:init_embeddings []
        trunc_normal_magic: !partial:.llama_init:trunc_normal_magic []
        trunc_normal: !partial:.llama_init:trunc_normal
            std: !call:.llama_init:llama_std [ !var "num_hidden_layers" ]
        init_output_layer: !partial:.llama_init:init_output_layer { d_model: !var "hidden_size" }
    # Print how each param is being initialized.
    debug: False

    [model_factory]
model_factory: &model_factory !factory:.causal_lm:CasualLM@model_factory
    loss_fn: *loss_fn
    input_encoder: *input_encoder
    output_decoder: *output_decoder
    layer_stack: *layer_stack
    init_weights: *init_weights

[model_config]
    == super()
    hidden_size: 4096
    num_attention_heads: 32
    # Default to MHA when null
    num_kv_heads: null
    d_head: 128 # Must be hidden_size // num_attention_heads
    num_hidden_layers: 32
    max_sequence_length: !singleton:getattr
        - *tokenizer
        - "model_max_length"
    dim_feedforward: 11008
    rope_theta: 10000.0
    embedding_dropout: 0.0
    rms_norm_eps: 1.0e-05
    layer_dropout: 0.0
    residual_dropout: 0.0
    attention_dropout: 0.0
    activation_dropout: 0.0
    enable_activation_checkpoint: False
    checkpoint_stride: 1
    compile_flex: True # For debug flex, set to False
    flex_attn_kernel_options: *flex_attn_kernel_options

[model_code_generator]
    == super()
    # Dynamic Llama
    supports_gradient_checkpointing: True
    supports_sdpa: True
    supports_flex_attn: True
    supports_flash_attn: True