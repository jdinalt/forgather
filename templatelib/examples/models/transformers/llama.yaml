-- extends "models/causal_lm/from_config.yaml"

[model_meta_config]
    == super()
    -- set ns.model_name = "Llama"
    -- set ns.model_description = "A newly initialized HF Llama model"
    -- set ns.model_source = ""
    -- set ns.model_config_cls = 'transformers:LlamaConfig'

[model_tokenizer]
## Select a 'llama-2' tokenizer
tokenizer: &tokenizer !call:transformers:AutoTokenizer.from_pretrained@tokenizer
    arg0: "TheBloke/Llama-2-7B-GPTQ"
    legacy: False
    model_max_length: 4096

[model_config]
# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/configuration_llama.py
    == super()
    max_position_embeddings: !singleton:getattr [ *tokenizer, 'model_max_length' ]
    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]
    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]
    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]