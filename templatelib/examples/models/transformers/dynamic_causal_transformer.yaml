-- extends 'models/causal_lm/custom_dynamic.yaml'

[model_meta_config]
    == super()
    -- set ns.model_name = "Dynamic Causal Transformer"
    -- set ns.model_description = "A vanilla causal transformer model."
    -- set ns.model_short_name = "dynamic_causal_transformer"

[model_tokenizer]
    ## Default tokenizer

## These are all modular 'bits' which all can be swapped out with other
## modules through the configuration, provided they share a compatible
## interface with the other modules.
##
## This effectively makes the implementation of each component a
## configurable model parameter.
##
## The default arguments are intended closely mirror the original 
## transformer archetecture, Attention is All You Need, obviously
## excepting this being a causal LM.
[model_bits]
    [loss_fn]
loss_fn: &loss_fn !singleton:.causal_loss:CausalLoss@loss_fn []

    [layer_norm_factory]
layer_norm_factory: &layer_norm_factory !partial:torch.nn:LayerNorm@layer_norm_factory
    normalized_shape: !var "hidden_size"

    [feedforward_factory]
feedforward_factory: &feedforward_factory !partial:.feedforward_layer:FeedforwardLayer@feedforward_factory
    d_model: !var "hidden_size"
    d_feedforward: !var "dim_feedforward"
    dropout: !var "activation_dropout"

    [rel_positional_encoder]
## Undefined
    
    [attention_factory]
attention_factory: &attention_factory !partial:.causal_multihead_attn:CausalMultiheadAttn@attention_factory
    d_model: !var "hidden_size"
    num_heads: !var "num_attention_heads"
    dropout: !var "attention_dropout"

    [layer_factory]
layer_factory: &layer_factory !partial:.post_ln_layer:PostLNLayer@layer_factory
    feedforward_factory: *feedforward_factory
    attention_factory: *attention_factory
    norm_factory: *layer_norm_factory
    dropout: !var "layer_dropout"
    residual_dropout: !var "residual_dropout"
    
    [layer_stack]
layer_stack: &layer_stack !factory:.checkpoint_layer_stack:LayerStack@layer_stack
    layer_factory: *layer_factory
    num_hidden_layers: !var "num_hidden_layers"
    enable_checkpoint: !var "enable_activation_checkpoint"
    checkpoint_stride: !var "checkpoint_stride"

    [output_decoder]
output_decoder: &output_decoder !factory:torch.nn:Linear@output_decoder
    - !var "hidden_size"
    - !var "vocab_size"

    [positional_encoder]
positional_encoder: &positional_encoder !factory:.sinusoidal_pe:SinusoidalPE@positional_encoder
    d_model: !var "hidden_size"
    max_sequence_length: !var "max_sequence_length"

    [input_encoder]
input_encoder: &input_encoder !factory:.input_encoder:InputEncoder@input_encoder
    d_model: !var "hidden_size"
    vocab_size: !var "vocab_size"
    dropout: !var "embedding_dropout"
    positional_encoder: *positional_encoder
    scale_sqrt_d_model: True

    [init_weights]
init_weights: &init_weights !partial:.init_weights:simple_weight_init@init_weights
    scale_rsqrt_d_model: True

    [model_factory]
model_factory: &model_factory !factory:.causal_lm:CasualLM@model_factory
    loss_fn: *loss_fn
    input_encoder: *input_encoder
    output_decoder: *output_decoder
    layer_stack: *layer_stack
    init_weights: *init_weights

[model_config]
    == super()
    hidden_size: 512
    num_attention_heads: 8
    num_hidden_layers: 6
    max_sequence_length: !singleton:getattr
        - *tokenizer
        - "model_max_length"
    dim_feedforward: 2048
    embedding_dropout: 0.10
    layer_dropout: 0.10
    residual_dropout: 0.0
    attention_dropout: 0.0
    activation_dropout: 0.0
    enable_activation_checkpoint: False
    checkpoint_stride: 1

[model_code_generator]
    == super()
    supports_gradient_checkpointing: True
    supports_sdpa: True