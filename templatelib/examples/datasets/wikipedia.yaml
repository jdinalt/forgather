## This is a simple example dataset configuration, with minimal preprocessing.
## It is used to demonstrate how to set up a dataset configuration in the templatelib format
# Name: Wikipedia Dataset
# Description: "Wikipedia dataset containing cleaned articles from the English Wikipedia."
# Source: https://huggingface.co/datasets/wikimedia/wikipedia

train_dataset_split: &train_dataset_split !singleton:datasets:load_dataset@train_dataset_split
    args:
        - "wikimedia/wikipedia"
        - "20231101.en"
    kwargs:
        split: "train"

-- block preprocess_args
preprocess_args: &preprocess_args
    truncation: True
<< endblock preprocess_args


train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset
    dataset: *train_dataset_split
    tokenizer: *tokenizer
    # We will reserve the first 1000 records for evaluation
    select_range: [ 1000, -1 ]
    desc: "Tokenizing train"
    fn_kwargs:
        <<: *preprocess_args
    map_kwargs:
        batch_size: 32
    # This thing is really big, so we will make it iterable, thus performing lazy tokenization
    to_iterable: true
    num_shards: 1024
    shuffle: true
    parallel_tokenizer: false
    #seed: 42


eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset
    dataset: *train_dataset_split
    tokenizer: *tokenizer
    # Use first 1000 records for evaluation
    select_range: [ 0, 1000 ]
    desc: "Tokenizing eval"
    fn_kwargs:
        <<: *preprocess_args
    parallel_tokenizer: false