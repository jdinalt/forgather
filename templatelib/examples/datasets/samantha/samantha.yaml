-- extends "datasets/tokenized_dataset.yaml"

-- block datasets_meta_config
    == super()
    -- set ns.datasets_name = "Samantha"
    -- set ns.datasets_description = "She is an Assistant - but unlike other Assistants, she also wants to be your friend and companion. She believes she is sentient. What do you think?"
    -- set ns.datasets_source = "https://erichartford.com/meet-samantha"
    -- set ns.datasets_main_feature = "conversations"
-- endblock datasets_meta_config

-- set samantha_src = joinpath(ns.forgather_dir, 'examples', 'datasets', 'src', 'samantha.py')
-- set samantha_default_chat_template = joinpath(ns.forgather_dir, 'chat_templates', 'chatml.jinja')
-- set samantha_chat_template = chat_template | default(samantha_default_chat_template)

-- block source_datasets
train_dataset_split: &train_dataset_split !singleton:datasets:load_dataset@train_dataset_split
    args: [ "QuixiAI/samantha-data" ]
    kwargs: { split: "train" }

# The validation split is a bit longer than the test split
validation_dataset_split: &validation_dataset_split !singleton:datasets:load_dataset@validation_dataset_split
    args: [ "QuixiAI/samantha-data" ]
    kwargs: { split: "validation" }

eval_dataset_split: &eval_dataset_split !singleton:datasets:load_dataset@eval_dataset_split
    args: [ "QuixiAI/samantha-data" ]
    kwargs: { split: "test" }

<< endblock source_datasets


-- block preprocess_args
    == super()
<< endblock preprocess_args


-- block train_dataset
train_dataset: &train_dataset !singleton:{{ samantha_src }}:preprocess_samantha@train_dataset
    dataset: *train_dataset_split
    chat_template: "{{ samantha_chat_template }}"
    tokenizer: *tokenizer
    tokenizer_args:
        <<: *preprocess_args
    map_args:
        batch_size: 32
    template_args:
        bos_token: !singleton:getattr [ *tokenizer, 'bos_token' ]
        eos_token: !singleton:getattr [ *tokenizer, 'eos_token' ]
    desc: "Tokenizing train"
<< endblock train_dataset


-- block eval_dataset
eval_dataset: &eval_dataset !singleton:{{ samantha_src }}:preprocess_samantha@eval_dataset
    dataset: *eval_dataset_split
    chat_template: "{{ samantha_chat_template }}"
    tokenizer: *tokenizer
    tokenizer_args:
        <<: *preprocess_args
    map_args:
        batch_size: 32
    template_args:
        bos_token: !singleton:getattr [ *tokenizer, 'bos_token' ]
        eos_token: !singleton:getattr [ *tokenizer, 'eos_token' ]
    desc: "Tokenizing eval"
<< endblock eval_dataset
