-- extends "datasets/tokenized_dataset.yaml"

-- block datasets_meta_config
    == super()
    -- set ns.datasets_name = "Tiny Stories"
    -- set ns.datasets_description = "Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary."
    -- set ns.datasets_source = "https://arxiv.org/abs/2305.07759"
-- endblock datasets_meta_config


-- block source_datasets
train_dataset_split: &train_dataset_split !singleton:datasets:load_dataset@train_dataset_split
    args: [ "roneneldan/TinyStories" ]
    kwargs: { split: "train" }

eval_dataset_split: &eval_dataset_split !singleton:datasets:load_dataset@eval_dataset_split
    args: [ "roneneldan/TinyStories" ]
    kwargs: { split: "validation" }
<< endblock source_datasets


-- block preprocess_args
    == super()
<< endblock preprocess_args


-- block train_dataset
train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset
    dataset: *train_dataset_split
    tokenizer: *tokenizer
    desc: "Tokenizing train"
    fn_kwargs:
        <<: *preprocess_args
<< endblock train_dataset


-- block eval_dataset
eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset
    dataset: *eval_dataset_split
    tokenizer: *tokenizer
    desc: "Tokenizing validation"
    fn_kwargs:
        <<: *preprocess_args
    select_range: 500
<< endblock eval_dataset
