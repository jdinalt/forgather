-- extends "datasets/tokenized_dataset.yaml"

-- block datasets_meta_config
    == super()
    -- set ns.datasets_name = "Togethercomputer RedPajama 1T-Book"
    -- set ns.datasets_description = "RPJ 1T-Book dataset."
    -- set ns.datasets_source = "https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T"
-- endblock datasets_meta_config

-- set block_tokenizer_src = joinpath(ns.forgather_dir, 'examples', 'datasets', 'src', 'block_tokenizer.py')

-- block source_datasets
# Note: Only the local one has been tested, as redownloading this beast for testing is not practical right now.
# This is /probably/ correct. If not, let me know.
train_dataset_split: &train_dataset_split !singleton:datasets:load_dataset@train_dataset_split
    args:
        - "togethercomputer/RedPajama-Data-1T"
    kwargs:
        split: "train"
<< endblock source_datasets


-- block preprocess_args
preprocess_args: &preprocess_args
    block_size: 512
    overflow: true
    stride: 0
<< endblock preprocess_args


-- block train_dataset
train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset
    dataset: *train_dataset_split
    tokenizer: *tokenizer
    map_fn: !partial:{{ block_tokenizer_src }}:block_tokenize_fn
    desc: "Tokenizing train"
    fn_kwargs:
        <<: *preprocess_args
    map_kwargs:
        batch_size: 32
    to_iterable: true
    num_shards: 1024
    shuffle: true
    parallel_tokenizer: false
    #seed: 42
<< endblock train_dataset


-- block eval_dataset
eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset
    dataset: *train_dataset_split
    tokenizer: *tokenizer
    map_fn: !partial:{{ block_tokenizer_src }}:block_tokenize_fn
    # Use first 1000 records for evaluation
    select_range: 10
    desc: "Tokenizing eval"
    fn_kwargs:
        <<: *preprocess_args
    parallel_tokenizer: false
<< endblock eval_dataset
