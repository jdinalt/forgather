-- extends 'datasets/base_datasets.yaml'

-- block datasets_meta_config
    == super()
    -- set datasets_ns.name = "Togethercomputer RedPajama 1T-Book"
    -- set datasets_ns.description = ""
    -- set datasets_ns.source = "https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T"

    ## Configured for load_from_disk method.
    ##-- set datasets_ns.train_dataset_id = "togethercomputer/RedPajama-Data-1T"
    -- set datasets_ns.train_load_method = "datasets:load_from_disk"
    -- set datasets_ns.train_dataset_id = joinpath(ns.datasets_dir, "togethercomputer-RedPajama-Data-1T-book")

    ## There is no official validation dataset. This was manually constructed taking a small random sub-sample of the main dataset.
    -- set datasets_ns.eval_load_method = "datasets:load_from_disk"
    -- set datasets_ns.eval_dataset_id = joinpath(ns.datasets_dir, "red_pajamas_books_validation")

-- endblock datasets_meta_config

-- block preprocess_args
preprocess_args: &preprocess_args
    block_size: 512
    overflow: true
    stride: 0
    
<< endblock preprocess_args


-- block tokenize_train
    == super()
    # This dataset uses entire books as rows. Block_tokenize to split these 
    # into blocks of 'block_size' tokens.
    map_fn: !lambda:forgather.ml.datasets:block_tokenize_fn

    # We definitely do not want to tokenize several hundred GBs up-front.
    # Convert to iterable to tokenize when loaded from the iterator.
    to_iterable: true

    # Break the dataset into this many shards for efficient shuffling.
    num_shards: 1024

    # The tokenizer will be running in the DataLoaders worker threads. Don't further parallelize
    # tokenization within the workers!
    parallel_tokenizer: false
    map_kwargs:
        # These records are huge! Don't process too many at a time.
        batch_size: 2

    # Shuffle, as this dataset is not very randomized 'as-is'
    shuffle: true
    seed: 42
<< endblock tokenize_train


-- block tokenize_eval
    == super()
    # Use the same pre-processor as the main dataset, but perform the pre-processing up-front.
    map_fn: !lambda:forgather.ml.datasets:block_tokenize_fn

    # Limit the number eval examples to this many 'books'
    select_range: 10
<< endblock tokenize_eval
