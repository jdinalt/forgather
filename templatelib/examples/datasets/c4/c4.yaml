-- extends "datasets/tokenized_dataset.yaml"

-- block datasets_meta_config
    -- set ns.datasets_name = "C4"
    -- set ns.datasets_description = "A colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset https://commoncrawl.org"
    -- set ns.datasets_source = "https://huggingface.co/datasets/allenai/c4"
-- endblock datasets_meta_config

-- block source_datasets
train_dataset_split: &train_dataset_split !singleton:datasets:load_dataset@train_dataset_split
    args:
        - "allenai/c4"
        - "en"
    kwargs:
        split: "train"


eval_dataset_split: &eval_dataset_split !singleton:datasets:load_dataset@eval_dataset_split
    args:
        - "allenai/c4"
        - "en"
    kwargs:
        split: "validation"
<< endblock source_datasets


-- block preprocess_args
    == super()
<< endblock preprocess_args

-- block train_dataset
train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset
    dataset: *train_dataset_split
    tokenizer: *tokenizer
    desc: "Tokenizing train"
    fn_kwargs:
        <<: *preprocess_args
    map_kwargs:
        batch_size: 32
    to_iterable: true
    num_shards: 1024
    shuffle: true
    parallel_tokenizer: false
    #seed: 42
<< endblock train_dataset

-- block eval_dataset
eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset
    dataset: *train_dataset_split
    tokenizer: *tokenizer
    # Use first 1000 records for evaluation
    select_range: [ 0, 1000 ]
    desc: "Tokenizing eval"
    fn_kwargs:
        <<: *preprocess_args
    parallel_tokenizer: false
<< endblock eval_dataset