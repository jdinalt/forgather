## Forgather Titan Trainer project template
-- set ns = namespace()
-- from 'inc/formatting.jinja' import h1, h2, h3
[head!]
    [base_directories]
    -- include "base_directories.yaml"

    [resource_directories]
    -- set ns.models_dir = joinpath(project_dir, 'output_models')
    -- set ns.tokenizers_dir = joinpath(ns.forgather_dir, 'tokenizers')

    [config_metadata]
    -- set ns.config_name = "Undefined"
    -- set ns.config_description = "Undefined"
    -- set ns.config_class = "type.training_script.fg_titan"
    -- set ns.model_name = "default_model"
    -- set ns.log_name = "tb"
    -- set ns.nproc_per_node = "gpu"

    [output_directories]
    -- set ns.output_dir = joinpath(ns.models_dir, ns.model_name)
    -- set ns.log_dir = joinpath(ns.output_dir, ns.log_name)

    [constants]
    ## The train_spec is defined and registered by the config
    ## These are the names under which they are registered, but
    ## the actual values do not matter.
    -- set ns.train_spec_name = "forgather_config_spec"
    -- set ns.train_spec_flavor = "default"

[header]
== h1(ns.config_name)
# {{ utcisotime() }}
# Description: {{ ns.config_description }}

[job_definition]
== h2("Job Definition")

    [job]
.define: &job !call:torchtitan.config.job_config:Job
    dump_folder: "{{ ns.output_dir }}"
    description: "{{ ns.config_description }}"

    [metrics]
.define: &metrics !call:torchtitan.config.job_config:Metrics
    log_freq: 10
    enable_tensorboard: false
    save_tb_folder: "{{ ns.log_name }}"

    [model]
.define: &model !call:torchtitan.config.job_config:Model
    ## Note that the tokenizer and model are defined below, not here.
    ## These values need to be defined for the glue to work.
    name: "{{ ns.train_spec_name }}"
    flavor: "{{ ns.train_spec_flavor }}"
    
    [training]
.define: &training !call:torchtitan.config.job_config:Training
    steps: 1000
    local_batch_size: 8
    seq_len: 2048
    gc_freq: 1000

    [validation]
.define: &validation !call:torchtitan.config.job_config:Validation
    enable: true
    freq: 100
    local_batch_size: 16
    seq_len: !call:getattr [ *training, "seq_len" ]

    [parallelism]
.define: &parallelism !call:torchtitan.config.job_config:Parallelism
    data_parallel_replicate_degree: 1
    data_parallel_shard_degree: -1
    fsdp_reshard_after_forward: "default" # default / never / always
    tensor_parallel_degree: 1
    enable_async_tensor_parallel: false
    pipeline_parallel_degree: 1
    context_parallel_degree: 1

    [checkpoint]
.define: &checkpoint !call:torchtitan.config.job_config:Checkpoint
    enable: false
    folder: "checkpoints"
    interval: 500
    keep_latest_k: 3
    last_save_model_only: false
    export_dtype: "float32"

    [activation_checkpoint]
.define: &activation_checkpoint !call:torchtitan.config.job_config:ActivationCheckpoint
    mode: "none"  # ["none", "selective", "full"]

    [profiling]
.define: &profiling !call:torchtitan.config.job_config:Profiling

    [compile]
.define: &compile !call:torchtitan.config.job_config:Compile
    enable: false
    components: ["model", "loss"]

    [quantize]
.define: &quantize !call:torchtitan.config.job_config:Quantize

    [comm]
.define: &comm !call:torchtitan.config.job_config:Comm

    [memory_estimation]
.define: &memory_estimation !call:torchtitan.config.job_config:MemoryEstimation

    [fault_tolerance]
.define: &fault_tolerance !call:torchtitan.config.job_config:FaultTolerance

    [experimental]
.define: &experimental !call:torchtitan.config.job_config:Experimental

[project_dependencies]
== h2("project_dependencies")

    [distributed_env]
distributed_env: &distributed_env !call:forgather.ml.distributed:DistributedEnvironment

[model_definition]
== h2("Model Definition")

## Required
## tokenizer: &tokenizer
## model_args: &model_args ...
## model_factory: &model_factory
## parallelize_fn: &parallelize_fn
## pipeline_fn: &pipeline_fn
## loss_fn: &loss_fn
## state_dict_adapter: &state_dict_adapter

[dataset_definition]
== h2("Dataset Definition")

    [tokenizer_args]
.define: &tokenizer_args !dict
    max_length: !call:getattr [ *training, "seq_len" ]
    truncation: True

    [dataset]
.define: &dataset !call:forgather:from_project
    targets: [ "train_dataset", "eval_dataset" ]
    preprocess_args: *tokenizer_args
    tokenizer: *tokenizer
    ## Required
    ## project_dir: "/path/to/dataset/project"
    ## config_template: "config_name.yaml"

    [train_dataset]
train_dataset: &train_dataset !call:getitem [ *dataset, 'train_dataset' ]

    [validation_dataset]
validation_dataset: &validation_dataset !call:getitem [ *dataset, 'eval_dataset' ]

[trainer_assets]
== h2("Trainer Assets")

    [optimizer_factory]
.define: &optimizer_factory !partial:torch:optim.AdamW
    lr: 1.0e-4

    [lr_scheduler_factory]
.define: &lr_scheduler_factory !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler
    warmup_steps: 500
    cooldown_steps: 50000
    constant_lr: 1.0e-6

    [data_collator]
.define: &data_collator !call:forgather.ml.data_collator:DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
    truncation: True
    max_length: !call:getattr [ *training, "seq_len" ]
    input_name: "input"
    labels_name: null

[torchtitan]
== h2("Torch Titan Config")
# This section defines the job_config and train_spec from the definitions from above

    [job_config]
job_config: &job_config !call:torchtitan.config.job_config:JobConfig@job_config
    job: *job
    profiling: *profiling
    metrics: *metrics
    model: *model
    training: *training
    parallelism: *parallelism
    checkpoint: *checkpoint
    activation_checkpoint: *activation_checkpoint
    compile: *compile
    quantize: *quantize
    comm: *comm
    memory_estimation: *memory_estimation
    fault_tolerance: *fault_tolerance
    experimental: *experimental
    validation: *validation

    [build_optimizers_fn]
.define: &build_optimizers_fn !partial:forgather.ml.trainer.torchtitan.helpers:build_optimizers
    # model_parts
    # optimizer_config
    # parallel_dims
    # ft_manager
    container_factory: !partial:torchtitan.components.optimizer:OptimizersContainer
        # [partial] model_parts
        optimizer_cls: *optimizer_factory
        optimizer_kwargs: !dict ## Redundant, when using partial functions

    [build_lr_schedulers_fn]
.define: &build_lr_schedulers_fn !partial:forgather.ml.trainer.torchtitan.helpers:build_lr_schedulers
    # optimizers
    # lr_scheduler_config
    # training_steps
    container_factory: !partial:forgather.ml.trainer.torchtitan.lr_scheduler:LRSchedulersContainer
        # optimizers
        lr_scheduler_factory: *lr_scheduler_factory

    [build_dataloader_fn]
.define: &build_dataloader_fn !partial:forgather.ml.trainer.torchtitan.helpers:build_dataloader
    # dp_world_size
    # dp_rank
    # tokenizer
    # job_config
    # infinite
    dataset: *train_dataset
    batch_size: !call:getattr [ *training, "local_batch_size" ]
    data_collator: *data_collator

    [build_validation_dataloader_fn]
.define: &build_validation_dataloader_fn !partial:forgather.ml.trainer.torchtitan.helpers:build_dataloader
    # dp_world_size
    # dp_rank
    # tokenizer
    # job_config
    # infinite
    dataset: *validation_dataset
    batch_size: !call:getattr [ *validation, "local_batch_size" ]
    data_collator: *data_collator

    [build_tokenizer_fn]
.define: &build_tokenizer_fn !partial:forgather.ml.trainer.torchtitan.helpers:build_tokenizer
    # job_config
    tokenizer: *tokenizer

    [build_loss_fn]
.define: &build_loss_fn !partial:forgather.ml.trainer.torchtitan.helpers:build_loss_fn
    # job_config
    loss_fn: *loss_fn

    [build_validator_fn]
.define: &build_validator_fn !partial:forgather.ml.trainer.torchtitan.validate:Validator
    # job_config
    # dp_world_size
    # dp_rank
    # tokenizer
    # parallel_dims
    # loss_fn
    # validation_context
    # maybe_enable_amp
    # metrics_processor
    # pp_schedule
    # pp_has_first_stage
    # pp_has_last_stage
    build_dataloader_fn: *build_validation_dataloader_fn

    [build_metrics_processor_fn]
.define: &build_metrics_processor_fn null

    [train_spec]
train_spec: &train_spec !call:torchtitan.protocols.train_spec:TrainSpec
    name: "{{ ns.train_spec_name }}"
    model_cls: *model_factory
    model_args: { {{ ns.train_spec_flavor }}: *model_args }
    parallelize_fn: *parallelize_fn
    pipelining_fn: *pipeline_fn
    build_optimizers_fn: *build_optimizers_fn
    build_lr_schedulers_fn: *build_lr_schedulers_fn
    build_dataloader_fn: *build_dataloader_fn
    build_tokenizer_fn: *build_tokenizer_fn
    build_loss_fn: *build_loss_fn
    build_validator_fn: *build_validator_fn
    build_metrics_processor_fn: *build_metrics_processor_fn
    state_dict_adapter: *state_dict_adapter

    [trainer]
trainer: &trainer !call:torchtitan.train:Trainer@trainer
    job_config: !call:forgather.ml.construct:dependency_list
        - *job_config
        # Register train_spec before constructing Trainer
        - !call:torchtitan.protocols.train_spec:register_train_spec
            train_spec: *train_spec

[meta]
meta: &meta_output !dict:@meta
    config_name: "{{ ns.config_name }}"
    config_description: "{{ ns.config_description }}"
    config_class: "{{ ns.config_class }}"
    output_dir: "{{ ns.output_dir }}"
    log_dir: "{{ ns.log_dir }}"
    forgather_dir: "{{ ns.forgather_dir }}"
    nproc_per_node: {{ ns.nproc_per_node }}

[dynamic_args]
dynamic_args: !dlist

[env]
## These values are passed as envrionment variables to torchrun
env: &env !dict

[main]
main: !call:forgather.ml.training_script:TrainingScript@training_script
    meta: *meta_output
    trainer: *trainer
