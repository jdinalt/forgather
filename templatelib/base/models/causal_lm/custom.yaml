-- extends 'models/base_language_model.yaml'

[model_meta_config]
    == super()
    -- set ns.model_name = "Custom Language Model"
    -- set ns.model_description = "A custom model definition."
    ## Define the following variables
##  -- set ns.model_cls = '<model class>'
##  -- set ns.model_cfg_cls = '<model configuration class>'
##  -- set ns.model_config_path = '<path to model config source>'
##  -- set ns.model_path = '<path to model source>'

[model_header]
    => super()
# ns.model_cls = "{{ ns.model_cls }}"
# ns.model_cfg_cls = "{{ ns.model_cfg_cls }}"
# ns.model_config_path = "{{ abspath(ns.model_config_path) }}"
# ns.model_path = "{{ abspath(ns.model_path) }}"

[model_tokenizer]
##.define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained
##    arg0: "tokenizer_path_or_id"
##     Needed for Transformers 5.0
##    trust_remote_code: True

[model_config_defs]
    == super()
model_submodule_searchpath: &model_submodule_searchpath
    -- block model_submodule_searchpath
    << endblock model_submodule_searchpath
    - "{{ ns.output_dir }}"

    [model_bits]
## Custom model definition

[model_config]
model_config: &model_config !singleton:{{ ns.model_config_path }}:{{ ns.model_cfg_cls }}@model_config
    submodule_searchpath: *model_submodule_searchpath
    # Set auto-map for custom model; this ensures that the source code stays with the model.
    auto_map:
        AutoConfig: "{{ modname_from_path(ns.model_config_path) + '.' + ns.model_cfg_cls }}"
        AutoModel: "{{ modname_from_path(ns.model_path) + '.' + ns.model_cls }}"
    # Get the vocab-size from the tokenizer definition.
    vocab_size: !singleton:len [ *tokenizer ]
    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]
    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]
    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]

[model_constructor]
# Construct and save pretrained config to output_dir
pretrained_config: &pretrained_config !singleton:forgather.ml.construct:build_rule
    target: "{{ joinpath(ns.output_dir, 'config.json') }}"
    recipe:
        - !partial:call [ !singleton:getattr [ *model_config, "save_pretrained"], "{{ ns.output_dir }}" ]
        - !partial:forgather.ml.construct:copy_package_files
            - "{{ ns.output_dir }}"
            - *model_config
    loader: !partial:transformers:AutoConfig.from_pretrained
        arg0: "{{ ns.output_dir }}"
        trust_remote_code: True

# Construct and save pretrained tokenizer to output_dir
pretrained_tokenizer: &pretrained_tokenizer !singleton:forgather.ml.construct:build_rule
    target: "{{ joinpath(ns.output_dir, 'tokenizer.json') }}"
    recipe: !partial:call [ !singleton:getattr [ *tokenizer, "save_pretrained"], "{{ ns.output_dir }}" ]
    loader: !partial:transformers:AutoTokenizer.from_pretrained
        arg0: "{{ ns.output_dir }}"
        trust_remote_code: True

# Callable, which constructs model instance from config in output_dir
model: &model !partial:transformers:AutoModelForCausalLM.from_config
    arg0: *pretrained_config
    ## latent_args defers binding until constructed, then merges the resulting dictionary
    ## This allows us to, optionally, pass in a dictionary or default to model_constructor_args
    latent_args: !var
        name: "model_constructor_args"
        default: *model_constructor_args