## Given a PretrainedConfig class, construct a new PretrainedModel
-- extends 'models/base_language_model.yaml'

-- block model_meta_config
    == super()
    -- set ns.model_name = "HF Language Model"
    -- set ns.model_description = "A newly initialized HF language model"
    -- set ns.model_source = ""
##  -- set ns.model_config_cls = '<model configuration cls>'
<< endblock model_meta_config


-- block model_header
    == super()

# ns.model_source = "{{ ns.model_source  }}"
# ns.model_config_cls = "{{ ns.model_config_cls }}"
<< endblock model_header


##-- block model_tokenizer
##.define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained
##    - "tokenizer_path_or_id"
##<< endblock model_tokenizer


-- block model_config
model_config: &model_config !singleton:{{ ns.model_config_cls }}
    vocab_size: !singleton:len [ *tokenizer ]
<< endblock model_config


-- block model_constructor
model: &model !lambda:transformers:AutoModelForCausalLM.from_config@model
    args:
        - *model_config
    kwargs:
        <<: *model_constructor_args
<< endblock model_constructor
