## Given a PretrainedConfig class, construct a new PretrainedModel
-- extends 'models/base_language_model.yaml'

[model_meta_config]
    == super()
    -- set ns.model_name = "HF Language Model"
    -- set ns.model_description = "A newly initialized HF language model"
    -- set ns.model_source = ""
##  -- set ns.model_config_cls = '<model configuration cls>'

[model_header]
    == super()
# ns.model_source = "{{ ns.model_source  }}"
# ns.model_config_cls = "{{ ns.model_config_cls }}"

[model_tokenizer]
.define: &tokenizer !call:transformers:AutoTokenizer.from_pretrained
    ## arg0: "tokenizer_path_or_id"

[model_config]
model_config: &model_config !call:{{ ns.model_config_cls }}
    vocab_size: !call:len [ *tokenizer ]

[model_constructor]
model: &model !lambda:transformers:AutoModelForCausalLM.from_config@model
    arg0: *model_config
    <<: *model_constructor_args