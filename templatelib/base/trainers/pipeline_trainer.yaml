-- extends 'trainers/trainer.yaml'
-- block trainer_meta_config
    == super()
    -- set ns.trainer_name = "forgather.ml.trainer.pipeline:PipelineTrainer"
    -- set ns.trainer_description = "Pipeline parallel trainer"
    -- set ns.trainer_config_class = "forgather.ml.trainer.pipeline:PipelineTrainingArguments"
    -- set ns.trainer_class = "forgather.ml.trainer.pipeline:PipelineTrainer"
    ## Set these in your sub-project
    ##-- set ns.pipeline_layers = 16
    ##-- set ns.pipeline_segments = 4
    ##-- set ns.pipeline_microbatches = 4
    ##-- set ns.split_layer_prefix = "causal_lm.layer_stack.layers."
    ## Set to match number of pipeline segments on each node
    ## -- set ns.nproc_per_node = N
-- endblock trainer_meta_config


-- block trainer_header
    == super()
# pipeline_layers: {{ ns.pipeline_layers }}
# pipeline_segments: {{ ns.pipeline_segments }}
# pipeline_microbatches: {{ ns.pipeline_microbatches }}
# split_layer_prefix: "{{ ns.split_layer_prefix }}"
<< endblock trainer_header


-- block trainer_dependencies
    == super()
    -- block split_spec
split_spec: &split_spec !dict@split_spec
    ## Define where to split the model, based upon the total number of layers and the number of segments.
    ## While not optimal, this is probalby a good starting point.
-- for i in range(1, ns.pipeline_segments):
    {{ ns.split_layer_prefix }}{{i * (ns.pipeline_layers // ns.pipeline_segments)}}: "beginning"
-- endfor
    -- endblock split_spec
-- endblock trainer_dependencies


-- block trainer_args
    == super()
    # Pipeline Trainer
    split_spec: *split_spec

    # Split batches into N micro-batches; batch-sizes must be divisble by this value.
    pipeline_chunks: {{ ns.pipeline_microbatches }}
    ## Must be 1 for PipelineScheduleSingle. Otherwise, see scheduler documentation for reqiriements.
    stages_per_rank: 1
    ## Set to True, if pipe_schedule_factory is a sub-class of PipelineScheduleMulti
    is_multistage: False

    # Set to True (or checkpoint-path) to init weights from checkpoint.
    # Loads from 'output_dir' if True.
    ## Note: Loading weights in you model constructor will not work with this trainer.
    ## See pipeline_trainer example project for checkpoint loading details.
    resume_from_checkpoint: False
    ##
    ## Optional:
    ## Enable Pipeline Debug logging
    ##   debug_pipeline: bool = False
    ##
    ## Extra debug info about model splitting.
    ##   debug_split_model: bool = False
    ##
    ## Extra debug info about model params
    ##   debug_model_params: bool = False
    ##
    ## Extra debug info about weight init
    ##   debug_model_init: bool = False
    ##
    ## ZBVZ requires the "v" option.
    ##   pp_stage_type: "v" | "loop" = "loop"
    ##
    ## Use the same pipeline for both train and eval. This is slower, as the train piepline 
    ## always runs the backward pass. It may also produce different loss results, as it was 
    ## traced with model.train(), rather than model.eval()
    ## This option is primarily for diagnostics, although it may also reduce memory overhead slightly.
    ## In the case of ZBVZ, it is presenlty required for the eval step, as seperate eval model is
    ## not working for ZBVZ yet.
    ##   unified_model: True
<< endblock trainer_args


-- block trainer_constructor
    == super()
    # Pipeline requires explicit loss function
    compute_loss_func: !singleton:forgather.ml.loss:CausalLoss
    distributed_env: *distributed_env
    # https://docs.pytorch.org/docs/stable/distributed.pipelining.html#module-torch.distributed.pipelining.schedules
    pipe_schedule_factory: !partial:torch.distributed.pipelining:ScheduleGPipe
<< endblock trainer_constructor
