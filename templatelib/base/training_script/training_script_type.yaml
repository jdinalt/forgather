## Base class for training script configurations
-- extends 'config_type.yaml'
-- from 'inc/formatting.jinja' import h2


-- block config_metadata
    == super()
    -- set ns.config_class = ns.config_class + ".training_script"
    ## This block introduces new settings and defines defautls.
    -- set ns.log_name = "log"
    -- set ns.model_name = "default_model"
    ## Assume single process training, unless overridden
    -- set ns.nproc_per_node = 1
<< endblock config_metadata


-- block globals
    ## Generate directories from above blocks.
    -- set ns.output_dir = output_dir | default(joinpath(ns.models_dir, ns.model_name))
    -- set ns.logging_dir = joinpath(ns.output_dir, "runs", ns.log_name + '_' + filetime())
<< endblock globals


-- block header
    == super()
# Model: {{ ns.model_name }}
# Hostname: {{ hostname }}
# Versions:
-- for key, value in versions.items()
    == '#     ' + key + ': ' + value
-- endfor
<< endblock header


-- block variable_listing
    == super()
# ns.model_src_dir: "{{ ns.model_src_dir }}"
# ns.output_dir: "{{ ns.output_dir }}"
# ns.logging_dir: "{{ ns.logging_dir }}"
# ns.nproc_per_node: {{ ns.nproc_per_node }}
<< endblock variable_listing


-- block main_body

== h2('Distributed Environment')

    -- filter trim()
    -- block distributed_environment
distributed_env: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env
    backend: {{ dist_backend | toyaml("cuda:nccl,cpu:gloo") }}
    no_accelerator: {{ no_accelerator | toyaml(False) }}
    << endblock distributed_environment
    -- endfilter


== h2('Dependencies')

    -- filter trim()
    -- block pre_model_setup
## Undefined
    -- endblock pre_model_setup
    -- endfilter


== h2('Model')

    -- filter trim()
    -- block model_constructor_args
model_constructor_args: &model_constructor_args {}
    << endblock model_constructor_args
    -- endfilter


    -- filter trim()
    -- block model_definition required
## tokenizer: &tokenizer
## model: &model
    << endblock model_definition
    -- endfilter


== h2('Datasets')

    -- filter trim()
    -- block datasets_preprocessor_args
    << endblock datasets_preprocessor_args
    -- endfilter
    

    -- filter trim()
    -- block datasets_definition required
## train_dataset: &train_dataset
## eval_dataset: &eval_dataset
    << endblock datasets_definition
    -- endfilter


== h2('Data Collator')

    -- filter trim()
    -- block datacollator
data_collator: &data_collator null
    << endblock datacollator
    -- endfilter


== h2('Trainer Callbacks')

    -- filter trim()
    -- block trainer_callbacks
trainer_callbacks: &trainer_callbacks !dlist:@trainer_callbacks
    null: ~
    << endblock trainer_callbacks
    -- endfilter


== h2('Optimizer')

    -- filter trim()
    -- block optimizer
optimizer: &optimizer ~
    << endblock optimizer
    -- endfilter


== h2('LR Scheduler')

    -- filter trim()
    -- block lr_scheduler
lr_scheduler: &lr_scheduler ~
    << endblock lr_scheduler
    -- endfilter

== h3('Trainer Args')

    -- filter trim()
    -- block trainer_args
trainer_args: &trainer_args !dict
    << endblock trainer_args
    -- endfilter

== h3('Trainer Dependencies')

    -- filter trim()
    -- block trainer_dependencies
    << endblock trainer_dependencies
    -- endfilter

== h2('Trainer')

    -- filter trim()
    -- block trainer_definition required
## trainer: &trainer
    << endblock trainer_definition
    -- endfilter
<< endblock main_body

-- block dynamic_args
    == super()
    max_steps:
        names: "--max-steps"
        type: "int"
        help: "Set maximum training steps"
    save_strategy:
        names: [ "--save-strategy", "-S" ]
        choices: [ "no", "steps", "epoch" ]
        type: "str"
        help: "When to save checkpoints"
    no_accelerator:
        names: "--no-accelerator"
        action: "store_true"
        help: "Disable use of accelerator, when available. e.g. 'don't use GPU'"
    dist_backend:
        names: "--dist-backend"
        help: "The name of the torch-distributed backend to use"
    output_dir:
        names: "--output-dir"
        type: path
        help: "Overrides default model output directory path"
<< endblock dynamic_args

-- block meta_output
    == super()
    output_dir: "{{ ns.output_dir }}"
    model_src_dir: "{{ ns.model_src_dir }}"
    logging_dir: "{{ ns.logging_dir }}"
    nproc_per_node: {{ ns.nproc_per_node }}
<< endblock meta_output


-- block main_output
main: !singleton:forgather.ml.training_script:TrainingScript@training_script
    meta: *meta_output
    do_train: True
    do_save: False
    do_eval: False
    distributed_env: *distributed_env
    trainer: *trainer
<< endblock main_output
