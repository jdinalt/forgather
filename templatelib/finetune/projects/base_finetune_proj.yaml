-- extends "training_script/causal_lm/causal_lm.yaml"

[resource_directories]
    == super()
    ## Change this to point to where your models are stored
    -- set ns.models_dir = models_dir | default(joinpath(user_home_dir(), 'models'))

    ## Directory in which local datasets are stored
    -- set ns.datasets_dir = datasets_dir | default(joinpath(user_home_dir(), 'datasets'))

[config_metadata]
    == super()
    -- set ns.config_name = 'Finetune'
    -- set ns.config_description = 'Experiments in finetuning'
    -- set ns.trainer_class = 'trainers/trainer.yaml'
    -- set ns.trust_remote_code = True
    -- set ns.model_name = 'default_model'
    -- set ns.log_name = log_name | default('log')
    
    ## memory debugging
    -- set ns.debug_memory_detials = False
    -- set ns.log_memory_to_tb = False

    ## Set project defaults
    -- set ns.default_dataset_proj = joinpath(ns.forgather_dir, 'examples', 'datasets', 'QuixiAI')
    -- set ns.default_dataset_config = "samantha.yaml"
    ## Use template from tokenizer or dataset default
    -- set ns.default_chat_template = ""

[globals]
	## If not specified, default to model-dir and model-name
	-- set ns.model_id_or_path = model_id_or_path | default(joinpath(ns.models_dir, ns.model_name))

    ## If explicity specified, use 'output_dir,' otherwise assume that 'ns.model_id_or_path'
    -- set ns.output_dir = output_dir | default(ns.model_id_or_path)
    -- set ns.logging_dir = joinpath(ns.output_dir, "runs", ns.log_name + '_' + filetime())

[variable_listing]
    == super()
# ns.dataset_proj: "{{ dataset_proj | default(ns.default_dataset_proj) }}"
# ns.dataset_config: "{{ dataset_config | default(ns.default_dataset_config) }}"
# ns.debug_memory_detials: {{ ns.debug_memory_detials }}
# ns.log_memory_to_tb: {{ ns.log_memory_to_tb }}
# ns.model_id_or_path: {{ ns.model_id_or_path }}


[datasets_preprocessor_args]
.define: &datasets_preprocessor_args !dict
    truncation: True

[construct_new_model]
    -- include 'models/causal_lm/from_pretrained_config.yaml'

[datasets_definition]
    [dataset_pp_kwargs]
.define: &dataset_pp_args
    chat_template: "{{ chat_template | default(ns.default_chat_template) }}"

    [dataset_dict]
.define: &dataset_dict !call:forgather:from_project
    project_dir: "{{ dataset_proj | default(ns.default_dataset_proj) }}"
    config_template: "{{ dataset_config | default(ns.default_dataset_config) }}"
    targets: [ "train_dataset", "eval_dataset" ] 
    preprocess_args: *datasets_preprocessor_args
    tokenizer: *tokenizer
    pp_kwargs:
        <<: *dataset_pp_args

    [train_dataset]
train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]

    [eval_dataset]
eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]

## Setup some defaults for this type of project
[trainer_args]
    == super()

    # **finetune**
    seed: 42
    logging_steps: 10
    eval_steps: 100
    eval_strategy: "steps"
    max_eval_steps: -1
    num_train_epochs: {{ epochs | default(1) }}
    dataloader_num_workers: 1
    float32_matmul_precision: "high"
    default_dtype: bfloat16
    gradient_checkpointing: {{ gradient_checkpointing | default(False) }}

    # Checkpoint Save Settings
    save_strategy: "{{ save_strategy | default('steps') }}" # Set to "steps" to save every "save_steps"; set to "no" to disable
    save_steps: 1000                 # Checkpoint every N steps
    save_safetensors: False          # Safe tensors don't support shared weights
    save_total_limit: 3              # Keep at most N checkpoints
    save_on_each_node: False         # Save common files on each node
    save_rng_state: True             # Save RNG state with checkpoint
    save_on_each_node: {{ save_on_each_node | default(False) }}

    # Load model from latest checkpoint
    resume_from_checkpoint: {{ resume_from_checkpoint | default(True) }}

    # Where to construct the model: ["default"|"device"|"meta"]
    construct_model_on: "device"    # Construct model on accelerator device
    # Enable all SDPA backends and let PyTorch choose which to use.
    sdpa_backend: [ "math", "flash", "efficient", "cudnn" ]
    sdpa_set_priority: False # If list, interpret as priority order

[fused_loss_factory]
# Fused output-linear x cross-entropy-loss kernel
.define: &fused_loss_factory !partial:forgather.ml.loss:LinearCrossEntropyLoss

[trainer_definition]
## Defaults to the basic trainer implementation
## Note: This is unsuitable for multiple GPUs.
## Override 'ns.trainer_class' to change the trainer class.
    ## See definition below
    -- include 'finetune.trainer'

[trainer_callbacks]
    ## See definition below
    -- include 'finetune.callbacks'

[datacollator]
data_collator: &data_collator !call:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
    truncation: True
    ## Put a reasonable limit on the sequence length
    max_length: 4096

[lr_scheduler]
# See https://arxiv.org/html/2503.02844v1
## This lr_scheduler is pretty flexible, as it can be used to implement many common LR schedules
## - Linear warmup
## - Constant LR
## - Cosine annealing
## - Exponential decay
## Combinations of any of the above, e.g.
## - Linear warmup + constant LR
## - Linear warmup + cosine annealing
## - Linear warmup + cosine annealing + constant
## - Linear warmup + cosine annealing + constant + exponential decay
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    # Linear warmup steps
    warmup_steps: {{ warmup_steps | default(200) }}

    # If 0, training will warmup to the lr specified in the optimizer and hold until checkpoint_step.
    # If > 0, training will warmup to the lr specified in the optimizer, followed by 
    # cosine annealing to constant_lr, completing at step warmup_steps + cooldown_steps.
    cooldown_steps: 0

    # Begin exponential decay, from constant_lr to minimum_lr, at this absolute step
    # Disable by setting to < 0
    checkpoint_step: -1

    # Hold at this LR after warmup.
    constant_lr: 5.0e-6

    # Exponential decay constant, tau
    tau: !!float 1.0e2

    # Exponetial decay limit
    min_lr: 1.0e-8

## Add a default memory efficient optimizer
[optimizer]
optimizer: &optimizer !partial:forgather.ml.optim.adafactor:Adafactor
    lr: !!float {{ lr | default(5.0e-6) }}
    weight_decay: 0.001

[dynamic_args]
    == super()
    epochs:
        names: "--epochs"
        type: "int"
        help: "Set the number of epochs to train for"
    log_peak_memory:
        names: [ "--log-peak-memory", "-P" ]
        action: "store_true"
        help: "Log peak GPU memory at each log step"
    dataset_config:
        names: "--dataset-config"
        type: "str"
        help: "The name of the dataset configuration to use"
    dataset_proj:
        names: "--dataset-proj"
        type: "path"
        help: "Path to dataset project to use"
    model_id_or_path:
        names: [ "--model-id-or-path", "-M" ]
        type: "path"
        help: "HF model ID or local path to model"
    output_dir:
        names: "--output-dir"
        type: "path"
        help: "Training output director. Defaults to model_id_or_path"
    gradient_checkpointing:
        names: [ "--gradient-checkpointing", "-G"]
        action: "store_true"
        help: "Enable gradient (activation) checkpoint, when supported"
    resume_from_checkpoint:
        names: "--resume-from-checkpoint"
        type: path
        help: "Explicit checkpoint path to load"
    save_on_each_node:
        names: "--save-on-each-node"
        action: "store_true"
        help: "Save common checkpoint files on each node"
    chat_template:
        names: [ "--chat-template", "-C" ]
        type: "path"
        help: "Path to the chat template to use"
    log_name:
        names: [ "--log-name" ]
        type: "str"
        help: "The name of the output log"
    lr:
        names: [ "--lr" ]
        type: "float"
        help: "Learning rate"
    warmup_steps:
        names: [ "--warmup-steps" ]
        type: "int"
        help: "The number of warmup steps to use"

#-------------------- finetune.callbacks --------------------
-- extends 'callbacks/loggers.yaml'
## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

[callback_dependencies]
    == super()

[callback_list]
    == super()
    peak_memory: !call:forgather.ml.trainer.callbacks:PeakMemory
        show_details: {{ ns.debug_memory_detials }}
        do_log: {{ log_peak_memory | default(False) }}
    -- if ns.log_memory_to_tb
        summary_writer: *summary_writer
    -- endif
    trainer_control: !call:forgather.ml.trainer.callbacks:TrainerControlCallback

#-------------------- finetune.trainer --------------------
-- extends ns.trainer_class
## Note: We use dynamic inheritance for the trainer-class
## This has a side effect of not being able to statically resolve the
## parent template, which is named in the 'ns.trainer_class' variable,
## the value of which is defined in the 'config_metadata' block, above.