-- extends 'trainers/pipeline/base_pipeline_trainer.yaml'

[trainer_meta_config]
    == super()
    ## Parallelism settings
    -- set ns.nproc_per_node = 4
    -- set ns.nnodes = 1
    -- set ns.pipeline_microbatches = 8

    ## Scheduler settings
    -- set ns.pipeline_is_multistage = True
    -- set ns.pipeline_stages_per_rank = 2
    -- set ns.pipeline_sched_class = "torch.distributed.pipelining:ScheduleInterleaved1F1B"

    ## Model parameters
    -- set ns.pipeline_layers = 32
    -- set ns.split_layer_prefix = "causal_lm.layer_stack.layers."
    -- set ns.pipeline_unified_model = False

[trainer_args]
    == super()
    # **i1f1b 4gpu**
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16