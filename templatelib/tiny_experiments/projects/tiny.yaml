## This is an example of a project template; useful where similar configurations
## may be used in multiple projects.
##
## Specifically, this sets up training for a very small model over one epoch
## on the 'abridged' version of the Tiny Stories dataset.
-- extends "training_script/causal_lm/causal_lm.yaml"

[config_metadata]
    == super()
    ## Overrides
    -- set ns.config_name = "Tiny Experiments"
    -- set ns.config_description = "A project template for running tiny model experiments."
    ## Defines
    -- set ns.trainer_class = 'trainers/trainer.yaml'
    ## We make use of custom models in these projects
    -- set ns.trust_remote_code = True

    ## memory debugging
    -- set ns.debug_memory_detials = False
    ## Debug logging to TensorBoard vs console
    -- set ns.log_memory_to_tb = False

    ## The dataset sub-project to use
    -- set ns.dataset_proj = dataset_proj | default(joinpath(ns.forgather_dir, 'examples', 'datasets', 'roneneldan'))

    ## The configuration in the dataset sub-project
    ## Note: Switch to 'tinystories-iter.yaml' for the full dataset.
    -- set ns.dataset_config = dataset_config | default("tinystories-abridged.yaml")
  
    ## The model project to import
    -- set ns.model_project_dir = joinpath(ns.forgather_dir, 'examples', 'models', 'causal_lm')
    -- set ns.model_project_config = "4M.yaml"
    -- set ns.debug_model_project = False

[pre_model_setup]
    ## Add assets needed for text-gen sampling.
    ## This adds a set of prompts and text-gen parameters.
    == super()
    -- include "prompts/tiny_stories.yaml"

[datasets_preprocessor_args]
tokenizer_args: &tokenizer_args !dict
    truncation: True

[datasets_definition]
    -- include 'datasets/llm_dataset_project.yaml'

## Defaults to the basic trainer implementation
## Note: This is unsuitable for multiple GPUs.
## Override 'ns.trainer_class' to change the trainer class.
[trainer_definition]
    ## See definition below
    -- include 'tiny.trainer_config'

[construct_new_model]
    -- include "models/causal_lm/import_model_project.yaml"

[trainer_callbacks]
    ## See definition below
    -- include 'tiny.callbacks'

[datacollator]
    == super()
    # Tiny Project
    ## Limit maximum sequence length 512 tokens, at the data-collator level.
    truncation: True
    max_length: 512

[lr_scheduler]
##-- include 'lr_schedulers/cosine_annealing_with_warmup.yaml'
# https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    # Linear warm-up steps
    warmup_steps: 500

    # Cosine decay from end of warmp-up to constant-lr
    cooldown_steps: 50000

    # See paper for experimentally derived values.
    constant_lr: 1.0e-4

[optimizer]
optimizer: &optimizer !partial:torch:optim.AdamW
    lr: 1.0e-3

[dynamic_args]
    == super()
    log_peak_memory:
        names: "--log-peak-memory"
        action: "store_true"
        help: "Log peak GPU memory at each log step"
    dataset_config:
        names: "--dataset-config"
        type: "str"
        help: "The name of the dataset configuration to use"
    dataset_proj:
        names: "--dataset-proj"
        type: "path"
        help: "Path to dataset project to use"


#-------------------- tiny.trainer_config --------------------
-- extends ns.trainer_class
## Note: We use dynamic inheritance for the trainer-class
## This has a side effect of not being able to statically resolve the
## parent template, which is named in the 'ns.trainer_class' variable,
## the value of which is defined in the 'config_metadata' block, above.

[trainer_args]
    == super()
    
    # **Tiny Project**
    seed: 42
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 64
    logging_steps: 100
    eval_strategy: "steps"
    eval_steps: 500
    save_steps: 10000
    num_train_epochs: 1
    dataloader_num_workers: 1
    # Safetensors can't handle tied parameters/buffers, so fallback to PyTorch format.
    save_safetensors: False

#-------------------- tiny.callbacks --------------------
-- extends 'callbacks/loggers.yaml'
## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

[callback_dependencies]
    == super()

    [text_gen_callback_args]
text_gen_callback_args: &text_gen_callback_args
    summary_writer: *summary_writer
    prompts: *testprompts
    generation_config: *generation_config
    max_new_tokens: 40
    generation_steps: 2000

## This adds a text-generationn sample every 'generation_steps'
[callback_list]
    == super()
    -- if attn_implementation | default("") != "flex_attention"
    text_gen_callback: !singleton:forgather.ml.trainer.callbacks:TextgenCallback
        <<: *text_gen_callback_args
    -- endif
    peak_memory: !singleton:forgather.ml.trainer.callbacks:PeakMemory
        show_details: {{ ns.debug_memory_detials }}
        do_log: {{ log_peak_memory | default(False) }}
    -- if ns.log_memory_to_tb
        summary_writer: *summary_writer
    -- endif
    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback
