-- extends 'projects/tiny.yaml'
## Extends tiny.yaml to include parameter for batch-size scaling

[config_metadata]
    == super()
    -- set ns.dataset_config = "tinystories-iter.yaml"
    -- set ns.normalized_lr = 1e-3
    -- set ns.effective_batch_size = 32
    -- set ns.gradient_accumulation_steps = 1

[globals]
    == super()
    ## For SGD-like optimizers
    -- set ns.lr_scale = ns.effective_batch_size / 32
    ## For Adam-like optimizers, use: ns.lr_scale ** 0.5

[trainer_definition]
    -- include 'project.trainer_config'

[lr_scheduler]
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: {{ 16000 // ns.effective_batch_size }}
    ## Targeted for one epoch of full tinystories dataset
    cooldown_steps: {{ 2000000 // ns.effective_batch_size }}
    constant_lr: 1.0e-5

[optimizer]
optimizer: &optimizer !partial:torch:optim.AdamW
    lr: {{ ns.normalized_lr * (ns.lr_scale ** 0.5) }}

#-------------------- project.trainer_config --------------------
-- extends 'tiny.trainer_config'

[trainer_args]
    == super()
    # project overrides
    #max_grad_norm: 0.5
    gradient_accumulation_steps: {{ ns.gradient_accumulation_steps }}
    per_device_train_batch_size: {{ ns.effective_batch_size // ns.gradient_accumulation_steps }}
    per_device_eval_batch_size: {{ 2 * ns.effective_batch_size // ns.gradient_accumulation_steps }}
    logging_steps: {{ 3200 // ns.effective_batch_size }}
    eval_steps: {{ 32000 // ns.effective_batch_size }}
