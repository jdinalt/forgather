{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../../notebooks/forgather.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Traning  for Fashion\n",
       "\n",
       "This project reproduces the configuration from a PyTorch tutorial, where a simple ML model is created and trained to recognize categories of clothing from the FashionMNIST dataset.\n",
       "\n",
       "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
       "\n",
       "This was chosen as it is a relatively simple project which can be relativley self contained. Still, it is far more complex than the previous examples.\n",
       "\n",
       "The model itself does not require any custom code. It's simply a stack of PyTorch layers, chained together with a nn.Sequential. If you would like to know more about the model itself, see:\n",
       "\n",
       "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
       "\n",
       "## Custom Code\n",
       "\n",
       "While Forgather is good at assembling objects, the language is not practical for defining logic. For this, we have defined a custom \"trainer\" class in the project's 'src' directory and we will use Forgather to dynamically import this code, injecting all the required dependencies.\n",
       "\n",
       "Unlike the previous projects, you will note that the \"Modules\" section not empty and has a link to the Trainer definition.\n",
       "\n",
       "## Project Structure\n",
       "\n",
       "Like the previous example, this project makes use of template inheritance, where there is a common 'project.yaml' file from which all of the configuratioins are derived.\n",
       "\n",
       "The template provides the basic structure, with 'blocks' which may be substituted or extended by child templates. We use this functionaity in the configurations to override various components of the base configuraiton.\n",
       "\n",
       "This is still a relatively simple project, as it does not reference any external template libraries. We will get to that in the next example.\n",
       "\n",
       "## Code Generation\n",
       "\n",
       "Note the output of the code generator. It has detected the inclusion of a dynamic import, thus it has automatically defined a function for importing dynamic modules.\n",
       "\n",
       "Also note that it knows how to translate some of the rather clunky expreressions from the original YAML file, like calling a method, into relatively clean Python code.\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/tutorials/basic_projects/project_gamma\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/tutorials/basic_projects/project_gamma/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/tutorials/basic_projects/project_gamma/templates](templates)\n",
       "\n",
       "## Available Configurations\n",
       "- [wider.yaml](templates/experiments/wider.yaml)\n",
       "- [deeper.yaml](templates/experiments/deeper.yaml)\n",
       "- [adam.yaml](templates/experiments/adam.yaml)\n",
       "- [baseline.yaml](templates/experiments/baseline.yaml)\n",
       "\n",
       "Default Configuration: baseline.yaml\n",
       "\n",
       "Active Configuration: baseline.yaml\n",
       "\n",
       "## Available Templates\n",
       "- [experiments/adam.yaml](templates/experiments/adam.yaml)\n",
       "- [experiments/baseline.yaml](templates/experiments/baseline.yaml)\n",
       "- [experiments/deeper.yaml](templates/experiments/deeper.yaml)\n",
       "- [experiments/wider.yaml](templates/experiments/wider.yaml)\n",
       "- [formatting.yaml](templates/formatting.yaml)\n",
       "- [project.yaml](templates/project.yaml)\n",
       "\n",
       "## Included Templates\n",
       "- [experiments/baseline.yaml](templates/experiments/baseline.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [formatting.yaml](templates/formatting.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'batch_size': 64,\n",
       " 'citation': 'https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html',\n",
       " 'description': 'Base configuration, based on Torch tutorial parameters.',\n",
       " 'epochs': 5,\n",
       " 'hidden_dim': 512,\n",
       " 'logging_steps': 100,\n",
       " 'lr': 0.001,\n",
       " 'name': 'Fashion MNIST Trainer'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "- [./src/trainer.py](src/trainer.py) : Trainer\n",
       "    - [/home/dinalt/ai_assets/forgather/tutorials/basic_projects/project_gamma/./src/trainer.py](src/trainer.py) : trainer\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "\n",
       "#---------------------------------------\n",
       "#          Fashion MNIST Trainer         \n",
       "#---------------------------------------\n",
       "# 2024-08-12T06:16:12\n",
       "# Description: Base configuration, based on Torch tutorial parameters.\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/tutorials/basic_projects/project_gamma\n",
       "# Citation: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
       "#---------------------------------------\n",
       "\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.hidden_dim = 512\n",
       "# ns.epochs = 5\n",
       "# ns.batch_size = 64\n",
       "# ns.lr = 0.001\n",
       "# ns.logging_steps = 100\n",
       "\n",
       "\n",
       "\n",
       "########### Model Definition ###########\n",
       "\n",
       ".define: &activation_fn !factory:torch.nn:ReLU@activation_fn []\n",
       "\n",
       ".define: &model !singleton:torch.nn:Sequential@model\n",
       "    - !factory:torch.nn:Flatten []\n",
       "    - !factory:torch.nn:Linear [ 784, 512 ]\n",
       "    - *activation_fn\n",
       "    - !factory:torch.nn:Linear [ 512, 512 ]\n",
       "    - *activation_fn\n",
       "    - !factory:torch.nn:Linear [ 512, 10 ]\n",
       "\n",
       "############### Dataset ################\n",
       "\n",
       ".define: &transform !factory:torchvision.transforms:ToTensor@transform []\n",
       "\n",
       ".define: &training_data !singleton:torchvision.datasets:FashionMNIST\n",
       "    root: \"data\"\n",
       "    train: True\n",
       "    download: True\n",
       "    transform: *transform\n",
       "\n",
       ".define: &test_data !singleton:torchvision.datasets:FashionMNIST\n",
       "    root: \"data\"\n",
       "    train: False\n",
       "    download: True\n",
       "    transform: *transform\n",
       "\n",
       ".define: &train_dataloader !singleton:torch.utils.data:DataLoader\n",
       "    args: [ *training_data ]\n",
       "    kwargs: { batch_size: 64 }\n",
       "\n",
       ".define: &test_dataloader !singleton:torch.utils.data:DataLoader\n",
       "    args: [ *test_data ]\n",
       "    kwargs: { batch_size: 64 }\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       ".define: &model_params !singleton:call [ !singleton:getattr [ *model, \"parameters\" ] ]\n",
       "\n",
       "# **Optimizer**\n",
       "\n",
       ".define: &optimizer !singleton:torch.optim:SGD\n",
       "    args: [ *model_params ]\n",
       "    kwargs:\n",
       "        lr: 0.001\n",
       "\n",
       "# **Loss Function**\n",
       "\n",
       ".define: &loss_fn !singleton:torch.nn:CrossEntropyLoss []\n",
       "\n",
       "# **Trainer**\n",
       "\n",
       ".define: &trainer !singleton:./src/trainer.py:Trainer@trainer\n",
       "    train_dataloader: *train_dataloader\n",
       "    test_dataloader: *test_dataloader\n",
       "    model: *model\n",
       "    loss_fn: *loss_fn\n",
       "    optimizer: *optimizer\n",
       "    epochs: 5\n",
       "    batch_size: 64\n",
       "    logging_steps: 100\n",
       "\n",
       "################ Output ################\n",
       "\n",
       "meta:\n",
       "    name: \"Fashion MNIST Trainer\"\n",
       "    description: \"Base configuration, based on Torch tutorial parameters.\"\n",
       "    citation: \"https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\"\n",
       "    hidden_dim: 512\n",
       "    epochs: 5\n",
       "    batch_size: 64\n",
       "    logging_steps: 100\n",
       "    lr: 0.001\n",
       "\n",
       "main:\n",
       "    model: *model\n",
       "    trainer: *trainer\n",
       "\n",
       "```\n",
       "\n",
       "## Loaded Configuration to YAML\n",
       "\n",
       "```yaml\n",
       ".define: &activation_fn !factory:torch.nn:ReLU@activation_fn []\n",
       "\n",
       ".define: &model !singleton:torch.nn:Sequential@model\n",
       "    - !factory:torch.nn:Flatten []\n",
       "    - !factory:torch.nn:Linear\n",
       "        - 784\n",
       "        - 512\n",
       "    - *activation_fn\n",
       "    - !factory:torch.nn:Linear\n",
       "        - 512\n",
       "        - 512\n",
       "    - *activation_fn\n",
       "    - !factory:torch.nn:Linear\n",
       "        - 512\n",
       "        - 10\n",
       "\n",
       ".define: &transform !factory:torchvision.transforms:ToTensor@transform []\n",
       "\n",
       ".define: &trainer !singleton:./src/trainer.py:Trainer@trainer\n",
       "    train_dataloader: !singleton:torch.utils.data:DataLoader\n",
       "        args:\n",
       "            - !singleton:torchvision.datasets:FashionMNIST\n",
       "                root: 'data'\n",
       "                train: True\n",
       "                download: True\n",
       "                transform: *transform\n",
       "        kwargs:\n",
       "            batch_size: 64\n",
       "    test_dataloader: !singleton:torch.utils.data:DataLoader\n",
       "        args:\n",
       "            - !singleton:torchvision.datasets:FashionMNIST\n",
       "                root: 'data'\n",
       "                train: False\n",
       "                download: True\n",
       "                transform: *transform\n",
       "        kwargs:\n",
       "            batch_size: 64\n",
       "    model: *model\n",
       "    loss_fn: !singleton:torch.nn:CrossEntropyLoss []\n",
       "    optimizer: !singleton:torch.optim:SGD\n",
       "        args:\n",
       "            - !singleton:call\n",
       "                - !singleton:getattr\n",
       "                    - *model\n",
       "                    - 'parameters'\n",
       "        kwargs:\n",
       "            lr: 0.001\n",
       "    epochs: 5\n",
       "    batch_size: 64\n",
       "    logging_steps: 100\n",
       "\n",
       "\n",
       "meta: \n",
       "    name: 'Fashion MNIST Trainer'\n",
       "    description: 'Base configuration, based on Torch tutorial parameters.'\n",
       "    citation: 'https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html'\n",
       "    hidden_dim: 512\n",
       "    epochs: 5\n",
       "    batch_size: 64\n",
       "    logging_steps: 100\n",
       "    lr: 0.001\n",
       "main: \n",
       "    model: *model\n",
       "    trainer: *trainer\n",
       "\n",
       "```\n",
       "\n",
       "### Generated Source Code\n",
       "\n",
       "```python\n",
       "from torch.nn import Linear\n",
       "from torch.nn import Flatten\n",
       "from torch.nn import CrossEntropyLoss\n",
       "from torch.nn import Sequential\n",
       "from torch.utils.data import DataLoader\n",
       "from torch.optim import SGD\n",
       "from torch.nn import ReLU\n",
       "from torchvision.transforms import ToTensor\n",
       "from torchvision.datasets import FashionMNIST\n",
       "from importlib.util import spec_from_file_location, module_from_spec\n",
       "import os\n",
       "import sys\n",
       "\n",
       "# Import a dynamic module.\n",
       "def dynimport(module, name, searchpath):\n",
       "    module_path = module\n",
       "    module_name = os.path.basename(module).split(\".\")[0]\n",
       "    module_spec = spec_from_file_location(\n",
       "        module_name,\n",
       "        module_path,\n",
       "        submodule_search_locations=searchpath,\n",
       "    )\n",
       "    mod = module_from_spec(module_spec)\n",
       "    sys.modules[module_name] = mod\n",
       "    module_spec.loader.exec_module(mod)\n",
       "    for symbol in name.split(\".\"):\n",
       "        mod = getattr(mod, symbol)\n",
       "    return mod\n",
       "\n",
       "Trainer = lambda: dynimport(\"./src/trainer.py\", \"Trainer\", [])\n",
       "\n",
       "def construct(\n",
       "):\n",
       "    activation_fn = lambda: ReLU()\n",
       "\n",
       "    model = Sequential(\n",
       "        Flatten(),\n",
       "        Linear(\n",
       "            784,\n",
       "            512,\n",
       "        ),\n",
       "        activation_fn(),\n",
       "        Linear(\n",
       "            512,\n",
       "            512,\n",
       "        ),\n",
       "        activation_fn(),\n",
       "        Linear(\n",
       "            512,\n",
       "            10,\n",
       "        ),\n",
       "    )\n",
       "\n",
       "    transform = lambda: ToTensor()\n",
       "\n",
       "    trainer = Trainer()(\n",
       "        train_dataloader=DataLoader(\n",
       "            FashionMNIST(\n",
       "                root='data',\n",
       "                train=True,\n",
       "                download=True,\n",
       "                transform=transform(),\n",
       "            ),\n",
       "            batch_size=64,\n",
       "        ),\n",
       "        test_dataloader=DataLoader(\n",
       "            FashionMNIST(\n",
       "                root='data',\n",
       "                train=False,\n",
       "                download=True,\n",
       "                transform=transform(),\n",
       "            ),\n",
       "            batch_size=64,\n",
       "        ),\n",
       "        model=model,\n",
       "        loss_fn=CrossEntropyLoss(),\n",
       "        optimizer=SGD(\n",
       "            model.parameters(),\n",
       "            lr=0.001,\n",
       "        ),\n",
       "        epochs=5,\n",
       "        batch_size=64,\n",
       "        logging_steps=100,\n",
       "    )\n",
       "    \n",
       "    return {\n",
       "        'meta': {\n",
       "            'name': 'Fashion MNIST Trainer',\n",
       "            'description': 'Base configuration, based on Torch tutorial parameters.',\n",
       "            'citation': 'https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html',\n",
       "            'hidden_dim': 512,\n",
       "            'epochs': 5,\n",
       "            'batch_size': 64,\n",
       "            'logging_steps': 100,\n",
       "            'lr': 0.001,\n",
       "        },\n",
       "        'main': {\n",
       "            'model': model,\n",
       "            'trainer': trainer,\n",
       "        },\n",
       "    }\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "# By setting materialize to 'False,' we can skip constructing the config\n",
    "# Constructing this configuration will result in downloading the required dataset,\n",
    "# so it may take a moment the first time it is constructed.\n",
    "nb.display_project_index(config_template=\"\", materialize=False, pp_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5eab2-edbe-47e1-b83e-b1858970740e",
   "metadata": {},
   "source": [
    "## Construct Baseline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf482ee-2bf6-4a40-bd81-f188a0f1e6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'name': 'Fashion MNIST Trainer',\n",
      "          'description': 'Base configuration, based on Torch tutorial '\n",
      "                         'parameters.',\n",
      "          'citation': 'https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html',\n",
      "          'hidden_dim': 512,\n",
      "          'epochs': 5,\n",
      "          'batch_size': 64,\n",
      "          'logging_steps': 100,\n",
      "          'lr': 0.001},\n",
      " 'main': {'model': Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=512, out_features=10, bias=True)\n",
      "),\n",
      "          'trainer': Trainer(train_dataloader=<torch.utils.data.dataloader.DataLoader object at 0x7fb14c7a71f0>, test_dataloader=<torch.utils.data.dataloader.DataLoader object at 0x7fb14c7a6e00>, model=Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=512, out_features=10, bias=True)\n",
      "), loss_fn=CrossEntropyLoss(), optimizer=SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      "), epochs=5, batch_size=64, logging_steps=100)}}\n"
     ]
    }
   ],
   "source": [
    "from forgather.project import Project\n",
    "from forgather.dotdict import DotDict\n",
    "from pprint import pp\n",
    "\n",
    "# Load default baseline config\n",
    "proj = Project(config_name=\"\")\n",
    "\n",
    "output = proj()\n",
    "pp(output)\n",
    "\n",
    "# Wrap the main output in DotDict for easier to read/type syntax by making dictionary keys look like attributes.\n",
    "config = DotDict(output['main'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df259d-4ce8-4a24-9c53-7ecfcef918c5",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "The trainer is started by simply calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a2a9f-bc40-464f-bd1e-f336d52f6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd1717-cf76-4661-a9ae-68df074acf06",
   "metadata": {},
   "source": [
    "## Construct and Train with Adam Optimizer\n",
    "\n",
    "This just goes straight from config to train, using the 'adam.yaml' configuration, where we have replaced the SGD optimizer with Adam.\n",
    "\n",
    "Take a look at the actual config definition to see what changes were required to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9131188-a9a2-46c4-84a9-3e207e194e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Project(config_name=\"adam.yaml\")()['main']['trainer']()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c182f-da2e-4ea4-8ef1-35a83ad7622c",
   "metadata": {},
   "source": [
    "## Run all Project Configurations\n",
    "\n",
    "You can directly load the meta-config only and use it to find and iterate over all configurations in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639bae8c-5113-4209-8a68-b52b33eee711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.meta_config import MetaConfig\n",
    "\n",
    "meta = MetaConfig()\n",
    "for config_name, _ in meta.find_templates(meta.config_prefix):\n",
    "    proj = Project(config_name=config_name)\n",
    "    print(f\"{ ' Starting ' + proj.config_name + ' ':-^60}\")\n",
    "    proj()['main']['trainer']()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9df0f2-6190-4692-85b7-a5034b4bc2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
