{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../notebooks/forgather.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Traning  for Fashion\n",
       "\n",
       "This project reproduces the configuration from a PyTorch tutorial, where a simple ML model is created and trained to recognize categories of clothing from the FashionMNIST dataset.\n",
       "\n",
       "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
       "\n",
       "This was chosen as it is a relatively simple project which can be relativley self contained. Still, it is far more complex than the previous examples.\n",
       "\n",
       "The model itself does not require any custom code. It's simply a stack of PyTorch layers, chained together with a nn.Sequential. If you would like to know more about the model itself, see:\n",
       "\n",
       "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
       "\n",
       "## Custom Code\n",
       "\n",
       "While Forgather is good at assembling objects, the language is not practical for defining logic. For this, we have defined a custom \"trainer\" class in the project's 'src' directory and we will use Forgather to dynamically import this code, injecting all the required dependencies.\n",
       "\n",
       "Unlike the previous projects, you will note that the \"Modules\" section not empty and has a link to the Trainer definition.\n",
       "\n",
       "## Project Structure\n",
       "\n",
       "Like the previous example, this project makes use of template inheritance, where there is a common 'project.yaml' file from which all of the configuratioins are derived.\n",
       "\n",
       "The template provides the basic structure, with 'blocks' which may be substituted or extended by child templates. We use this functionaity in the configurations to override various components of the base configuraiton.\n",
       "\n",
       "This is still a relatively simple project, as it does not reference any external template libraries. We will get to that in the next example.\n",
       "\n",
       "## Code Generation\n",
       "\n",
       "Note the output of the code generator. It has detected the inclusion of a dynamic import, thus it has automatically defined a function for importing dynamic modules.\n",
       "\n",
       "Also note that it knows how to translate some of the rather clunky expreressions from the original YAML file, like calling a method, into relatively clean Python code.\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/tutorials/project_gamma\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/tutorials/project_gamma/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/tutorials/project_gamma/templates](templates)\n",
       "\n",
       "## Available Configurations\n",
       "- [wider.yaml](templates/experiments/wider.yaml)\n",
       "- [deeper.yaml](templates/experiments/deeper.yaml)\n",
       "- [adam.yaml](templates/experiments/adam.yaml)\n",
       "- [baseline.yaml](templates/experiments/baseline.yaml)\n",
       "\n",
       "Default Configuration: baseline.yaml\n",
       "\n",
       "Active Configuration: baseline.yaml\n",
       "\n",
       "## Included Templates\n",
       "- [experiments/baseline.yaml](templates/experiments/baseline.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [formatting.yaml](templates/formatting.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'citation': 'https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html',\n",
       " 'config_class': 'fashion_trainer',\n",
       " 'config_description': 'Base configuration, based on Torch tutorial '\n",
       "                       'parameters.',\n",
       " 'config_name': 'Fashion MNIST Trainer'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "- [./src/trainer.py](src/trainer.py) : Trainer\n",
       "    - [/home/dinalt/ai_assets/forgather/tutorials/project_gamma/./src/trainer.py](src/trainer.py) : trainer\n",
       "## Output Targets\n",
       "- meta\n",
       "- main\n",
       "- args\n",
       "- model_params\n",
       "- model\n",
       "- training_data\n",
       "- test_data\n",
       "- train_dataloader\n",
       "- test_dataloader\n",
       "- loss_fn\n",
       "- optimizer\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "\n",
       "#---------------------------------------\n",
       "#          Fashion MNIST Trainer         \n",
       "#---------------------------------------\n",
       "# 2024-08-16T04:31:35\n",
       "# Description: Base configuration, based on Torch tutorial parameters.\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/tutorials/project_gamma\n",
       "# Citation: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
       "#---------------------------------------\n",
       "\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.hidden_dim = 512\n",
       "# ns.epochs = 5\n",
       "# ns.batch_size = 64\n",
       "# ns.lr = 0.001\n",
       "# ns.logging_steps = 100\n",
       "\n",
       "\n",
       "\n",
       "########### Model Definition ###########\n",
       "\n",
       ".define: &activation_fn !factory:torch.nn:ReLU@activation_fn []\n",
       "\n",
       ".define: &model !singleton:torch.nn:Sequential@model\n",
       "    - !factory:torch.nn:Flatten []\n",
       "    - !factory:torch.nn:Linear [ 784, 512 ]\n",
       "    - *activation_fn\n",
       "    - !factory:torch.nn:Linear [ 512, 512 ]\n",
       "    - *activation_fn\n",
       "    - !factory:torch.nn:Linear [ 512, 10 ]\n",
       "\n",
       "############### Dataset ################\n",
       "\n",
       ".define: &transform !factory:torchvision.transforms:ToTensor@transform []\n",
       "\n",
       ".define: &training_data !singleton:torchvision.datasets:FashionMNIST\n",
       "    root: \"data\"\n",
       "    train: True\n",
       "    download: True\n",
       "    transform: *transform\n",
       "\n",
       ".define: &test_data !singleton:torchvision.datasets:FashionMNIST\n",
       "    root: \"data\"\n",
       "    train: False\n",
       "    download: True\n",
       "    transform: *transform\n",
       "\n",
       ".define: &train_dataloader !singleton:torch.utils.data:DataLoader\n",
       "    args: [ *training_data ]\n",
       "    kwargs: { batch_size: 64 }\n",
       "\n",
       ".define: &test_dataloader !singleton:torch.utils.data:DataLoader\n",
       "    args: [ *test_data ]\n",
       "    kwargs: { batch_size: 64 }\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       ".define: &model_params !singleton:call [ !singleton:getattr [ *model, \"parameters\" ] ]\n",
       "\n",
       "# **Optimizer**\n",
       "\n",
       ".define: &optimizer !singleton:torch.optim:SGD\n",
       "    args: [ *model_params ]\n",
       "    kwargs:\n",
       "        lr: 0.001\n",
       "\n",
       "# **Loss Function**\n",
       "\n",
       ".define: &loss_fn !singleton:torch.nn:CrossEntropyLoss []\n",
       "\n",
       "# **Trainer**\n",
       "\n",
       ".define: &trainer !singleton:./src/trainer.py:Trainer@trainer\n",
       "    train_dataloader: *train_dataloader\n",
       "    test_dataloader: *test_dataloader\n",
       "    model: *model\n",
       "    loss_fn: *loss_fn\n",
       "    optimizer: *optimizer\n",
       "    epochs: 5\n",
       "    batch_size: 64\n",
       "    logging_steps: 100\n",
       "\n",
       "################ Output ################\n",
       "\n",
       "meta:\n",
       "    config_name: \"Fashion MNIST Trainer\"\n",
       "    config_description: \"Base configuration, based on Torch tutorial parameters.\"\n",
       "    config_class: \"fashion_trainer\"\n",
       "    citation: \"https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\"\n",
       "\n",
       "main: *trainer\n",
       "\n",
       "args:\n",
       "    hidden_dim: 512\n",
       "    epochs: 5\n",
       "    batch_size: 64\n",
       "    logging_steps: 100\n",
       "    lr: 0.001\n",
       "model_params: *model_params\n",
       "model: *model\n",
       "training_data: *training_data\n",
       "test_data: *test_data\n",
       "train_dataloader: *train_dataloader\n",
       "test_dataloader: *test_dataloader\n",
       "loss_fn: *loss_fn\n",
       "optimizer: *optimizer\n",
       "\n",
       "```\n",
       "\n",
       "## Generated Code\n",
       "\n",
       "```python\n",
       "from torch.utils.data import DataLoader\n",
       "from torch.nn import Sequential\n",
       "from torch.nn import Linear\n",
       "from torch.optim import SGD\n",
       "from torch.nn import Flatten\n",
       "from torchvision.datasets import FashionMNIST\n",
       "from torchvision.transforms import ToTensor\n",
       "from torch.nn import CrossEntropyLoss\n",
       "from torch.nn import ReLU\n",
       "from importlib.util import spec_from_file_location, module_from_spec\n",
       "import os\n",
       "import sys\n",
       "\n",
       "# Import a dynamic module.\n",
       "def dynimport(module, name, searchpath):\n",
       "    module_path = module\n",
       "    module_name = os.path.basename(module).split(\".\")[0]\n",
       "    module_spec = spec_from_file_location(\n",
       "        module_name,\n",
       "        module_path,\n",
       "        submodule_search_locations=searchpath,\n",
       "    )\n",
       "    mod = module_from_spec(module_spec)\n",
       "    sys.modules[module_name] = mod\n",
       "    module_spec.loader.exec_module(mod)\n",
       "    for symbol in name.split(\".\"):\n",
       "        mod = getattr(mod, symbol)\n",
       "    return mod\n",
       "\n",
       "Trainer = lambda: dynimport(\"./src/trainer.py\", \"Trainer\", [])\n",
       "\n",
       "def construct(\n",
       "):\n",
       "    transform = lambda: ToTensor()\n",
       "\n",
       "    activation_fn = lambda: ReLU()\n",
       "\n",
       "    model = Sequential(\n",
       "        Flatten(),\n",
       "        Linear(\n",
       "            784,\n",
       "            512,\n",
       "        ),\n",
       "        activation_fn(),\n",
       "        Linear(\n",
       "            512,\n",
       "            512,\n",
       "        ),\n",
       "        activation_fn(),\n",
       "        Linear(\n",
       "            512,\n",
       "            10,\n",
       "        ),\n",
       "    )\n",
       "\n",
       "    trainer = Trainer()(\n",
       "        train_dataloader=DataLoader(\n",
       "            FashionMNIST(\n",
       "                root='data',\n",
       "                train=True,\n",
       "                download=True,\n",
       "                transform=transform(),\n",
       "            ),\n",
       "            batch_size=64,\n",
       "        ),\n",
       "        test_dataloader=DataLoader(\n",
       "            FashionMNIST(\n",
       "                root='data',\n",
       "                train=False,\n",
       "                download=True,\n",
       "                transform=transform(),\n",
       "            ),\n",
       "            batch_size=64,\n",
       "        ),\n",
       "        model=model,\n",
       "        loss_fn=CrossEntropyLoss(),\n",
       "        optimizer=SGD(\n",
       "            model.parameters(),\n",
       "            lr=0.001,\n",
       "        ),\n",
       "        epochs=5,\n",
       "        batch_size=64,\n",
       "        logging_steps=100,\n",
       "    )\n",
       "    \n",
       "    return trainer\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "nb.display_project_index(show_pp_config=True, show_generated_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5eab2-edbe-47e1-b83e-b1858970740e",
   "metadata": {},
   "source": [
    "## Construct Baseline Configuration\n",
    "\n",
    "The \"main\" output is the model trainer, but we also get the model and the test-dataset as auxiliary outputs as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf482ee-2bf6-4a40-bd81-f188a0f1e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "import forgather.nb.notebooks as nb\n",
    "from pprint import pp\n",
    "\n",
    "# Load default baseline config\n",
    "proj = Project()\n",
    "\n",
    "outputs = proj([\"main\", \"model\", \"test_data\"])\n",
    "pp(outputs)\n",
    "\n",
    "# For easier access\n",
    "trainer = outputs[\"main\"]\n",
    "model = outputs[\"model\"]\n",
    "test_data = outputs[\"test_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55e6b8-7c2d-402e-8008-4d7f2073e5d6",
   "metadata": {},
   "source": [
    "### Examine Dataset and Predict\n",
    "\n",
    "We can take a look at what's in the dataset using [Meerkat](http://meerkat.wiki/docs/start/tutorials/tutorial-data-frames.html).\n",
    "\n",
    "The raw image data is a 32x32 tensor, which we can convert to a greyscale image for rendering.\n",
    "\n",
    "The \"label\" is the is the ground-truth target the model is expected to predict.\n",
    "\n",
    "We will also add a lambda to the DataPanel, which will get the model's prediction for the image. Without any training, the untrained model is expected to fail miserabl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343d53c-1ce9-4992-bab8-b31e87875202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import meerkat as mk\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Show target index key\n",
    "print(\"Label Key: \", test_data.class_to_idx)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(x):\n",
    "    \"\"\"\n",
    "    Given a raw image, get the model's prediction.\n",
    "    \"\"\"\n",
    "    # The image is stored as a 32x32 uint8 Tensor\n",
    "    # Convert to float and add batch dimension\n",
    "    model_input = (x / 255).unsqueeze(0)\n",
    "    model.eval()\n",
    "\n",
    "    # Get model's prediciton logits for input\n",
    "    logits = model(model_input)\n",
    "\n",
    "    # Get the index for the strongest prediction.\n",
    "    return logits.argmax(1).item()\n",
    "\n",
    "def make_datapanel():\n",
    "    # Convert the test dataset's raw images into a Meerkat TensorColumn\n",
    "    raw_img_column = mk.TensorColumn(test_data.data)\n",
    "    \n",
    "    dp = mk.DataPanel(\n",
    "        {\n",
    "            # Get label and convert to Python int\n",
    "            \"label\": mk.TensorColumn(test_data.targets).defer(lambda x: x.item()),\n",
    "            # Lazy model inference; get model's prediction from raw image\n",
    "            \"prediction\": raw_img_column.defer(predict),\n",
    "            # Lazy conversion of raw images to images\n",
    "            \"image\": raw_img_column.defer(lambda x: Image.fromarray(x.numpy()))\n",
    "        }\n",
    "    )\n",
    "    return dp\n",
    "\n",
    "# Construct the Meerkat DataPanel and display the first 10 cells.\n",
    "dp = make_datapanel()\n",
    "dp[:10]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df259d-4ce8-4a24-9c53-7ecfcef918c5",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "The trainer is started by simply calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a2a9f-bc40-464f-bd1e-f336d52f6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cd550-ac32-4dcb-ad2e-e6e5c94135ed",
   "metadata": {},
   "source": [
    "### Test the Trained Model\n",
    "\n",
    "We will display the same 10 data-cells as before, but now that the model has been trained, it's much less awful at the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccffa4f-d932-441d-b906-8fe30fd921ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp[:10]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd1717-cf76-4661-a9ae-68df074acf06",
   "metadata": {},
   "source": [
    "## Construct and Train with Adam Optimizer\n",
    "\n",
    "This just goes straight from config to train, using the 'adam.yaml' configuration, where we have replaced the SGD optimizer with Adam.\n",
    "\n",
    "Take a look at the actual config definition to see what changes were required to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9131188-a9a2-46c4-84a9-3e207e194e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = Project(config_name=\"adam.yaml\")\n",
    "outputs = proj([\"main\", \"model\", \"test_data\"])\n",
    "\n",
    "trainer = outputs[\"main\"]\n",
    "trainer()\n",
    "\n",
    "# Regenerate the DataPanel and show predictions.\n",
    "dp = make_datapanel()\n",
    "dp[:10]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c182f-da2e-4ea4-8ef1-35a83ad7622c",
   "metadata": {},
   "source": [
    "## Run all Project Configurations\n",
    "\n",
    "You can directly load the meta-config only and use it to find and iterate over all configurations in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639bae8c-5113-4209-8a68-b52b33eee711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.meta_config import MetaConfig\n",
    "\n",
    "meta = MetaConfig()\n",
    "for config_name, _ in meta.find_templates(meta.config_prefix):\n",
    "    proj = Project(config_name=config_name)\n",
    "    print(f\"{ ' Starting ' + proj.config_name + ' ':-^60}\")\n",
    "    trainer = proj()\n",
    "    trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9df0f2-6190-4692-85b7-a5034b4bc2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
