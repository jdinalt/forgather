{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../notebooks/forgather.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Traning  for Fashion\n",
       "\n",
       "This project reproduces the configuration from a PyTorch tutorial, where a simple ML model is created and trained to recognize categories of clothing from the FashionMNIST dataset.\n",
       "\n",
       "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
       "\n",
       "This was chosen as it is a relatively simple project which can be relativley self contained. Still, it is far more complex than the previous examples.\n",
       "\n",
       "The model itself does not require any custom code. It's simply a stack of PyTorch layers, chained together with a nn.Sequential. If you would like to know more about the model itself, see:\n",
       "\n",
       "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
       "\n",
       "## Custom Code\n",
       "\n",
       "While Forgather is good at assembling objects, the language is not practical for defining logic. For this, we have defined a custom \"trainer\" class in the project's 'src' directory and we will use Forgather to dynamically import this code, injecting all the required dependencies.\n",
       "\n",
       "Unlike the previous projects, you will note that the \"Modules\" section not empty and has a link to the Trainer definition.\n",
       "\n",
       "## Project Structure\n",
       "\n",
       "Like the previous example, this project makes use of template inheritance, where there is a common 'project.yaml' file from which all of the configuratioins are derived.\n",
       "\n",
       "The template provides the basic structure, with 'blocks' which may be substituted or extended by child templates. We use this functionaity in the configurations to override various components of the base configuraiton.\n",
       "\n",
       "This is still a relatively simple project, as it does not reference any external template libraries. We will get to that in the next example.\n",
       "\n",
       "## Code Generation\n",
       "\n",
       "Note the output of the code generator. It has detected the inclusion of a dynamic import, thus it has automatically defined a function for importing dynamic modules.\n",
       "\n",
       "Also note that it knows how to translate some of the rather clunky expreressions from the original YAML file, like calling a method, into relatively clean Python code.\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/tutorials/project_gamma\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/tutorials/project_gamma/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/tutorials/project_gamma/templates](templates)\n",
       "\n",
       "## Available Configurations\n",
       "- [wider.yaml](templates/experiments/wider.yaml)\n",
       "- [deeper.yaml](templates/experiments/deeper.yaml)\n",
       "- [adam.yaml](templates/experiments/adam.yaml)\n",
       "- [baseline.yaml](templates/experiments/baseline.yaml)\n",
       "\n",
       "Default Configuration: baseline.yaml\n",
       "\n",
       "Active Configuration: baseline.yaml\n",
       "\n",
       "## Included Templates\n",
       "- [experiments/baseline.yaml](templates/experiments/baseline.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [formatting.yaml](templates/formatting.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'citation': 'https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html',\n",
       " 'config_class': 'fashion_trainer',\n",
       " 'config_description': 'Base configuration, based on Torch tutorial '\n",
       "                       'parameters.',\n",
       " 'config_name': 'Fashion MNIST Trainer'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "- [./src/trainer.py](src/trainer.py) : Trainer\n",
       "    - [/home/dinalt/ai_assets/forgather/tutorials/project_gamma/./src/trainer.py](src/trainer.py) : trainer\n",
       "## Output Targets\n",
       "- meta\n",
       "- main\n",
       "- args\n",
       "- model_params\n",
       "- model\n",
       "- training_data\n",
       "- test_data\n",
       "- train_dataloader\n",
       "- test_dataloader\n",
       "- loss_fn\n",
       "- optimizer\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "\n",
       "#---------------------------------------\n",
       "#          Fashion MNIST Trainer         \n",
       "#---------------------------------------\n",
       "# 2024-08-15T07:00:22\n",
       "# Description: Base configuration, based on Torch tutorial parameters.\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/tutorials/project_gamma\n",
       "# Citation: https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
       "#---------------------------------------\n",
       "\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.hidden_dim = 512\n",
       "# ns.epochs = 5\n",
       "# ns.batch_size = 64\n",
       "# ns.lr = 0.001\n",
       "# ns.logging_steps = 100\n",
       "\n",
       "\n",
       "\n",
       "########### Model Definition ###########\n",
       "\n",
       ".define: &activation_fn !factory:torch.nn:ReLU@activation_fn []\n",
       "\n",
       ".define: &model !singleton:torch.nn:Sequential@model\n",
       "    - !factory:torch.nn:Flatten []\n",
       "    - !factory:torch.nn:Linear [ 784, 512 ]\n",
       "    - *activation_fn\n",
       "    - !factory:torch.nn:Linear [ 512, 512 ]\n",
       "    - *activation_fn\n",
       "    - !factory:torch.nn:Linear [ 512, 10 ]\n",
       "\n",
       "############### Dataset ################\n",
       "\n",
       ".define: &transform !factory:torchvision.transforms:ToTensor@transform []\n",
       "\n",
       ".define: &training_data !singleton:torchvision.datasets:FashionMNIST\n",
       "    root: \"data\"\n",
       "    train: True\n",
       "    download: True\n",
       "    transform: *transform\n",
       "\n",
       ".define: &test_data !singleton:torchvision.datasets:FashionMNIST\n",
       "    root: \"data\"\n",
       "    train: False\n",
       "    download: True\n",
       "    transform: *transform\n",
       "\n",
       ".define: &train_dataloader !singleton:torch.utils.data:DataLoader\n",
       "    args: [ *training_data ]\n",
       "    kwargs: { batch_size: 64 }\n",
       "\n",
       ".define: &test_dataloader !singleton:torch.utils.data:DataLoader\n",
       "    args: [ *test_data ]\n",
       "    kwargs: { batch_size: 64 }\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       ".define: &model_params !singleton:call [ !singleton:getattr [ *model, \"parameters\" ] ]\n",
       "\n",
       "# **Optimizer**\n",
       "\n",
       ".define: &optimizer !singleton:torch.optim:SGD\n",
       "    args: [ *model_params ]\n",
       "    kwargs:\n",
       "        lr: 0.001\n",
       "\n",
       "# **Loss Function**\n",
       "\n",
       ".define: &loss_fn !singleton:torch.nn:CrossEntropyLoss []\n",
       "\n",
       "# **Trainer**\n",
       "\n",
       ".define: &trainer !singleton:./src/trainer.py:Trainer@trainer\n",
       "    train_dataloader: *train_dataloader\n",
       "    test_dataloader: *test_dataloader\n",
       "    model: *model\n",
       "    loss_fn: *loss_fn\n",
       "    optimizer: *optimizer\n",
       "    epochs: 5\n",
       "    batch_size: 64\n",
       "    logging_steps: 100\n",
       "\n",
       "################ Output ################\n",
       "\n",
       "meta:\n",
       "    config_name: \"Fashion MNIST Trainer\"\n",
       "    config_description: \"Base configuration, based on Torch tutorial parameters.\"\n",
       "    config_class: \"fashion_trainer\"\n",
       "    citation: \"https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\"\n",
       "\n",
       "main: *trainer\n",
       "\n",
       "args:\n",
       "    hidden_dim: 512\n",
       "    epochs: 5\n",
       "    batch_size: 64\n",
       "    logging_steps: 100\n",
       "    lr: 0.001\n",
       "model_params: *model_params\n",
       "model: *model\n",
       "training_data: *training_data\n",
       "test_data: *test_data\n",
       "train_dataloader: *train_dataloader\n",
       "test_dataloader: *test_dataloader\n",
       "loss_fn: *loss_fn\n",
       "optimizer: *optimizer\n",
       "\n",
       "```\n",
       "\n",
       "## Generated Code\n",
       "\n",
       "```python\n",
       "from torch.nn import ReLU\n",
       "from torchvision.transforms import ToTensor\n",
       "from torch.nn import CrossEntropyLoss\n",
       "from torch.nn import Flatten\n",
       "from torch.utils.data import DataLoader\n",
       "from torch.nn import Sequential\n",
       "from torch.optim import SGD\n",
       "from torch.nn import Linear\n",
       "from torchvision.datasets import FashionMNIST\n",
       "from importlib.util import spec_from_file_location, module_from_spec\n",
       "import os\n",
       "import sys\n",
       "\n",
       "# Import a dynamic module.\n",
       "def dynimport(module, name, searchpath):\n",
       "    module_path = module\n",
       "    module_name = os.path.basename(module).split(\".\")[0]\n",
       "    module_spec = spec_from_file_location(\n",
       "        module_name,\n",
       "        module_path,\n",
       "        submodule_search_locations=searchpath,\n",
       "    )\n",
       "    mod = module_from_spec(module_spec)\n",
       "    sys.modules[module_name] = mod\n",
       "    module_spec.loader.exec_module(mod)\n",
       "    for symbol in name.split(\".\"):\n",
       "        mod = getattr(mod, symbol)\n",
       "    return mod\n",
       "\n",
       "Trainer = lambda: dynimport(\"./src/trainer.py\", \"Trainer\", [])\n",
       "\n",
       "def construct(\n",
       "):\n",
       "    transform = lambda: ToTensor()\n",
       "\n",
       "    activation_fn = lambda: ReLU()\n",
       "\n",
       "    model = Sequential(\n",
       "        Flatten(),\n",
       "        Linear(\n",
       "            784,\n",
       "            512,\n",
       "        ),\n",
       "        activation_fn(),\n",
       "        Linear(\n",
       "            512,\n",
       "            512,\n",
       "        ),\n",
       "        activation_fn(),\n",
       "        Linear(\n",
       "            512,\n",
       "            10,\n",
       "        ),\n",
       "    )\n",
       "\n",
       "    trainer = Trainer()(\n",
       "        train_dataloader=DataLoader(\n",
       "            FashionMNIST(\n",
       "                root='data',\n",
       "                train=True,\n",
       "                download=True,\n",
       "                transform=transform(),\n",
       "            ),\n",
       "            batch_size=64,\n",
       "        ),\n",
       "        test_dataloader=DataLoader(\n",
       "            FashionMNIST(\n",
       "                root='data',\n",
       "                train=False,\n",
       "                download=True,\n",
       "                transform=transform(),\n",
       "            ),\n",
       "            batch_size=64,\n",
       "        ),\n",
       "        model=model,\n",
       "        loss_fn=CrossEntropyLoss(),\n",
       "        optimizer=SGD(\n",
       "            model.parameters(),\n",
       "            lr=0.001,\n",
       "        ),\n",
       "        epochs=5,\n",
       "        batch_size=64,\n",
       "        logging_steps=100,\n",
       "    )\n",
       "    \n",
       "    return trainer\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "nb.display_project_index(show_pp_config=True, show_generated_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5eab2-edbe-47e1-b83e-b1858970740e",
   "metadata": {},
   "source": [
    "## Construct Baseline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf482ee-2bf6-4a40-bd81-f188a0f1e6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main': Trainer(train_dataloader=<torch.utils.data.dataloader.DataLoader object at 0x7ff2a59e3220>, test_dataloader=<torch.utils.data.dataloader.DataLoader object at 0x7ff2a594e050>, model=Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=512, out_features=10, bias=True)\n",
      "), loss_fn=CrossEntropyLoss(), optimizer=SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      "), epochs=5, batch_size=64, logging_steps=100),\n",
      " 'model': Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=512, out_features=10, bias=True)\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "from forgather.project import Project\n",
    "import forgather.nb.notebooks as nb\n",
    "from pprint import pp\n",
    "\n",
    "# Load default baseline config\n",
    "proj = Project()\n",
    "\n",
    "outputs = proj([\"main\", \"model\"])\n",
    "pp(outputs)\n",
    "\n",
    "# For easier access\n",
    "trainer = outputs[\"main\"]\n",
    "model = outputs[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df259d-4ce8-4a24-9c53-7ecfcef918c5",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "The trainer is started by simply calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f92a2a9f-bc40-464f-bd1e-f336d52f6b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ec275a309a467bb08158d586a20f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:04:43          100  0.11  train-loss: 2.281     \n",
      "2024-08-15 07:04:44          200  0.21  train-loss: 2.26253   \n",
      "2024-08-15 07:04:44          300  0.32  train-loss: 2.25173   \n",
      "2024-08-15 07:04:45          400  0.43  train-loss: 2.2478    \n",
      "2024-08-15 07:04:45          500  0.53  train-loss: 2.22504   \n",
      "2024-08-15 07:04:46          600  0.64  train-loss: 2.20235   \n",
      "2024-08-15 07:04:46          700  0.75  train-loss: 2.20056   \n",
      "2024-08-15 07:04:47          800  0.85  train-loss: 2.18143   \n",
      "2024-08-15 07:04:47          900  0.96  train-loss: 2.14116   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:04:48          938  1.0   eval-loss:  2.14191   accuracy: 50.9\n",
      "2024-08-15 07:04:48        1,000  1.07  train-loss: 2.13721   \n",
      "2024-08-15 07:04:49        1,100  1.17  train-loss: 2.07268   \n",
      "2024-08-15 07:04:49        1,200  1.28  train-loss: 2.08282   \n",
      "2024-08-15 07:04:50        1,300  1.39  train-loss: 2.06193   \n",
      "2024-08-15 07:04:50        1,400  1.49  train-loss: 2.05358   \n",
      "2024-08-15 07:04:51        1,500  1.6   train-loss: 1.98428   \n",
      "2024-08-15 07:04:51        1,600  1.71  train-loss: 1.91841   \n",
      "2024-08-15 07:04:52        1,700  1.81  train-loss: 1.9116    \n",
      "2024-08-15 07:04:52        1,800  1.92  train-loss: 1.83476   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:04:54        1,876  2.0   eval-loss:  1.8467    accuracy: 61.0\n",
      "2024-08-15 07:04:54        1,900  2.03  train-loss: 1.81424   \n",
      "2024-08-15 07:04:54        2,000  2.13  train-loss: 1.82146   \n",
      "2024-08-15 07:04:55        2,100  2.24  train-loss: 1.73323   \n",
      "2024-08-15 07:04:55        2,200  2.35  train-loss: 1.70688   \n",
      "2024-08-15 07:04:56        2,300  2.45  train-loss: 1.66776   \n",
      "2024-08-15 07:04:56        2,400  2.56  train-loss: 1.6131    \n",
      "2024-08-15 07:04:57        2,500  2.67  train-loss: 1.60799   \n",
      "2024-08-15 07:04:57        2,600  2.77  train-loss: 1.55674   \n",
      "2024-08-15 07:04:58        2,700  2.88  train-loss: 1.52204   \n",
      "2024-08-15 07:04:58        2,800  2.99  train-loss: 1.4762    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:04:59        2,814  3.0   eval-loss:  1.47434   accuracy: 60.8\n",
      "2024-08-15 07:04:59        2,900  3.09  train-loss: 1.40874   \n",
      "2024-08-15 07:05:00        3,000  3.2   train-loss: 1.5162    \n",
      "2024-08-15 07:05:00        3,100  3.3   train-loss: 1.44716   \n",
      "2024-08-15 07:05:01        3,200  3.41  train-loss: 1.37735   \n",
      "2024-08-15 07:05:01        3,300  3.52  train-loss: 1.33895   \n",
      "2024-08-15 07:05:02        3,400  3.62  train-loss: 1.35463   \n",
      "2024-08-15 07:05:02        3,500  3.73  train-loss: 1.32036   \n",
      "2024-08-15 07:05:03        3,600  3.84  train-loss: 1.38361   \n",
      "2024-08-15 07:05:03        3,700  3.94  train-loss: 1.28251   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:05:04        3,752  4.0   eval-loss:  1.22872   accuracy: 62.2\n",
      "2024-08-15 07:05:05        3,800  4.05  train-loss: 1.20158   \n",
      "2024-08-15 07:05:05        3,900  4.16  train-loss: 1.21234   \n",
      "2024-08-15 07:05:06        4,000  4.26  train-loss: 1.01742   \n",
      "2024-08-15 07:05:06        4,100  4.37  train-loss: 1.15856   \n",
      "2024-08-15 07:05:07        4,200  4.48  train-loss: 1.22419   \n",
      "2024-08-15 07:05:07        4,300  4.58  train-loss: 1.02989   \n",
      "2024-08-15 07:05:08        4,400  4.69  train-loss: 1.06938   \n",
      "2024-08-15 07:05:08        4,500  4.8   train-loss: 1.05031   \n",
      "2024-08-15 07:05:09        4,600  4.9   train-loss: 1.12351   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:05:10        4,690  5.0   eval-loss:  1.07941   accuracy: 63.7\n"
     ]
    }
   ],
   "source": [
    "trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd1717-cf76-4661-a9ae-68df074acf06",
   "metadata": {},
   "source": [
    "## Construct and Train with Adam Optimizer\n",
    "\n",
    "This just goes straight from config to train, using the 'adam.yaml' configuration, where we have replaced the SGD optimizer with Adam.\n",
    "\n",
    "Take a look at the actual config definition to see what changes were required to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9131188-a9a2-46c4-84a9-3e207e194e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca2f32e99a54dbb83217a90e9e8d257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:05:48          100  0.11  train-loss: 0.62368   \n",
      "2024-08-15 07:05:49          200  0.21  train-loss: 0.62747   \n",
      "2024-08-15 07:05:49          300  0.32  train-loss: 0.34301   \n",
      "2024-08-15 07:05:50          400  0.43  train-loss: 0.61325   \n",
      "2024-08-15 07:05:50          500  0.53  train-loss: 0.2276    \n",
      "2024-08-15 07:05:51          600  0.64  train-loss: 0.33854   \n",
      "2024-08-15 07:05:51          700  0.75  train-loss: 0.65316   \n",
      "2024-08-15 07:05:52          800  0.85  train-loss: 0.40484   \n",
      "2024-08-15 07:05:53          900  0.96  train-loss: 0.31205   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:05:53          938  1.0   eval-loss:  0.42341   accuracy: 84.5\n",
      "2024-08-15 07:05:54        1,000  1.07  train-loss: 0.38436   \n",
      "2024-08-15 07:05:54        1,100  1.17  train-loss: 0.41055   \n",
      "2024-08-15 07:05:55        1,200  1.28  train-loss: 0.45073   \n",
      "2024-08-15 07:05:55        1,300  1.39  train-loss: 0.24093   \n",
      "2024-08-15 07:05:56        1,400  1.49  train-loss: 0.4748    \n",
      "2024-08-15 07:05:57        1,500  1.6   train-loss: 0.20765   \n",
      "2024-08-15 07:05:57        1,600  1.71  train-loss: 0.32071   \n",
      "2024-08-15 07:05:58        1,700  1.81  train-loss: 0.60491   \n",
      "2024-08-15 07:05:58        1,800  1.92  train-loss: 0.28574   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:05:59        1,876  2.0   eval-loss:  0.39034   accuracy: 85.3\n",
      "2024-08-15 07:06:00        1,900  2.03  train-loss: 0.249     \n",
      "2024-08-15 07:06:00        2,000  2.13  train-loss: 0.41074   \n",
      "2024-08-15 07:06:01        2,100  2.24  train-loss: 0.32845   \n",
      "2024-08-15 07:06:01        2,200  2.35  train-loss: 0.3349    \n",
      "2024-08-15 07:06:02        2,300  2.45  train-loss: 0.37223   \n",
      "2024-08-15 07:06:02        2,400  2.56  train-loss: 0.48951   \n",
      "2024-08-15 07:06:03        2,500  2.67  train-loss: 0.33798   \n",
      "2024-08-15 07:06:03        2,600  2.77  train-loss: 0.35123   \n",
      "2024-08-15 07:06:04        2,700  2.88  train-loss: 0.46253   \n",
      "2024-08-15 07:06:05        2,800  2.99  train-loss: 0.1777    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:06:05        2,814  3.0   eval-loss:  0.37397   accuracy: 86.2\n",
      "2024-08-15 07:06:06        2,900  3.09  train-loss: 0.40315   \n",
      "2024-08-15 07:06:06        3,000  3.2   train-loss: 0.51675   \n",
      "2024-08-15 07:06:07        3,100  3.3   train-loss: 0.34919   \n",
      "2024-08-15 07:06:07        3,200  3.41  train-loss: 0.172     \n",
      "2024-08-15 07:06:08        3,300  3.52  train-loss: 0.17076   \n",
      "2024-08-15 07:06:09        3,400  3.62  train-loss: 0.2518    \n",
      "2024-08-15 07:06:09        3,500  3.73  train-loss: 0.30597   \n",
      "2024-08-15 07:06:10        3,600  3.84  train-loss: 0.30211   \n",
      "2024-08-15 07:06:10        3,700  3.94  train-loss: 0.23603   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:06:11        3,752  4.0   eval-loss:  0.37112   accuracy: 86.2\n",
      "2024-08-15 07:06:12        3,800  4.05  train-loss: 0.25203   \n",
      "2024-08-15 07:06:12        3,900  4.16  train-loss: 0.31518   \n",
      "2024-08-15 07:06:13        4,000  4.26  train-loss: 0.14734   \n",
      "2024-08-15 07:06:13        4,100  4.37  train-loss: 0.44975   \n",
      "2024-08-15 07:06:14        4,200  4.48  train-loss: 0.27402   \n",
      "2024-08-15 07:06:14        4,300  4.58  train-loss: 0.23364   \n",
      "2024-08-15 07:06:15        4,400  4.69  train-loss: 0.09645   \n",
      "2024-08-15 07:06:15        4,500  4.8   train-loss: 0.09581   \n",
      "2024-08-15 07:06:16        4,600  4.9   train-loss: 0.29859   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 07:06:17        4,690  5.0   eval-loss:  0.35356   accuracy: 87.1\n"
     ]
    }
   ],
   "source": [
    "trainer = Project(config_name=\"adam.yaml\")()\n",
    "trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c182f-da2e-4ea4-8ef1-35a83ad7622c",
   "metadata": {},
   "source": [
    "## Run all Project Configurations\n",
    "\n",
    "You can directly load the meta-config only and use it to find and iterate over all configurations in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639bae8c-5113-4209-8a68-b52b33eee711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.meta_config import MetaConfig\n",
    "\n",
    "meta = MetaConfig()\n",
    "for config_name, _ in meta.find_templates(meta.config_prefix):\n",
    "    proj = Project(config_name=config_name)\n",
    "    print(f\"{ ' Starting ' + proj.config_name + ' ':-^60}\")\n",
    "    trainer = proj()\n",
    "    trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9df0f2-6190-4692-85b7-a5034b4bc2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
