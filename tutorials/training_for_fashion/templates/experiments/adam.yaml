-- extends "project.yaml"

-- block config_metadata
    == super()
    -- set ns.config_name = "Adam Optimizer"
    -- set ns.config_description = "Base configuration + Adam optimizer"
    -- set ns.model_name = "adam_optimized"
    -- set ns.log_name = ns.model_name
<< endblock config_metadata

## Override the trainer configuration from the project definition.
-- block trainer_definition
    -- include 'experiment.trainer_config'
-- endblock trainer_definition


#-------------------- experiment.trainer_config --------------------
-- extends 'project.trainer_config'

-- block trainer_constructor
    == super()

    ## Override the optimizer with an Adam optimizer
    # {{ ns.config_name }} Overrides
    optimizer_factory: !lambda:torch.optim:Adam
        ## Note: Lambdas are translated to partial functions. The tl;dr is that you need to
        ## explicitly specify positional args via argN, whereas kvargs are handled automatically.
        - !var "arg0"
-- endblock trainer_constructor