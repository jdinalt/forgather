{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173c6fe4-0561-4294-9fca-10eb3ad26c2c",
   "metadata": {},
   "source": [
    "# Advanced Forgather Syntax\n",
    "\n",
    "Below the \"project\" abstraction lies a lower level API. We use it here, as it's easier to use for syntax experimentation.\n",
    "\n",
    "We will start with a few simple examples and work our way to constructing a modular transformer model and feeding a training example through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576f49ed-6e78-4a86-ae9f-494d2a41fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..', 'src')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "\n",
    "from pprint import pp, pformat\n",
    "\n",
    "from IPython import display as ds\n",
    "\n",
    "from forgather.latent import Latent\n",
    "from forgather.config import ConfigEnvironment\n",
    "from forgather.preprocess import PPEnvironment\n",
    "from forgather.codegen import generate_code\n",
    "from forgather.yaml_encoder import to_yaml\n",
    "import forgather.nb.notebooks as nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162bace-b2fb-4041-8a75-c8ed1d613c1e",
   "metadata": {},
   "source": [
    "## Trivial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3ccc83-c952-4fc2-b78f-45d6258aa4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0921,  0.2509],\n",
       "        [-1.2511,  1.3328]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from forgather.config import ConfigEnvironment\n",
    "\n",
    "# Construct a configuration environment\n",
    "env = ConfigEnvironment()\n",
    "\n",
    "# Define a configuration\n",
    "# Here, we construct a 2x2 random tensor.\n",
    "document = \"\"\"\n",
    "!call:torch:randn [ 2, 2 ]\n",
    "\"\"\"\n",
    "\n",
    "# Convert the configuration to a graph\n",
    "graph = env.load_from_string(document).config\n",
    "\n",
    "# Construct the graph\n",
    "graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd881bb-4b85-4f65-835d-31942dcf1a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a function which computes the square-root of its argument.\n",
    "graph = env.load_from_string(\"main: !partial:math:sqrt []\").config\n",
    "graph.main(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ca26e-8977-40a2-896f-2acb1cb83afb",
   "metadata": {},
   "source": [
    "## Complex Example\n",
    "\n",
    "The following template defines a fairly simple causal transformer model. We will make use of the reusable model components library.\n",
    "\n",
    "[./templates/model_def.yaml](./templates/model_def.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c209c6a0-0ace-471a-ae84-25e295fbaf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Configuration Template\n",
       "```yaml\n",
       "-- set ns = namespace()\n",
       "-- from 'templates/formatting.jinja' import h1, h2, h3\n",
       "-- filter trim() ## This removes whitespace before the header.\n",
       "\n",
       "## Jina2 block definitions; we can override these in derived templates.\n",
       "-- block meta_config\n",
       "    -- set ns.model_src = '../../../modelsrc/transformer/'\n",
       "    -- set ns.config_name = 'Control'\n",
       "    -- set ns.config_description = \"Baseline Control\"\n",
       "    ## Example of variable set by jinja2 template.\n",
       "    -- set ns.vocab_size = 1024\n",
       "<< endblock meta_config\n",
       "\n",
       "\n",
       "-- endfilter\n",
       "-- block header\n",
       "== h1(ns.config_name)\n",
       "# {{ utcisotime() }}\n",
       "# Description: {{ ns.config_description }}\n",
       "# model_src = {{ ns.model_src }}\n",
       "# Current Working Dir: \"{{ getcwd() }}\"\n",
       "# Forgather Config Dir: \"{{ abspath(forgather_config_dir()) }}\"\n",
       "<< endblock header\n",
       "\n",
       "\n",
       "== h2(\"Model Definition\")\n",
       "\n",
       "== h3(\"Layer Norm Factory\")\n",
       "\n",
       "-- block layer_norm_factory\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    - !var \"hidden_size\"\n",
       "<< endblock layer_norm_factory\n",
       "\n",
       "\n",
       "== h3(\"Activation Factory\")\n",
       "\n",
       "-- block activation_factory\n",
       ".define: &activation_factory !partial:torch.nn:ReLU@activation_factory []\n",
       "<< endblock activation_factory\n",
       "\n",
       "\n",
       "== h3(\"Feedforward Factory\")\n",
       "\n",
       "-- block feedforward_factory\n",
       ".define: &feedforward_factory !partial:{{ns.model_src}}feedforward_layer.py:FeedforwardLayer@feedforward_factory\n",
       "    activation_factory: *activation_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "<< endblock feedforward_factory\n",
       "\n",
       "\n",
       "== h3(\"Attention Factory\")\n",
       "\n",
       "-- block attention_factory\n",
       ".define: &attention_factory !partial:{{ns.model_src}}single_head_attn.py:SingleHeadAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "<< endblock attention_factory\n",
       "\n",
       "\n",
       "== h3(\"Layer Factory\")\n",
       "\n",
       "-- block layer_factory\n",
       ".define: &layer_factory !partial:{{ns.model_src}}pre_ln_layer.py:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "<< endblock layer_factory\n",
       "\n",
       "\n",
       "== h3(\"Layer Stack Factory\")\n",
       "\n",
       "-- block layer_stack_factory\n",
       ".define: &layer_stack_factory !factory:{{ns.model_src}}layer_stack.py:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    num_hidden_layers: !var \"n_layers\"\n",
       "<< endblock layer_stack_factory\n",
       "\n",
       "\n",
       "== h3(\"Model\")\n",
       "\n",
       "-- block model\n",
       "## This block is not nearly as factored-out as the others, using inline-definiions.\n",
       ".define: &model !call:{{ns.model_src}}causal_lm.py:CasualLM@model\n",
       "    loss_fn: !factory:{{ns.model_src}}causal_loss.py:CausalLoss\n",
       "    input_encoder: !factory:{{ns.model_src}}input_encoder.py:InputEncoder\n",
       "        d_model: !var \"hidden_size\"\n",
       "        vocab_size: {{ ns.vocab_size }}\n",
       "    output_decoder: !factory:torch.nn:Linear [ !var \"hidden_size\", {{ ns.vocab_size }} ]\n",
       "    init_weights: !partial:{{ns.model_src}}init_weights.py:simple_weight_init\n",
       "    layer_stack: *layer_stack_factory\n",
       "<< endblock model\n",
       "\n",
       "== h3(\"Optimizer\")\n",
       "\n",
       "-- block optimizer\n",
       "## Define an optimizer\n",
       "optimizer: !partial:torch.optim:AdamW\n",
       "    lr: 1.0e-3\n",
       "<< endblock optimizer\n",
       "\n",
       "meta:\n",
       "    d_model: !var \"hidden_size\"\n",
       "    vocab_size: {{ ns.vocab_size }}\n",
       "\n",
       "## Main output\n",
       "main: *model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "template_path = os.path.join('templates', 'model_def.yaml')\n",
    "with open(template_path, 'r') as f:\n",
    "    nb.display_codeblock(\"yaml\", f.read(), \"### Configuration Template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df2615-dde6-4965-9849-1db05fa7051a",
   "metadata": {},
   "source": [
    "## Preprocess the Template\n",
    "\n",
    "This will only run the Jinja preprocessor.\n",
    "\n",
    "This more or less looks like the original template..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3aa9dd-688c-417e-80d7-a527d427d0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Pre Processed Template\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#                 Control                \n",
       "#---------------------------------------\n",
       "# 2025-06-17T08:07:23\n",
       "# Description: Baseline Control\n",
       "# model_src = ../../../modelsrc/transformer/\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/basic/syntax\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "\n",
       "########### Model Definition ###########\n",
       "\n",
       "# **Layer Norm Factory**\n",
       "\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    - !var \"hidden_size\"\n",
       "\n",
       "# **Activation Factory**\n",
       "\n",
       ".define: &activation_factory !partial:torch.nn:ReLU@activation_factory []\n",
       "\n",
       "# **Feedforward Factory**\n",
       "\n",
       ".define: &feedforward_factory !partial:../../../modelsrc/transformer/feedforward_layer.py:FeedforwardLayer@feedforward_factory\n",
       "    activation_factory: *activation_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "\n",
       "# **Attention Factory**\n",
       "\n",
       ".define: &attention_factory !partial:../../../modelsrc/transformer/single_head_attn.py:SingleHeadAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "\n",
       "# **Layer Factory**\n",
       "\n",
       ".define: &layer_factory !partial:../../../modelsrc/transformer/pre_ln_layer.py:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "\n",
       "# **Layer Stack Factory**\n",
       "\n",
       ".define: &layer_stack_factory !factory:../../../modelsrc/transformer/layer_stack.py:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    num_hidden_layers: !var \"n_layers\"\n",
       "\n",
       "# **Model**\n",
       "\n",
       ".define: &model !call:../../../modelsrc/transformer/causal_lm.py:CasualLM@model\n",
       "    loss_fn: !factory:../../../modelsrc/transformer/causal_loss.py:CausalLoss\n",
       "    input_encoder: !factory:../../../modelsrc/transformer/input_encoder.py:InputEncoder\n",
       "        d_model: !var \"hidden_size\"\n",
       "        vocab_size: 1024\n",
       "    output_decoder: !factory:torch.nn:Linear [ !var \"hidden_size\", 1024 ]\n",
       "    init_weights: !partial:../../../modelsrc/transformer/init_weights.py:simple_weight_init\n",
       "    layer_stack: *layer_stack_factory\n",
       "# **Optimizer**\n",
       "\n",
       "optimizer: !partial:torch.optim:AdamW\n",
       "    lr: 1.0e-3\n",
       "meta:\n",
       "    d_model: !var \"hidden_size\"\n",
       "    vocab_size: 1024\n",
       "\n",
       "main: *model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = ConfigEnvironment()\n",
    "\n",
    "pp_config = env.preprocess(template_path)\n",
    "nb.display_codeblock(\"yaml\", pp_config, \"### Pre Processed Template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427961e-dc0c-4cf0-bb27-7a4dee8743d2",
   "metadata": {},
   "source": [
    "## Construct Model Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6192bc1d-72dd-4b92-998a-35561ea993d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CasualLM(\n",
       "  loss_fn=CausalLoss()\n",
       "  (input_encoder): InputEncoder(\n",
       "    d_model=64, vocab_size=1024\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (embedding): Embedding(1024, 64)\n",
       "  )\n",
       "  (output_decoder): Linear(in_features=64, out_features=1024, bias=True)\n",
       "  (layer_stack): LayerStack(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x PreLNLayer(\n",
       "        (feedforward): FeedforwardLayer(\n",
       "          d_model=64, d_feedforward=256\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Identity()\n",
       "          (activation): ReLU()\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "        (attention): SingleHeadAttn(\n",
       "          d_model=64, bias=True\n",
       "          (query_key_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (residual_dropout): Identity()\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = env.load(template_path).config\n",
    "\n",
    "model_args =dict(\n",
    "    hidden_size=64,\n",
    "    dim_feedforward=256,\n",
    "    n_layers=2,\n",
    ")\n",
    "\n",
    "graph.main(context_vars=model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1fdb25-72c8-43be-b8d2-da3cd91c6b1e",
   "metadata": {},
   "source": [
    "### Override Something\n",
    "\n",
    "For our experiment, we will want to change just one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3f3930-a245-4472-801a-3ff683af52a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Pre Processed Experiment Config\n",
       "```yaml\n",
       "\n",
       "#---------------------------------------\n",
       "#                 No Bias                \n",
       "#---------------------------------------\n",
       "# 2025-06-17T08:07:25\n",
       "# Description: Disabled bias in attention. Does it matter?\n",
       "# model_src = ../../../modelsrc/transformer/\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/basic/syntax\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "\n",
       "########### Model Definition ###########\n",
       "\n",
       "# **Layer Norm Factory**\n",
       "\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    - !var \"hidden_size\"\n",
       "\n",
       "# **Activation Factory**\n",
       "\n",
       ".define: &activation_factory !partial:torch.nn:ReLU@activation_factory []\n",
       "\n",
       "# **Feedforward Factory**\n",
       "\n",
       ".define: &feedforward_factory !partial:../../../modelsrc/transformer/feedforward_layer.py:FeedforwardLayer@feedforward_factory\n",
       "    activation_factory: *activation_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "\n",
       "# **Attention Factory**\n",
       "\n",
       ".define: &attention_factory !partial:../../../modelsrc/transformer/single_head_attn.py:SingleHeadAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "\n",
       "    # Experiment override.\n",
       "    bias: False\n",
       "\n",
       "# **Layer Factory**\n",
       "\n",
       ".define: &layer_factory !partial:../../../modelsrc/transformer/pre_ln_layer.py:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "\n",
       "# **Layer Stack Factory**\n",
       "\n",
       ".define: &layer_stack_factory !factory:../../../modelsrc/transformer/layer_stack.py:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    num_hidden_layers: !var \"n_layers\"\n",
       "\n",
       "# **Model**\n",
       "\n",
       ".define: &model !call:../../../modelsrc/transformer/causal_lm.py:CasualLM@model\n",
       "    loss_fn: !factory:../../../modelsrc/transformer/causal_loss.py:CausalLoss\n",
       "    input_encoder: !factory:../../../modelsrc/transformer/input_encoder.py:InputEncoder\n",
       "        d_model: !var \"hidden_size\"\n",
       "        vocab_size: 1024\n",
       "    output_decoder: !factory:torch.nn:Linear [ !var \"hidden_size\", 1024 ]\n",
       "    init_weights: !partial:../../../modelsrc/transformer/init_weights.py:simple_weight_init\n",
       "    layer_stack: *layer_stack_factory\n",
       "# **Optimizer**\n",
       "\n",
       "optimizer: !partial:torch.optim:AdamW\n",
       "    lr: 1.0e-3\n",
       "meta:\n",
       "    d_model: !var \"hidden_size\"\n",
       "    vocab_size: 1024\n",
       "\n",
       "main: *model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_config = \"\"\"\n",
    "-- extends 'templates/model_def.yaml'\n",
    "\n",
    "-- block meta_config\n",
    "    ## This includes the definition from the parent.\n",
    "    == super()\n",
    "    -- set ns.config_name = \"No Bias\"\n",
    "    -- set ns.config_description = \"Disabled bias in attention. Does it matter?\"\n",
    "<< endblock meta_config\n",
    "\n",
    "-- block attention_factory\n",
    "    == super()\n",
    "\n",
    "    ## And add an override. We are essentially just appending more arguments to the definition.\n",
    "    # Experiment override.\n",
    "    bias: False\n",
    "<< endblock attention_factory\n",
    "\"\"\"\n",
    "\n",
    "output = env.load_from_string(experiment_config)\n",
    "nb.display_codeblock(\"yaml\", output.pp_config, \"#### Pre Processed Experiment Config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a6c5b-40d1-4abd-9420-091266812b07",
   "metadata": {},
   "source": [
    "## Construct Experiment Model\n",
    "\n",
    "This model now has been modified. The bias is now disabled on the attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2522af56-9f94-4692-85bd-d6d9e36a36d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CasualLM(\n",
       "  loss_fn=CausalLoss()\n",
       "  (input_encoder): InputEncoder(\n",
       "    d_model=64, vocab_size=1024\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (embedding): Embedding(1024, 64)\n",
       "  )\n",
       "  (output_decoder): Linear(in_features=64, out_features=1024, bias=True)\n",
       "  (layer_stack): LayerStack(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x PreLNLayer(\n",
       "        (feedforward): FeedforwardLayer(\n",
       "          d_model=64, d_feedforward=256\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Identity()\n",
       "          (activation): ReLU()\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "        (attention): SingleHeadAttn(\n",
       "          d_model=64, bias=False\n",
       "          (query_key_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (residual_dropout): Identity()\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = output.config\n",
    "graph.main(context_vars=model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b5cbb-d506-445e-8199-82b3eb81727b",
   "metadata": {},
   "source": [
    "### Implementation Override\n",
    "\n",
    "Unlike most configuration systems, we can not only change numerical parameters, we can alter the implementatinon!\n",
    "\n",
    "Let's replace the simple single-head attention module with a multihead-attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62131420-1e08-4e9d-bf49-cf9c90654ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Pre Processed Experiment Config\n",
       "```yaml\n",
       "\n",
       "#---------------------------------------\n",
       "#           Multihead Attention          \n",
       "#---------------------------------------\n",
       "# 2025-06-17T08:07:28\n",
       "# Description: Swapped singlehead attention for multihead attention.\n",
       "# model_src = ../../../modelsrc/transformer/\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/basic/syntax\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "\n",
       "########### Model Definition ###########\n",
       "\n",
       "# **Layer Norm Factory**\n",
       "\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    - !var \"hidden_size\"\n",
       "\n",
       "# **Activation Factory**\n",
       "\n",
       ".define: &activation_factory !partial:torch.nn:ReLU@activation_factory []\n",
       "\n",
       "# **Feedforward Factory**\n",
       "\n",
       ".define: &feedforward_factory !partial:../../../modelsrc/transformer/feedforward_layer.py:FeedforwardLayer@feedforward_factory\n",
       "    activation_factory: *activation_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "\n",
       "# **Attention Factory**\n",
       "\n",
       "# Experiment Override.\n",
       ".define: &attention_factory !partial:../../../modelsrc/transformer/causal_multihead_attn.py:CausalMultiheadAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    num_heads: 2\n",
       "\n",
       "# **Layer Factory**\n",
       "\n",
       ".define: &layer_factory !partial:../../../modelsrc/transformer/pre_ln_layer.py:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "\n",
       "# **Layer Stack Factory**\n",
       "\n",
       ".define: &layer_stack_factory !factory:../../../modelsrc/transformer/layer_stack.py:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    num_hidden_layers: !var \"n_layers\"\n",
       "\n",
       "# **Model**\n",
       "\n",
       ".define: &model !call:../../../modelsrc/transformer/causal_lm.py:CasualLM@model\n",
       "    loss_fn: !factory:../../../modelsrc/transformer/causal_loss.py:CausalLoss\n",
       "    input_encoder: !factory:../../../modelsrc/transformer/input_encoder.py:InputEncoder\n",
       "        d_model: !var \"hidden_size\"\n",
       "        vocab_size: 1024\n",
       "    output_decoder: !factory:torch.nn:Linear [ !var \"hidden_size\", 1024 ]\n",
       "    init_weights: !partial:../../../modelsrc/transformer/init_weights.py:simple_weight_init\n",
       "    layer_stack: *layer_stack_factory\n",
       "# **Optimizer**\n",
       "\n",
       "optimizer: !partial:torch.optim:AdamW\n",
       "    lr: 1.0e-3\n",
       "meta:\n",
       "    d_model: !var \"hidden_size\"\n",
       "    vocab_size: 1024\n",
       "\n",
       "main: *model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_config = \"\"\"\n",
    "-- extends 'templates/model_def.yaml'\n",
    "\n",
    "-- block meta_config\n",
    "    == super()\n",
    "    -- set ns.config_name = \"Multihead Attention\"\n",
    "    -- set ns.config_description = \"Swapped singlehead attention for multihead attention.\"\n",
    "    -- set ns.attention_heads = 2\n",
    "<< endblock meta_config\n",
    "\n",
    "\n",
    "-- block attention_factory\n",
    "# Experiment Override.\n",
    ".define: &attention_factory !partial:{{ns.model_src}}causal_multihead_attn.py:CausalMultiheadAttn@attention_factory\n",
    "    d_model: !var \"hidden_size\"\n",
    "    num_heads: {{ ns.attention_heads }}\n",
    "<< endblock attention_factory\n",
    "\"\"\"\n",
    "\n",
    "output = env.load_from_string(experiment_config)\n",
    "nb.display_codeblock(\"yaml\", output.pp_config, \"#### Pre Processed Experiment Config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df356bd-f0cd-4515-aee7-ed8693388517",
   "metadata": {},
   "source": [
    "### Examine the Graph\n",
    "\n",
    "Internally, the processed configuraiton is represented as an abstract node graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0956c00c-5950-45fd-b35c-932b2d141591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Node Graph\n",
       "```python\n",
       "{'main': SingletonNode('../../../modelsrc/transformer/causal_lm.py:CasualLM', *(), identity='model', **{'loss_fn': FactoryNode('../../../modelsrc/transformer/causal_loss.py:CausalLoss', *(), identity=140135030590480, **{}), 'input_encoder': FactoryNode('../../../modelsrc/transformer/input_encoder.py:InputEncoder', *(), identity=140135030589424, **{'d_model': VarNode('hidden_size', identity=140135030602960, value=Undefined), 'vocab_size': 1024}), 'output_decoder': FactoryNode('torch.nn:Linear', *(VarNode('hidden_size', identity=140135030589664, value=Undefined), 1024), identity=140135030589472, **{}), 'init_weights': LambdaNode('../../../modelsrc/transformer/init_weights.py:simple_weight_init', *(), identity=140135030589232, **{}), 'layer_stack': FactoryNode('../../../modelsrc/transformer/layer_stack.py:LayerStack', *(), identity='layer_stack_factory', **{'layer_factory': LambdaNode('../../../modelsrc/transformer/pre_ln_layer.py:PreLNLayer', *(), identity='layer_factory', **{'feedforward_factory': LambdaNode('../../../modelsrc/transformer/feedforward_layer.py:FeedforwardLayer', *(), identity='feedforward_factory', **{'activation_factory': LambdaNode('torch.nn:ReLU', *(), identity='activation_factory', **{}), 'd_model': VarNode('hidden_size', identity=140135030603728, value=Undefined), 'd_feedforward': VarNode('dim_feedforward', identity=140135030600608, value=Undefined)}), 'attention_factory': LambdaNode('../../../modelsrc/transformer/single_head_attn.py:SingleHeadAttn', *(), identity='attention_factory', **{'d_model': VarNode('hidden_size', identity=140135030590192, value=Undefined), 'bias': False}), 'norm_factory': LambdaNode('torch.nn:LayerNorm', *(VarNode('hidden_size', identity=140135030591872, value=Undefined),), identity='layer_norm_factory', **{})}), 'post_norm_factory': LambdaNode('torch.nn:LayerNorm', *(VarNode('hidden_size', identity=140135030591872, value=Undefined),), identity='layer_norm_factory', **{}), 'num_hidden_layers': VarNode('n_layers', identity=140135030600416, value=Undefined)})}),\n",
       " 'meta': {'d_model': VarNode('hidden_size', identity=140135030589520, value=Undefined),\n",
       "          'vocab_size': 1024},\n",
       " 'optimizer': LambdaNode('torch.optim:AdamW', *(), identity=140135030589376, **{'lr': 0.001})}\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_codeblock(\"python\", pformat(graph), \"### Node Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c95e8-a4d0-4d8d-b1cb-407a7979e57b",
   "metadata": {},
   "source": [
    "### Convert Graph to YAML\n",
    "\n",
    "Convert the node-graph to a YAML representation. This may not be exactly the same as it was in the source template, but should be symantically equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f0d0c1b-abb3-47bc-aad8-f0d857f15e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       ".define: &activation_factory !lambda:torch.nn:ReLU@activation_factory []\n",
       "\n",
       ".define: &feedforward_factory !lambda:../../../modelsrc/transformer/feedforward_layer.py:FeedforwardLayer@feedforward_factory\n",
       "    activation_factory: *activation_factory\n",
       "    d_model: !var 'hidden_size'\n",
       "    d_feedforward: !var 'dim_feedforward'\n",
       "\n",
       ".define: &attention_factory !lambda:../../../modelsrc/transformer/single_head_attn.py:SingleHeadAttn@attention_factory\n",
       "    d_model: !var 'hidden_size'\n",
       "    bias: False\n",
       "\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    - !var 'hidden_size'\n",
       "\n",
       ".define: &layer_factory !lambda:../../../modelsrc/transformer/pre_ln_layer.py:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "\n",
       ".define: &layer_stack_factory !factory:../../../modelsrc/transformer/layer_stack.py:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    num_hidden_layers: !var 'n_layers'\n",
       "\n",
       ".define: &model !singleton:../../../modelsrc/transformer/causal_lm.py:CasualLM@model\n",
       "    loss_fn: !factory:../../../modelsrc/transformer/causal_loss.py:CausalLoss []\n",
       "    input_encoder: !factory:../../../modelsrc/transformer/input_encoder.py:InputEncoder\n",
       "        d_model: !var 'hidden_size'\n",
       "        vocab_size: 1024\n",
       "    output_decoder: !factory:torch.nn:Linear\n",
       "        - !var 'hidden_size'\n",
       "        - 1024\n",
       "    init_weights: !lambda:../../../modelsrc/transformer/init_weights.py:simple_weight_init []\n",
       "    layer_stack: *layer_stack_factory\n",
       "\n",
       "\n",
       "optimizer: !lambda:torch.optim:AdamW\n",
       "    lr: 0.001\n",
       "meta: \n",
       "    d_model: !var 'hidden_size'\n",
       "    vocab_size: 1024\n",
       "main: *model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_codeblock(\"yaml\", to_yaml(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad4788-09fc-46ca-b6ce-9e604a3b3ea7",
   "metadata": {},
   "source": [
    "### Convert Graph to Python\n",
    "\n",
    "This function takes the output from Latent.to_py(graph) and uses it to render Pyhon code using a Jinja2 template. If the template is unspecified, an implicit \"built-in\" template is used, which will generate appropriate import and dynamic import statements, where required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec86456-4baa-4395-b005-6e5e768bff9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Generated Code\n",
       "```python\n",
       "from torch.nn import LayerNorm\n",
       "from torch.nn import Linear\n",
       "from torch.nn import ReLU\n",
       "from importlib.util import spec_from_file_location, module_from_spec\n",
       "import os\n",
       "import sys\n",
       "from functools import partial\n",
       "\n",
       "# Import a dynamic module.\n",
       "def dynimport(module, name, searchpath):\n",
       "    module_path = module\n",
       "    module_name = os.path.basename(module).split(\".\")[0]\n",
       "    module_spec = spec_from_file_location(\n",
       "        module_name,\n",
       "        module_path,\n",
       "        submodule_search_locations=searchpath,\n",
       "    )\n",
       "    mod = module_from_spec(module_spec)\n",
       "    sys.modules[module_name] = mod\n",
       "    module_spec.loader.exec_module(mod)\n",
       "    for symbol in name.split(\".\"):\n",
       "        mod = getattr(mod, symbol)\n",
       "    return mod\n",
       "\n",
       "CasualLM = lambda: dynimport(\"../../../modelsrc/transformer/causal_lm.py\", \"CasualLM\", ())\n",
       "CausalLoss = lambda: dynimport(\"../../../modelsrc/transformer/causal_loss.py\", \"CausalLoss\", ())\n",
       "FeedforwardLayer = lambda: dynimport(\"../../../modelsrc/transformer/feedforward_layer.py\", \"FeedforwardLayer\", ())\n",
       "simple_weight_init = lambda: dynimport(\"../../../modelsrc/transformer/init_weights.py\", \"simple_weight_init\", ())\n",
       "InputEncoder = lambda: dynimport(\"../../../modelsrc/transformer/input_encoder.py\", \"InputEncoder\", ())\n",
       "LayerStack = lambda: dynimport(\"../../../modelsrc/transformer/layer_stack.py\", \"LayerStack\", ())\n",
       "PreLNLayer = lambda: dynimport(\"../../../modelsrc/transformer/pre_ln_layer.py\", \"PreLNLayer\", ())\n",
       "SingleHeadAttn = lambda: dynimport(\"../../../modelsrc/transformer/single_head_attn.py\", \"SingleHeadAttn\", ())\n",
       "\n",
       "def construct(\n",
       "    dim_feedforward,\n",
       "    hidden_size,\n",
       "    n_layers,\n",
       "):\n",
       "    activation_factory = partial(ReLU, )\n",
       "\n",
       "    feedforward_factory = partial(FeedforwardLayer(), \n",
       "        activation_factory=activation_factory,\n",
       "        d_model=hidden_size,\n",
       "        d_feedforward=dim_feedforward,\n",
       "    )\n",
       "\n",
       "    attention_factory = partial(SingleHeadAttn(), \n",
       "        d_model=hidden_size,\n",
       "        bias=False,\n",
       "    )\n",
       "\n",
       "    layer_norm_factory = partial(LayerNorm, \n",
       "        hidden_size,\n",
       "    )\n",
       "\n",
       "    layer_factory = partial(PreLNLayer(), \n",
       "        feedforward_factory=feedforward_factory,\n",
       "        attention_factory=attention_factory,\n",
       "        norm_factory=layer_norm_factory,\n",
       "    )\n",
       "\n",
       "    layer_stack_factory = partial(LayerStack(), \n",
       "        layer_factory=layer_factory,\n",
       "        post_norm_factory=layer_norm_factory,\n",
       "        num_hidden_layers=n_layers,\n",
       "    )\n",
       "\n",
       "    model = CasualLM()(\n",
       "        loss_fn=CausalLoss()(),\n",
       "        input_encoder=InputEncoder()(\n",
       "            d_model=hidden_size,\n",
       "            vocab_size=1024,\n",
       "        ),\n",
       "        output_decoder=Linear(\n",
       "            hidden_size,\n",
       "            1024,\n",
       "        ),\n",
       "        init_weights=partial(simple_weight_init(), ),\n",
       "        layer_stack=layer_stack_factory(),\n",
       "    )\n",
       "    \n",
       "    return model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from forgather.graph_encoder import NamePolicy # NamePolicy.REQUIRED | NamePolicy.ALL | NamePolicy.NAMED\n",
    "generated_code = generate_code(graph.main, name_policy=None)\n",
    "nb.display_codeblock(\"python\", generated_code, \"### Generated Code\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5eeef-511f-4bc2-ad87-d5d4b810befd",
   "metadata": {},
   "source": [
    "### Custom Code Template\n",
    "\n",
    "The above code is pretty generic. How about we wrap this class with a HF PreTrainedModel?  \n",
    "[./templates/causal_lm.py](./templates/causal_lm.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52e24273-ff22-4f46-8ec8-23720fb656d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Generated Code\n",
       "```python\n",
       "# See: https://huggingface.co/docs/transformers/custom_models\n",
       "# This is a template model, with the details filled-in by the code-generator.\n",
       "from typing import Optional, Tuple\n",
       "\n",
       "from functools import partial\n",
       "from torch import nn, Tensor, LongTensor, FloatTensor\n",
       "import torch\n",
       "from transformers.modeling_outputs import CausalLMOutput\n",
       "from transformers import (\n",
       "    PreTrainedModel,\n",
       "    PretrainedConfig,\n",
       "    AutoConfig,\n",
       "    AutoModelForCausalLM,\n",
       "    GenerationMixin,\n",
       ")\n",
       "\n",
       "from torch.nn import LayerNorm\n",
       "from torch.nn import Linear\n",
       "from torch.nn import ReLU\n",
       "\n",
       "from importlib.util import spec_from_file_location, module_from_spec\n",
       "import os\n",
       "import sys\n",
       "from functools import partial\n",
       "\n",
       "# Import a dynamic module.\n",
       "def dynimport(module, name, searchpath):\n",
       "    module_path = module\n",
       "    module_name = os.path.basename(module).split(\".\")[0]\n",
       "    module_spec = spec_from_file_location(\n",
       "        module_name,\n",
       "        module_path,\n",
       "        submodule_search_locations=searchpath,\n",
       "    )\n",
       "    mod = module_from_spec(module_spec)\n",
       "    sys.modules[module_name] = mod\n",
       "    module_spec.loader.exec_module(mod)\n",
       "    for symbol in name.split(\".\"):\n",
       "        mod = getattr(mod, symbol)\n",
       "    return mod\n",
       "\n",
       "CasualLM = lambda: dynimport(\"../../../modelsrc/transformer/causal_lm.py\", \"CasualLM\", ())\n",
       "CausalLoss = lambda: dynimport(\"../../../modelsrc/transformer/causal_loss.py\", \"CausalLoss\", ())\n",
       "FeedforwardLayer = lambda: dynimport(\"../../../modelsrc/transformer/feedforward_layer.py\", \"FeedforwardLayer\", ())\n",
       "simple_weight_init = lambda: dynimport(\"../../../modelsrc/transformer/init_weights.py\", \"simple_weight_init\", ())\n",
       "InputEncoder = lambda: dynimport(\"../../../modelsrc/transformer/input_encoder.py\", \"InputEncoder\", ())\n",
       "LayerStack = lambda: dynimport(\"../../../modelsrc/transformer/layer_stack.py\", \"LayerStack\", ())\n",
       "PreLNLayer = lambda: dynimport(\"../../../modelsrc/transformer/pre_ln_layer.py\", \"PreLNLayer\", ())\n",
       "SingleHeadAttn = lambda: dynimport(\"../../../modelsrc/transformer/single_head_attn.py\", \"SingleHeadAttn\", ())\n",
       "\n",
       "model_type = \"my_model\"\n",
       "\n",
       "\n",
       "class DynamicCausalLMConfig(PretrainedConfig):\n",
       "    model_type = model_type\n",
       "\n",
       "\n",
       "class DynamicCasualLM(PreTrainedModel, GenerationMixin):\n",
       "    config_class = DynamicCausalLMConfig\n",
       "    model_type = model_type\n",
       "\n",
       "    def __init__(self, config: PretrainedConfig):\n",
       "        super().__init__(config)\n",
       "        self.causal_lm = self.construct_model(**config.to_dict())\n",
       "        if \"torch_dtype\" in config:\n",
       "            self.to(config.torch_dtype)\n",
       "\n",
       "    @staticmethod\n",
       "    def construct_model(\n",
       "        dim_feedforward,\n",
       "        hidden_size,\n",
       "        n_layers,\n",
       "        **kwargs\n",
       "    ):\n",
       "        activation_factory = partial(ReLU, )\n",
       "\n",
       "        feedforward_factory = partial(FeedforwardLayer(), \n",
       "            activation_factory=activation_factory,\n",
       "            d_model=hidden_size,\n",
       "            d_feedforward=dim_feedforward,\n",
       "        )\n",
       "\n",
       "        attention_factory = partial(SingleHeadAttn(), \n",
       "            d_model=hidden_size,\n",
       "            bias=False,\n",
       "        )\n",
       "\n",
       "        layer_norm_factory = partial(LayerNorm, \n",
       "            hidden_size,\n",
       "        )\n",
       "\n",
       "        layer_factory = partial(PreLNLayer(), \n",
       "            feedforward_factory=feedforward_factory,\n",
       "            attention_factory=attention_factory,\n",
       "            norm_factory=layer_norm_factory,\n",
       "        )\n",
       "\n",
       "        layer_stack_factory = partial(LayerStack(), \n",
       "            layer_factory=layer_factory,\n",
       "            post_norm_factory=layer_norm_factory,\n",
       "            num_hidden_layers=n_layers,\n",
       "        )\n",
       "\n",
       "        model = CasualLM()(\n",
       "            loss_fn=CausalLoss()(),\n",
       "            input_encoder=InputEncoder()(\n",
       "                d_model=hidden_size,\n",
       "                vocab_size=1024,\n",
       "            ),\n",
       "            output_decoder=Linear(\n",
       "                hidden_size,\n",
       "                1024,\n",
       "            ),\n",
       "            init_weights=partial(simple_weight_init(), ),\n",
       "            layer_stack=layer_stack_factory(),\n",
       "        )\n",
       "        \n",
       "        return model\n",
       "\n",
       "    def forward(\n",
       "        self,\n",
       "        input_ids: LongTensor,\n",
       "        labels: Optional[LongTensor] = None,\n",
       "        position_ids: Optional[LongTensor] = None,\n",
       "        attention_mask: Optional[FloatTensor] = None,\n",
       "        return_dict: bool = False,\n",
       "        **kwargs,\n",
       "    ) -> CausalLMOutput | Tuple[FloatTensor, dict[str, FloatTensor]] | FloatTensor:\n",
       "\n",
       "        outputs = self.causal_lm(\n",
       "            input_ids=input_ids,\n",
       "            labels=labels,\n",
       "            position_ids=position_ids,\n",
       "            attention_mask=attention_mask,\n",
       "            **kwargs,\n",
       "        )\n",
       "\n",
       "        # Return type depends on arguments.\n",
       "        if return_dict:\n",
       "            return CausalLMOutput(**outputs)\n",
       "        elif labels is not None:\n",
       "            return (outputs[\"loss\"], outputs[\"logits\"])\n",
       "        else:\n",
       "            return outputs[\"logits\"]\n",
       "\n",
       "    # Bare-minimum for HF text generation interface to work.\n",
       "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
       "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
       "        model_inputs = {\n",
       "            \"input_ids\": input_ids,\n",
       "            \"attention_mask\": attention_mask,\n",
       "        }\n",
       "        return model_inputs\n",
       "\n",
       "\n",
       "AutoConfig.register(model_type, DynamicCausalLMConfig)\n",
       "AutoModelForCausalLM.register(DynamicCausalLMConfig, DynamicCasualLM)\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_code = generate_code(graph.main, template_name=\"templates/causal_lm.py\", model_type=\"my_model\")\n",
    "nb.display_codeblock(\"python\", generated_code, \"### Generated Code\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d4314-7137-4424-b711-38a80003a13f",
   "metadata": {},
   "source": [
    "## Execute Generated Code\n",
    "\n",
    "Execute the generated code, then call the generated 'construct' function to construct the objects.\n",
    "\n",
    "Note: When directly constucting objects from a graph, there is no intermediate code-generation step; objects are directly constructed from the graph. This example is primarily to illustrate that you can export a graph as code, which can be useful if you are going wish to export the code with the model weighs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44dc50cb-1111-4e1d-ab6e-ace13da13fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cc48126-a989-4a76-a70e-90f241c3f378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicCasualLM(\n",
       "  (causal_lm): CasualLM(\n",
       "    loss_fn=CausalLoss()\n",
       "    (input_encoder): InputEncoder(\n",
       "      d_model=128, vocab_size=1024\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (embedding): Embedding(1024, 128)\n",
       "    )\n",
       "    (output_decoder): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (layer_stack): LayerStack(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x PreLNLayer(\n",
       "          (feedforward): FeedforwardLayer(\n",
       "            d_model=128, d_feedforward=512\n",
       "            (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dropout): Identity()\n",
       "            (activation): ReLU()\n",
       "            (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (attention): SingleHeadAttn(\n",
       "            d_model=128, bias=False\n",
       "            (query_key_linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (value_linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (residual_dropout): Identity()\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = DynamicCausalLMConfig(hidden_size=128, dim_feedforward=512, n_layers=3)\n",
    "model = DynamicCasualLM(model_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3555c732-0bc0-4a5c-9839-d371d655bf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(functools.partial(LambdaNode('torch.optim:AdamW', *(), identity=140135030589376, **{'lr': 0.001}), context_vars={'hidden_size': 64, 'dim_feedforward': 256, 'n_layers': 2}),\n",
       " {'d_model': 64, 'vocab_size': 1024})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get the meta-data and the optimizer constructor\n",
    "# We can construct other target in the graph like this...\n",
    "outputs = Latent.materialize(\n",
    "   graph, mtargets=[\"optimizer\", \"meta\" ], context_vars=model_args\n",
    ")\n",
    "optim_ctor = outputs[\"optimizer\"]\n",
    "meta = outputs[\"meta\"]\n",
    "optim_ctor, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37314b-bf75-421f-a4d6-d66a18a1681d",
   "metadata": {},
   "source": [
    "This is how partial functions are useful. We created a callable object, where the hyper-parameters have already been specified. All we have to do is pass in the missing argument(s), the model's parameters, as 'lr' has already been baked in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aa60723-3709-43ac-a7e0-72adc89c0160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: True\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    param_names: ['causal_lm.input_encoder.embedding.weight', 'causal_lm.output_decoder.weight', 'causal_lm.output_decoder.bias', 'causal_lm.layer_stack.layers.0.feedforward.linear1.weight', 'causal_lm.layer_stack.layers.0.feedforward.linear1.bias', 'causal_lm.layer_stack.layers.0.feedforward.linear2.weight', 'causal_lm.layer_stack.layers.0.feedforward.linear2.bias', 'causal_lm.layer_stack.layers.0.attention.query_key_linear.weight', 'causal_lm.layer_stack.layers.0.attention.value_linear.weight', 'causal_lm.layer_stack.layers.0.norm1.weight', 'causal_lm.layer_stack.layers.0.norm1.bias', 'causal_lm.layer_stack.layers.0.norm2.weight', 'causal_lm.layer_stack.layers.0.norm2.bias', 'causal_lm.layer_stack.layers.1.feedforward.linear1.weight', 'causal_lm.layer_stack.layers.1.feedforward.linear1.bias', 'causal_lm.layer_stack.layers.1.feedforward.linear2.weight', 'causal_lm.layer_stack.layers.1.feedforward.linear2.bias', 'causal_lm.layer_stack.layers.1.attention.query_key_linear.weight', 'causal_lm.layer_stack.layers.1.attention.value_linear.weight', 'causal_lm.layer_stack.layers.1.norm1.weight', 'causal_lm.layer_stack.layers.1.norm1.bias', 'causal_lm.layer_stack.layers.1.norm2.weight', 'causal_lm.layer_stack.layers.1.norm2.bias', 'causal_lm.layer_stack.layers.2.feedforward.linear1.weight', 'causal_lm.layer_stack.layers.2.feedforward.linear1.bias', 'causal_lm.layer_stack.layers.2.feedforward.linear2.weight', 'causal_lm.layer_stack.layers.2.feedforward.linear2.bias', 'causal_lm.layer_stack.layers.2.attention.query_key_linear.weight', 'causal_lm.layer_stack.layers.2.attention.value_linear.weight', 'causal_lm.layer_stack.layers.2.norm1.weight', 'causal_lm.layer_stack.layers.2.norm1.bias', 'causal_lm.layer_stack.layers.2.norm2.weight', 'causal_lm.layer_stack.layers.2.norm2.bias', 'causal_lm.layer_stack.layer_norm.weight', 'causal_lm.layer_stack.layer_norm.bias']\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim_ctor(model.named_parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2ed1472-882a-43f0-869f-8f9b0cc26963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 225,   62,  989,  864,  337,  196,   24,  847,  584, 1019,  447,  484,\n",
       "          187,  861,  433,  640],\n",
       "        [ 816,  304,  517,  326,  277,  393,  160,  560,  927,   19,  906,   28,\n",
       "          517,  146,   86,   38]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We can then grab the vocab_size from the meta-data and construct a batch of random input-ids.\n",
    "# Create a batch of 2 x 16 input-ids\n",
    "input_ids = torch.randint(meta['vocab_size'], (2, 16,))\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74153189-5d83-43da-8980-4c1f6ac3c71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.038168907165527"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed the inputs through the model and get loss and logits.\n",
    "loss, logits = model(input_ids, labels=input_ids)\n",
    "\n",
    "# Backward step\n",
    "loss.backward()\n",
    "\n",
    "# Step the optimizer\n",
    "optimizer.step()\n",
    "\n",
    "# Reset grad\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Show loss\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a071fa0-470d-4e08-8fe6-04e0daa33332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
