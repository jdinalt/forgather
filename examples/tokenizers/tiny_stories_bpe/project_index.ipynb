{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../../notebooks/forgather.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Tiny Stories BPE Tokenizer\n",
       "\n",
       "An example BPE tokenizer trained on the Tiny Stories dataset.\n",
       "\n",
       "### 2k:\n",
       "- vocabulary_size: 2000\n",
       "- model_max_length: 2048\n",
       "\n",
       "### 8k\n",
       "- vocabulary_size: 8000\n",
       "- model_max_length: 2048\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/tiny_experiments](../../../templates/tiny_experiments)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/base](../../../templates/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [256.yaml](templates/configs/256.yaml)\n",
       "- [2k.yaml](templates/configs/2k.yaml)\n",
       "- [8k.yaml](templates/configs/8k.yaml)\n",
       "\n",
       "Default Configuration: 2k.yaml\n",
       "\n",
       "Active Configuration: 2k.yaml\n",
       "\n",
       "## Available Templates\n",
       "- [project.yaml](templates/project.yaml)\n",
       "- [configs/256.yaml](templates/configs/256.yaml)\n",
       "- [configs/2k.yaml](templates/configs/2k.yaml)\n",
       "- [configs/8k.yaml](templates/configs/8k.yaml)\n",
       "- [projects/tiny.yaml](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "- [datasets/tiny/tiny_stories.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories.yaml)\n",
       "- [datasets/tiny/tiny_stories_abridged.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories_abridged.yaml)\n",
       "- [models/tiny/tiny_causal.yaml](../../../templates/tiny_experiments/models/tiny/tiny_causal.yaml)\n",
       "- [models/tiny/tiny_gpt2.yaml](../../../templates/tiny_experiments/models/tiny/tiny_gpt2.yaml)\n",
       "- [models/tiny/tiny_llama.yaml](../../../templates/tiny_experiments/models/tiny/tiny_llama.yaml)\n",
       "- [models/tiny/tiny_d128_l2.yaml](../../../templates/tiny_experiments/models/tiny/tiny_d128_l2.yaml)\n",
       "- [prompts/tiny_stories.yaml](../../../templates/tiny_experiments/prompts/tiny_stories.yaml)\n",
       "- [tokenizers/tiny_2k.yaml](../../../templates/tiny_experiments/tokenizers/tiny_2k.yaml)\n",
       "- [tokenizers/tiny_8k.yaml](../../../templates/tiny_experiments/tokenizers/tiny_8k.yaml)\n",
       "- [trainers/accel_trainer.yaml](../../../templates/base/trainers/accel_trainer.yaml)\n",
       "- [trainers/trainer.yaml](../../../templates/base/trainers/trainer.yaml)\n",
       "- [trainers/hf_trainer.yaml](../../../templates/base/trainers/hf_trainer.yaml)\n",
       "- [trainers/base_trainer.yaml](../../../templates/base/trainers/base_trainer.yaml)\n",
       "- [datasets/abstract/pretokenized_dataset.yaml](../../../templates/base/datasets/abstract/pretokenized_dataset.yaml)\n",
       "- [datasets/abstract/base_datasets.yaml](../../../templates/base/datasets/abstract/base_datasets.yaml)\n",
       "- [models/abstract/causal_lm_from_config.yaml](../../../templates/base/models/abstract/causal_lm_from_config.yaml)\n",
       "- [models/abstract/base_language_model.yaml](../../../templates/base/models/abstract/base_language_model.yaml)\n",
       "- [models/abstract/custom_causal_lm.yaml](../../../templates/base/models/abstract/custom_causal_lm.yaml)\n",
       "- [models/abstract/causal_lm_from_pretrained.yaml](../../../templates/base/models/abstract/causal_lm_from_pretrained.yaml)\n",
       "- [models/abstract/dynamic_causal_lm.yaml](../../../templates/base/models/abstract/dynamic_causal_lm.yaml)\n",
       "- [models/abstract/load_model.yaml](../../../templates/base/models/abstract/load_model.yaml)\n",
       "- [callbacks/base_callbacks.yaml](../../../templates/base/callbacks/base_callbacks.yaml)\n",
       "- [callbacks/loggers.yaml](../../../templates/base/callbacks/loggers.yaml)\n",
       "- [types/meta_template.yaml](../../../templates/base/types/meta_template.yaml)\n",
       "- [types/type.yaml](../../../templates/base/types/type.yaml)\n",
       "- [types/tokenizer/tokenizer.yaml](../../../templates/base/types/tokenizer/tokenizer.yaml)\n",
       "- [types/tokenizer/bpe/bpe.yaml](../../../templates/base/types/tokenizer/bpe/bpe.yaml)\n",
       "- [types/model/model_type.yaml](../../../templates/base/types/model/model_type.yaml)\n",
       "- [types/training_script/training_script.yaml](../../../templates/base/types/training_script/training_script.yaml)\n",
       "- [types/training_script/causal_lm/causal_lm.yaml](../../../templates/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "\n",
       "## Included Templates\n",
       "- [configs/2k.yaml](templates/configs/2k.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [types/tokenizer/bpe/bpe.yaml](../../../templates/base/types/tokenizer/bpe/bpe.yaml)\n",
       "            - [types/tokenizer/tokenizer.yaml](../../../templates/base/types/tokenizer/tokenizer.yaml)\n",
       "                - [types/type.yaml](../../../templates/base/types/type.yaml)\n",
       "                - [inc/formatting.jinja](../../../templates/base/inc/formatting.jinja)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_description': 'BPE tokenizer trained on Tiny Stories dataset w/ 2K '\n",
       "                       'tokens',\n",
       " 'config_name': 'Tiny Stories 2K',\n",
       " 'datasets_dir': '../../../datasets',\n",
       " 'model_max_length': '2048',\n",
       " 'models_dir': './output_models',\n",
       " 'output_dir': '../../../tokenizers/tiny_stories_2k',\n",
       " 'project_dir': '.',\n",
       " 'tokenizer_name': 'tiny_stories_2k',\n",
       " 'tokenizers_dir': '../../../tokenizers',\n",
       " 'vocab_size': '2000'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#             Tiny Stories 2K            \n",
       "#---------------------------------------\n",
       "# 2024-08-10T19:27:48\n",
       "# Description: BPE tokenizer trained on Tiny Stories dataset w/ 2K tokens\n",
       "# Project Dir: .\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"../../..\"\n",
       "# ns.models_dir: \"./output_models\"\n",
       "# ns.project_model_src_dir: \"./model_src\"\n",
       "# ns.tokenizers_dir: \"../../../tokenizers\"\n",
       "# ns.datasets_dir: \"../../../datasets\"\n",
       "# tokenizer_name: 'tiny_stories_2k'\n",
       "# output_dir: '../../../tokenizers/tiny_stories_2k'\n",
       "# model_max_length: '2048'\n",
       "# vocab_size: '2000'\n",
       "# dataset_id: 'roneneldan/TinyStories'\n",
       "# dataset_split: 'train'\n",
       "\n",
       "########## Special Tokens Map ##########\n",
       "\n",
       ".define: &special_tokens_map !dict:@special_tokens_map\n",
       "    bos: \"<|BOS|>\" # Beginning of Sequence; the first token in a sequence\n",
       "    pad: \"<|PAD|>\" # Padding, used to pad out samples in a batch.\n",
       "    eos: \"<|EOS|>\" # End of Sequence; typically is used to stop generation.\n",
       "    unk: \"<|UNK|>\" # Unknown; used when a symbol can't be represented.\n",
       "\n",
       "#### Pretrained Tokenizer Fast Args ####\n",
       "\n",
       ".define: &tokenizer_args !dict:@tokenizer_args\n",
       "    bos_token: \"<|BOS|>\"\n",
       "    eos_token: \"<|EOS|>\"\n",
       "    unk_token: \"<|UNK|>\"\n",
       "    pad_token: \"<|PAD|>\"\n",
       "    return_special_tokens_mask: True\n",
       "    model_max_length: 2048\n",
       "    padding_side: \"right\"\n",
       "    truncation_side: \"right\"\n",
       "\n",
       "###### Tokenizer Training Dataset ######\n",
       "\n",
       ".define: &tokenizer_dataset !singleton:operator:getitem@tokenizer_dataset\n",
       "    - !singleton:datasets:load_dataset [ \"roneneldan/TinyStories\" ]\n",
       "    - \"train\"\n",
       "\n",
       "########## Tokenizer Trainer ###########\n",
       "\n",
       ".define: &tokenizer_trainer !lambda:forgather.ml.tokenizer:train_tokenizer@tokenizer_trainer\n",
       "    output_dir: \"../../../tokenizers/tiny_stories_2k\"\n",
       "    dataset: *tokenizer_dataset\n",
       "    args: *tokenizer_args\n",
       "\n",
       "    model: !singleton:tokenizers:models.BPE\n",
       "        cache_capacity: 16\n",
       "        unk_token: \"<|UNK|>\"\n",
       "        byte_fallback: True\n",
       "    normalizer: !singleton:tokenizers:normalizers.NFC []\n",
       "    pre_tokenizer: !singleton:tokenizers:pre_tokenizers.ByteLevel []\n",
       "    decoder: !singleton:tokenizers:decoders.ByteLevel []\n",
       "    # Automatically add bos token to sequence start\n",
       "    post_processor: !singleton:tokenizers:processors.TemplateProcessing\n",
       "        single: \"<bos> $A\"\n",
       "        special_tokens: [ !tuple [ \"<bos>\", 0 ] ]\n",
       "    trainer: !singleton:tokenizers.trainers:BpeTrainer\n",
       "        vocab_size: 2000\n",
       "        initial_alphabet: !singleton:tokenizers:pre_tokenizers.ByteLevel.alphabet []\n",
       "        special_tokens: !singleton:list [!singleton:values [*special_tokens_map]]\n",
       "        show_progress: False\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Tiny Stories 2K\"\n",
       "    config_description: \"BPE tokenizer trained on Tiny Stories dataset w/ 2K tokens\"\n",
       "    project_dir: \".\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"../../../tokenizers\"\n",
       "    datasets_dir: \"../../../datasets\"\n",
       "    tokenizer_name: \"tiny_stories_2k\"\n",
       "    output_dir: \"../../../tokenizers/tiny_stories_2k\"\n",
       "    vocab_size: \"2000\"\n",
       "    model_max_length: \"2048\"\n",
       "\n",
       "main: !singleton:forgather.ml.construct:build_rule\n",
       "    target: \"../../../tokenizers/tiny_stories_2k/tokenizer.json\"\n",
       "    recipe: *tokenizer_trainer\n",
       "    loader: !lambda:transformers:AutoTokenizer.from_pretrained\n",
       "        - \"../../../tokenizers/tiny_stories_2k\"\n",
       "\n",
       "```\n",
       "\n",
       "## Loaded Configuration to YAML\n",
       "\n",
       "```yaml\n",
       ".define: &meta !singleton:named_dict@meta\n",
       "    config_name: 'Tiny Stories 2K'\n",
       "    config_description: 'BPE tokenizer trained on Tiny Stories dataset w/ 2K tokens'\n",
       "    project_dir: '.'\n",
       "    models_dir: './output_models'\n",
       "    tokenizers_dir: '../../../tokenizers'\n",
       "    datasets_dir: '../../../datasets'\n",
       "    tokenizer_name: 'tiny_stories_2k'\n",
       "    output_dir: '../../../tokenizers/tiny_stories_2k'\n",
       "    vocab_size: '2000'\n",
       "    model_max_length: '2048'\n",
       "\n",
       ".define: &tokenizer_dataset !singleton:operator:getitem@tokenizer_dataset\n",
       "    - !singleton:datasets:load_dataset\n",
       "        - 'roneneldan/TinyStories'\n",
       "    - 'train'\n",
       "\n",
       ".define: &tokenizer_args !singleton:named_dict@tokenizer_args\n",
       "    bos_token: '<|BOS|>'\n",
       "    eos_token: '<|EOS|>'\n",
       "    unk_token: '<|UNK|>'\n",
       "    pad_token: '<|PAD|>'\n",
       "    return_special_tokens_mask: True\n",
       "    model_max_length: 2048\n",
       "    padding_side: 'right'\n",
       "    truncation_side: 'right'\n",
       "\n",
       ".define: &special_tokens_map !singleton:named_dict@special_tokens_map\n",
       "    bos: '<|BOS|>'\n",
       "    pad: '<|PAD|>'\n",
       "    eos: '<|EOS|>'\n",
       "    unk: '<|UNK|>'\n",
       "\n",
       ".define: &tokenizer_trainer !lambda:forgather.ml.tokenizer:train_tokenizer@tokenizer_trainer\n",
       "    output_dir: '../../../tokenizers/tiny_stories_2k'\n",
       "    dataset: *tokenizer_dataset\n",
       "    args: *tokenizer_args\n",
       "    model: !singleton:tokenizers:models.BPE\n",
       "        cache_capacity: 16\n",
       "        unk_token: '<|UNK|>'\n",
       "        byte_fallback: True\n",
       "    normalizer: !singleton:tokenizers:normalizers.NFC []\n",
       "    pre_tokenizer: !singleton:tokenizers:pre_tokenizers.ByteLevel []\n",
       "    decoder: !singleton:tokenizers:decoders.ByteLevel []\n",
       "    post_processor: !singleton:tokenizers:processors.TemplateProcessing\n",
       "        single: '<bos> $A'\n",
       "        special_tokens: \n",
       "            - !singleton:named_tuple!tuple\n",
       "                - '<bos>'\n",
       "                - 0\n",
       "    trainer: !singleton:tokenizers.trainers:BpeTrainer\n",
       "        vocab_size: 2000\n",
       "        initial_alphabet: !singleton:tokenizers:pre_tokenizers.ByteLevel.alphabet []\n",
       "        special_tokens: !singleton:list\n",
       "            - !singleton:values\n",
       "                - *special_tokens_map\n",
       "        show_progress: False\n",
       "\n",
       "\n",
       "meta: *meta\n",
       "main: !singleton:forgather.ml.construct:build_rule\n",
       "    target: '../../../tokenizers/tiny_stories_2k/tokenizer.json'\n",
       "    recipe: *tokenizer_trainer\n",
       "    loader: !lambda:transformers:AutoTokenizer.from_pretrained\n",
       "        - '../../../tokenizers/tiny_stories_2k'\n",
       "\n",
       "```\n",
       "\n",
       "### Generated Source Code\n",
       "\n",
       "```python\n",
       "from forgather.ml.tokenizer import train_tokenizer\n",
       "from datasets import load_dataset\n",
       "from tokenizers import pre_tokenizers.ByteLevel.alphabet\n",
       "from tokenizers.trainers import BpeTrainer\n",
       "from tokenizers import models.BPE\n",
       "from tokenizers import pre_tokenizers.ByteLevel\n",
       "from transformers import AutoTokenizer.from_pretrained\n",
       "from tokenizers import processors.TemplateProcessing\n",
       "from forgather.ml.construct import build_rule\n",
       "from tokenizers import decoders.ByteLevel\n",
       "from tokenizers import normalizers.NFC\n",
       "\n",
       "def construct(\n",
       "):\n",
       "    meta = {\n",
       "        'config_name': 'Tiny Stories 2K',\n",
       "        'config_description': 'BPE tokenizer trained on Tiny Stories dataset w/ 2K tokens',\n",
       "        'project_dir': '.',\n",
       "        'models_dir': './output_models',\n",
       "        'tokenizers_dir': '../../../tokenizers',\n",
       "        'datasets_dir': '../../../datasets',\n",
       "        'tokenizer_name': 'tiny_stories_2k',\n",
       "        'output_dir': '../../../tokenizers/tiny_stories_2k',\n",
       "        'vocab_size': '2000',\n",
       "        'model_max_length': '2048',\n",
       "    }\n",
       "\n",
       "    tokenizer_dataset = load_dataset(\n",
       "            'roneneldan/TinyStories',\n",
       "        )['train']\n",
       "\n",
       "    tokenizer_args = {\n",
       "        'bos_token': '<|BOS|>',\n",
       "        'eos_token': '<|EOS|>',\n",
       "        'unk_token': '<|UNK|>',\n",
       "        'pad_token': '<|PAD|>',\n",
       "        'return_special_tokens_mask': True,\n",
       "        'model_max_length': 2048,\n",
       "        'padding_side': 'right',\n",
       "        'truncation_side': 'right',\n",
       "    }\n",
       "\n",
       "    special_tokens_map = {\n",
       "        'bos': '<|BOS|>',\n",
       "        'pad': '<|PAD|>',\n",
       "        'eos': '<|EOS|>',\n",
       "        'unk': '<|UNK|>',\n",
       "    }\n",
       "\n",
       "    tokenizer_trainer = lambda: train_tokenizer(\n",
       "        output_dir='../../../tokenizers/tiny_stories_2k',\n",
       "        dataset=tokenizer_dataset,\n",
       "        args=tokenizer_args,\n",
       "        model=models.BPE(\n",
       "            cache_capacity=16,\n",
       "            unk_token='<|UNK|>',\n",
       "            byte_fallback=True,\n",
       "        ),\n",
       "        normalizer=normalizers.NFC(),\n",
       "        pre_tokenizer=pre_tokenizers.ByteLevel(),\n",
       "        decoder=decoders.ByteLevel(),\n",
       "        post_processor=processors.TemplateProcessing(\n",
       "            single='<bos> $A',\n",
       "            special_tokens=[\n",
       "                (\n",
       "                    '<bos>',\n",
       "                    0,\n",
       "                ),\n",
       "            ],\n",
       "        ),\n",
       "        trainer=BpeTrainer(\n",
       "            vocab_size=2000,\n",
       "            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
       "            special_tokens=list(\n",
       "                special_tokens_map.values(),\n",
       "            ),\n",
       "            show_progress=False,\n",
       "        ),\n",
       "    )\n",
       "    \n",
       "    return {\n",
       "        'meta': meta,\n",
       "        'main': build_rule(\n",
       "            target='../../../tokenizers/tiny_stories_2k/tokenizer.json',\n",
       "            recipe=tokenizer_trainer,\n",
       "            loader=lambda: AutoTokenizer.from_pretrained(\n",
       "                '../../../tokenizers/tiny_stories_2k',\n",
       "            ),\n",
       "        ),\n",
       "    }\n",
       "\n",
       "```\n",
       "\n",
       "## Constructed Project\n",
       "\n",
       "```python\n",
       "{'main': PreTrainedTokenizerFast(name_or_path='../../../tokenizers/tiny_stories_2k', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|PAD|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "},\n",
       " 'meta': {'config_description': 'BPE tokenizer trained on Tiny Stories dataset '\n",
       "                                'w/ 2K tokens',\n",
       "          'config_name': 'Tiny Stories 2K',\n",
       "          'datasets_dir': '../../../datasets',\n",
       "          'model_max_length': '2048',\n",
       "          'models_dir': './output_models',\n",
       "          'output_dir': '../../../tokenizers/tiny_stories_2k',\n",
       "          'project_dir': '.',\n",
       "          'tokenizer_name': 'tiny_stories_2k',\n",
       "          'tokenizers_dir': '../../../tokenizers',\n",
       "          'vocab_size': '2000'}}\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.ml.notebooks as nb\n",
    "\n",
    "nb.display_project_index(config_template=\"\", materialize=True, pp_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0a06f-f425-41ef-878b-017a2d2c2718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
