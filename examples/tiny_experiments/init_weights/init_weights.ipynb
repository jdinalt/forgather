{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Test Various Weight Initialization Methods\n",
       "\n",
       "### Control\n",
       "\n",
       "This uses the standard PyTorch initializaiton methods for Linear and Embedding layers.\n",
       "\n",
       "Torch uses code equivalent to the followning for initializing linear layers:\n",
       "\n",
       "```python\n",
       "stdv = 1. / math.sqrt(self.weight.size(1))\n",
       "self.weight.data.uniform_(-stdv, stdv)\n",
       "```\n",
       "\n",
       "See interesting discussions about this method:\n",
       "\n",
       "https://github.com/pytorch/pytorch/issues/57109\n",
       "https://soumith.ch/files/20141213_gplus_nninit_discussion.htm\n",
       "\n",
       "### Regex\n",
       "\n",
       "This is the same initialization as \"Control,\" but it uses regular-expressions to control how the parameters are initialized.\n",
       "\n",
       "This is more complex, but far more flexible.\n",
       "\n",
       "### Xavier Uniform\n",
       "\n",
       "Here. we use regex again, but use it to replace the torch default init with Xavier Unifrom initializaiton.\n",
       "\n",
       "This performs relatively poorly.\n",
       "\n",
       "### Xavier Uniform No Feedforward\n",
       "\n",
       "This is the same as Xavier Unifrom, except for the feedforward layers, which are initialized with the \"torch\" method.\n",
       "\n",
       "This demonstrates that the primary issue is with using fan-out to compute the scaling-factor, where fan-out is 4x fan-in.\n",
       "\n",
       "Note that both methods are effectively the same for symetric matrices, like those used by the attention layers.\n",
       "\n",
       "The only difference in this case is with the initialization of the output layers.\n",
       "\n",
       "### Deepnet\n",
       "\n",
       "DeepNet: Scaling Transformers to 1,000 Layers  \n",
       "https://arxiv.org/pdf/2203.00555\n",
       "\n",
       "Here we try using the method described in the above paper. Among the changes, this rescales both the feedforward initialization and that of the \n",
       "attention value and output layers by \"beta,\" which is computed from the number of transformer layers and scales the residuals by \"alpha,\" \n",
       "also dervived from the number of layers.\n",
       "\n",
       "Even though this is using Xavier Uniform initializaiton, this performs on-par with the control, thus not showing the issue identified when \n",
       "testing with a simple Xavier Unifrom initialization.\n",
       "\n",
       "### Deepnet Init\n",
       "\n",
       "This uses the deepnet initialization method, but not the residual scaling factor. Performance is close to the other good methods.\n",
       "\n",
       "### Deepnet Torch\n",
       "\n",
       "This is the same as Deepnet, but we replace Xavier Uniform with the \"Torch\" method. Again, similar performance.\n",
       "\n",
       "### No sqrt(d_model)\n",
       "\n",
       "With transformer models, we typically initialize the embedding layer to have a standard-deviation of 1/sqrt(d_model) and then scale the embedding outputs by sqrt(d_model); this is our default.\n",
       "\n",
       "This was originally a clever trick to allow the same embedding weights to be used by both the input encoder and the output decoder, but does it matter when these weights are not tied?\n",
       "\n",
       "Let's see what happens if we instead initialize the embeddings to std = 1.0 and scale the embeddings by 1.0? This should produce the same results, right?\n",
       "\n",
       "\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/trainers/init_weights\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/trainers/init_weights/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "    - [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/trainers/init_weights/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/forgather_workspace](../../../forgather_workspace)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/tiny_experiments](../../../templates/tiny_experiments)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/modellib](../../../templates/modellib)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/base](../../../templates/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [deepnet_torch.yaml](templates/configs/deepnet_torch.yaml)\n",
       "- [deepnet.yaml](templates/configs/deepnet.yaml)\n",
       "- [xavier_uniform_noff.yaml](templates/configs/xavier_uniform_noff.yaml)\n",
       "- [norm_all_the_weights.yaml](templates/configs/norm_all_the_weights.yaml)\n",
       "- [regex.yaml](templates/configs/regex.yaml)\n",
       "- [xavier_uniform.yaml](templates/configs/xavier_uniform.yaml)\n",
       "- [no_sqrt_dmodel.yaml](templates/configs/no_sqrt_dmodel.yaml)\n",
       "- [deepnet_init.yaml](templates/configs/deepnet_init.yaml)\n",
       "- [he_relu.yaml](templates/configs/he_relu.yaml)\n",
       "- [control.yaml](templates/configs/control.yaml)\n",
       "\n",
       "Default Configuration: control.yaml\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "nb.display_project_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85086ab4-2ed8-475e-b8b4-6ccc539d5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.display_config(config_template=\"\", show_pp_config=False, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c0dab-e03d-4ba9-a3cb-86a0319a6f08",
   "metadata": {},
   "source": [
    "## Constuct Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6c458-ec39-4c4b-a978-f54ed6a544d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "from forgather import Project\n",
    "\n",
    "# Pass config name\n",
    "proj = Project(\"control.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68e2a0-4aa7-479a-bec8-09d2e5399e04",
   "metadata": {},
   "source": [
    "## Train Model in Notebook\n",
    "This only works for a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f419ac-aec8-4edb-93ef-2850487ec34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use default config and default output target (training script, in this example).\n",
    "training_script = proj()\n",
    "training_script.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30fa78-b4e4-44f2-a12d-ce0138fd8c48",
   "metadata": {},
   "source": [
    "## Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0f013-5494-49c6-b6d4-432ea0555af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show command to run tensorboard; local_host should be false if tensorboard should run on all network interfaces.\n",
    "nb.display_tb_command(proj, local_host=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232926b8-13ba-4742-846a-08d07fbe0935",
   "metadata": {},
   "source": [
    "## Generate Trainingscript\n",
    "The preferred way of running training is via the command-line. This generates a simple bash script to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0188b-8ade-43e7-ba5d-11785e5b528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second arg specifies which GPUs may be used. For example, \"0,2\" only allows the first and third GPU.\n",
    "# Note that multi-GPU training requires a trainer implementation which supports this. e.g. \"accel_trainer\"\n",
    "nb.generate_trainingscript(proj, \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16903487-2a0f-4c8b-9928-66794a3978e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
