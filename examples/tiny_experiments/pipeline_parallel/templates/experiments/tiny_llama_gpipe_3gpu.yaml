-- extends 'base_pipeline_project.yaml'

## This is the manual-splitting counter-part to the auto config. This is here to create
## a functional baseline to compare the "auto" version against. See comments in that
## config for details of the issue we are testing for.
[config_metadata]
    == super()
    -- set ns.config_name = "Tiny Llama GPipe with 3 GPUs"
    -- set ns.config_description = "Tiny Llama on 3 GPUs -- baseline test"
    -- set ns.log_name = "tiny_llama_gpipe_3gpu"
    -- set ns.model_name = "tiny_llama"

[construct_new_model]
    -- include 'models/llama_pipeline_model.yaml'

[trainer_definition]
    -- include 'experiment.llama_trainer_config'

#-------------------- experiment.llama_trainer_config --------------------
-- extends 'base_pipeline_project.trainer_config'

[trainer_meta_config]
    == super()
    -- set ns.nproc_per_node = 3

[trainer_args]
    == super()
    
    # Quick testing - short run
    max_steps: {{ max_steps | default(100) }}
    eval_steps: 25

    # Safe Tensors can't handle tied params/weights
    save_safetensors: False

    # Test checkpoint loading
    #load_weights_from_checkpoint: True
    
    # Smaller batch sizes for tiny model
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 16
    n_microbatches: 4

    # Debug settings
    debug_pipeline: False
    debug_split_model: False
    debug_model_params: False
    debug_model_init: False