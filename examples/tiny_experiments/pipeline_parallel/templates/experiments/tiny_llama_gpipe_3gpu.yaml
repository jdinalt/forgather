-- extends 'base_pipeline_project.yaml'

## This is the manual-splitting counter-part to the auto config. This is here to create
## a functional baseline to compare the "auto" version against. See comments in that
## config for details of the issue we are testing for.
[config_metadata]
    == super()
    -- set ns.config_name = "Tiny Llama GPipe with 3 GPUs"
    -- set ns.config_description = "Tiny Llama on 3 GPUs -- baseline test"
    -- set ns.log_name = "tiny_llama_gpipe_3gpu"
    -- set ns.model_name = "tiny_llama"
    -- set ns.model_project_dir = joinpath(project_dir, 'models/llama')
    -- set ns.model_project_config = "tiny_pipeline_llama.yaml"
    -- set ns.nproc_per_node = 3

[trainer_args]
    == super()
    
    # Quick testing - short run
    max_steps: {{ max_steps | default(100) }}
    eval_steps: 25

    # Safe Tensors can't handle tied params/weights
    save_safetensors: False

    # Test checkpoint loading
    #load_weights_from_checkpoint: True
    
    # Smaller batch sizes for tiny model
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 16
    n_microbatches: 4
