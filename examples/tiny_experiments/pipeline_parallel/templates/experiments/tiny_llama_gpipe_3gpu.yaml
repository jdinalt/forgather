-- extends 'base_pipeline_project.yaml'

-- block config_metadata
    == super()
    -- set ns.config_name = "Tiny Llama GPipe with 3 GPUs"
    -- set ns.config_description = "Pipeline model testing with 3 GPUs (3 stages)"
    -- set ns.log_name = "tiny_llama_gpipe_3gpu"
    -- set ns.model_name = "tiny_llama"
    -- set ns.nproc_per_node = 3
    -- set ns.save_model = True
-- endblock config_metadata

-- block construct_new_model
    -- include 'models/llama_pipeline_model.yaml'
-- endblock construct_new_model

-- block trainer_definition
    -- include 'experiment.llama_trainer_config'
-- endblock trainer_definition


#-------------------- experiment.llama_trainer_config --------------------
-- extends 'base_pipeline_project.trainer_config'

-- block trainer_meta_config
    == super()
    -- set trainer_def.pipeline_layers = 8
    -- set trainer_def.pipeline_segments = 3
    -- set trainer_def.pipeline_microbatches = 4
-- endblock trainer_meta_config

-- block trainer_args
    == super()
    
    # Quick testing - short run
    max_steps: 100
    eval_steps: 25

    # Safe Tensors can't handle tied params/weights
    save_safetensors: False

    # Test checkpoint loading
    #load_weights_from_checkpoint: True
    
    # Smaller batch sizes for tiny model
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 16

    # Debug settings
    debug_pipeline: True
    debug_split_model: False
    debug_model_params: False
    debug_model_init: False
-- endblock trainer_args


