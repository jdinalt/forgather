-- extends 'base_pipeline_project.yaml'

-- block config_metadata
    == super()
    -- set ns.config_name = "Tiny Llama GPipe with 3 GPUs"
    -- set ns.config_description = "Pipeline model testing with 3 GPUs (3 stages)"
    -- set ns.log_name = "tiny_llama_gpipe_3gpu"
    -- set ns.model_name = "tiny_llama_gpipe_3gpu"
    -- set ns.nproc_per_node = 3
-- endblock config_metadata

-- block construct_new_model
    -- include 'experiment.llama_model'
-- endblock construct_new_model

-- block trainer_definition
    -- include 'experiment.llama_trainer_config'
-- endblock trainer_definition


#-------------------- experiment.llama_trainer_config --------------------
-- extends 'base_pipeline_project.trainer_config'

-- block trainer_meta_config
    == super()
    -- set trainer_def.pipeline_layers = 8
    -- set trainer_def.pipeline_segments = 3
    -- set trainer_def.pipeline_microbatches = 4
-- endblock trainer_meta_config

-- block trainer_args
    == super()
    
    # Quick testing - short run
    max_steps: 100
    
    # Smaller batch sizes for tiny model
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 16

    # Debug settings
    debug_pipeline: True
    debug_split_model: False
    debug_model_params: False
    debug_model_init: False
-- endblock trainer_args

#-------------------- experiment.llama_model --------------------
-- extends 'models/tiny/tiny_dyn_llama.yaml'


-- block model_config
    == super()
    
    # Tiny Llama GPipe Overrides
    hidden_size: 256
    dim_feedforward: 1024
    num_attention_heads: 2
    d_head: 128
    num_hidden_layers: 8
    embedding_dropout: 0.1
    layer_dropout: 0.1
<< endblock model_config


