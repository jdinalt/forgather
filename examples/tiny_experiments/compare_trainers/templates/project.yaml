-- extends 'projects/tiny.yaml'

-- block config_metadata
    == super()
    ## Overrides
    -- set ns.config_name = "Compare Trainers"
    -- set ns.config_description = "Compare trainer performance."
    -- set ns.create_new_model = True
    -- set ns.save_model = False
    ## Defines
    -- set ns.trainer_class = 'trainers/trainer.yaml'
-- endblock config_metadata


-- block datasets_definition
    ## Note: Switch to 'datasets/tiny/tiny_stories.yaml' for the full dataset.
    -- include 'datasets/tinystories/tinystories_abridged.yaml'
-- endblock datasets_definition


-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


-- block construct_new_model
    -- include 'project.model_config'
-- endblock construct_new_model


-- block trainer_callbacks
    ## Go back to basic loggers, as we may be interesting in comparing the relative speed of the trainers.
    -- include 'callbacks/loggers.yaml'
<< endblock trainer_callbacks


-- block lr_scheduler
# HF Trainer does not support LR Scheduler factories. We must specify in trainer_args
lr_scheduler: &lr_scheduler ~
<< endblock lr_scheduler


-- block optimizer
# This would work with the HF trainer, if not for a bug, where they try to get the class name from the callable object.
# TODO: File bug report and propose PR to resolve.
optimizer: &optimizer ~
<< endblock optimizer

#-------------------- project.trainer_config --------------------
-- extends 'tiny.trainer_config'


-- block trainer_args
    == super()
    # HF Trainer compat. LR scheduler args.
    lr_scheduler_type: "linear"
    warmup_steps: 500

    # HR optimizer args
    learning_rate: 1.0e-3
-- endblock trainer_args

#-------------------- project.model_config --------------------
-- extends 'models/tiny/tiny_causal.yaml'
