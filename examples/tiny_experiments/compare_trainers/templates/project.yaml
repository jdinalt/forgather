-- extends 'projects/tiny.yaml'

[config_metadata]
    == super()
    ## Overrides
    -- set ns.config_name = "Compare Trainers"
    -- set ns.config_description = "Compare trainer performance."
    -- set ns.create_new_model = True
    -- set ns.save_model = False
    ## Defines
    -- set ns.trainer_class = 'trainers/trainer.yaml'

[trainer_definition]
    -- include 'project.trainer_config'

[trainer_callbacks]
    ## Go back to basic loggers, as we may be interesting in comparing the relative speed of the trainers.
    -- include 'callbacks/loggers.yaml'

[lr_scheduler]
# HF Trainer does not support LR Scheduler factories. We must specify in trainer_args
lr_scheduler: &lr_scheduler ~

[optimizer]
# This would work with the HF trainer, if not for a bug, where they try to get the class name from the callable object.
# TODO: File bug report and propose PR to resolve.
optimizer: &optimizer ~

#-------------------- project.trainer_config --------------------
-- extends 'tiny.trainer_config'

[trainer_meta_config]
    == super()
    ## Disable DDP training, when supported
    -- set ns.nproc_per_node = 1

[trainer_args]
    == super()
    # HF Trainer compat. LR scheduler args.
    lr_scheduler_type: "linear"
    warmup_steps: 500

    # HR optimizer args
    learning_rate: 1.0e-3

    # Grad norm is disabled by default in our trainer. Set value to enable it in all trainers.
    max_grad_norm: 1.0
