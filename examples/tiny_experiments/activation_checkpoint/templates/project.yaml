-- extends 'projects/tiny.yaml'

-- block config_metadata
    == super()
    -- set ns.create_new_model = True
    -- set ns.save_model = False
    -- set ns.log_peak_memmory = True
    -- set ns.log_memory_to_tb = False ## Log to console
-- endblock config_metadata

-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


-- block datasets_definition
    -- include 'project.dataset_config'
-- endblock datasets_definition


-- block trainer_callbacks
    -- include 'project.callbacks'
<< endblock trainer_callbacks


-- block construct_new_model
    -- include 'project.model'
-- endblock construct_new_model

-- block optimizer
optimizer: &optimizer !lambda:forgather.ml.optim.adafactor:Adafactor
    lr: 2.0e-4
    weight_decay: 0.01
    
<< endblock optimizer

-- block lr_scheduler
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 2000
    cooldown_steps: 100000
    checkpoint_step: -1
    constant_lr: 3.0e-5
<< endblock lr_scheduler

#-------------------- project.trainer_config --------------------
-- extends 'tiny.trainer_config'


-- block trainer_args
    == super()
    # project overrides
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16
    logging_steps: 25
    eval_steps: 200
    max_grad_norm: 1.0
    ## max_steps: 3
-- endblock trainer_args


#-------------------- project.dataset_config --------------------
##-- extends 'datasets/tiny_stories_abridged.yaml'
-- extends 'datasets/tiny_stories.yaml'


#-------------------- project.callbacks --------------------
-- extends 'tiny.callbacks'


#-------------------- project.model --------------------
-- extends 'models/deepone.yaml'

## Add the project model_src directory to the search path
-- block model_submodule_searchpath
    - "{{ ns.project_model_src_dir }}"
    == super()
<< endblock model_submodule_searchpath


-- block model_tokenizer
    ## Use a smaller tokenizer.
    -- include 'tokenizers/tiny_8k.yaml'
<< endblock model_tokenizer

-- block model_config
    == super()
    enable_activation_checkpoint: False
    checkpoint_stride: 1
<< endblock model_config