-- extends 'projects/tiny.yaml'

[config_metadata]
    == super()
    ## Switch to 1.7B Deepone model definition
    -- set ns.model_project_dir = joinpath(ns.forgather_dir, 'examples', 'models', 'deepone')
    -- set ns.model_project_config = "1_7B.yaml"

[trainer_definition]
    -- include 'project.trainer_config'

[trainer_callbacks]
    -- include 'project.callbacks'

[optimizer]
optimizer: &optimizer !lambda:forgather.ml.optim.adafactor:Adafactor
    lr: 2.0e-4
    weight_decay: 0.01

[datacollator]
# To control for sequence length, always use the same length -- padding out to max_length, if short, and 
# truncating if long.
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
    # Comment out this line to use variable length batches.
    padding: "max_length"
    truncation: True
    max_length: 768

#-------------------- project.trainer_config --------------------
-- extends 'tiny.trainer_config'

[trainer_args]
    == super()
    # project overrides
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 8
    logging_steps: 10
    eval_steps: 100
    max_steps: {{ max_steps | default(100) }}
    # See: https://docs.pytorch.org/docs/stable/generated/torch.nn.attention.SDPBackend.html
    sdpa_backend: "efficient"
    # Set on NVIDIA Ampere or later GPUs when training in 32-bit precision for a significant speedup
    # float32_matmul_precision: "high"

#-------------------- project.callbacks --------------------
-- extends 'tiny.callbacks'
## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

[callback_list]
    == super()
    text_gen_callback: null # Disable text gen

    # Override peak_memory
    peak_memory: !singleton:forgather.ml.trainer.callbacks:PeakMemory
        show_details: True
        do_log: True
        summary_writer: *summary_writer
        # Uncomment to write a PyTorch memory snampshot file at the first log step.
        # See: https://pytorch.org/blog/understanding-gpu-memory-1/
        # enable_memory_snapshot: True
        file_prefix: "memory_snapshot"
