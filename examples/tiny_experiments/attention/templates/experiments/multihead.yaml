-- extends 'project.yaml'

-- block config_metadata
    == super()
    -- set ns.config_name = "Standard Multi-Head Attention"
    -- set ns.config_description = "Baseline configuration using standard multi-head attention with PyTorch SDPA"
    -- set ns.model_name = "attention_multihead"
    -- set ns.log_name = "attention_multihead"
-- endblock config_metadata


-- block construct_new_model
    -- include 'experiment.model'
-- endblock construct_new_model

#-------------------- experiment.model --------------------
-- extends 'project.model_config'


-- block attention_factory
# Standard multi-head attention with injectable SDPA (tiny model defaults)
attention_factory: &attention_factory !partial:.causal_multihead_attn:CausalMultiheadAttn@attention_factory
    d_model: !var "hidden_size"
    num_heads: !var "num_attention_heads"
    dropout: !var "attention_dropout"
<< endblock attention_factory