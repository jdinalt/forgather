-- extends 'models/control.yaml'

-- block model_meta_config
    == super()
    -- set model_def.name = "ReLU-GLU"
    -- set model_def.description = "Base + ReLU-GLU FF layer"
    -- set model_def.short_name = "relu-glu"
-- endblock model_meta_config


## Note: It would also be possible to override the swi-glu config's activation function.
-- block feedforward_factory
# ReluGLU config override.
# GLU Variants Improve Transformer: https://arxiv.org/pdf/2002.05202v1
feedforward_factory: &feedforward_factory !lambda:.glu_feedforward:GLUFeedforwardLayer
    d_model: !var "hidden_size"
    d_feedforward: !var "dim_feedforward"
    dropout: !var "activation_dropout"
    # Replace the default activation (SiLU) with ReLU.
    activation_factory: !lambda:torch.nn:ReLU []
-- endblock feedforward_factory


-- block model_config
== super()
    # Scale down, to compensate for increased parameter count
    # GLU-feedforward increases parameter could by 1.5, so scale down
    # 1024 * 2/3 ~ 682
    dim_feedforward: 682
<< endblock model_config