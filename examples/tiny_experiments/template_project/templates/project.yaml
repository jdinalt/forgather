## Project level definitions and overrides go here.
-- extends "types/training_script/causal_lm/causal_lm.yaml"

-- block resource_directories
    == super()
    ## Directory in which pre-trained models are located.
    ##-- set ns.models_dir = joinpath(user_home_dir(), 'ai_assets', 'models')
    
    ## Directory in which local datasets are stored
    ##-- set ns.datasets_dir = joinpath(user_home_dir(), 'ai_assets', 'datasets')
<< endblock resource_directories


## Set Project level defaults
-- block config_metadata
    == super()
    -- set ns.config_name = "Project Name"
    -- set ns.config_description = "An example configuration"
    
    ## Initialize a new model from scratch
    -- set ns.create_new_model = True

    ## Save model, when training is complete
    -- set ns.save_model = True

    ## Required to load a custom model
    -- set ns.trust_remote_code = True
-- endblock config_metadata


-- block datasets_definition
    -- include 'datasets/tiny_stories_abridged.yaml'
-- endblock datasets_definition


-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


-- block construct_new_model
    -- include 'project.model_config'
-- endblock construct_new_model


-- block trainer_callbacks
    -- include 'project.callbacks'
<< endblock trainer_callbacks


-- block datacollator
# Data collator for causal model
# Batches are dynamically padded to longest sequence
# labels are set to input_ids, with pad tokens set to -100
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
-- endblock datacollator


-- block lr_scheduler
    ##-- include 'lr_schedulers/cosine_annealing_with_warmup.yaml'
# https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 5000
    cooldown_steps: 50000
    constant_lr: 1.0e-4
<< endblock lr_scheduler


-- block optimizer
optimizer: &optimizer !lambda:torch:optim.AdamW
    lr: 1.0e-3
<< endblock optimizer


#-------------------- project.trainer_config --------------------
## Select one-of:
## trainers/( trainer.yaml | accel_trainer.yaml | hf_trainer.yaml )
-- extends 'trainers/accel_trainer.yaml'


-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
<< endblock trainer_meta_config


-- block trainer_args
    == super()
    # Project Overrides
    learning_rate: 5.0e-4
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 64
    logging_steps: 100
    eval_steps: 500
    lr_scheduler_type: "cosine"
    
    ## Stop early for quick test.
    ## max_steps: 1000
<< endblock trainer_args

#-------------------- project.model_config --------------------
-- extends 'models/causal_transformer.yaml'

-- block model_config
    == super()
    
    # Project Overrides
<< endblock model_config


## Add the project model_src directory to the search path
-- block model_submodule_searchpath
    - "{{ ns.project_model_src_dir }}"
    == super()
<< endblock model_submodule_searchpath

#-------------------- project.callbacks --------------------
-- extends 'callbacks/loggers.yaml'
