-- extends 'project.yaml'

-- block config_metadata
    == super()
    -- set ns.config_name = "RoPE with GQA - Rotary Position Embedding + Grouped Query Attention"
    -- set ns.config_description = "Larger model configuration using RoPE and GQA for efficiency testing"
    -- set ns.model_name = "rope_gqa"
    -- set ns.log_name = "rope_gqa"
-- endblock config_metadata


-- block construct_new_model
    -- include 'experiment.model'
-- endblock construct_new_model

#-------------------- experiment.model --------------------
-- extends 'tiny.model_config'

-- block model_config
    == super()
    # Larger model configuration for GQA testing
    hidden_size: 512
    num_attention_heads: 8
    num_kv_heads: 2  # 4:1 ratio for meaningful GQA testing
    dim_feedforward: 1024
    num_hidden_layers: 6
<< endblock model_config


-- block positional_encoder
# Use NullPE since RoPE is embedded in attention layers
positional_encoder: &positional_encoder !factory:.null_pe:NullPE@positional_encoder
<< endblock positional_encoder


-- block attention_factory
# Use improved RoPE-enabled attention with injectable SDPA and GQA
attention_factory: &attention_factory !partial:.causal_rpe_attn:CausalRpeAttn@attention_factory
    d_model: !var "hidden_size"
    num_heads: !var "num_attention_heads"
    num_kv_heads: !var "num_kv_heads"
    dropout: !var "attention_dropout"
    apply_pos_emb: !partial:.rotary_embeddings:apply_rotary_emb []
<< endblock attention_factory


-- block model_factory
model_factory: &model_factory !factory:.causal_rpe_lm:CausalRpeLM@model_factory
    loss_fn: *loss_fn
    input_encoder: *input_encoder
    output_decoder: *output_decoder
    layer_stack: *layer_stack
    init_weights: *init_weights
    relative_pe: !partial:.rotary_embeddings:RotaryPE
        d_head: 64  # 512 / 8 = 64
        max_sequence_length: !var "max_sequence_length"
        rope_theta: 10000.0
<< endblock model_factory