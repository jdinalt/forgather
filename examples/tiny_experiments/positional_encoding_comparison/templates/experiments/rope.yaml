-- extends 'project.yaml'

-- block config_metadata
    == super()
    -- set ns.config_name = "RoPE - Rotary Position Embedding"
    -- set ns.config_description = "Configuration using Rotary Position Embeddings (RoPE) instead of sinusoidal PE"
    -- set ns.model_name = "rope"
    -- set ns.log_name = "rope"
-- endblock config_metadata

-- block construct_new_model
    -- include 'experiment.model'
-- endblock construct_new_model

#-------------------- experiment.model --------------------
-- extends 'project.model'

-- block positional_encoder
# Use NullPE since RoPE is embedded in attention layers
positional_encoder: &positional_encoder !singleton:.null_pe:NullPE@positional_encoder
<< endblock positional_encoder

-- block attention_factory
# Use RoPE-enabled attention instead of standard attention
attention_factory: &attention_factory !partial:.causal_multihead_rope_attn:CausalMultiheadRoPEAttn@attention_factory
    d_model: !var "hidden_size"
    num_heads: !var "num_attention_heads"
    max_seq_len: !var "max_sequence_length"
    dropout: !var "attention_dropout"
    theta: 10000.0
<< endblock attention_factory