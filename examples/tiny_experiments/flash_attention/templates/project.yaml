-- extends 'projects/tiny.yaml'

-- block config_metadata
    == super()
    -- set ns.create_new_model = True
    -- set ns.save_model = False
    ## float16 or bfloat16 are required for flash
    ## OTOH, torch "memory-efficient" attention can work with float32.
    -- set ns.model_dtype = "bfloat16"
-- endblock config_metadata

-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


-- block datasets_definition
    -- include 'project.dataset_config'
-- endblock datasets_definition


-- block trainer_callbacks
    -- include 'project.callbacks'
<< endblock trainer_callbacks


-- block construct_new_model
    -- include 'project.model_flash'
-- endblock construct_new_model

#-------------------- project.trainer_config --------------------
-- extends 'tiny.trainer_config'


-- block trainer_args
    == super()
    # project overrides
    ## max_steps: 3
-- endblock trainer_args


#-------------------- project.dataset_config --------------------
-- extends 'datasets/tiny/tiny_stories_abridged.yaml'
##-- extends 'datasets/tiny/tiny_stories.yaml'


-- block tokenize_args
## See: https://huggingface.co/docs/transformers/main_classes/tokenizer
    == super()
    # project overrides
    max_length: 512
<< endblock tokenize_args

#-------------------- project.callbacks --------------------
-- extends 'tiny.callbacks'
##-- extends 'callbacks/grad_logger.yaml'

#-------------------- project.model_base --------------------
-- extends 'models/tiny/tiny_causal.yaml'

## Add the project model_src directory to the search path
-- block model_submodule_searchpath
    - "{{ ns.project_model_src_dir }}"
    == super()
<< endblock model_submodule_searchpath

-- block model_config
    == super()
    
    # Flash Attention Project Overrides
    # Make the model big enough to be interesting.
    hidden_size: 512
    dim_feedforward: 2048
    num_attention_heads: 4
    num_hidden_layers: 8
    embedding_dropout: 0.1
    layer_dropout: 0.1
<< endblock model_config

#-------------------- project.model_flash --------------------
-- extends 'project.model_base'

-- block attention_factory
attention_factory: &attention_factory !lambda:.causal_multihead_flash_attn:CausalMultiheadFlashAttn@attention_factory
    d_model: !var "hidden_size"
    num_heads: !var "num_attention_heads"
    dropout: !var "attention_dropout"
    attn_type: !var "attention_type"
<< endblock attention_factory

