{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Testing Project\n",
    "\n",
    "This project demonstrates the new checkpoint functionality for optimizer and learning rate scheduler states.\n",
    "\n",
    "## Features Tested\n",
    "\n",
    "- **Optimizer State Checkpointing**: Save and restore optimizer state including parameter-specific momentum and other state variables\n",
    "- **LR Scheduler State Checkpointing**: Save and restore learning rate scheduler state to maintain proper learning rate schedules across training interruptions\n",
    "- **Automatic Checkpoint Discovery**: Automatically find and resume from the most recent valid checkpoint\n",
    "- **Modification Time-based Selection**: Use file modification times rather than step numbers for robust checkpoint discovery\n",
    "\n",
    "## Configurations\n",
    "\n",
    "1. **train.yaml**: Initial training with checkpointing enabled\n",
    "2. **resume.yaml**: Resume training from the latest checkpoint\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "### Step 1: Initial Training\n",
    "Run the initial training configuration to create checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "nb.display_model_project_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.display_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather import Project\n",
    "\n",
    "# Load the checkpointing project to display trainer args\n",
    "proj = Project(\"train.yaml\")\n",
    "trainer_args = proj(\"trainer_args\")\n",
    "print(\"Trainer Arguments Configuration:\")\n",
    "for key, value in trainer_args.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display configuration details for train.yaml\n",
    "print(\"=== Initial Training Configuration (train.yaml) ===\")\n",
    "config = proj.environment.load(\"configs/train.yaml\") \n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Uses project template with checkpointing enabled\")\n",
    "\n",
    "# Show key checkpointing-related trainer arguments\n",
    "print(\"\\n=== Key Checkpoint Settings ===\")\n",
    "trainer_args = proj(\"trainer_args\")\n",
    "checkpoint_settings = {\n",
    "    'save_strategy': trainer_args.get('save_strategy'),\n",
    "    'save_steps': trainer_args.get('save_steps'), \n",
    "    'save_total_limit': trainer_args.get('save_total_limit'),\n",
    "    'max_steps': trainer_args.get('max_steps'),\n",
    "    'save_optimizer_state': trainer_args.get('save_optimizer_state'),\n",
    "    'save_scheduler_state': trainer_args.get('save_scheduler_state'),\n",
    "    'resume_from_checkpoint': trainer_args.get('resume_from_checkpoint')\n",
    "}\n",
    "\n",
    "for key, value in checkpoint_settings.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run Initial Training\n",
    "\n",
    "To run the training, use the command line:\n",
    "\n",
    "```bash\n",
    "cd examples/tiny_experiments/checkpointing\n",
    "fgcli.py -t train.yaml -d 0\n",
    "```\n",
    "\n",
    "This will:\n",
    "- Train for 500 steps\n",
    "- Save checkpoints every 100 steps \n",
    "- Save optimizer and scheduler state with each checkpoint\n",
    "- Create checkpoints in `output_models/default_model/checkpoints/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_script = proj()\n",
    "training_script.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if checkpoints were created\n",
    "checkpoint_dir = \"output_models/default_model/checkpoints\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
    "    print(f\"Found {len(checkpoints)} checkpoints:\")\n",
    "    for cp in sorted(checkpoints):\n",
    "        cp_path = os.path.join(checkpoint_dir, cp)\n",
    "        files = os.listdir(cp_path)\n",
    "        has_training_state = 'training_state.pt' in files\n",
    "        print(f\"  {cp}: {len(files)} files {'✓ Has training state' if has_training_state else '✗ No training state'}\")\n",
    "else:\n",
    "    print(\"No checkpoints found. Run the initial training first.\")\n",
    "    print(f\"Expected checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Resume Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the resume configuration to show trainer args\n",
    "print(\"=== Resume Training Configuration (resume.yaml) ===\")\n",
    "proj_resume = Project(\"resume.yaml\")\n",
    "resume_trainer_args = proj_resume(\"trainer_args\")\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Uses project template with checkpoint resumption enabled\")\n",
    "\n",
    "# Show key resume-related trainer arguments\n",
    "print(\"\\n=== Key Resume Settings ===\")\n",
    "resume_settings = {\n",
    "    'max_steps': resume_trainer_args.get('max_steps'),\n",
    "    'resume_from_checkpoint': resume_trainer_args.get('resume_from_checkpoint'),\n",
    "    'restore_optimizer_state': resume_trainer_args.get('restore_optimizer_state'),\n",
    "    'restore_scheduler_state': resume_trainer_args.get('restore_scheduler_state'),\n",
    "    'save_strategy': resume_trainer_args.get('save_strategy'),\n",
    "    'save_steps': resume_trainer_args.get('save_steps')\n",
    "}\n",
    "\n",
    "for key, value in resume_settings.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Resume Training\n",
    "\n",
    "To resume training from the latest checkpoint:\n",
    "\n",
    "```bash\n",
    "cd examples/tiny_experiments/checkpointing\n",
    "fgcli.py -t resume.yaml -d 0\n",
    "```\n",
    "\n",
    "This will:\n",
    "- Automatically find the latest checkpoint by modification time\n",
    "- Restore the model weights, optimizer state, and scheduler state\n",
    "- Continue training from step 500 to step 800\n",
    "- Maintain the learning rate schedule continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = Project(\"resume.yaml\")\n",
    "training_script = proj()\n",
    "training_script.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Verify Checkpoint Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Examine a checkpoint's training state\n",
    "checkpoint_dir = \"output_models/default_model/checkpoints\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = sorted(checkpoints)[-1]\n",
    "        training_state_path = os.path.join(checkpoint_dir, latest_checkpoint, \"training_state.pt\")\n",
    "        \n",
    "        print(f\"=== Examining {latest_checkpoint} ===\")\n",
    "        if os.path.exists(training_state_path):\n",
    "            training_state = torch.load(training_state_path, map_location=\"cpu\")\n",
    "            print(f\"Training state keys: {list(training_state.keys())}\")\n",
    "            \n",
    "            if 'global_step' in training_state:\n",
    "                print(f\"Global step: {training_state['global_step']}\")\n",
    "            \n",
    "            if 'optimizer' in training_state:\n",
    "                opt_state = training_state['optimizer']\n",
    "                print(f\"Optimizer state keys: {list(opt_state.keys())}\")\n",
    "                if 'param_groups' in opt_state:\n",
    "                    print(f\"Learning rate: {opt_state['param_groups'][0].get('lr', 'N/A')}\")\n",
    "            \n",
    "            if 'lr_scheduler' in training_state:\n",
    "                sched_state = training_state['lr_scheduler']\n",
    "                print(f\"Scheduler state keys: {list(sched_state.keys())}\")\n",
    "                print(f\"Last epoch: {sched_state.get('last_epoch', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"No training state found in {latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"No checkpoints found\")\n",
    "else:\n",
    "    print(\"Checkpoint directory does not exist\")\n",
    "    print(f\"Expected: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Behavior\n",
    "\n",
    "1. **Initial Training**: Creates checkpoints with both model weights and training state\n",
    "2. **Resume Training**: \n",
    "   - Finds latest checkpoint by modification time\n",
    "   - Loads model weights\n",
    "   - Restores optimizer state (momentum, etc.)\n",
    "   - Restores scheduler state (step count, learning rate schedule)\n",
    "   - Continues training seamlessly\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "The checkpoint functionality can be controlled with these training arguments:\n",
    "\n",
    "- `save_optimizer_state`: Save optimizer state in checkpoints\n",
    "- `save_scheduler_state`: Save LR scheduler state in checkpoints  \n",
    "- `restore_optimizer_state`: Restore optimizer state when resuming\n",
    "- `restore_scheduler_state`: Restore scheduler state when resuming\n",
    "- `resume_from_checkpoint`: Boolean (auto-find) or string (specific path)\n",
    "- `save_total_limit`: Maximum number of checkpoints to keep"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
