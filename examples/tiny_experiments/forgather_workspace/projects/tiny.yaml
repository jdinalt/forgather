## This is an example of a project template; useful where similar configurations
## may be used in multiple projects.
##
## Specifically, this sets up training for a very small model over one epoch
## on the 'abridged' version of the Tiny Stories dataset.
-- extends "training_script/causal_lm/causal_lm.yaml"


-- block config_metadata
    == super()
    ## Overrides
    -- set ns.config_name = "Tiny Experiments"
    -- set ns.config_description = "A project template for running tiny model experiments."
    ## Defines
    -- set ns.trainer_class = 'trainers/trainer.yaml'
    ## We make use of custom models in these projects
    -- set ns.trust_remote_code = True

    ## memory debugging
    -- set ns.debug_memory_detials = False
    ## Debug logging to TensorBoard vs console
    -- set ns.log_memory_to_tb = False

    ## The dataset sub-project to use
    -- set ns.dataset_proj = dataset_proj | default(joinpath(ns.forgather_dir, 'examples', 'datasets', 'roneneldan'))

    ## The configuration in the dataset sub-project
    ## Note: Switch to 'tinystories.yaml' for the full dataset.
    -- set ns.dataset_config = dataset_config | default("tinystories-abridged.yaml")
  
    ## The model project to import
    -- set ns.model_project_dir = joinpath(workspace_root, 'models')
    -- set ns.model_project_config = "tiny.yaml"
    -- set ns.debug_model_project = False
-- endblock config_metadata


-- block pre_model_setup
    ## Add assets needed for text-gen sampling.
    ## This adds a set of prompts and text-gen parameters.
    == super()
    -- include "prompts/tiny_stories.yaml"
-- endblock pre_model_setup


-- block datasets_preprocessor_args
tokenizer_args: &tokenizer_args !dict
    truncation: True
-- endblock datasets_preprocessor_args


-- block datasets_definition
    -- include 'datasets/llm_dataset_project.yaml'
-- endblock datasets_definition


## Defaults to the basic trainer implementation
## Note: This is unsuitable for multiple GPUs.
## Override 'ns.trainer_class' to change the trainer class.
-- block trainer_definition
    ## See definition below
    -- include 'tiny.trainer_config'
-- endblock trainer_definition


-- block construct_new_model
    -- include "models/causal_lm/import_model_project.yaml"
-- endblock construct_new_model


-- block trainer_callbacks
    ## See definition below
    -- include 'tiny.callbacks'
<< endblock trainer_callbacks


-- block datacollator
    == super()
    # Tiny Project
    ## Limit maximum sequence length 512 tokens, at the data-collator level.
    truncation: True
    max_length: 512
-- endblock datacollator


-- block lr_scheduler
    ##-- include 'lr_schedulers/cosine_annealing_with_warmup.yaml'
# https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    # Linear warm-up steps
    warmup_steps: 500

    # Cosine decay from end of warmp-up to constant-lr
    cooldown_steps: 50000

    # See paper for experimentally derived values.
    constant_lr: 1.0e-4
<< endblock lr_scheduler


-- block optimizer
optimizer: &optimizer !partial:torch:optim.AdamW
    lr: 1.0e-3

<< endblock optimizer


-- block dynamic_args
    == super()
    log_peak_memory:
        names: "--log-peak-memory"
        action: "store_true"
        help: "Log peak GPU memory at each log step"
    dataset_config:
        names: "--dataset-config"
        type: "str"
        help: "The name of the dataset configuration to use"
    dataset_proj:
        names: "--dataset-proj"
        type: "path"
        help: "Path to dataset project to use"
<< endblock dynamic_args

#-------------------- tiny.trainer_config --------------------
-- extends ns.trainer_class
## Note: We use dynamic inheritance for the trainer-class
## This has a side effect of not being able to statically resolve the
## parent template, which is named in the 'ns.trainer_class' variable,
## the value of which is defined in the 'config_metadata' block, above.

-- block trainer_args
    == super()
    
    # **Tiny Project**
    seed: 42
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 64
    logging_steps: 100
    eval_strategy: "steps"
    eval_steps: 500
    save_steps: 10000
    num_train_epochs: 1
    dataloader_num_workers: 1
-- endblock trainer_args

#-------------------- tiny.callbacks --------------------
-- extends 'callbacks/loggers.yaml'
## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

-- block callback_dependencies
    == super()

    -- filter trim()
    -- block text_gen_callback_args
text_gen_callback_args: &text_gen_callback_args
    summary_writer: *summary_writer
    prompts: *testprompts
    generation_config: *generation_config
    max_new_tokens: 40
    generation_steps: 2000
    << endblock text_gen_callback_args
    -- endfilter
<< endblock callback_dependencies

## This adds a text-generationn sample every 'generation_steps'
-- block callback_list
    == super()
    text_gen_callback: !singleton:forgather.ml.trainer.callbacks:TextgenCallback
        <<: *text_gen_callback_args
    peak_memory: !singleton:forgather.ml.trainer.callbacks:PeakMemory
        show_details: {{ ns.debug_memory_detials }}
        do_log: {{ log_peak_memory | default(False) }}
    -- if ns.log_memory_to_tb
        summary_writer: *summary_writer
    -- endif
    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback []
<< endblock callback_list
