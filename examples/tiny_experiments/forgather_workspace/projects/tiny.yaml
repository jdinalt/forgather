## This is an example of a project template; useful where similar configurations
## may be used in multiple projects.
##
## Specifically, this sets up training for a very small model over one epoch
## on the 'abridged' version of the Tiny Stories dataset.
-- extends "types/training_script/causal_lm/causal_lm.yaml"


-- block config_metadata
    == super()
    ## Overrides
    -- set ns.config_name = "Tiny Experiments"
    -- set ns.config_description = "A project template for running tiny model experiments."
    ## Defines
    -- set ns.trainer_class = 'trainers/trainer.yaml'
    ## We make use of custom models in these projects
    -- set ns.trust_remote_code = True

    ## memory debugging
    -- set ns.debug_memory_detials = False
    -- set ns.log_peak_memmory = False
    ## Debug logging to TensorBoard vs console
    -- set ns.log_memory_to_tb = False
-- endblock config_metadata


-- block pre_model_setup
    ## Add assets needed for text-gen sampling.
    ## This adds a set of prompts and text-gen parameters.
    == super()
    -- include "prompts/tiny_stories.yaml"
-- endblock pre_model_setup


-- block datasets_definition
    -- include 'tiny.dataset_config'
-- endblock datasets_definition


## Defaults to the basic trainer implementation
## Note: This is unsuitable for multiple GPUs.
## Override 'ns.trainer_class' to change the trainer class.
-- block trainer_definition
    ## See definition below
    -- include 'tiny.trainer_config'
-- endblock trainer_definition


-- block construct_new_model
    ## See definition below
    -- include 'tiny.model_config'
-- endblock construct_new_model


-- block trainer_callbacks
    ## See definition below
    -- include 'tiny.callbacks'
<< endblock trainer_callbacks


-- block datacollator
    == super()
    # Tiny Project
    ## Limit maximum sequence length 512 tokens, at the data-collator level.
    truncation: True
    max_length: 512
-- endblock datacollator


-- block lr_scheduler
    ##-- include 'lr_schedulers/cosine_annealing_with_warmup.yaml'
# https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    # Linear warm-up steps
    warmup_steps: 500

    # Cosine decay from end of warmp-up to constant-lr
    cooldown_steps: 50000

    # See paper for experimentally derived values.
    constant_lr: 1.0e-4
<< endblock lr_scheduler


-- block optimizer
optimizer: &optimizer !partial:torch:optim.AdamW
    lr: 1.0e-3

<< endblock optimizer

#-------------------- tiny.trainer_config --------------------
-- extends ns.trainer_class
## Note: We use dynamic inheritance for the trainer-class
## This has a side effect of not being able to statically resolve the
## parent template, which is named in the 'ns.trainer_class' variable,
## the value of which is defined in the 'config_metadata' block, above.

## Set sane trainer defaults for 'tiny' projects.
-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
-- endblock trainer_meta_config


-- block trainer_args
    == super()
    # Tiny Project Overrides
    seed: 42
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 64
    logging_steps: 100
    eval_steps: 500
    num_train_epochs: 1
    dataloader_num_workers: 1
-- endblock trainer_args

#-------------------- tiny.model_config --------------------
-- extends 'models/tiny/tiny_causal.yaml'
## Default to tiny-causal model definition.

#-------------------- tiny.callbacks --------------------
-- extends 'callbacks/loggers.yaml'
## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

-- block callback_dependencies
    == super()

    -- filter trim()
    -- block text_gen_callback_args
text_gen_callback_args: &text_gen_callback_args
    summary_writer: *summary_writer
    prompts: *testprompts
    generation_config: *generation_config
    max_new_tokens: 40
    generation_steps: 2000
    << endblock text_gen_callback_args
    -- endfilter
<< endblock callback_dependencies

## This adds a text-generationn sample every 'generation_steps'
-- block callback_list
    == super()
    - !singleton:forgather.ml.trainer.callbacks:TextgenCallback
        <<: *text_gen_callback_args
    - !singleton:forgather.ml.trainer.callbacks:PeakMemory
        show_details: {{ ns.debug_memory_detials }}
        do_log: {{ ns.log_peak_memmory }}
    -- if ns.log_memory_to_tb
        summary_writer: *summary_writer
    -- endif
<< endblock callback_list

#-------------------- tiny.dataset_config --------------------
## Note: Switch to 'datasets/roneneldan/tinystories.yaml' for the full dataset.
-- extends 'datasets/roneneldan/tinystories-abridged.yaml'

-- block tokenize_args
## See: https://huggingface.co/docs/transformers/main_classes/tokenizer
    == super()
    # project overrides
    max_length: 512
<< endblock tokenize_args
