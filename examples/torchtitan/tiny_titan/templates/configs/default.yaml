-- set ns = namespace()
-- include "base_directories.yaml"

.define: &config_name "Example Trainer"
.define: &config_description "Rough draft"
.define: &output_dir {{ joinpath(project_dir, 'output_models', 'test_model') }}
.define: &seq_len 512
.define: &local_batch_size 32

.define: &job !call:torchtitan.config.job_config:Job
    dump_folder: *output_dir
    description: *config_description
    print_args: true

.define: &profiling !call:torchtitan.config.job_config:Profiling

.define: &metrics !call:torchtitan.config.job_config:Metrics
    log_freq: 100
    disable_color_printing: false
    enable_tensorboard: true
    save_tb_folder: "runs"

.define: &training !call:torchtitan.config.job_config:Training
    steps: 6625 # 10% of dataset
    local_batch_size: *local_batch_size
    seq_len: *seq_len
    gc_freq: 1000

.define: &parallelism !call:torchtitan.config.job_config:Parallelism
    data_parallel_replicate_degree: 1
    data_parallel_shard_degree: -1
    fsdp_reshard_after_forward: "default" # default / never / always
    tensor_parallel_degree: 1
    enable_async_tensor_parallel: false
    pipeline_parallel_degree: 1
    context_parallel_degree: 1

.define: &checkpoint !call:torchtitan.config.job_config:Checkpoint
    enable: true
    folder: "checkpoints"
    interval: 1000
    keep_latest_k: 3
    last_save_model_only: false
    export_dtype: "float32"

.define: &activation_checkpoint !call:torchtitan.config.job_config:ActivationCheckpoint
    mode: "none"  # ["none", "selective", "full"]

.define: &compile !call:torchtitan.config.job_config:Compile
    enable: false
    components: ["model", "loss"]

.define: &float8 !call:torchtitan.config.job_config:Float8

.define: &mx !call:torchtitan.config.job_config:MX

.define: &comm !call:torchtitan.config.job_config:Comm

.define: &memory_estimation !call:torchtitan.config.job_config:MemoryEstimation

.define: &fault_tolerance !call:torchtitan.config.job_config:FaultTolerance

.define: &experimental !call:torchtitan.config.job_config:Experimental

.define: &validation !call:torchtitan.config.job_config:Validation
    enable: true
    freq: 500

job_config: &job_config !call:torchtitan.config.job_config:JobConfig@job_config
    job: *job
    profiling: *profiling
    metrics: *metrics
    training: *training
    parallelism: *parallelism
    checkpoint: *checkpoint
    activation_checkpoint: *activation_checkpoint
    compile: *compile
    float8: *float8
    mx: *mx
    comm: *comm
    memory_estimation: *memory_estimation
    fault_tolerance: *fault_tolerance
    experimental: *experimental
    validation: *validation

distributed_env: &distributed_env !call:forgather.ml.distributed:DistributedEnvironment

parallel_dims: &parallel_dims !call:forgather.ml.trainer.torchtitan.titan_trainer:ExtendedParallelDims.from_config
    distributed_env: *distributed_env
    parallelism_config: *parallelism

ft_manager: &ft_manager !call:forgather.ml.trainer.torchtitan.titan_trainer:ExtendedFTManager
    ft_config: *fault_tolerance
    parallel_dims: *parallel_dims

.define: &dp_rank !call:getattr [ *ft_manager, "dp_rank" ]
.define: &dp_degree !call:getattr [ *ft_manager, "dp_degree" ]

tokenizer_args: &tokenizer_args !dict
    max_length: *seq_len
    truncation: True

# Import tokenizer from sub-project
tokenizer: &tokenizer !call:forgather:from_project
    project_dir: {{ joinpath(ns.forgather_dir, 'examples', 'tokenizers', 'tiny_stories_bpe') }}
    config_template: "2k.yaml"

# Import dataset from sub-project
.define: &dataset_dict !call:forgather:from_project
    project_dir: {{ joinpath(ns.forgather_dir, 'examples', 'datasets', 'roneneldan') }}
    config_template: "tinystories-iter.yaml"
    targets: [ "train_dataset", "eval_dataset" ]
    preprocess_args: *tokenizer_args
    tokenizer: *tokenizer

train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]
eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]

data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM                                                                                                                                                                                                                                                                                                                  
    tokenizer: *tokenizer
    return_tensors: pt
    truncation: True
    input_name: "input"
    labels_name: null

train_dataloader: &train_dataloader !call:torchtitan.components.dataloader:ParallelAwareDataloader
    dataset: *train_dataset
    dp_rank: *dp_rank
    dp_world_size: *dp_degree
    batch_size: *local_batch_size
    collate_fn: *data_collator

validation_dataloader: &validation_dataloader !call:torchtitan.components.dataloader:ParallelAwareDataloader
    dataset: *eval_dataset
    dp_rank: *dp_rank
    dp_world_size: *dp_degree
    batch_size: 64
    collate_fn: *data_collator

model_args: &model_args !call:torchtitan.models.llama3.model.args:TransformerModelArgs
    dim: 256
    n_layers: 4
    n_heads: 2
    vocab_size: !call:len [ *tokenizer ]
    eos_id: !call:getattr [ *tokenizer, 'eos_token_id' ]
    max_seq_len: !call:getattr [ *tokenizer, "model_max_length" ]
    rope_theta: !!float 10000

model_factory: &model_factory !partial:torchtitan.models.llama3.model.model:Transformer
    model_args: *model_args

parallelize_fn: &parallelize_fn !partial:torchtitan.models.llama3.infra.parallelize:parallelize_llama

pipeline_fn: &pipeline_fn !partial:torchtitan.models.llama3.infra.pipeline:pipeline_llama

loss_fn: &loss_fn !call:forgather.ml.loss:CausalLoss

state_dict_adapter: &state_dict_adapter !call:torchtitan.models.llama3.model.state_dict_adapter:Llama3StateDictAdapter
    model_args: *model_args
    hf_assets_path: null

optimizer_factory: &optimizer_factory !partial:torch:optim.AdamW
    lr: 1.0e-3

lr_scheduler_factory: &lr_scheduler_factory !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler
    warmup_steps: 500
    cooldown_steps: 50000
    constant_lr: 1.0e-6

validator_factory: &validator_factory !partial:forgather.ml.trainer.torchtitan.validate:Validator

trainer: &trainer !call:forgather.ml.trainer.torchtitan.titan_trainer:Trainer@trainer
    job_config: *job_config
    parallel_dims: *parallel_dims
    ft_manager: *ft_manager
    distributed_env: *distributed_env
    train_dataloader: *train_dataloader
    validation_dataloader: *validation_dataloader
    tokenizer: *tokenizer
    model_factory: *model_factory
    optimizer_factory: *optimizer_factory
    lr_scheduler_factory: *lr_scheduler_factory
    parallelize_fn: *parallelize_fn
    pipelining_fn: *pipeline_fn
    loss_fn: *loss_fn
    state_dict_adapter: *state_dict_adapter
    validator_factory: *validator_factory
    model_args: *model_args

meta: &meta_output !dict:@meta
    config_name: *config_name
    config_description: *config_description
    config_class: "type.training_script.titan_trainer"
    output_dir: *output_dir
    forgather_dir: {{ ns.forgather_dir }}
    nproc_per_node: 1

dynamic_args: !dlist

main: !call:forgather.ml.training_script:TrainingScript@training_script
    meta: *meta_output
    trainer: *trainer