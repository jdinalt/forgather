-- extends "llama3.yaml"

## Derived from: torchtitan/config/models/llama3/model/train_configs/llamba3_8b.toml
[config_metadata]
    == super()
    -- set ns.config_name = "Llama3 8B"
    -- set ns.config_description = "Llama 3 8B training"
    -- set ns.model_name = "llama3_8b"

[profiling]
    => super()
    enable_profiling: true
    save_traces_folder: "profile_trace"
    profile_freq: 100

[metrics]
    => super()
    log_freq: 10
    enable_tensorboard: true
    save_tb_folder: "tb"

[model]
    => super()
    flavor: "8B"
    # converters: ["float8"]

[optimizer]
    => super()
    name: "AdamW"
    lr: !!float 3e-4
    eps: !!float 1e-8

[lr_scheduler]
    => super()
    warmup_steps: 200  # lr scheduler warm up

[training]
    => super()
    local_batch_size: 1
    seq_len: 8192
    max_norm: 1.0  # grad norm clipping

[parallelism]
    => super()
    data_parallel_replicate_degree: 1
    data_parallel_shard_degree: -1
    tensor_parallel_degree: 1
    pipeline_parallel_degree: 1
    context_parallel_degree: 1

[checkpoint]
    => super()
    enable: false
    folder: "checkpoint"
    interval: 500
    last_save_model_only: true
    export_dtype: "float32"
    async_mode: "disabled" # ["disabled", "async", "async_with_pinned_mem"]

[compile]
    => super()
    enable: false
    components: ["model", "loss"]

[activation_checkpoint]
    => super()
    mode: "selective"  # ["none", "selective", "full"]
    selective_ac_option: "op"  # "int" = ac every positive int layer or 'op', ac based on ops policy

[float8]
    => super()
    enable_fsdp_float8_all_gather: false
    precompute_float8_dynamic_scale_for_fsdp: false
    filter_fqns: ["output"]

[validation]
    => super()
    enable: false
    dataset: "c4_validation"
    freq: 500
    steps: 1200 # Recommend value for c4_validation with world-size=8 and seq_len=8192