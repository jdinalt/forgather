## Project level definitions and overrides go here.
-- extends "types/training_script/causal_lm/causal_lm.yaml"

-- block resource_directories
    == super()
    ## Directory in which pre-trained models are located.
    ##-- set ns.models_dir = joinpath(user_home_dir(), 'ai_assets', 'models')
    
    ## Directory in which local datasets are stored
    ##-- set ns.datasets_dir = joinpath(user_home_dir(), 'ai_assets', 'datasets')
<< endblock resource_directories


## Set Project level defaults
-- block config_metadata
    == super()
    -- set ns.config_name = "Project Name"
    -- set ns.config_description = "An example configuration"
    
    ## Initialize a new model from scratch
    -- set ns.create_new_model = True

    ## Save model, when training is complete
    -- set ns.save_model = True

    ## Required to load a custom model
    -- set ns.trust_remote_code = True
-- endblock config_metadata


-- block datasets_definition
    -- include 'project.dataset'
-- endblock datasets_definition


-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


-- block construct_new_model
    -- include 'project.model_config'
-- endblock construct_new_model


-- block trainer_callbacks
    -- include 'project.callbacks'
<< endblock trainer_callbacks


-- block lr_scheduler
##-- include 'lr_schedulers/cosine_annealing_with_warmup.yaml'
# https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    # This seems sufficient to maintain stability.
    warmup_steps: 5000

    # Don't both with cooldown.
    # Note: It non-zero, warm-up will proceed to the LR configured in the optimizer, rather than to
    # constant_lr, then perform cosine annealing for this many steps to the constant_lr
    cooldown_steps: 50000
    
    # Start annealing after first checkpoint.
    # This is an exponential decay after checkpoint_step.
    checkpoint_step: 100000

    # Hold at this LR after cooldown.
    constant_lr: 5.0e-6

    # Decay time constant for checkpoint
    tau: !!float 1.0e2
<< endblock lr_scheduler


-- block optimizer
optimizer: &optimizer !lambda:torch:optim.AdamW
    lr: 1.0e-4
<< endblock optimizer


-- block datacollator
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
    truncation: True
    ## Limit maximum sequence length 512 tokens, at the data-collator level.
    ## This can be useful for avoiding OOM errors, but can also be configured 
    ## by overriding the dataset's arguments.
    ## max_length: 512
    ##
    ## Setting 'padding' to "max_length," combined with "truncation," will produce a constant batch length
    ## padding: "max_length"
-- endblock datacollator


#-------------------- project.trainer_config --------------------
## Select one-of:
## trainers/( trainer.yaml | accel_trainer.yaml | hf_trainer.yaml | pipeline_trainer.yaml )
-- extends 'trainers/trainer.yaml'


-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
<< endblock trainer_meta_config


-- block trainer_args
    == super()
    # Project overrides
    seed: 42
    per_device_train_batch_size: 64
    per_device_eval_batch_size: 128
    logging_steps: 100
    eval_steps: 1000
    num_train_epochs: 2
    dataloader_num_workers: 1
    max_grad_norm: 1.0               # Gradient norm clipping threshold

    ## Stop early for quick test.
    ## max_steps: 1000
    
    # Checkpoint Save Settings
    save_strategy: "steps"           # Set to "steps" to save every "save_steps"; set to "no" to disable
    save_steps: 10000                # Checkpoint every N steps
    save_safetensors: True           # Safe tensors?
    save_total_limit: 3              # Keep at most N checkpoints
    save_on_each_node: False         # Save common files on each node
    save_optimizer_state: True       # Save optimizer state with checkpoint
    save_scheduler_state: True       # Save LR scheduler state with checkpoint
    save_rng_state: True             # Save RNG state with checkpoint
    
    # Checkpoint Load Settings
    resume_from_checkpoint: False    # Load latest checkpoint
    restore_optimizer_state: True    # Test optimizer state restoration per rank
    restore_scheduler_state: True    # Test scheduler state restoration per rank
    restore_rng_state: True          # Test RNG state restoration per rank
    restore_dataset_state: False     # Not fully implemented yet!
    
    # Only used if lr_scheduler is null
    lr_scheduler_type: "cosine"
    warmup_steps: 500
    
    # Only used if optimizer is null
    learning_rate: 1.0e-3
    
    # Torch Compile Settings
    torch_compile: True
    torch_compile_backend: "inductor"
    torch_compile_dynamic: True
    torch_compile_mode: "default"
    torch_compile_full_graph: False
    activation_memory_budget: 0.6

    float32_matmul_precision: "high" # Improves 32-bit MM performance on Ampere and later GPUs
    # Set SDPA Kernel backend(s)
    # https://docs.pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html#torch.nn.attention.sdpa_kernel
    sdpa_backend: [ "math", "flash", "efficient", "cudnn" ]
    sdpa_set_priority: False # If list, interpret as priority order

    ## Low memory options
    ## max_grad_norm: null
    ## fuse_optim_with_backward: True
    ## enable_activation_offloading: True
<< endblock trainer_args


## -- block model_preprocessor
## Convert model to bfloat16 format
## model_preprocessor: &model_preprocessor !partial:forgather.ml.construct:module_to_dtype [ *model, "bfloat16" ]
### << endblock model_preprocessor

#-------------------- project.model_config --------------------
-- extends 'models/example_model_config.yaml'


#-------------------- project.callbacks --------------------
-- extends 'callbacks/loggers.yaml'

-- block callback_list
    == super()
<< endblock callback_list

#-------------------- project.dataset --------------------
-- extends 'datasets/roneneldan/tinystories-abridged.yaml'

-- block tokenize_args
## See: https://huggingface.co/docs/transformers/main_classes/tokenizer
    == super()
    # project overrides
    max_length: 512
<< endblock tokenize_args
