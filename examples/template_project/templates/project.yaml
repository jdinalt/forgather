## Project level definitions and overrides go here.
-- extends "training_script/causal_lm/causal_lm.yaml"

[resource_directories]
    == super()
    ## Directory in which pre-trained models are located.
    ##-- set ns.models_dir = joinpath(user_home_dir(), 'ai_assets', 'models')
    
    ## Directory in which local datasets are stored
    ##-- set ns.datasets_dir = joinpath(user_home_dir(), 'ai_assets', 'datasets')

## Set Project level defaults
[config_metadata]
    == super()
    -- set ns.config_name = "Template Project"
    -- set ns.config_description = "An example configuration"

    ## Required to load a custom model
    -- set ns.trust_remote_code = True

    ## ** Dataset Project Import **
    -- set ns.dataset_proj = dataset_proj | default(joinpath(ns.forgather_dir, 'examples', 'datasets', 'roneneldan'))
    -- set ns.dataset_config = dataset_config | default("tinystories-abridged.yaml")

    ## ** Model Project Import **
    -- set ns.model_project_dir = joinpath(ns.forgather_dir, 'examples', 'models', 'causal_lm')
    -- set ns.model_project_config = "4M.yaml"
    -- set ns.debug_model_project = False

[datasets_preprocessor_args]
tokenizer_args: &tokenizer_args !dict
    max_length: 512
    truncation: True

[datasets_definition]
    -- include 'datasets/llm_dataset_project.yaml'

[trainer_definition]
    -- include 'project.trainer_config'

[construct_new_model]
    ## Import model definition from another project
    -- include "models/causal_lm/import_model_project.yaml"

[optimizer]
optimizer: &optimizer !lambda:torch:optim.AdamW
    lr: 1.0e-4

[lr_scheduler]
# https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    # This seems sufficient to maintain stability.
    warmup_steps: 200

    # Note: It non-zero, warm-up will proceed to the LR configured in the optimizer, rather than to
    # constant_lr, then perform cosine annealing for this many steps to the constant_lr
    cooldown_steps: 50000

    # Hold at this LR after cooldown.
    constant_lr: 5.0e-6

[datacollator]
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
    truncation: True
    ## Limit maximum sequence length 512 tokens, at the data-collator level.
    ## This can be useful for avoiding OOM errors, but can also be configured 
    ## by overriding the dataset's arguments.
    ## max_length: 512
    ##
    ## Setting 'padding' to "max_length," combined with "truncation," will produce a constant batch length
    ## padding: "max_length"

[trainer_callbacks]
    -- include 'project.callbacks'

## Dyamic args are added to the "forgather" CLI interface
[dynamic_args]
    == super()
    max_steps:
        names: "--max-steps"
        help: "Set maximum training steps"
    dataset_config:
        names: "--dataset-config"
        help: "The name of the dataset configuration to use"
    dataset_proj:
        names: "--dataset-proj"
        help: "Path to dataset project to use"

#-------------------- project.trainer_config --------------------
## Select one-of:
## trainers/( trainer.yaml | accel_trainer.yaml | hf_trainer.yaml | pipeline_trainer.yaml )
-- extends 'trainers/trainer.yaml'

[trainer_args]
    == super()
    # Project overrides
    seed: 42
    per_device_train_batch_size: 64
    per_device_eval_batch_size: 128
    logging_steps: 100
    eval_steps: 1000
    num_train_epochs: 2
    dataloader_num_workers: 1
    max_grad_norm: 1.0               # Gradient norm clipping threshold

    ## Stop early for quick test.
    max_steps: {{ max_steps | default(-1) }}
    
    # Checkpoint Save Settings
    save_strategy: "{{ save_strategy | default('steps') }}"           # Set to "steps" to save every "save_steps"; set to "no" to disable
    save_steps: 10000                # Checkpoint every N steps
    save_safetensors: True           # Safe tensors or PyTorch pickle?
    save_total_limit: 3              # Keep at most N checkpoints
    
    # Checkpoint Load Settings
    resume_from_checkpoint: False    # Load latest checkpoint
    
    # Only used if lr_scheduler is null
    lr_scheduler_type: "cosine"
    warmup_steps: 500
    
    # Only used if optimizer is null
    learning_rate: 1.0e-3
    
    # Torch Compile Settings
    torch_compile: False
    torch_compile_backend: "inductor"
    torch_compile_dynamic: True
    torch_compile_mode: "default"
    torch_compile_full_graph: False
    activation_memory_budget: 0.6

    float32_matmul_precision: "high" # Improves 32-bit MM performance on Ampere and later GPUs
    # Set SDPA Kernel backend(s)
    # https://docs.pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html#torch.nn.attention.sdpa_kernel
    sdpa_backend: [ "math", "flash", "efficient", "cudnn" ]
    sdpa_set_priority: False # If list, interpret as priority order

    ## Save memory by combining optimizer with backward step -- best combined with activation offloading
    fuse_optim_with_backward: False
    enable_activation_offloading: False

[model_preprocessor]
# Convert model to bfloat16 format
model_preprocessor: &model_preprocessor !partial:forgather.ml.construct:module_to_dtype [ *model, "bfloat16" ]

#-------------------- project.callbacks --------------------
-- extends 'callbacks/loggers.yaml'

[callback_list]
    == super()
    peak_memory: !singleton:forgather.ml.trainer.callbacks:PeakMemory
        do_log: {{ log_peak_memory | default(False) }}
    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback []
