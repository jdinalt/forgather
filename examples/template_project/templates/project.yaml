## Project level definitions and overrides go here.
-- extends "types/training_script/causal_lm/causal_lm.yaml"

-- block resource_directories
    == super()
    ## Directory in which pre-trained models are located.
    ##-- set ns.models_dir = joinpath(user_home_dir(), 'ai_assets', 'models')
    
    ## Directory in which local datasets are stored
    ##-- set ns.datasets_dir = joinpath(user_home_dir(), 'ai_assets', 'datasets')
<< endblock resource_directories


## Set Project level defaults
-- block config_metadata
    == super()
    -- set ns.config_name = "Project Name"
    -- set ns.config_description = "An example configuration"
    
    ## Initialize a new model from scratch
    -- set ns.create_new_model = True

    ## Save model, when training is complete
    -- set ns.save_model = True

    ## Required to load a custom model
    -- set ns.trust_remote_code = True
-- endblock config_metadata


-- block datasets_definition
    -- include 'project.dataset'
-- endblock datasets_definition


-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


-- block construct_new_model
    -- include 'project.model_config'
-- endblock construct_new_model


-- block trainer_callbacks
    -- include 'project.callbacks'
<< endblock trainer_callbacks


-- block lr_scheduler
    ##-- include 'lr_schedulers/cosine_annealing_with_warmup.yaml'
# https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 5000
    cooldown_steps: 50000
    constant_lr: 1.0e-4
<< endblock lr_scheduler


-- block optimizer
optimizer: &optimizer !lambda:torch:optim.AdamW
    lr: 1.0e-3
<< endblock optimizer


-- block datacollator
    == super()
    # Project Overrides
    ## Limit maximum sequence length 512 tokens, at the data-collator level.
    ## This can be useful for avoiding OOM errors, but can also be configured 
    ## by overriding the dataset's arguments.
    truncation: True
    max_length: 512
-- endblock datacollator


#-------------------- project.trainer_config --------------------
## Select one-of:
## trainers/( trainer.yaml | accel_trainer.yaml | hf_trainer.yaml | pipeline_trainer.yaml )
-- extends 'trainers/trainer.yaml'


-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
<< endblock trainer_meta_config


-- block trainer_args
    == super()
    # Project Overrides
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 64
    logging_steps: 100
    eval_steps: 500
    warmup_steps: 500
    ## Stop early for quick test.
    ## max_steps: 1000
    
    # Only used if lr_scheduler is null
    lr_scheduler_type: "cosine"

    # Only used if optimizer is null
    learning_rate: 1.0e-3
    
<< endblock trainer_args

#-------------------- project.model_config --------------------
-- extends 'models/example_model_config.yaml'


#-------------------- project.callbacks --------------------
-- extends 'callbacks/loggers.yaml'


#-------------------- project.dataset --------------------
-- extends 'datasets/tinystories/tinystories_abridged.yaml'

-- block tokenize_args
## See: https://huggingface.co/docs/transformers/main_classes/tokenizer
    == super()
    # project overrides
    max_length: 512
<< endblock tokenize_args
