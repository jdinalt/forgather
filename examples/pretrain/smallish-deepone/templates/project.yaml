-- extends "training_script/causal_lm/causal_lm.yaml"

[config_metadata]
    == super()
    ## Overrides
    -- set ns.config_name = "Pretrain Deepone"
    -- set ns.config_description = "Pretrain a smallish Deepone model"
    -- set ns.model_name = "default"

    ## The dataset sub-project to use
    -- set ns.dataset_proj = abspath(joinpath(ns.forgather_dir, 'examples/datasets/HuggingFaceTB/'))

    ## The configuration in the dataset sub-project
    ## Note that this config assumes packed-sequences
    -- set ns.dataset_config = "smollm-corpus/interleaved-packed.yaml"

    ## Set predefined model project to import
    -- set ns.model_project_dir = abspath(joinpath(ns.forgather_dir, "examples/models/deepone/"))
    -- set ns.model_project_config = "117M.yaml"

[datasets_preprocessor_args]
.define: &datasets_preprocessor_args !dict
    truncation: True
    max_length: 4096

[datasets_definition]
.define: &dataset_dict !call:forgather:from_project
    project_dir: "{{ ns.dataset_proj }}"
    config_template: "{{ ns.dataset_config }}"
    targets: [  "train_dataset", "eval_dataset" ]
    preprocess_args: *datasets_preprocessor_args
    tokenizer: *tokenizer

train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]
eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]

[construct_new_model]
.define: &model_dict !call:forgather:from_project
    project_dir: "{{ model_project | default(ns.model_project_dir) }}"
    config_template: "{{ model_config | default(ns.model_project_config) }}"
    targets: [ "pretrained_tokenizer", "model" ] 
    pp_kwargs:
        output_dir: "{{ ns.output_dir }}"
    model_constructor_args: *model_constructor_args

tokenizer: &tokenizer !call:getitem [ *model_dict, 'pretrained_tokenizer' ]
model: &model !call:getitem [ *model_dict, 'model' ]

[datacollator]
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    packed_sequences: True
    return_tensors: pt

[optimizer]
optimizer: &optimizer !partial:torch:optim.AdamW
    lr: 2.0e-4

[lr_scheduler]
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 50
    cooldown_steps: 0 # Use constant learning-rate, after warmup

[trainer_definition]
    -- include 'project.trainer_config'

[trainer_callbacks]
    -- include 'project.logger_config'

[dynamic_args]
    == super()
    # **Pretrain**
    model_project:
        names: "--model-project"
        type: path
        help: "Path to model project for model initialization"
    model_config:
        names: "--model-config"
        help: "Model project configuration for model init"
    model_init:
        names: "--init-model"
        action: "store_true"
        help: "Initialize model weights"
    no_restore_dataset_state:
        names: "--no-restore-dataset-state"
        action: "store_true"
        help: "Don't restore dataset state from checkpoint"

#-------------------- project.trainer_config --------------------
-- extends 'trainers/trainer.yaml'

[trainer_args]
    == super()
    # **Pretrain**
    seed: 42

    eval_strategy: "steps"
    max_eval_steps: -1
    eval_steps: 50
    eval_delay: 0

    save_strategy: "{{ save_strategy | default('steps') }}"
    save_safetensors: False
    save_total_limit: 3
    save_steps: 200

    logging_steps: 10

    # epoch_train_steps: 100000
    num_train_epochs: 1

    resume_from_checkpoint: {{ not (init_model | default(False)) }}
    # Useful when changing datasets
    restore_dataset_state: {{ not (no_restore_dataset_state | default(False)) }}

    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4

    dataloader_num_workers: 1
    dataloader_drop_last: True
    dataloader_persistent_workers: True
    dataloader_prefetch_factor: 32

    float32_matmul_precision: "high"
    default_dtype: "bfloat16"

    torch_compile: True
    torch_compile_dynamic: True

    gradient_accumulation_steps: 2
    max_grad_norm: 5.0

[trainer_constructor]
    == super()
    fused_loss_factory: !partial:forgather.ml.loss:LinearCrossEntropyLoss

#-------------------- project.logger_config --------------------
-- extends 'callbacks/loggers.yaml'

[callback_dependencies]
    == super()

    [generation_config]
generation_config: &generation_config !dict:@generation_config
    do_sample: True
    temperature: 0.7
    repetition_penalty: 1.15

    [text_gen_callback_args]
text_gen_callback_args: &text_gen_callback_args
    summary_writer: *summary_writer
    prompts: {{ abspath(joinpath(ns.forgather_dir, "prompts/short_stories.yaml")) }}
    generation_config: *generation_config
    max_new_tokens: 120
    generation_steps: 2000

[callback_list]
    == super()
    text_gen_callback: !singleton:forgather.ml.trainer.callbacks:TextgenCallback
        <<: *text_gen_callback_args
    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback
