-- extends "training_script/causal_lm/causal_lm.yaml"

[config_metadata]
    == super()
    ## Overrides
    -- set ns.config_name = "Pretrain Deepone"
    -- set ns.config_description = "Pretrain a smallish Deepone model"
    -- set ns.model_name = "deepone"

    ## The dataset sub-project to use
    -- set ns.dataset_proj = abspath(joinpath(ns.forgather_dir, 'examples/datasets/HuggingFaceTB/'))

    ## The configuration in the dataset sub-project
    ## Note that this config assumes packed-sequences
    -- set ns.dataset_config = "smollm-corpus/interleaved.yaml"

    ## Set predefined model project to import
    -- set ns.model_project_dir = abspath(joinpath(ns.forgather_dir, "examples/models/deepone/"))
    -- set ns.model_project_config = "117M.yaml"

[datasets_preprocessor_args]
.define: &datasets_preprocessor_args !dict
    truncation: True
    max_length: 4096

[datasets_definition]
.define: &dataset_dict !call:forgather:from_project
    project_dir: "{{ ns.dataset_proj }}"
    config_template: "{{ ns.dataset_config }}"
    targets: [  "train_dataset", "eval_dataset" ]
    preprocess_args: *datasets_preprocessor_args
    tokenizer: *tokenizer

train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]
eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]

[construct_new_model]
.define: &model_dict !call:forgather:from_project
    project_dir: "{{ ns.model_project_dir }}"
    config_template: "{{ ns.model_project_config }}"
    targets: [ "pretrained_tokenizer", "model" ] 
    pp_kwargs:
        output_dir: "{{ ns.output_dir }}"
    model_constructor_args: *model_constructor_args

tokenizer: &tokenizer !call:getitem [ *model_dict, 'pretrained_tokenizer' ]
model: &model !call:getitem [ *model_dict, 'model' ]

[datacollator]
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    packed_sequences: True
    return_tensors: pt

[optimizer]
optimizer: &optimizer !partial:torch:optim.AdamW
    lr: 2.0e-4

[lr_scheduler]
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 50
    cooldown_steps: 0 # Use constant learning-rate, after warmup

[trainer_definition]
    -- include 'project.trainer_config'

[trainer_callbacks]
    -- include 'project.logger_config'

#-------------------- project.trainer_config --------------------
-- extends 'trainers/trainer.yaml'

[trainer_args]
    == super()
    # **Project**
    eval_strategy: "steps"
    save_strategy: "{{ save_strategy | default('steps') }}"
    save_steps: 50000
    save_safetensors: False
    seed: 42
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 8
    logging_steps: 50
    eval_steps: 500
    num_train_epochs: 1
    dataloader_num_workers: 1
    float32_matmul_precision: "high"
    default_dtype: "bfloat16"
    torch_compile: True
    torch_compile_dynamic: True

    resume_from_checkpoint: True
    restore_dataset_state: False

[trainer_constructor]
    == super()
    fused_loss_factory: !partial:forgather.ml.loss:LinearCrossEntropyLoss

#-------------------- project.logger_config --------------------
-- extends 'callbacks/loggers.yaml'

[callback_dependencies]
    == super()

    [generation_config]
generation_config: &generation_config !dict:@generation_config
    do_sample: True
    temperature: 0.9
    repetition_penalty: 1.10

    [text_gen_callback_args]
text_gen_callback_args: &text_gen_callback_args
    summary_writer: *summary_writer
    prompts: {{ abspath(joinpath(ns.forgather_dir, "prompts/short_stories.yaml")) }}
    generation_config: *generation_config
    max_new_tokens: 120
    generation_steps: 2000

[callback_list]
    == super()
    text_gen_callback: !singleton:forgather.ml.trainer.callbacks:TextgenCallback
        <<: *text_gen_callback_args
    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback
