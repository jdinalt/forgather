-- extends "training_script/causal_lm/causal_lm.yaml"

[config_metadata]
    == super()
    ## Overrides
    -- set ns.config_name = "Pretrain Deepone"
    -- set ns.config_description = "Pretrain a smallish Deepone model"
    -- set ns.model_name = "default"

    ## The dataset sub-project to use
    -- set ns.dataset_proj = abspath(joinpath(ns.forgather_dir, 'examples/datasets/HuggingFaceTB/'))

    ## The configuration in the dataset sub-project
    ## Note that this config assumes packed-sequences
    -- set ns.dataset_config = "smollm-corpus/interleaved-packed.yaml"

    ## Set predefined model project to import
    -- set ns.model_project_dir = abspath(joinpath(ns.forgather_dir, "examples/models/deepone/"))
    -- set ns.model_project_config = "117M.yaml"

    -- set ns.per_device_train_batch_size = 4
    -- set ns.base_learning_rate = 1.0e-4

    -- set ns.world_size = (getenv("WORLD_SIZE", "1") | int)
    -- set ns.effective_batch_size = ns.per_device_train_batch_size * ns.world_size

    # Learning Rate, scaled by sqrt(ns.effective_batch_size)
    -- set ns.scaled_lr = ns.base_learning_rate * (ns.effective_batch_size ** 0.5)

[datasets_preprocessor_args]
.define: &tokenizer_args !dict
    truncation: True
    max_length: 4096

[datasets_definition]
   [dataset_project_pp_args]
.define: &dataset_project_pp_args !dict

    [dataset_project]
.define: &dataset_dict !call:forgather:from_project
    project_dir: "{{ ns.dataset_proj }}"
    config_template: "{{ ns.dataset_config }}"
    targets: [ "train_dataset", "eval_dataset" ]
    # These are passed to the project preprocessor
    pp_kwargs: *dataset_project_pp_args
    # These are injected as variables as runtime
    preprocess_args: *tokenizer_args
    tokenizer: *tokenizer
    #shard_dataset: True

    [dataset_splits]
train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]
eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]

[construct_new_model]
    [model_project_pp_args]
.define: &model_project_pp_args
    output_dir: "{{ ns.output_dir }}"

    [model_project]
.define: &model_dict !call:forgather:from_project
    project_dir: "{{ ns.model_project_dir }}"
    config_template: "{{ ns.model_project_config }}"
    targets: [ "pretrained_tokenizer", "model" ] 
    pp_kwargs: *model_project_pp_args
    model_constructor_args: *model_constructor_args

    [model_assets]
tokenizer: &tokenizer !call:getitem [ *model_dict, 'pretrained_tokenizer' ]
model: &model !call:getitem [ *model_dict, 'model' ]

[datacollator]
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    packed_sequences: True
    return_tensors: pt

[optimizer]
optimizer: &optimizer !partial:torch:optim.AdamW
    # Scale LR by sqrt(WORLD_SIZE)
    lr: {{ ns.scaled_lr | toyaml }}

[lr_scheduler]
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 50
    cooldown_steps: 0 # Use constant learning-rate, after warmup

[trainer_definition]
    -- include 'project.trainer_config'

[trainer_callbacks]
    -- include 'project.logger_config'

[dynamic_args]
    == super()
    # **Pretrain**
    model_project:
        names: "--model-project"
        type: path
        help: "Path to model project for model initialization"
    model_config:
        names: "--model-config"
        help: "Model project configuration for model init"
    model_init:
        names: "--init-model"
        action: "store_true"
        help: "Initialize model weights"
    no_restore_dataset_state:
        names: "--no-restore-dataset-state"
        action: "store_true"
        help: "Don't restore dataset state from checkpoint"

#-------------------- project.trainer_config --------------------
-- extends 'trainers/ddp_trainer.yaml'

[trainer_meta_config]
    == super()
    -- set ns.nproc_per_node = "gpu"

[trainer_args]
    == super()
    # **Pretrain**
    seed: 42

    eval_strategy: "steps"
    max_eval_steps: -1
    eval_steps: {{ 400 // ns.per_device_train_batch_size }}
    eval_delay: 0

    save_strategy: "{{ save_strategy | default('steps') }}"
    save_safetensors: False
    save_total_limit: 3
    save_steps: {{ 800 // ns.per_device_train_batch_size }}

    logging_steps: {{ 50 // ns.per_device_train_batch_size }}

    num_train_epochs: 1

    resume_from_checkpoint: {{ not (init_model | default(False)) }}
    # Useful when changing datasets
    restore_dataset_state: {{ not (no_restore_dataset_state | default(False)) }}

    per_device_train_batch_size: {{ ns.per_device_train_batch_size }}
    per_device_eval_batch_size: {{ ns.per_device_train_batch_size * 2 }}

    dataloader_num_workers: 1
    dataloader_drop_last: True
    dataloader_persistent_workers: False
    dataloader_prefetch_factor: 32

    float32_matmul_precision: "high"
    default_dtype: "bfloat16"

    torch_compile: True
    torch_compile_dynamic: True

    gradient_accumulation_steps: 1
    max_grad_norm: 5.0

    # We are sharding the dataset. No need to dispatch batches.
    #dispatch_batches: False

[trainer_constructor]
    == super()
    #fused_loss_factory: !partial:forgather.ml.loss:LinearCrossEntropyLoss

#-------------------- project.logger_config --------------------
-- extends 'callbacks/loggers.yaml'

[callback_dependencies]
    == super()

    [generation_config]
generation_config: &generation_config !dict:@generation_config
    do_sample: True
    temperature: 0.7
    repetition_penalty: 1.15

    [text_gen_callback_args]
text_gen_callback_args: &text_gen_callback_args
    summary_writer: *summary_writer
    prompts: {{ abspath(joinpath(ns.forgather_dir, "prompts/short_stories.yaml")) }}
    generation_config: *generation_config
    max_new_tokens: 120
    generation_steps: 2000

[callback_list]
    == super()
    text_gen_callback: !singleton:forgather.ml.trainer.callbacks:TextgenCallback
        <<: *text_gen_callback_args
    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback
