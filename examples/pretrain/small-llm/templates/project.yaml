-- extends "training_script/causal_lm/causal_lm.yaml"

[config_metadata]
    == super()
    ## Overrides
    -- set ns.config_name = "Pretrain Small-LLM"
    -- set ns.config_description = "Pretrain a small LLM"
    -- set ns.model_name = "default"

    ## The dataset sub-project to use
    -- set ns.dataset_proj = abspath(joinpath(ns.forgather_dir, 'examples/datasets/HuggingFaceTB/'))

    ## The configuration in the dataset sub-project
    -- set ns.dataset_config = "smollm-corpus/interleaved-packed.yaml"

    ## Set predefined model project to import
    -- set ns.model_project_dir = abspath(model_project | default((joinpath(project_dir, "custom_deepone"))))
    -- set ns.model_project_config = model_config | default("custom_deepone.yaml")

    ## Training settings
    -- set ns.per_device_train_batch_size = batch_size | default(4)
    -- set ns.gradient_accumulation_steps = 1
    -- set ns.base_learning_rate = learning_rate | default(2.0e-05)
    -- set ns.max_length = max_length | default(4096)
    
    ## Shard the dataset, rather than dispatching from rank0
    -- set ns.dispatch_batches = False

    ## This scales the number of steps per log/eval/save cycle
    -- set ns.step_cadence = step_cadence | default(1.0)

[globals]
    ## Computed config. values
    == super()
    -- set ns.world_size = (getenv("WORLD_SIZE", "1") | int)
    -- set ns.effective_batch_size = ns.gradient_accumulation_steps * ns.per_device_train_batch_size * ns.world_size
    ## Learning Rate, scaled by sqrt(ns.effective_batch_size)
    -- set ns.scaled_lr = ns.base_learning_rate * (ns.effective_batch_size ** 0.5)

[variable_listing]
    == super()
# Pretrain
# ns.per_device_train_batch_size: {{ ns.per_device_train_batch_size }}
# ns.gradient_accumulation_steps: {{ ns.gradient_accumulation_steps }}
# ns.world_size: {{ ns.world_size }}
# ns.effective_batch_size: {{ ns.effective_batch_size }}
# ns.base_learning_rate: {{ ns.base_learning_rate }}
# ns.step_cadence: {{ ns.step_cadence }}
# ns.scaled_lr: {{ ns.scaled_lr }}

[datasets_preprocessor_args]
.define: &tokenizer_args !dict
    truncation: True
    max_length: {{ ns.max_length }}

[datasets_definition]
   [dataset_project_pp_args]
.define: &dataset_project_pp_args !dict

    [dataset_project]
.define: &dataset_dict !call:forgather:from_project
    project_dir: "{{ ns.dataset_proj }}"
    config_template: "{{ ns.dataset_config }}"
    targets: [ "train_dataset", "eval_dataset" ]
    # These are passed to the project preprocessor
    pp_kwargs: *dataset_project_pp_args
    # These are injected as variables as runtime
    preprocess_args: *tokenizer_args
    tokenizer: *tokenizer
    shard_dataset: {{ ns.dispatch_batches == False }}

    [dataset_splits]
train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]
eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]

[model_constructor_args]
.define: &model_constructor_args
    attn_implementation: "{{ attn_implementation | default('sdpa') }}"

[construct_new_model]
    [model_project_pp_args]
.define: &model_project_pp_args
    output_dir: "{{ ns.output_dir }}"

    [model_project]
.define: &model_dict !call:forgather:from_project
    project_dir: "{{ ns.model_project_dir }}"
    config_template: "{{ ns.model_project_config }}"
    targets: [ "pretrained_tokenizer", "model" ] 
    pp_kwargs: *model_project_pp_args
    model_constructor_args: *model_constructor_args

    [model_assets]
tokenizer: &tokenizer !call:getitem [ *model_dict, 'pretrained_tokenizer' ]
model: &model !call:getitem [ *model_dict, 'model' ]

[datacollator]
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    # Pad to max-length. This maintains a constant input shape
    # With packed examples, there should be very little padding and this can speed things up a little.
    padding: "max_length"
    max_length: {{ ns.max_length }}
    packed_sequences: True
    return_tensors: pt

[optimizer]
optimizer: &optimizer !partial:torch:optim.AdamW
    # Scale LR by sqrt(WORLD_SIZE)
    lr: {{ ns.scaled_lr | toyaml }}

[lr_scheduler]
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 50
    cooldown_steps: 0 # Use constant learning-rate, after warmup

[trainer_definition]
    -- include 'project.trainer_config'

[trainer_callbacks]
    -- include 'project.logger_config'

[dynamic_args]
    == super()
    # **Pretrain**
    batch_size:
        names: "--batch-size"
        type: int
        default: 4
        help: "Set the per-device-training batch size"
    model_project:
        names: "--model-project"
        type: path
        help: "Path to model project for model initialization"
    model_config:
        names: "--model-config"
        help: "Model project configuration for model init"
    model_init:
        names: "--init-model"
        action: "store_true"
        help: "Initialize model weights"
    no_restore_dataset_state:
        names: "--no-restore-dataset-state"
        action: "store_true"
        help: "Don't restore dataset state from checkpoint"
    no_compile:
        names: "--no-compile"
        action: "store_true"
        help: "Disable Torch compile for faster startup"
    learning_rate:
        names: [ "--learning-rate", "--lr" ]
        type: float
        default: !!float 1.0e-4
        help: "Set the base learning rate"
    max_length:
        names: [ "--max_length" ]
        type: int
        default: 4096
        help: "Set maximum sequence length"
    step_cadence:
        names: "--step-cadence"
        type: float
        default: 1.0
        help: "Scale size of train/eval/save steps by this factor"

#-------------------- project.trainer_config --------------------
-- extends 'trainers/ddp_trainer.yaml'

[trainer_meta_config]
    == super()
    -- set ns.nproc_per_node = "gpu"

[trainer_args]
    == super()
    # --- Pretrain Config ---

    # **Cadence**
    logging_steps: {{ (100 * ns.step_cadence / ns.per_device_train_batch_size) | int }}
    eval_steps: {{ (1000 * ns.step_cadence / ns.per_device_train_batch_size) | int }}
    save_steps: {{ (20000 * ns.step_cadence / ns.per_device_train_batch_size) | int }}

    # **Dataloader Config**
    num_train_epochs: 1
    per_device_train_batch_size: {{ ns.per_device_train_batch_size }}
    per_device_eval_batch_size: {{ ns.per_device_train_batch_size * 2 }}
    dataloader_num_workers: 1
    dataloader_drop_last: True
    dataloader_persistent_workers: False
    dataloader_prefetch_factor: 32
    # When True, only rank0 reads the datasets -- and dispatches batches to other ranks.
    dispatch_batches: False

    # **Eval Config**
    eval_strategy: "steps"
    max_eval_steps: -1 # This limits the number of examples in the eval. step.

    # **Checkpoing saving**
    save_strategy: {{ save_strategy | toyaml('steps') }}
    save_safetensors: False
    save_total_limit: 3
    preserve_best_model: True
    best_model_metric: "loss"
    preserve_n_best: 2
    eval_on_save: True

    # **Checkpoint loading**
    resume_from_checkpoint: {{ not (init_model | default(False)) }}

    # **Torch Compile Config**
    torch_compile: {{ not (no_compile | default(False)) }}
    torch_compile_dynamic: True

    # **Misc.**
    seed: 42
    float32_matmul_precision: "high"
    default_dtype: "bfloat16"
    gradient_accumulation_steps: {{ ns.gradient_accumulation_steps }}
    max_grad_norm: 1.0

[trainer_constructor]
    == super()
    fused_loss_factory: !partial:forgather.ml.loss:LinearCrossEntropyLoss

#-------------------- project.logger_config --------------------
-- extends 'callbacks/loggers.yaml'

[callback_dependencies]
    == super()

    [generation_config]
generation_config: &generation_config !dict:@generation_config
    do_sample: True
    temperature: 0.7
    repetition_penalty: 1.15

    [text_gen_callback_args]
# Periodically generate text at eval. step for subjective model evaluation.
text_gen_callback_args: &text_gen_callback_args
    summary_writer: *summary_writer
    prompts: {{ abspath(joinpath(ns.forgather_dir, "prompts/short_stories.yaml")) }}
    generation_config: *generation_config
    max_new_tokens: 120
    generation_steps: 10000

[callback_list]
    == super()
    #text_gen_callback: !singleton:forgather.ml.trainer.callbacks:TextgenCallback
    #    <<: *text_gen_callback_args
    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback
    peak_memory: !singleton:forgather.ml.trainer.callbacks:PeakMemory
    divergence_detector: !singleton:forgather.ml.trainer.callbacks:DualTimeScaleDivergenceDetector
        short_alpha: 0.1        # Fast EMA (~10 step window)
        long_alpha: 0.01        # Slow EMA (~100 step window)
        threshold: 1.0          # Stop if short - long >= 1.0
        action: "abort"
