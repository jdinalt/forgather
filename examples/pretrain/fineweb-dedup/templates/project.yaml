-- extends "training_script/causal_lm/causal_lm.yaml"

[config_metadata]
    == super()
    ## Overrides
    -- set ns.config_name = "Pretrain "
    -- set ns.config_description = "Pretrain on FineWeb-Edu-Dedup"
    -- set ns.model_name = "default"

    ## The dataset sub-project to use
    -- set ns.dataset_proj = abspath(joinpath(ns.forgather_dir, 'examples/datasets/HuggingFaceTB/'))

    ## The configuration in the dataset sub-project
    ## Note that this config assumes packed-sequences
    -- set ns.dataset_config = "smollm-corpus/fineweb-edu-packed.yaml"

    ## Set predefined model project to import
    -- set ns.model_project_dir = abspath(joinpath(ns.forgather_dir, "examples/models/llama/"))
    -- set ns.model_project_config = "117M.yaml"

[datasets_preprocessor_args]
# Overrides for forgather.ml.datasets:block_tokenize_fn
.define: &datasets_preprocessor_args !dict
    max_length: 4096

[datasets_definition]
.define: &dataset_dict !call:forgather:from_project
    project_dir: "{{ ns.dataset_proj }}"
    config_template: "{{ ns.dataset_config }}"
    targets: [  "train_dataset", "eval_dataset" ]
    pp_kwargs:
        # For quick test
        abridged: {{ abridged | default(False) }}
    preprocess_args: *datasets_preprocessor_args
    tokenizer: *tokenizer

train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]
eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]

[construct_new_model]
.define: &model_dict !call:forgather:from_project
    project_dir: "{{ ns.model_project_dir }}"
    config_template: "{{ ns.model_project_config }}"
    targets: [ "pretrained_tokenizer", "model" ] 
    pp_kwargs:
        output_dir: "{{ ns.output_dir }}"
    model_constructor_args: *model_constructor_args

tokenizer: &tokenizer !call:getitem [ *model_dict, 'pretrained_tokenizer' ]
model: &model !call:getitem [ *model_dict, 'model' ]

[datacollator]
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    packed_sequences: True
    return_tensors: pt

[optimizer]
optimizer: &optimizer !partial:torch:optim.AdamW
    lr: 2.0e-4

[lr_scheduler]
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 50
    cooldown_steps: 0 # Use constant learning-rate, after warmup

[trainer_definition]
    -- include 'project.trainer_config'

[trainer_callbacks]
    -- include 'project.logger_config'

[dynamic_args]
    == super()
    abridged:
        names: "--abridged"
        action: "store_true"
        help: "'Abridged'  length for faster loading"

#-------------------- project.trainer_config --------------------
-- extends 'trainers/trainer.yaml'

[trainer_args]
    == super()
    # **Project**
    eval_strategy: "steps"
    save_strategy: "{{ save_strategy | default('steps') }}"
    save_steps: 1000
    save_safetensors: False
    seed: 42
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 8
    logging_steps: 10
    eval_steps: 100
    num_train_epochs: 1
    dataloader_num_workers: 1
    float32_matmul_precision: "high"
    default_dtype: "bfloat16"

#-------------------- project.logger_config --------------------
-- extends 'callbacks/loggers.yaml'

[callback_list]
    -- include 'prompts/short_stories.yaml'

    == super()
-- if attn_implementation | default("") != "flex_attention"
    ## This adds a text-generationn sample every 'generation_steps'
    text_gen_callback: !singleton:forgather.ml.trainer.callbacks:TextgenCallback
        summary_writer: *summary_writer
        prompts: *testprompts
        generation_config: *generation_config
        max_new_tokens: 40
        generation_steps: 500
-- endif
    # Allow remote control of the training process
    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback
