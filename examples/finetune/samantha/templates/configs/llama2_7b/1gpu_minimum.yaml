-- extends 'samantha.yaml'

[config_metadata]
    == super()
    -- set ns.config_name = "Samantha Llama7B 16 GB"
    -- set ns.config_description = "Train on 16 GB GPU"
    -- set ns.log_name = "16gb"

[trainer_args]
    == super()

    # **config**
    logging_steps: 50
    eval_steps: 250
    max_eval_steps: 50
    save_steps: 1000
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 2
    gradient_checkpointing: True
    fuse_optim_with_backward: True
    enable_activation_offloading: True

[datacollator]
    == super()
    # Demonstrate that we /can/ train with this sequence length! But not fast...
    max_length: 512
    ## Set to always use max_length. This can reduce peak memory slightly.
    padding: "max_length"

[optimizer]
    == super()
    lr: 1.41e-6 # Rescale LR for smaller batch-size
