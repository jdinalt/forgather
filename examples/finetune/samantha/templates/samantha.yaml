-- extends 'projects/finetune.yaml'

## Default Assumptions:
## The
-- block resource_directories
    == super()
    ## Change this to point to where your models are stored
    -- set ns.models_dir = joinpath(user_home_dir(), 'ai_assets', 'models')
<< endblock resource_directories

-- block config_metadata
    == super()
    # Set ot name of model to train
    -- set ns.model_name = "llama-2-7b-fg"
    -- set ns.debug_memory_detials = False
    -- set ns.log_peak_memmory = True
    -- set ns.log_memory_to_tb = True
    -- set ns.dataset_proj = joinpath(ns.forgather_dir, 'examples', 'datasets', 'QuixiAI')
    -- set ns.dataset_config = "samantha.yaml"
    -- set ns.samantha_default_chat_template = joinpath(ns.forgather_dir, 'chat_templates', 'chatml.jinja')
    -- set ns.samantha_chat_template = chat_template | default(ns.samantha_default_chat_template)
-- endblock config_metadata


-- block trainer_definition
    -- include 'samantha.trainer_config'
-- endblock trainer_definition


#-------------------- samantha.trainer_config --------------------
-- extends 'finetune.trainer_config'

-- block trainer_args
    == super()
    # project overrides
    sdpa_backend: [ "flash", "efficient" ]
    sdpa_set_priority: True
    ## max_steps: 500
-- endblock trainer_args
