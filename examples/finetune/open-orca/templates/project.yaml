-- extends 'projects/base_finetune_proj.yaml'

[config_metadata]
    == super()
    -- set ns.config_name = "Open-Orca Finetune"
    -- set ns.config_description = "Train a model on the Open-Orca dataset"
    -- set ns.log_name = "default"
    -- set ns.default_dataset_proj = joinpath(ns.forgather_dir, 'examples', 'datasets', 'Open-Orca')
    -- set ns.default_dataset_config = "openorca.yaml"

[optimizer]
optimizer: &optimizer !partial:forgather.ml.optim.adafactor:Adafactor
    lr: 4.0e-6
    weight_decay: 0.001

[lr_scheduler]
lr_scheduler: &lr_scheduler !partial:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 50
    cooldown_steps: 0 # Use constant learning-rate, after warmup

#-------------------- project.trainer --------------------
-- extends "finetune.trainer"

[trainer_args]
    == super()
    # **open-orca**
    logging_steps: 10
    eval_steps: 50
    max_eval_steps: 50
    save_steps: 1000
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16
