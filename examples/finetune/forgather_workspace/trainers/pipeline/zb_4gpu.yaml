-- extends 'trainers/pipeline/base_pipeline_trainer.yaml'


-- block trainer_meta_config
    == super()
    ## Parallelism settings
    -- set ns.nproc_per_node = 4
    -- set ns.nnodes = 1
    -- set ns.pipeline_microbatches = 8

    ## Scheduler settings
    -- set ns.pipeline_is_multistage = True
    -- set ns.pipeline_stages_per_rank = 2
    -- set ns.pipeline_sched_class = "torch.distributed.pipelining:ScheduleZBVZeroBubble"

    ## Batch settings
    -- set ns.per_device_train_batch_size = 8
    -- set ns.per_device_eval_batch_size = 8
    -- set ns.pipeline_unified_model = True

    ## Model parameters
    -- set ns.pipeline_layers = 32
    -- set ns.split_layer_prefix = "causal_lm.layer_stack.layers."
-- endblock trainer_meta_config


-- block trainer_args
    == super()
    # ZBVZeroBubble uses a different stage indexing layout than the others.
    pp_stage_type: "v"
-- endblock trainer_args 