-- extends 'trainers/base_finetune_trainer.yaml'

-- block trainer_meta_config
    == super()
    ## Parallelism settings
    -- set ns.nproc_per_node = 2
    -- set ns.nnodes = 1
    -- set ns.pipeline_microbatches = 8

    ## Scheduler settings
    -- set ns.pipeline_is_multistage = False
    -- set ns.pipeline_stages_per_rank = 1
    -- set ns.pipeline_sched_class = "torch.distributed.pipelining:Schedule1F1B"

    ## Batch settings
    -- set ns.per_device_train_batch_size = 8
    -- set ns.per_device_eval_batch_size = 16
    -- set ns.pipeline_unified_model = False

    ## Model parameters
    -- set ns.pipeline_layers = 32
    -- set ns.split_layer_prefix = "causal_lm.layer_stack.layers."
-- endblock trainer_meta_config


-- block trainer_globals
-- if ns.pipeline_unified_model
    -- set ns.per_device_eval_batch_size = ns.per_device_train_batch_size
-- endif
    -- set ns.pipeline_segments = ns.nproc_per_node * ns.nnodes * ns.pipeline_stages_per_rank
<< endblock trainer_globals

-- block trainer_header
    == super()
# pipeline_is_multistage: {{ ns.pipeline_is_multistage }}
# nnodes: {{ ns.nnodes }}
# nproc_per_node: {{ ns.nproc_per_node }}
# pipeline_unified_model: {{ ns.pipeline_unified_model }}
# per_device_train_batch_size: {{ ns.per_device_train_batch_size }}
# per_device_eval_batch_size: {{ ns.per_device_eval_batch_size }}
# pipeline_is_multistage: {{ ns.pipeline_is_multistage }}
# pipeline_stages_per_rank: {{ ns.pipeline_stages_per_rank }}
<< endblock trainer_header


-- block split_spec
split_spec: &split_spec !dict@split_spec
    ## Define where to split the model, based upon the total number of layers and the number of segments.
    ## While not optimal, this is probalby a good starting point.
-- for i in range(1, ns.pipeline_segments):
    {{ ns.split_layer_prefix }}{{i * (ns.pipeline_layers // ns.pipeline_segments)}}: "beginning"
-- endfor
-- endblock split_spec


-- block trainer_constructor
    == super()
    # Config Overrides
    pipe_schedule_factory: !partial:{{ ns.pipeline_sched_class }}
-- endblock trainer_constructor


-- block trainer_args
    == super()
    # base pipeline overrides
    pipeline_chunks: {{ ns.pipeline_microbatches }}
    is_multistage: {{ ns.pipeline_is_multistage }}
    stages_per_rank: {{ ns.pipeline_stages_per_rank }}
    unified_model: {{ ns.pipeline_unified_model }}

    per_device_train_batch_size: {{ ns.per_device_train_batch_size }}
    per_device_eval_batch_size: {{ ns.per_device_eval_batch_size }}
-- endblock trainer_args

