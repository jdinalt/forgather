-- extends 'trainers/pipeline/base_pipeline_trainer.yaml'


-- block trainer_meta_config
    == super()
    ## Parallelism settings
    -- set ns.nproc_per_node = 4
    -- set ns.nnodes = 1
    -- set ns.pipeline_microbatches = 8

    ## Scheduler settings
    -- set ns.pipeline_is_multistage = False
    -- set ns.pipeline_stages_per_rank = 1
    -- set ns.pipeline_sched_class = "torch.distributed.pipelining:Schedule1F1B"

    ## Model parameters
    -- set ns.pipeline_layers = 32
    -- set ns.split_layer_prefix = "causal_lm.layer_stack.layers."
    -- set ns.pipeline_unified_model = False
-- endblock trainer_meta_config


-- block trainer_args
    == super()
    
    # **1f1b 4gpu**
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16
<< endblock trainer_args