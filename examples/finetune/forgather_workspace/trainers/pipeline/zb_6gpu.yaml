-- extends 'trainers/pipeline/base_pipeline_trainer.yaml'

[trainer_meta_config]
    == super()
    ## Parallelism settings
    -- set ns.nproc_per_node = 6
    -- set ns.nnodes = 1
    -- set ns.pipeline_microbatches = 12

    ## Scheduler settings
    -- set ns.pipeline_is_multistage = True
    -- set ns.pipeline_stages_per_rank = 2
    -- set ns.pipeline_sched_class = "torch.distributed.pipelining:ScheduleZBVZeroBubble"

    ## Model parameters
    -- set ns.pipeline_layers = 32
    -- set ns.split_layer_prefix = "causal_lm.layer_stack.layers."
    -- set ns.pipeline_unified_model = True

[trainer_args]
    == super()
    # **zb 6gpu**
    per_device_train_batch_size: 12
    per_device_eval_batch_size: 12

    # ZBVZeroBubble uses a different stage indexing layout than the others.
    pp_stage_type: "v"