-- extends "types/training_script/causal_lm/causal_lm.yaml"

-- block resource_directories
    == super()
    ## Change this to point to where your models are stored
    -- set ns.models_dir = models_dir | default(joinpath(user_home_dir(), 'models'))

    ## Directory in which local datasets are stored
    -- set ns.datasets_dir = datasets_dir | default(joinpath(user_home_dir(), 'datasets'))
<< endblock resource_directories


-- block config_metadata
    == super()
    -- set ns.config_name = 'Finetune'
    -- set ns.config_description = 'Experiments in finetuning'
    -- set ns.trainer_class = 'trainers/trainer.yaml'
    -- set ns.trust_remote_code = True
    -- set ns.model_name = 'default_model'
    -- set ns.log_name = 'log'
    
    ## Some models may have issues loading using our checkpoint method. Set safe_load to fallback
    ## to constructing model with AutoModelForCausalLM.from_pretrained().
    -- set ns.safe_load = safe_load | default(False)

    ## memory debugging
    -- set ns.debug_memory_detials = False
    -- set ns.log_memory_to_tb = False
-- endblock config_metadata


-- block globals
	# If not specified, default to model-dir and model-name
	-- set ns.model_id_or_path = model_id_or_path | default(joinpath(ns.models_dir, ns.model_name))

    ## If explicity specified, use 'output_dir,' otherwise assume that 'ns.model_id_or_path'
    -- set ns.output_dir = output_dir | default(ns.model_id_or_path)
    -- set ns.logging_dir = joinpath(ns.output_dir, "runs", ns.log_name + '_' + filetime())
<< endblock globals


-- block variable_listing
    == super()
# ns.safe_load: {{ ns.safe_load }}
# ns.dataset_proj: {{ ns.dataset_proj }}
# ns.dataset_config: {{ ns.dataset_config }}
# ns.debug_memory_detials: {{ ns.debug_memory_detials }}
# ns.log_memory_to_tb: {{ ns.log_memory_to_tb }}
# ns.model_id_or_path: {{ ns.model_id_or_path }}
-- endblock variable_listing


-- block datasets_preprocessor_args
tokenizer_args: &tokenizer_args !dict
    truncation: True
-- endblock datasets_preprocessor_args


-- block construct_new_model
    -- include 'models/causal_lm/from_pretrained_config.yaml'
-- endblock construct_new_model


-- block datasets_definition
    -- include 'datasets/llm_dataset_project.yaml'
-- endblock datasets_definition


## Defaults to the basic trainer implementation
## Note: This is unsuitable for multiple GPUs.
## Override 'ns.trainer_class' to change the trainer class.
-- block trainer_definition
    ## See definition below
    -- include 'finetune.trainer_config'
-- endblock trainer_definition


-- block trainer_callbacks
    ## See definition below
    -- include 'finetune.callbacks'
<< endblock trainer_callbacks


-- block datacollator
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
    truncation: True
    ## Put a reasonable limit on the sequence length
    max_length: 4096
-- endblock datacollator


-- block lr_scheduler
# See https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    # This seems sufficient to maintain stability.
    warmup_steps: 200

    # Don't both with cooldown.
    # Note: It non-zero, warm-up will proceed to the LR configured in the optimizer, rather than to
    # constant_lr, then perform cosine annealing for this many steps to the constant_lr
    cooldown_steps: 0
    
    # Set this to the last checkpoint before stopping.
    # This is an exponential decay after checkpoint_step.
    checkpoint_step: 1000

    # Hold at this LR after warmup.
    constant_lr: 5.0e-6

    # Exponential decay constant, tau
    tau: !!float 1.0e2
<< endblock lr_scheduler


## Add a default memory efficient optimizer
-- block optimizer
optimizer: &optimizer !partial:forgather.ml.optim.adafactor:Adafactor
    lr: 5.0e-6
    weight_decay: 0.001
<< endblock optimizer


-- block dynamic_args
    == super()
    train_epochs:
        names: "--train-epochs"
        type: "int"
        help: "Set the number of epochs to train for"
    log_peak_memory:
        names: "--log-peak-memory"
        action: "store_true"
        help: "Log peak GPU memory at each log step"
    dataset_config:
        names: "--dataset-config"
        type: "str"
        help: "The name of the dataset configuration to use"
    dataset_proj:
        names: "--dataset-proj"
        type: "path"
        help: "Path to dataset project to use"
    model_id_or_path:
        names: "--model-id-or-path"
        type: "path"
        help: "HF model ID or local path to model"
    output_dir:
        names: [ "--output-dir", "-o" ]
        type: "path"
        help: "Training output director. Defaults to model_id_or_path"
    safe_load:
        names: "--safe-load"
        action: "store_true"
        help: "Fallback to the more compatible HF model loading method"
    gradient_checkpointing:
        names: "--gradient-checkpointing"
        action: "store_true"
        help: "Enable gradient (activation) checkpoint, when supported"

<< endblock dynamic_args


#-------------------- finetune.trainer_config --------------------
-- extends ns.trainer_class
## Note: We use dynamic inheritance for the trainer-class
## This has a side effect of not being able to statically resolve the
## parent template, which is named in the 'ns.trainer_class' variable,
## the value of which is defined in the 'config_metadata' block, above.

## Setup some defaults for this type of project
-- block trainer_args
    == super()
    # Finetune overrides
    seed: 42
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16
    logging_steps: 10
    eval_steps: 50
    num_train_epochs: {{ train_epochs | default(1) }}
    dataloader_num_workers: 1
    max_grad_norm: 1.0
    float32_matmul_precision: "high"
    gradient_checkpointing: {{ gradient_checkpointing | default(False) }}

    # Checkpoint Save Settings
    save_strategy: "{{ save_strategy | default('steps') }}" # Set to "steps" to save every "save_steps"; set to "no" to disable
    save_steps: 1000                 # Checkpoint every N steps
    save_safetensors: False          # Safe tensors don't support shared weights
    save_total_limit: 3              # Keep at most N checkpoints
    save_on_each_node: False         # Save common files on each node
    save_optimizer_state: True       # Save optimizer state with checkpoint
    save_scheduler_state: True       # Save LR scheduler state with checkpoint
    save_rng_state: True             # Save RNG state with checkpoint

    # We load the model via the newest checkpoint
    resume_from_checkpoint: True
    restore_optimizer_state: True    # Test optimizer state restoration per rank
    restore_scheduler_state: True    # Test scheduler state restoration per rank
    restore_rng_state: True          # Test RNG state restoration per rank
    restore_dataset_state: False     # Not fully implemented yet!
    ## WARNING: If the model has any buffers with "peristent=False," construct the model
    ## on the "default" device; otherwise, there is no way to initialize these buffers from
    ## a checkpoint! OTOH, not constructing the model on "meta" is very slow...
-- if ns.safe_load
    construct_model_on: "default"    # Construct model on CPU
-- else
    construct_model_on: "meta"       # Construct empty model on meta device
-- endif
    # Enable all SDPA backends and let PyTorch choose which to use.
    sdpa_backend: [ "math", "flash", "efficient", "cudnn" ]
    sdpa_set_priority: False # If list, interpret as priority order
-- endblock trainer_args


-- block model_preprocessor
# Convert model to bfloat16 format
model_preprocessor: &model_preprocessor !partial:forgather.ml.construct:module_to_dtype [ *model, "bfloat16" ]
<< endblock model_preprocessor

#-------------------- finetune.callbacks --------------------
-- extends 'callbacks/loggers.yaml'
## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

-- block callback_dependencies
    == super()
<< endblock callback_dependencies

-- block callback_list
    == super()
    - !singleton:forgather.ml.trainer.callbacks:PeakMemory
        show_details: {{ ns.debug_memory_detials }}
        do_log: {{ log_peak_memory | default(False) }}
    -- if ns.log_memory_to_tb
        summary_writer: *summary_writer
    -- endif
<< endblock callback_list
