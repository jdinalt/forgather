-- extends "types/training_script/causal_lm/causal_lm.yaml"

-- block resource_directories
    == super()
    ## Change this to point to where your models are stored
    -- set ns.models_dir = joinpath(user_home_dir(), 'ai_assets', 'models')

    ## Directory in which local datasets are stored
    -- set ns.datasets_dir = joinpath(user_home_dir(), 'ai_assets', 'datasets')

    ## Set these to the dataset project to use
    ##-- set ns.dataset_proj = joinpath(ns.forgather_dir, 'examples', 'datasets', 'QuixiAI')
    ##-- set ns.dataset_config = "samantha.yaml"
<< endblock resource_directories


-- block config_metadata
    == super()
    -- set ns.config_name = "Finetune"
    -- set ns.config_description = "Experiments in finetuning"
    -- set ns.trainer_class = 'trainers/trainer.yaml'
    -- set ns.trust_remote_code = True
    -- set ns.create_new_model = False
    -- set ns.save_model = False
    -- set ns.log_name = "project_default"

    ## Set to name of model, in "ns.models_dir"
    ## -- set ns.model_name = "llama-2-7b-fg"
    -- set ns.train = True
    -- set ns.eval = False
    -- set ns.create_new_model = False
    -- set ns.trust_remote_code = True

    ## memory debugging
    -- set ns.debug_memory_detials = False
    -- set ns.log_peak_memmory = False
    -- set ns.log_memory_to_tb = False
-- endblock config_metadata


-- block datasets_preprocessor_args
tokenizer_args: &tokenizer_args !dict
    truncation: True
-- endblock datasets_preprocessor_args


-- block datasets_definition
    -- include 'datasets/llm_dataset_project.yaml'
-- endblock datasets_definition


## Defaults to the basic trainer implementation
## Note: This is unsuitable for multiple GPUs.
## Override 'ns.trainer_class' to change the trainer class.
-- block trainer_definition
    ## See definition below
    -- include 'finetune.trainer_config'
-- endblock trainer_definition


-- block trainer_callbacks
    ## See definition below
    -- include 'finetune.callbacks'
<< endblock trainer_callbacks


## Override, to construct model from config
-- block load_model
    -- include 'models/causal_lm/from_pretrained_config.yaml'
-- endblock load_model


-- block datacollator
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
    truncation: True
    max_length: 4096
-- endblock datacollator


-- block lr_scheduler
# See https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    # This seems sufficient to maintain stability.
    warmup_steps: 200

    # Don't both with cooldown.
    # Note: It non-zero, warm-up will proceed to the LR configured in the optimizer, rather than to
    # constant_lr, then perform cosine annealing for this many steps to the constant_lr
    cooldown_steps: 0
    
    # Start annealing after first checkpoint.
    # This is an exponential decay after checkpoint_step.
    checkpoint_step: 1000

    # Hold at this LR after warmup.
    constant_lr: 5.0e-6

    # Decay time constant
    tau: !!float 1.0e2
<< endblock lr_scheduler


## Add a default memory efficient optimizer
-- block optimizer
optimizer: &optimizer !partial:forgather.ml.optim.adafactor:Adafactor
    lr: 5.0e-6
    weight_decay: 0.001
<< endblock optimizer

#-------------------- finetune.trainer_config --------------------
-- extends ns.trainer_class
## Note: We use dynamic inheritance for the trainer-class
## This has a side effect of not being able to statically resolve the
## parent template, which is named in the 'ns.trainer_class' variable,
## the value of which is defined in the 'config_metadata' block, above.

-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
-- endblock trainer_meta_config


## Setup some defaults for this type of project
-- block trainer_args
    == super()
    # Finetune overrides
    seed: 42
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16
    logging_steps: 10
    eval_steps: 50
    num_train_epochs: 3
    dataloader_num_workers: 1
    max_grad_norm: 1.0
    float32_matmul_precision: "high"

    # Checkpoint Save Settings
    save_strategy: "steps"           # Set to "steps" to save every "save_steps"; set to "no" to disable
    save_steps: 10000                # Checkpoint every N steps
    save_safetensors: False          # Safe tensors don't support shared weights
    save_total_limit: 3              # Keep at most N checkpoints
    save_on_each_node: False         # Save common files on each node
    save_optimizer_state: True       # Save optimizer state with checkpoint
    save_scheduler_state: True       # Save LR scheduler state with checkpoint
    save_rng_state: True             # Save RNG state with checkpoint

    # We load the model via the newest checkpoint
    resume_from_checkpoint: True
    restore_optimizer_state: True    # Test optimizer state restoration per rank
    restore_scheduler_state: True    # Test scheduler state restoration per rank
    restore_rng_state: True          # Test RNG state restoration per rank
    restore_dataset_state: False     # Not fully implemented yet!
    construct_model_on: "meta"       # Construct empty model on meta device

    # Enable all SDPA backends and let PyTorch choose which to use.
    sdpa_backend: [ "math", "flash", "efficient", "cudnn" ]
    sdpa_set_priority: False # If list, interpret as priority order
-- endblock trainer_args


-- block model_preprocessor
# Convert model to bfloat16 format
model_preprocessor: &model_preprocessor !partial:forgather.ml.construct:module_to_dtype [ *model, "bfloat16" ]
<< endblock model_preprocessor


#-------------------- finetune.callbacks --------------------
-- extends 'callbacks/loggers.yaml'
## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

-- block callback_dependencies
    == super()
<< endblock callback_dependencies

-- block callback_list
    == super()
    - !singleton:forgather.ml.trainer.callbacks:PeakMemory
        show_details: {{ ns.debug_memory_detials }}
        do_log: {{ ns.log_peak_memmory }}
    -- if ns.log_memory_to_tb
        summary_writer: *summary_writer
    -- endif
<< endblock callback_list