-- extends "types/training_script/causal_lm/causal_lm.yaml"

-- block resource_directories
    == super()
    ## Change this to point to where your models are stored
    -- set ns.models_dir = models_dir | default(joinpath(user_home_dir(), 'models'))

    ## Directory in which local datasets are stored
    -- set ns.datasets_dir = datasets_dir | default(joinpath(user_home_dir(), 'datasets'))

    ## Set these to the dataset project to use
    -- set ns.dataset_proj = dataset_proj | default(joinpath(ns.forgather_dir, 'examples', 'datasets', 'QuixiAI'))
    -- set ns.dataset_config = dataset_config | default("samantha.yaml")

    ## Some models may have issues loading using our checkpoint method. Set safe_load to fallback
    ## to constructing model with AutoModelForCausalLM.from_pretrained().
    -- set ns.safe_load = safe_load | default(False)

    ## Some HF models support enabling gradient (activation) checkpointing via the 
    ## gradient_checkpointing_enable() method. If True, try to use this API to enable checkpointing.
    -- set ns.gradient_checkpointing = gradient_checkpointing | default(False)
<< endblock resource_directories


-- block config_metadata
    == super()
    -- set ns.config_name = "Finetune"
    -- set ns.config_description = "Experiments in finetuning"
    -- set ns.trainer_class = 'trainers/trainer.yaml'
    -- set ns.trust_remote_code = True
    -- set ns.create_new_model = False
    ## We use the checkpointing functionality to save.
    ## If set to True, the model weights of the original model will be overwritten.
    -- set ns.save_model = False
    -- set ns.log_name = "default"
    -- set ns.model_name = model_name | default('undefined-model')
    -- set ns.train = True
    -- set ns.eval = False
    -- set ns.create_new_model = False
    -- set ns.trust_remote_code = True

    ## memory debugging
    -- set ns.debug_memory_detials = False
    -- set ns.log_memory_to_tb = False
-- endblock config_metadata


-- block datasets_preprocessor_args
tokenizer_args: &tokenizer_args !dict
    truncation: True
-- endblock datasets_preprocessor_args


-- block datasets_definition
    -- include 'datasets/llm_dataset_project.yaml'
-- endblock datasets_definition


## Defaults to the basic trainer implementation
## Note: This is unsuitable for multiple GPUs.
## Override 'ns.trainer_class' to change the trainer class.
-- block trainer_definition
    ## See definition below
    -- include 'finetune.trainer_config'
-- endblock trainer_definition


-- block trainer_callbacks
    ## See definition below
    -- include 'finetune.callbacks'
<< endblock trainer_callbacks


## Override, to construct model from config
-- block load_model
    -- if ns.safe_load
        -- include 'models/causal_lm/from_pretrained.yaml'
    -- else
        -- include 'models/causal_lm/from_pretrained_config.yaml'
    -- endif
-- endblock load_model


-- block datacollator
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
    truncation: True
    ## Put a reasonable limit on the sequence length
    max_length: 4096
-- endblock datacollator


-- block lr_scheduler
# See https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    # This seems sufficient to maintain stability.
    warmup_steps: 200

    # Don't both with cooldown.
    # Note: It non-zero, warm-up will proceed to the LR configured in the optimizer, rather than to
    # constant_lr, then perform cosine annealing for this many steps to the constant_lr
    cooldown_steps: 0
    
    # Set this to the last checkpoint before stopping.
    # This is an exponential decay after checkpoint_step.
    checkpoint_step: 1000

    # Hold at this LR after warmup.
    constant_lr: 5.0e-6

    # Exponential decay constant, tau
    tau: !!float 1.0e2
<< endblock lr_scheduler


## Add a default memory efficient optimizer
-- block optimizer
optimizer: &optimizer !partial:forgather.ml.optim.adafactor:Adafactor
    lr: 5.0e-6
    weight_decay: 0.001
<< endblock optimizer


-- block dynamic_args
    == super()
    max_steps:
        names: "--max-steps"
        help: "Set maximum training steps"
    train_epoch:
        names: "--train-epochs"
        help: "Set the number of epochs to train for"
    log_peak_memory:
        names: "--log-peak-memory"
        action: "store_true"
        help: "Log peak GPU memory at each log step"
    dataset_config:
        names: "--dataset-config"
        help: "The name of the dataset configuration to use"
    dataset_proj:
        names: "--dataset-proj"
        help: "Path to dataset project to use"
    models_dir:
        names: "--models-dir"
        help: "The directory in which to look for models"
    model_name:
        names: "--model-name"
        help: "The name of the model to train in models_dir"
    safe_load:
        names: "--safe-load"
        action: "store_true"
        help: "Fallback to the more compatible HF model loading method"
    gradient_checkpointing:
        names: "--gradient-checkpointing"
        action: "store_true"
        help: "Enable gradient (activation) checkpoint, when supported"

<< endblock dynamic_args


#-------------------- finetune.trainer_config --------------------
-- extends ns.trainer_class
## Note: We use dynamic inheritance for the trainer-class
## This has a side effect of not being able to statically resolve the
## parent template, which is named in the 'ns.trainer_class' variable,
## the value of which is defined in the 'config_metadata' block, above.

-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
-- endblock trainer_meta_config


## Setup some defaults for this type of project
-- block trainer_args
    == super()
    # Finetune overrides
    seed: 42
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16
    logging_steps: 10
    eval_steps: 50
    num_train_epochs: {{ train_epochs | default(1) }}
    max_steps: {{ max_steps | default(-1) }}
    dataloader_num_workers: 1
    max_grad_norm: 1.0
    float32_matmul_precision: "high"
    gradient_checkpointing: {{ ns.gradient_checkpointing }}

    # Checkpoint Save Settings
    save_strategy: "steps"           # Set to "steps" to save every "save_steps"; set to "no" to disable
    save_steps: 10000                # Checkpoint every N steps
    save_safetensors: False          # Safe tensors don't support shared weights
    save_total_limit: 3              # Keep at most N checkpoints
    save_on_each_node: False         # Save common files on each node
    save_optimizer_state: True       # Save optimizer state with checkpoint
    save_scheduler_state: True       # Save LR scheduler state with checkpoint
    save_rng_state: True             # Save RNG state with checkpoint

    # We load the model via the newest checkpoint
-- if ns.safe_load
    resume_from_checkpoint: False
-- else
    resume_from_checkpoint: True
-- endif
    restore_optimizer_state: True    # Test optimizer state restoration per rank
    restore_scheduler_state: True    # Test scheduler state restoration per rank
    restore_rng_state: True          # Test RNG state restoration per rank
    restore_dataset_state: False     # Not fully implemented yet!
    ## WARNING: If the model has any buffers with "peristent=False," construct the model
    ## on the "default" device; otherwise, there is no way to initialize these buffers from
    ## a checkpoint! OTOH, not constructing the model on "meta" is very slow...
-- if ns.safe_load
    construct_model_on: "default"    # Construct model on CPU
-- else
    construct_model_on: "meta"       # Construct empty model on meta device
-- endif
    # Enable all SDPA backends and let PyTorch choose which to use.
    sdpa_backend: [ "math", "flash", "efficient", "cudnn" ]
    sdpa_set_priority: False # If list, interpret as priority order
-- endblock trainer_args


-- block model_preprocessor
# Convert model to bfloat16 format
model_preprocessor: &model_preprocessor !partial:forgather.ml.construct:module_to_dtype [ *model, "bfloat16" ]
<< endblock model_preprocessor

#-------------------- finetune.callbacks --------------------
-- extends 'callbacks/loggers.yaml'
## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

-- block callback_dependencies
    == super()
<< endblock callback_dependencies

-- block callback_list
    == super()
    - !singleton:forgather.ml.trainer.callbacks:PeakMemory
        show_details: {{ ns.debug_memory_detials }}
        do_log: {{ log_peak_memory | default(False) }}
    -- if ns.log_memory_to_tb
        summary_writer: *summary_writer
    -- endif
<< endblock callback_list
