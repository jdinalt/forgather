-- extends "models/model_type.yaml"

[config_metadata]
    == super()
    -- set ns.config_name = "Meta Llama"
    -- set ns.config_description = "Meta Llama 7B"
    -- set ns.model_name = model_name | default("llama_7b")

[model_definition]
    -- include "config.default.model"

[dynamic_args]
    == super()
    output_dir:
        names: "--output-dir"
        type: "path"
        help: "Model output directory"
    model_name:
        names: "--model-name"
        type: "str"
        help: "Model name"
    attention_dropout:
        names: "--attention-dropout"
        type: float
        help: "Attention dropout probability"
    hidden_size:
        names: "--hidden-size"
        type: "int"
        help: "Model hidden dimension size (d_model)"
    num_attention_heads:
        names: --num-attention-heads"
        type: "int"
        help: "Number of query heads (and KV heads, if otherwise unspecified)"
    num_kv_heads:
        names: "--num-kv-heads"
        type: "int"
        help: "Number of Key/Value heads"
    d_head:
        names: "--d-head"
        type: "int"
        help: "Head dimension"
    num_hidden_layers:
        names: "--num-hidden-layers"
        type: "int"
        help: "Number of hidden layers"
    dim_feedforward:
        names: "--dim-feedforward"
        type: "int"
        help: "Feedforward dimension"
    rope_theta:
        names: "--rope-theta"
        type: "float"
        help: "RoPE Theta"
    rms_norm_eps:
        names: "--rms-norm-eps"
        type: "float"
        help: "RMS Norm eps"

#------------- config.default.model --------------
-- extends "models/transformers/dynamic_llama.yaml"

[model_tokenizer]
tokenizer: &tokenizer !singleton:transformers:AutoTokenizer.from_pretrained@tokenizer
    arg0: "{{ tokenizer_id_or_path | default("TheBloke/Llama-2-7B-GPTQ") }}"
    legacy: False
    model_max_length: {{ max_model_length | default(4096) }}

[model_config]
    == super()
    # **Custom Llama**
    attention_dropout: !!float {{ attention_dropout | default(0.0) }}
    hidden_size: {{ hidden_size | default(4096) }}
    num_attention_heads: {{ num_attention_heads | default(32) }}
    num_kv_heads: {{ num_kv_heads | default('null') }}
    d_head: {{ d_head | default(128) }}
    num_hidden_layers: {{ num_hidden_layers | default(32) }}
    dim_feedforward: {{ dim_feedforward | default(11008) }}
    rope_theta: !!float {{ rope_theta | default(10000.0) }}
    rms_norm_eps: !!float {{ rms_norm_eps | default(1.0e-05) }}
