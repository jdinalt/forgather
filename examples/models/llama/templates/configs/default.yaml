-- extends "projects/causal_lm_def.yaml"

## The default primarily exists for
[config_metadata]
    == super()
    -- set ns.config_name = "Default Llama"
    -- set ns.config_description = "Meta Llama"
    -- set ns.model_name = model_name | default("llama")

[model_definition]
    -- include "config.default.model"

#------------- config.default.model --------------
-- extends "models/transformers/dynamic_llama.yaml"

[model_meta_config]
    == super()
    -- set ns.model_name = ns.config_name
    -- set ns.model_description = ns.config_description
    -- set ns.model_short_name = "llama"

[model_tokenizer]
tokenizer: &tokenizer !singleton:transformers:AutoTokenizer.from_pretrained@tokenizer
    ## Set default to something which it not gated.
    arg0: {{ tokenizer_id_or_path | toyaml('TheBloke/Llama-2-7B-GPTQ') }}
    legacy: False
    max_position_embeddings: {{ max_position_embeddings | toyaml(4096) }}
