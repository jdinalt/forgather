-- extends "projects/causal_lm_def.yaml"

[config_metadata]
    == super()
    -- set ns.config_name = "Llama3.2 1B"
    -- set ns.config_description = "Meta Llama3.2 1B"
    -- set ns.model_name = model_name | default("llama3_2_1b")

[model_definition]
    -- include "config.llam3_2_1b.model"

#------------- config.llam3_2_1b.model --------------
-- extends "models/transformers/dynamic_llama.yaml"

[model_meta_config]
    == super()
    -- set ns.model_name = ns.config_name
    -- set ns.model_description = ns.config_description
    -- set ns.model_short_name = "llama3_2"

[model_tokenizer]
tokenizer: &tokenizer !singleton:transformers:AutoTokenizer.from_pretrained@tokenizer
    arg0: {{ tokenizer_id_or_path | toyaml('meta-llama/Llama-3.2-1B') }}
    model_max_length: {{ max_model_length | toyaml(131072) }}

[model_config]
    == super()
    # **Llama 3.2 1B Overrides**
    hidden_size: {{ hidden_size | toyaml(2048) }}
    intermediate_size: {{ intermediate_size | toyaml(8192) }}
    num_attention_heads: {{ num_attention_heads | toyaml(32) }}
    num_key_value_heads: {{ num_key_value_heads | toyaml(8) }}
    num_hidden_layers: {{ num_hidden_layers | toyaml(16) }}
    rope_theta: {{ rope_theta | toyaml(500000.0) }}
    tie_word_embeddings: {{ tie_word_embeddings | toyaml(True) }}
    rope_scaling:
        factor: 32.0
        high_freq_factor: 4.0
        low_freq_factor: 1.0
        original_max_position_embeddings: 8192
        rope_type: "llama3"