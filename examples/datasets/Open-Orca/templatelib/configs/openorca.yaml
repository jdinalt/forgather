-- extends "datasets/tokenized_dataset.yaml"

[config_metadata]
    == super()
    -- set ns.config_name = "Open Orca"
    -- set ns.config_description = " This rich collection of augmented FLAN data aligns, as best as possible, with the distributions outlined in the Orca paper. It has been instrumental in generating high-performing model checkpoints and serves as a valuable resource for all NLP researchers and developers!"
    -- set ns.source = "https://arxiv.org/abs/2306.02707"
    -- set ns.pp_src = joinpath(project_dir, 'src', 'orca.py')
    -- set ns.main_feature = "question"
    -- set ns.default_chat_template = joinpath(ns.forgather_dir, 'chat_templates', 'chatml.jinja')
    -- set ns.chat_template = chat_template | default(ns.default_chat_template)

[train_dataset_split]
train_dataset_split: &train_dataset_split !singleton:datasets:load_dataset@train_dataset_split
    arg0: "Open-Orca/OpenOrca"
    split: "train[10000:{{ train_split_size | default('') }}]"
    num_proc: 8

[validation_dataset_split]
validation_dataset_split: &validation_dataset_split !singleton:datasets:load_dataset@validation_dataset_split
    arg0: "Open-Orca/OpenOrca"
    split: "train[:200]"

[test_dataset_split]
test_dataset_split: &test_dataset_split !singleton:datasets:load_dataset@test_dataset_split
    arg0: "Open-Orca/OpenOrca"
    split: "train[1000:10000]"

[train_dataset]
train_dataset: &train_dataset !singleton:{{ ns.pp_src }}:preprocess_orca@train_dataset
    dataset: *train_dataset_split
    chat_template: "{{ ns.chat_template }}"
    tokenizer: *tokenizer
    tokenizer_args: !var "preprocess_args"
    map_args:
        batch_size: 32
    template_args:
        bos_token: !call:getattr [ *tokenizer, 'bos_token' ]
        eos_token: !call:getattr [ *tokenizer, 'eos_token' ]
    to_iterable: True
    desc: "Tokenizing train"

[eval_dataset]
eval_dataset: &eval_dataset !singleton:{{ ns.pp_src }}:preprocess_orca@eval_dataset
    dataset: *validation_dataset_split
    chat_template: "{{ ns.chat_template }}"
    tokenizer: *tokenizer
    tokenizer_args: !var "preprocess_args"
    map_args:
        batch_size: 32
    template_args:
        bos_token: !call:getattr [ *tokenizer, 'bos_token' ]
        eos_token: !call:getattr [ *tokenizer, 'eos_token' ]
    desc: "Tokenizing eval"

[test_dataset]
test_dataset: &test_dataset !singleton:{{ ns.pp_src }}:preprocess_orca@test_dataset
    dataset: *test_dataset_split
    chat_template: "{{ ns.chat_template }}"
    tokenizer: *tokenizer
    tokenizer_args: !var "preprocess_args"
    map_args:
        batch_size: 32
    template_args:
        bos_token: !call:getattr [ *tokenizer, 'bos_token' ]
        eos_token: !call:getattr [ *tokenizer, 'eos_token' ]
    desc: "Tokenizing test"

[dynamic_args]
    == super()
    max_steps:
        names: "--chat-template"
        help: "Path to chat template"
        type: "path"
    train_split_size:
        names: "--train-split-size"
        help: "Number of examples to use from the train-split"
        type: "int"
