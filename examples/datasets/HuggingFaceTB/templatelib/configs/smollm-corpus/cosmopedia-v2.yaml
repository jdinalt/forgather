-- extends "datasets/tokenized_dataset.yaml"

[config_metadata]
    == super()
    -- set ns.config_name = "Cosmopedia v2"
    -- set ns.config_description = "Cosmopedia v2 is an enhanced version of Cosmopedia, the largest synthetic dataset for pre-training, consisting of over 39 million textbooks, blog posts, and stories..."
    -- set ns.source = "https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus"

[train_dataset_split]
train_dataset_split: &train_dataset_split !singleton:datasets:load_dataset@train_dataset_split
    arg0: "HuggingFaceTB/smollm-corpus"
    arg1: "cosmopedia-v2"
    split: "train[10000:]"
    num_proc: 16

[validation_dataset_split]
validation_dataset_split: &validation_dataset_split !singleton:datasets:load_dataset@validation_dataset_split
    arg0: "HuggingFaceTB/smollm-corpus"
    arg1: "cosmopedia-v2"
    split: "train[0:1000]"

[test_dataset_split]
test_dataset_split: &test_dataset_split !singleton:datasets:load_dataset@test_dataset_split
    arg0: "HuggingFaceTB/smollm-corpus"
    arg1: "cosmopedia-v2"
    split: "train[1000:10000]"

[train_dataset]
train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset
    dataset: *train_dataset_split
    tokenizer: *tokenizer
    desc: "Tokenizing train"
    fn_kwargs: !var "preprocess_args"
    map_kwargs:
        batch_size: 32
    to_iterable: true
    num_shards: 1024

[eval_dataset]
eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset
    dataset: *validation_dataset_split
    tokenizer: *tokenizer
    desc: "Tokenizing eval"
    fn_kwargs: !var "preprocess_args"
