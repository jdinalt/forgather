-- extends 'datasets/tokenized_dataset.yaml'

[config_metadata]
    == super()
    -- set ns.config_name = "Tiny Stories"
    -- set ns.config_description = "Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary."
    -- set ns.source = "https://arxiv.org/abs/2305.07759"

[map_function]
.define: &map_function !partial:forgather.ml.datasets:default_tokenize_map_fn
    add_eos: True

[train_dataset_split]
train_dataset_split: &train_dataset_split !singleton:datasets:load_dataset@train_dataset_split
    arg0: "roneneldan/TinyStories"
    split: "train"

[validation_dataset_split]
validation_dataset_split: &validation_dataset_split !singleton:datasets:load_dataset@validation_dataset_split
    arg0: "roneneldan/TinyStories"
    split: "validation[0:500]"

[test_dataset_split]
test_dataset_split: &test_dataset_split !singleton:datasets:load_dataset@test_dataset_split
    arg0: "roneneldan/TinyStories"
    split: "validation[500:]"

[train_dataset]
train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset
    <<: &common_args
        map_fn: *map_function
        tokenizer: *tokenizer
        shard_dataset: !var [ "shard_dataset", False ]
        fn_kwargs: !var "preprocess_args"
    select_range: !var [ "select_range", null ]
    dataset: *train_dataset_split
    desc: "Tokenizing train"
   
[eval_dataset]
eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset
    <<: *common_args
    dataset: *validation_dataset_split
    desc: "Tokenizing validation"

[test_dataset]
test_dataset: &test_dataset !singleton:forgather.ml.datasets:preprocess_dataset@test_dataset
    <<: *common_args
    dataset: *test_dataset_split
    desc: "Tokenizing test"