-- extends "datasets/load_dataset.yaml"

[config_metadata]
    == super()
    -- set ns.config_name = "Wikepedia Dataset"
    -- set ns.config_description = "Wikipedia dataset containing cleaned articles from the English Wikipedia."
    -- set ns.source = "https://huggingface.co/datasets/wikimedia/wikipedia"
    -- set ns.load_method = "forgather.ml.datasets:fast_load_iterable_dataset"

[map_kwargs]
    == super()
    batch_size: 32

[load_dataset_args]
    == super()
    path: "wikimedia/wikipedia"
    name: "20231101.en"

[train_dataset_split]
    == super()
    split: "train[10000:]"

[validation_dataset_split]
    == super()
    split: "train[0:1000]"

[test_dataset_split]
    == super()
    split: "train[1000:10000]"