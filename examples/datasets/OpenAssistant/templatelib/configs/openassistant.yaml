-- extends "datasets/tokenized_dataset.yaml"

[config_metadata]
    == super()
    -- set ns.config_name = "OpenAssistant"
    -- set ns.config_description = "OpenAssistant conversation dataset with dynamic thread generation and quality-weighted sampling"
    -- set ns.source = "https://github.com/LAION-AI/Open-Assistant"
    -- set ns.openassistant_src = joinpath(project_dir, 'src', 'openassistant.py')
    -- set ns.main_feature = "text"
    -- set ns.openassistant_chat_template = chat_template | default("")

## Generate all splits from a single configuration
[dataset_dict]
dataset_dict: &dataset_dict !singleton:{{ ns.openassistant_src }}:OpenAssistantDatasetDict@dataset_dict
    languages: {{ languages | default(['en']) }}
    min_quality: {{ min_quality | default('0.0') }}
    min_thread_length: {{ min_thread_length | default(2) }}
    max_thread_length: {{ max_thread_length | default(1000) }}
    exclude_deleted: {{ exclude_deleted | default(True) }}
    exclude_synthetic: {{ exclude_synthetic | default(True) }}
    branch_temperature: {{ branch_temperature | default(1.0) }}
    val_split: {{ val_split | default(5) }}
    test_split: {{ test_split | default(10) }}
    seed: {{ seed | default(42) }}
    tokenizer: *tokenizer # For chat-template only
    chat_template: "{{ ns.openassistant_chat_template }}"

[train_dataset_split]
train_dataset_split: &train_dataset_split !call:getitem [ *dataset_dict, "train" ]

[validation_dataset_split]
validation_dataset_split: &validation_dataset_split !call:getitem [ *dataset_dict, "validation" ]

[test_dataset_split]
test_dataset_split: &test_dataset_split !call:getitem [ *dataset_dict, "test" ]

[train_dataset]
train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset
    <<: &common_preprocess_args
        tokenizer: *tokenizer
        fn_kwargs: !var "preprocess_args"
        map_kwargs:
            batch_size: 32
        dataset_type: "iterable"
    dataset_length: {{ dataset_length | default(10000) }}
    dataset: *train_dataset_split

[eval_dataset]
eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset
    <<: *common_preprocess_args
    dataset: *validation_dataset_split
    dataset_length: {{ eval_length | default(500) }}

[test_dataset]
test_dataset: &test_dataset !singleton:forgather.ml.datasets:preprocess_dataset@test_dataset
    <<: *common_preprocess_args
    dataset: *test_dataset_split
    dataset_length: 1000

[dynamic_args]
    == super()
    chat_template:
        names: "--chat-template"
        help: "Path to chat template"
        type: "path"
    languages:
        names: "--languages"
        help: "Languages to include (comma-separated)"
        type: "str"
    min_quality:
        names: "--min-quality"
        help: "Minimum quality threshold"
        type: "float"
    min_thread_length:
        names: "--min-thread-length"
        help: "Minimum thread length"
        type: "int"
    max_thread_length:
        names: "--max-thread-length"
        help: "Maximum thread length"
        type: "int"
    branch_temperature:
        names: "--branch-temperature"
        help: "Temperature for quality-weighted branch sampling"
        type: "float"
    dataset_length:
        names: "--dataset-length"
        help: "Number of examples in train dataset"
        type: "int"
    eval_length:
        names: "--eval-length"
        help: "Number of examples in eval dataset"
        type: "int"
    seed:
        names: "--seed"
        help: "Random seed"
        type: "int"
