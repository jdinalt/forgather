-- extends "types/training_script/causal_lm/causal_lm.yaml"

-- block resource_directories
    == super()
    ## Change this to point to where your models are stored
    -- set ns.models_dir = joinpath(user_home_dir(), 'ai_assets', 'models')

    ## Directory in which local datasets are stored
    -- set ns.datasets_dir = joinpath(user_home_dir(), 'ai_assets', 'datasets')
<< endblock resource_directories


## Common project settings
-- block config_metadata
    == super()
    -- set ns.config_name = "Pipeline Llama"
    -- set ns.config_description = "Experimental pipeline parallel llama trainer"
    -- set ns.create_new_model = False
    -- set ns.save_model = False
    -- set ns.log_name = "project_default"
    -- set ns.model_name = "llama-2-7b-fg"
    -- set ns.train = True
    -- set ns.eval = False
    -- set ns.create_new_model = False
    -- set ns.trust_remote_code = True
-- endblock config_metadata

-- block trainer_definition
    -- include 'project.trainer_config'
<< endblock trainer_definition


-- block datasets_definition
    -- include 'project.dataset_config'
-- endblock datasets_definition


-- block trainer_callbacks
    -- include 'project.callbacks'
<< endblock trainer_callbacks


-- block construct_new_model
    -- include 'project.model'
-- endblock construct_new_model


-- block datacollator
# Pipeline Parallel requires constant input tensor shapes. Use the datacollator to
# generate constant sequence length batches, padding if too short and truncating if too long.
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
    padding: "max_length"
    truncation: True
    max_length: 1300
-- endblock datacollator


-- block optimizer
optimizer: &optimizer !partial:forgather.ml.optim.adafactor:Adafactor
    lr: 5.0e-6
    weight_decay: 0.001
<< endblock optimizer


-- block lr_scheduler
# See https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    # This seems sufficient to maintain stability.
    warmup_steps: 200

    # Don't both with cooldown.
    # Note: It non-zero, warm-up will proceed to the LR configured in the optimizer, rather than to
    # constant_lr, then perform cosine annealing for this many steps to the constant_lr
    cooldown_steps: 0
    
    # Start annealing after first checkpoint.
    # This is an exponential decay after checkpoint_step.
    checkpoint_step: 1000

    # Hold at this LR after warmup.
    constant_lr: 5.0e-6

    # Decay time constant
    tau: !!float 1.0e2
<< endblock lr_scheduler


-- block load_model
    -- include 'models/causal_lm/from_pretrained_config.yaml'
-- endblock load_model

#-------------------- project.trainer_config --------------------
-- extends 'trainers/pipeline_trainer.yaml'

-- block trainer_meta_config
    == super()
    -- set trainer_def.pipeline_layers = 32
    -- set trainer_def.split_layer_prefix = "causal_lm.layer_stack.layers."
-- endblock trainer_meta_config


-- block trainer_args
    == super()
    # Project overrides
    seed: 42
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16
    logging_steps: 10
    eval_steps: 50
    num_train_epochs: 2
    dataloader_num_workers: 1
    max_grad_norm: 1.0
    # max_steps: 11
    
    # fuse_optim_with_backward: True
    # max_grad_norm: null              # Disable when using fuse_optim_with_backward
    #sdpa_backend: "flash"
    #torch_compile: True
    #torch_compile_dynamic: False
    #activation_memory_budget: 0.2
    float32_matmul_precision: "high" # May still help for explicit float32 precision...
    
    # Checkpoint test settings - create multiple checkpoints with final step not multiple of save_steps
    save_strategy: "steps"           # Set to "steps" to save every "save_steps"; set to "no" to disable
    save_steps: 1000
    save_safetensors: False          # Safe tensors don't support shared weights
    save_total_limit: 3              # Keep 3 checkpoints
    save_on_each_node: False         # Save common files on each node
    save_optimizer_state: True       # Test optimizer state saving per rank
    save_scheduler_state: True       # Test scheduler state saving per rank
    save_rng_state: True             # Test RNG state saving per rank

    resume_from_checkpoint: True
    restore_optimizer_state: True    # Test optimizer state restoration per rank
    restore_scheduler_state: True    # Test scheduler state restoration per rank
    restore_rng_state: True          # Test RNG state restoration per rank
    restore_dataset_state: False     # Not fully implemented yet!

    # Pipeline args
    # enable_activation_checkpoints: True # Experimental: Inject activation checkpoints into graph
    debug_pipeline: False
    debug_split_model: False
    debug_model_params: False
    debug_model_init: False
-- endblock trainer_args


-- block model_preprocessor
# Convert model to bfloat16 format
model_preprocessor: &model_preprocessor !partial:forgather.ml.construct:module_to_dtype [ *model, "bfloat16" ]
<< endblock model_preprocessor


#-------------------- project.dataset_config --------------------
## Note: Switch to 'datasets/tinystories/tinystories.yaml' for the full dataset.
-- extends 'datasets/samantha/samantha.yaml'

-- block tokenize_args
    == super()
    # project overrides
<< endblock tokenize_args

#-------------------- project.callbacks --------------------
-- extends 'callbacks/loggers.yaml'

-- block callback_list
    == super()
    - !singleton:forgather.ml.trainer.callbacks:PeakMemory
        show_details: False
        #summary_writer: *summary_writer
        do_log: True
<< endblock callback_list
