-- extends "types/training_script/causal_lm/causal_lm.yaml"

-- block resource_directories
    == super()
    ## Change this to point to where your models are stored
    -- set ns.models_dir = joinpath(user_home_dir(), 'ai_assets', 'models')

    ## Directory in which local datasets are stored
    -- set ns.datasets_dir = joinpath(user_home_dir(), 'ai_assets', 'datasets')
<< endblock resource_directories


## Common project settings
-- block config_metadata
    == super()
    -- set ns.config_name = "Pipeline Llama"
    -- set ns.config_description = "Experimental pipeline parallel llama trainer"
    -- set ns.create_new_model = False
    -- set ns.save_model = False
    -- set ns.log_name = "project_default"
    -- set ns.model_name = "llama-2-7b-fg"
    -- set ns.train = True
    -- set ns.eval = False
    -- set ns.create_new_model = False
    -- set ns.trust_remote_code = True
-- endblock config_metadata

-- block trainer_definition
    -- include 'project.trainer_config'
<< endblock trainer_definition


-- block datasets_definition
    -- include 'project.dataset_config'
-- endblock datasets_definition


-- block trainer_callbacks
    -- include 'project.callbacks'
<< endblock trainer_callbacks


-- block construct_new_model
    -- include 'project.model'
-- endblock construct_new_model


-- block datacollator
# Pipeline Parallel requires constant input tensor shapes. Use the datacollator to
# generate constant sequence length batches, padding if too short and truncating if too long.
data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM
    tokenizer: *tokenizer
    return_tensors: pt
    padding: "max_length"
    truncation: True
    max_length: 1300
-- endblock datacollator


-- block optimizer
optimizer: &optimizer !partial:forgather.ml.optim.adafactor:Adafactor
    lr: 4.0e-5
    weight_decay: 0.001
<< endblock optimizer


-- block lr_scheduler
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 1000
    cooldown_steps: 2000
    checkpoint_step: 3000
    constant_lr: 1.0e-5
<< endblock lr_scheduler


-- block load_model
    -- include 'models/causal_lm/from_pretrained_config.yaml'
-- endblock load_model

#-------------------- project.trainer_config --------------------
-- extends 'trainers/pipeline_trainer.yaml'

-- block trainer_meta_config
    == super()
    -- set trainer_def.pipeline_layers = 32
    -- set trainer_def.split_layer_prefix = "causal_lm.layer_stack.layers."
-- endblock trainer_meta_config


-- block trainer_args
    == super()
    # Project overrides
    seed: 42
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16
    logging_steps: 10
    eval_steps: 100
    num_train_epochs: 5
    dataloader_num_workers: 1
    #max_steps: 500
    
    # Checkpoint test settings - create multiple checkpoints with final step not multiple of save_steps
    save_strategy: "steps"           # Set to "steps" to save every "save_steps"; set to "no" to disable
    save_steps: 1000
    save_safetensors: False          # Safe tensors don't support shared weights
    save_total_limit: 2              # Keep 2 checkpoints
    save_on_each_node: False         # Save common files on each node
    save_optimizer_state: True       # Test optimizer state saving per rank
    save_scheduler_state: True       # Test scheduler state saving per rank
    save_rng_state: True             # Test RNG state saving per rank

    resume_from_checkpoint: True
    restore_optimizer_state: True    # Test optimizer state restoration per rank
    restore_scheduler_state: True    # Test scheduler state restoration per rank
    restore_rng_state: True          # Test RNG state restoration per rank

    # Pipeline args
    debug_pipeline: False
    debug_split_model: False
    debug_model_params: False
    debug_model_init: False
-- endblock trainer_args


-- block model_preprocessor
# Convert model to bfloat16 format
model_preprocessor: &model_preprocessor !partial:forgather.ml.construct:module_to_dtype [ *model, "bfloat16" ]
<< endblock model_preprocessor


#-------------------- project.dataset_config --------------------
## Note: Switch to 'datasets/tiny_stories.yaml' for the full dataset.
-- extends 'datasets/samantha.yaml'

-- block tokenize_args
    == super()
    # project overrides
<< endblock tokenize_args

#-------------------- project.callbacks --------------------
-- extends 'callbacks/loggers.yaml'

-- block callback_list
    == super()
    - !singleton:forgather.ml.peak_memory:PeakMemory
        show_details: False
<< endblock callback_list
