## Project level definitions and overrides go here.
-- extends "types/training_script/causal_lm/causal_lm.yaml"

-- block resource_directories
    == super()
    ## Directory in which pre-trained models are located.
    -- set ns.models_dir = joinpath(user_home_dir(), 'ai_assets', 'models')

    ## Directory in which local datasets are stored
    -- set ns.datasets_dir = joinpath(user_home_dir(), 'ai_assets', 'datasets')
<< endblock resource_directories

## Set Project level defaults
-- block config_metadata
    == super()
    -- set ns.config_name = "Finetune"
    -- set ns.config_description = "Example configuration for finetuning a pre-trained model."

    ## The name of the model to train in the models directory
    -- set ns.model_name = 'walsh_test'
    -- set ns.log_name = "project_default"
    
    ## Initialize a new model from scratch
    -- set ns.create_new_model = False

    ## Save model, when training is complete -- unsupported without model definition.
    -- set ns.save_model = False

    -- set ns.train = True
    -- set ns.eval = False

    ## Required to load a custom model
    -- set ns.trust_remote_code = True
-- endblock config_metadata


-- block pre_model_setup
    ## Add assets needed for text-gen sampling.
    ## This adds a set of prompts and text-gen parameters.
    == super()
    -- include "prompts/short_stories.yaml"
-- endblock pre_model_setup


-- block model_constructor_args
model_constructor_args: &model_constructor_args
    # Load in bfloat16 ; disable if not on GPU with support for this format.
    torch_dtype: !singleton:forgather.ml.construct:torch_dtype [ "bfloat16" ]

    # Use flash-attention 2; Disable, if unsupported.
    attn_implementation: "flash_attention_2"
<< endblock model_constructor_args


-- block optimizer
optimizer: &optimizer !lambda:torchao.optim:Adam8bit
    lr: 5.0e-6
    #bf16_stochastic_round: true
<< endblock optimizer


-- block lr_scheduler
    ##-- include 'lr_schedulers/cosine_annealing_with_warmup.yaml'
# https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 500
    cooldown_steps: 0
    constant_lr: 5.0e-6
<< endblock lr_scheduler


-- block datasets_definition
    -- include 'project.dataset'
-- endblock datasets_definition


-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


-- block trainer_callbacks
    -- include 'project.callbacks'
<< endblock trainer_callbacks


#-------------------- project.trainer_config --------------------
## Select one-of:
## trainers/( trainer.yaml | accel_trainer.yaml | hf_trainer.yaml )
-- extends 'trainers/trainer.yaml'


-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
<< endblock trainer_meta_config


-- block trainer_args
    == super()
    # Project Overrides
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 16
    logging_steps: 50
    eval_steps: 200
    
    ## Stop early for quick test.
    max_steps: 2001

    # Eval after the first step
    eval_delay: -1
<< endblock trainer_args


#-------------------- project.callbacks --------------------
-- extends 'callbacks/loggers.yaml'

## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

-- block callback_dependencies
    == super()

    -- filter trim()
    -- block text_gen_callback_args
text_gen_callback_args: &text_gen_callback_args
    summary_writer: *summary_writer
    prompts: *testprompts
    generation_config: *generation_config
    max_new_tokens: 50
    generation_steps: 500
    << endblock text_gen_callback_args
    -- endfilter
<< endblock callback_dependencies

## This adds a text-generationn sample every 'generation_steps'
-- block callback_list
    == super()
    - !singleton:forgather.ml.textgen_callback:TextgenCallback
        <<: *text_gen_callback_args
<< endblock callback_list

#-------------------- project.dataset --------------------
-- extends 'datasets/books.yaml'

