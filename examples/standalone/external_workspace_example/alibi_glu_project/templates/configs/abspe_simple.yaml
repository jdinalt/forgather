-- extends 'training_script/causal_lm/causal_lm.yaml'

-- block config_metadata
    == super()
    -- set ns.config_name = "AbsPE Experiment"
    -- set ns.config_description = "GLU transformer with absolute positional encoding for comparison"
    -- set ns.model_name = "abspe_simple"
    -- set ns.create_new_model = True
    -- set ns.save_model = True
-- endblock config_metadata

-- block model_definition
    -- include 'tokenizers/tiny_2k.yaml'
    -- include 'models/glu_transformer_abspe.yaml'
-- endblock model_definition

-- block datasets_definition
    -- include 'datasets/roneneldan/tinystories-abridged.yaml'
-- endblock datasets_definition

-- block trainer_definition
    -- include 'trainers/trainer.yaml'
-- endblock trainer_definition

-- block trainer_args
    == super()
    
    output_dir: "./output_models/abspe_simple"
    
    ## Training schedule
    num_train_epochs: 1
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 8
    
    ## Optimization - same as ALiBi model for fair comparison
    learning_rate: 3e-4
    weight_decay: 0.01
    warmup_steps: 100
    
    ## Logging and evaluation
    logging_steps: 50
    eval_steps: 200
    eval_strategy: "steps"
    
    ## Checkpointing
    save_steps: 500
    save_strategy: "{{ save_strategy | default('steps') }}"
    save_total_limit: 2
    
-- endblock trainer_args

-- block datacollator
    == super()
    ## Limit maximum sequence length to 512 tokens to prevent OOM
    truncation: True
    max_length: 512
-- endblock datacollator

-- block tokenize_args
    == super()
    ## Truncate during tokenization as well
    max_length: 512
-- endblock tokenize_args
