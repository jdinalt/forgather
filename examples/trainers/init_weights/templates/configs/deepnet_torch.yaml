## The project defines the defaults
-- extends 'project.yaml'

-- block config_metadata
    == super()
    ## Experiment Metadata
    -- set ns.config_name = 'Deepnet Torch'
    -- set ns.config_description = "Deepnet with default torch Linear init"
    -- set ns.model_name = 'deepnet_torch'
    
    ## Name the logging directory to match the model name
    -- set ns.log_name = ns.model_name
-- endblock config_metadata

## Override default project definition
-- block construct_new_model
    -- include 'experiment.model_config'
-- endblock construct_new_model

#-------------------- experiment.model_config --------------------
-- extends 'project.model_config'

-- block init_weights
.define: &deepnet_beta 

.define: &init_weights !lambda:.init_weights_by_regex:init_weights_by_regex@init_weights
    regex_list:
        - [ 'norm', "pass" ]
        - [ 'bias', "zeros" ]
        - [ 'embedding\.weight', "embedding" ]
        - [ 'feedforward|value_linear|output_linear', "deepnet" ]
        - [ 'attention|output_decoder', "linear" ]
    init_f_map:
        pass: !lambda:.init_weights_by_regex:init_pass
        zeros: !lambda:torch.nn.init:zeros_ []
        embedding: !lambda:.init_weights:init_embeddings
            padding_index: !var "pad_token_id"
        linear: !lambda:.init_weights:init_torch_linear_default []
        deepnet: !lambda:.init_weights:init_torch_linear_default
            gain: !singleton:.deepnet:deepnet_beta [ !var "num_hidden_layers", 0 ]
<< endblock init_weights

-- block layer_factory
.define: &deepnet_alpha 

.define: &layer_factory !lambda:.deepnet:DeepnetLayer@layer_factory
    feedforward_factory: *feedforward_factory
    attention_factory: *attention_factory
    norm_factory: *layer_norm_factory
    dropout: !var "layer_dropout"
    residual_dropout: !var "residual_dropout"
    alpha: !singleton:.deepnet:deepnet_alpha [ !var "num_hidden_layers", 0 ]
<< endblock layer_factory