{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../../notebooks/forgather.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "nb.display_project_index(config_template=\"test_cp.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49321e2-1fc7-4ea3-83e6-3e6fda4d0ff4",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/train_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba48601-bae1-44cd-ab21-7f7b7245c329",
   "metadata": {},
   "source": [
    "#### View Memory Snapshot Files Here\n",
    "\n",
    "https://docs.pytorch.org/memory_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09251b9-1080-4894-9da4-0554fd45ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "from forgather.ml.utils import count_parameters\n",
    "\n",
    "# Load project assests\n",
    "proj = Project(\"test_cp.yaml\")\n",
    "outputs = proj([\"model\", \"data_collator\", \"train_dataset\", \"optimizer\",])\n",
    "model, data_collator, train_dataset, optimizer = outputs[\"model\"], outputs[\"data_collator\"], outputs[\"train_dataset\"], outputs[\"optimizer\"]\n",
    "print(model)\n",
    "print(\"model dtype: \", model.causal_lm.input_encoder.embedding.weight.dtype)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d956a85-ada7-4101-a2a8-230080b044cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pickle import dump\n",
    "import torch\n",
    "from pickle import dump\n",
    "import forgather\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "from forgather.ml.optim.adafactor import Adafactor\n",
    "from pprint import pp\n",
    "\n",
    "# Substitute with the project optimizer to test memory requirements with these.\n",
    "adamw_factory = partial(torch.optim.AdamW, lr=1e-3)\n",
    "sgd_factory = partial(torch.optim.SGD, lr=1e-1)\n",
    "adafactor_factory = partial(Adafactor, lr=1e-4)\n",
    "\n",
    "def profile_training_memory(\n",
    "    model,\n",
    "    dataset,\n",
    "    data_collator,\n",
    "    opt_factory,\n",
    "    device,\n",
    "    batch_size,\n",
    "    max_steps,\n",
    "    truncate_to=None,\n",
    "    shuffle=False,\n",
    "    show_details=False,\n",
    "    profiler_file=None\n",
    "):\n",
    "    try:\n",
    "        train_progress_bar = tqdm(total=max_steps, dynamic_ncols=True)\n",
    "        \n",
    "        if profiler_file:\n",
    "            torch.cuda.memory._record_memory_history(enabled='all')\n",
    "            \n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle()\n",
    "        \n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator, pin_memory=True)\n",
    "        opt = opt_factory(model.named_parameters())\n",
    "        max_sequence = 0\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            labels = batch[\"labels\"]\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "\n",
    "            if truncate_to:\n",
    "                labels = labels[:, :truncate_to]\n",
    "                input_ids = labels[:, :truncate_to]\n",
    "            \n",
    "            if max_sequence < input_ids.shape[1]:\n",
    "                max_sequence = input_ids.shape[1]\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            try:\n",
    "                loss, logits = model(input_ids=input_ids, labels=labels)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            except:\n",
    "                print(f\"Exception raised on batch {step} of {max_steps} : {input_ids.shape}\")\n",
    "                raise\n",
    "            opt.zero_grad()\n",
    "            train_progress_bar.update()\n",
    "            train_progress_bar.write(f\"loss: {loss.item():.4}\")\n",
    "            if step == max_steps:\n",
    "                break\n",
    "        del opt\n",
    "        \n",
    "        # save a snapshot of the memory allocation to file\n",
    "        if profiler_file:\n",
    "            s = torch.cuda.memory._snapshot()\n",
    "            with open(profiler_file, \"wb\") as f:\n",
    "                dump(s, f)\n",
    "        torch.cuda.memory._record_memory_history(enabled=None)\n",
    "        max_allocated = torch.cuda.max_memory_allocated()\n",
    "        model.cpu()\n",
    "        print(f\"maximum_sequence_length={max_sequence}\")\n",
    "        print(f\"final loss={loss.item()}\")\n",
    "        print(f\"max_allocated={max_allocated / 1000000000.:.3f} GB\")\n",
    "        if show_details:\n",
    "            pp(torch.cuda.memory_stats(device))\n",
    "    finally:\n",
    "        train_progress_bar.close()\n",
    "        train_progress_bar = None\n",
    "\n",
    "profile_training_memory(\n",
    "    model=model,\n",
    "    dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    opt_factory=adafactor_factory,\n",
    "    device=\"cuda:0\",\n",
    "    batch_size=32,\n",
    "    max_steps=3,\n",
    "    truncate_to=None,\n",
    "    shuffle=True,\n",
    "    show_details=True,\n",
    "    profiler_file=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2b368-deae-40d8-95b8-d143376523dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.reset_max_memory_allocated(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972b0f7-0b7f-464c-82e4-338500bd85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "\n",
    "proj = Project(\"test_cp.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165e7bd-df76-43f1-a62d-7889a28685d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.generate_trainingscript(proj, \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ce13d-3db8-4329-a34c-5690f423befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.display_tb_command(proj, local_host=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea93abb-107d-4bcf-8fdb-bb145f7e0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.ml.model_test import kick_test\n",
    "\n",
    "# Note: Any combination of the \"output targets\" may be constructed.\n",
    "model = proj(\"model\")\n",
    "\n",
    "kick_test(model, batch_size=2, seq_len=7, pad_probability=0.9, device=\"cuda:0\", dtype=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
