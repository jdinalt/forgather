## Project level definitions and overrides go here.
-- extends "types/training_script/causal_lm/causal_lm.yaml"

-- block resource_directories
    == super()
    ## Directory in which pre-trained models are located.
    -- set ns.models_dir = joinpath(user_home_dir(), 'ai_assets', 'models')

    ## Directory in which local datasets are stored
    -- set ns.datasets_dir = joinpath(user_home_dir(), 'ai_assets', 'datasets')
<< endblock resource_directories

## Set Project level defaults
-- block config_metadata
    == super()
    -- set ns.config_name = "Finetune"
    -- set ns.config_description = "An example configuration"

    ## The name of the model to train in the models directory
    -- set ns.model_name = 'walsh_test'
    -- set ns.log_name = ns.model_name
    
    ## Initialize a new model from scratch
    -- set ns.create_new_model = False

    ## Save model, when training is complete -- unsupported without model definition.
    -- set ns.save_model = False

    -- set ns.train = True
    -- set ns.eval = False

    ## Required to load a custom model
    -- set ns.trust_remote_code = True
-- endblock config_metadata


-- block pre_model_setup
    ## Add assets needed for text-gen sampling.
    ## This adds a set of prompts and text-gen parameters.
    == super()
    -- include "prompts/tiny_stories.yaml"
-- endblock pre_model_setup


-- block model_constructor_args
.define: &model_constructor_args
    # Load in bfloat16 ; disable if not on GPU with support for this format.
    torch_dtype: !singleton:forgather.ml.construct:torch_dtype [ "bfloat16" ]

    # Use flash-attention 2; Disable, if unsupported.
    attn_implementation: "flash_attention_2"
<< endblock model_constructor_args


-- block optimizer
#.define: &optimizer !lambda:torchao.optim:Adam8bit
.define: &optimizer !lambda:transformers:Adafactor
    kwargs:
        scale_parameter: False
        relative_step: False
        lr: 1.0e-5
        #bf16_stochastic_round: true
<< endblock optimizer


-- block datasets_definition
    -- include 'project.dataset'
-- endblock datasets_definition


-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


-- block trainer_callbacks
    -- include 'project.callbacks'
<< endblock trainer_callbacks


#-------------------- project.trainer_config --------------------
## Select one-of:
## trainers/( trainer.yaml | accel_trainer.yaml | hf_trainer.yaml )
-- extends 'trainers/accel_trainer.yaml'


-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
<< endblock trainer_meta_config


-- block trainer_args
    == super()
    # Project Overrides
    learning_rate: 1.0e-5
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 16
    logging_steps: 50
    eval_steps: 200
    lr_scheduler_type: "none"
    
    ## Stop early for quick test.
    max_steps: 1001

    # Eval after the first step
    eval_delay: 1
<< endblock trainer_args


#-------------------- project.callbacks --------------------
-- extends 'callbacks/loggers.yaml'

## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

-- block callback_dependencies
    == super()

    -- filter trim()
    -- block text_gen_callback_args
.define: &text_gen_callback_args
    summary_writer: *summary_writer
    prompts: *testprompts
    generation_config: *generation_config
    max_new_tokens: 40
    generation_steps: 500
    << endblock text_gen_callback_args
    -- endfilter
<< endblock callback_dependencies

## This adds a text-generationn sample every 'generation_steps'
-- block callback_list
    == super()
    - !singleton:forgather.ml.textgen_callback:TextgenCallback
        <<: *text_gen_callback_args
<< endblock callback_list

#-------------------- project.dataset --------------------
-- extends 'datasets/abstract/base_datasets.yaml'

-- block datasets_meta_config
    == super()
    -- set datasets_ns.name = "Local Dataset"
    -- set datasets_ns.description = "A dataset on the local filesystem."
    -- set datasets_ns.train_dataset_id = joinpath(ns.datasets_dir, "roneneldan-TinyStories")
    -- set datasets_ns.eval_dataset_id = datasets_ns.train_dataset_id
    -- set datasets_ns.train_load_method = "datasets:load_from_disk"
    -- set datasets_ns.eval_load_method = datasets_ns.train_load_method

    -- set datasets_ns.train_select_range = 0.1
    -- set datasets_ns.eval_select_range = 500
-- endblock datasets_meta_config

-- block tokenize_args
## See: https://huggingface.co/docs/transformers/main_classes/tokenizer
.define: &tokenize_args
    truncation: True
    max_length: 512
<< endblock tokenize_args
