{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../../notebooks/forgather.ipynb)  \n",
    "\n",
    "https://arxiv.org/pdf/2412.05270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Finetune Pretrained Model\n",
       "\n",
       "A work-in-progress to create templates for model finetuning.\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/trainers/finetune\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/trainers/finetune/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "    - [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/trainers/finetune/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/forgather_workspace](../../../forgather_workspace)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/tiny_experiments](../../../templates/tiny_experiments)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/modellib](../../../templates/modellib)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/base](../../../templates/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [adafactor.yaml](templates/configs/adafactor.yaml)\n",
       "- [default.yaml](templates/configs/default.yaml)\n",
       "\n",
       "Default Configuration: default.yaml\n",
       "\n",
       "Active Configuration: default.yaml\n",
       "\n",
       "## Available Templates\n",
       "- [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "- [callbacks/base_callbacks.yaml](../../../templates/base/callbacks/base_callbacks.yaml)\n",
       "- [callbacks/loggers.yaml](../../../templates/base/callbacks/loggers.yaml)\n",
       "- [configs/adafactor.yaml](templates/configs/adafactor.yaml)\n",
       "- [configs/default.yaml](templates/configs/default.yaml)\n",
       "- [datasets/abstract/base_datasets.yaml](../../../templates/base/datasets/abstract/base_datasets.yaml)\n",
       "- [datasets/abstract/pretokenized_dataset.yaml](../../../templates/base/datasets/abstract/pretokenized_dataset.yaml)\n",
       "- [datasets/examples/books.yaml](../../../templates/base/datasets/examples/books.yaml)\n",
       "- [datasets/examples/tiny_stories.yaml](../../../templates/base/datasets/examples/tiny_stories.yaml)\n",
       "- [datasets/examples/wikipedia.yaml](../../../templates/base/datasets/examples/wikipedia.yaml)\n",
       "- [datasets/tiny/tiny_stories.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories.yaml)\n",
       "- [datasets/tiny/tiny_stories_abridged.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories_abridged.yaml)\n",
       "- [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "- [model_ctor/args.yaml](../../../templates/modellib/model_ctor/args.yaml)\n",
       "- [model_test/base.yaml](../../../templates/base/model_test/base.yaml)\n",
       "- [model_test/sub_project.yaml](../../../templates/base/model_test/sub_project.yaml)\n",
       "- [models/abstract/base_language_model.yaml](../../../templates/base/models/abstract/base_language_model.yaml)\n",
       "- [models/abstract/causal_lm_from_config.yaml](../../../templates/base/models/abstract/causal_lm_from_config.yaml)\n",
       "- [models/abstract/causal_lm_from_pretrained.yaml](../../../templates/base/models/abstract/causal_lm_from_pretrained.yaml)\n",
       "- [models/abstract/custom_causal_lm.yaml](../../../templates/base/models/abstract/custom_causal_lm.yaml)\n",
       "- [models/abstract/dynamic_causal_lm.yaml](../../../templates/base/models/abstract/dynamic_causal_lm.yaml)\n",
       "- [models/abstract/load_model.yaml](../../../templates/base/models/abstract/load_model.yaml)\n",
       "- [models/causal_transformer.yaml](../../../templates/modellib/models/causal_transformer.yaml)\n",
       "- [models/dynamic_causal_transformer.yaml](../../../templates/modellib/models/dynamic_causal_transformer.yaml)\n",
       "- [models/gpt2.yaml](../../../templates/modellib/models/gpt2.yaml)\n",
       "- [models/llama.yaml](../../../templates/modellib/models/llama.yaml)\n",
       "- [models/tiny/tiny_causal.yaml](../../../templates/tiny_experiments/models/tiny/tiny_causal.yaml)\n",
       "- [models/tiny/tiny_d128_l2.yaml](../../../templates/tiny_experiments/models/tiny/tiny_d128_l2.yaml)\n",
       "- [models/tiny/tiny_gpt2.yaml](../../../templates/tiny_experiments/models/tiny/tiny_gpt2.yaml)\n",
       "- [models/tiny/tiny_llama.yaml](../../../templates/tiny_experiments/models/tiny/tiny_llama.yaml)\n",
       "- [project.yaml](templates/project.yaml)\n",
       "- [projects/tiny.yaml](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "- [projects/tiny_model.yaml](../../../templates/tiny_experiments/projects/tiny_model.yaml)\n",
       "- [prompts/short_stories.yaml](../../../templates/base/prompts/short_stories.yaml)\n",
       "- [prompts/tiny_stories.yaml](../../../templates/tiny_experiments/prompts/tiny_stories.yaml)\n",
       "- [tokenizers/tiny_2k.yaml](../../../templates/tiny_experiments/tokenizers/tiny_2k.yaml)\n",
       "- [tokenizers/tiny_8k.yaml](../../../templates/tiny_experiments/tokenizers/tiny_8k.yaml)\n",
       "- [trainers/accel_trainer.yaml](../../../templates/base/trainers/accel_trainer.yaml)\n",
       "- [trainers/base_trainer.yaml](../../../templates/base/trainers/base_trainer.yaml)\n",
       "- [trainers/hf_trainer.yaml](../../../templates/base/trainers/hf_trainer.yaml)\n",
       "- [trainers/minimal_trainer.yaml](../../../templates/base/trainers/minimal_trainer.yaml)\n",
       "- [trainers/simple_trainer.yaml](../../../templates/base/trainers/simple_trainer.yaml)\n",
       "- [trainers/trainer.yaml](../../../templates/base/trainers/trainer.yaml)\n",
       "- [types/meta_template.yaml](../../../templates/base/types/meta_template.yaml)\n",
       "- [types/model/model_type.yaml](../../../templates/base/types/model/model_type.yaml)\n",
       "- [types/tokenizer/bpe/bpe.yaml](../../../templates/base/types/tokenizer/bpe/bpe.yaml)\n",
       "- [types/tokenizer/tokenizer.yaml](../../../templates/base/types/tokenizer/tokenizer.yaml)\n",
       "- [types/training_script/causal_lm/causal_lm.yaml](../../../templates/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "- [types/training_script/training_script.yaml](../../../templates/base/types/training_script/training_script.yaml)\n",
       "- [types/type.yaml](../../../templates/base/types/type.yaml)\n",
       "\n",
       "## Included Templates\n",
       "- [configs/default.yaml](templates/configs/default.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [prompts/short_stories.yaml](../../../templates/base/prompts/short_stories.yaml)\n",
       "        - [types/training_script/causal_lm/causal_lm.yaml](../../../templates/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [trainers/trainer.yaml](../../../templates/base/trainers/trainer.yaml)\n",
       "                - [trainers/base_trainer.yaml](../../../templates/base/trainers/base_trainer.yaml)\n",
       "                    - [trainers/minimal_trainer.yaml](../../../templates/base/trainers/minimal_trainer.yaml)\n",
       "            - [callbacks/loggers.yaml](../../../templates/base/callbacks/loggers.yaml)\n",
       "                - [callbacks/base_callbacks.yaml](../../../templates/base/callbacks/base_callbacks.yaml)\n",
       "            - [models/abstract/load_model.yaml](../../../templates/base/models/abstract/load_model.yaml)\n",
       "                - [models/abstract/causal_lm_from_pretrained.yaml](../../../templates/base/models/abstract/causal_lm_from_pretrained.yaml)\n",
       "                    - [models/abstract/base_language_model.yaml](../../../templates/base/models/abstract/base_language_model.yaml)\n",
       "            - [types/training_script/training_script.yaml](../../../templates/base/types/training_script/training_script.yaml)\n",
       "                - [types/type.yaml](../../../templates/base/types/type.yaml)\n",
       "                    - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "            - [inc/formatting.jinja](../../../templates/base/inc/formatting.jinja)\n",
       "        - [project.callbacks](templates/project.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "        - [project.dataset](templates/project.yaml)\n",
       "            - [datasets/examples/books.yaml](../../../templates/base/datasets/examples/books.yaml)\n",
       "                - [datasets/abstract/base_datasets.yaml](../../../templates/base/datasets/abstract/base_datasets.yaml)\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#                Finetune                \n",
       "#---------------------------------------\n",
       "# 2025-05-19T07:20:25\n",
       "# Description: Example configuration for finetuning a pre-trained model.\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/trainers/finetune\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/finetune\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: walsh_test\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.7.0\n",
       "#     transformers: 4.51.3\n",
       "#     accelerate: 1.7.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/finetune/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"/home/dinalt/ai_assets/models/walsh_test\"\n",
       "# ns.logging_dir: \"/home/dinalt/ai_assets/models/walsh_test/runs/project_default_2025-05-19T07-20-25\"\n",
       "# ns.create_new_model: False\n",
       "# ns.save_model: False\n",
       "# ns.train: True\n",
       "# ns.eval: False\n",
       "# ns.trust_remote_code: True\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       ".define: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "# GPT3 generated opening lines to short stories.\n",
       ".define: &testprompts !list:@testprompts\n",
       "    - \"As the first snowflake fell, Sarah knew this winter would be different.\"\n",
       "    - \"The old bookstore at the corner of Elm Street had always held secrets, but today, something unusual caught Emma's eye.\"\n",
       "    - \"It was a rainy evening when Detective Williams received a letter from an anonymous sender, setting in motion a chilling investigation.\"\n",
       "    - \"In the quiet town of Willowbrook, strange occurrences had become a daily routine, and it all started with the arrival of that peculiar antique mirror.\"\n",
       "    - \"The sound of a violin playing in the distance drew Alex deeper into the forest, where an unexpected discovery awaited.\"\n",
       "    - \"It was a summer like no other, the summer that brought a talking cat into Lucy's life.\"\n",
       "    - \"The dusty attic had always been off-limits, but when Grace found a mysterious key, her curiosity got the best of her.\"\n",
       "    - \"On the night of the meteor shower, Max made a wish that would change his life in ways he could never have imagined.\"\n",
       "    - \"In the bustling city of Neon Heights, where technology reigned supreme, Maya stumbled upon an ancient relic with unimaginable power.\"\n",
       "    - \"The scent of freshly baked bread filled the air, and for the first time in years, Claire felt a sense of belonging.\"\n",
       "\n",
       "# Conservative text-generation parameters.\n",
       ".define: &generation_config !dict:@generation_config\n",
       "    identity: generation_config\n",
       "    do_sample: True\n",
       "    top_k: 20\n",
       "    top_p: 0.9\n",
       "    temperature: 0.7\n",
       "    repitition_penalty: 1.15\n",
       "\n",
       "################ Model #################\n",
       "\n",
       ".define: &model_constructor_args\n",
       "    # Load in bfloat16 ; disable if not on GPU with support for this format.\n",
       "    torch_dtype: !singleton:forgather.ml.construct:torch_dtype [ \"bfloat16\" ]\n",
       "\n",
       "    # Use flash-attention 2; Disable, if unsupported.\n",
       "    attn_implementation: \"flash_attention_2\"\n",
       "\n",
       "# Name: walsh_test\n",
       "# Description: Load locally save model from disk.\n",
       "\n",
       "# model_def.model_id_or_path = \"/home/dinalt/ai_assets/models/walsh_test\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       ".define: &tokenizer !singleton:transformers:AutoTokenizer.from_pretrained@tokenizer\n",
       "    - \"/home/dinalt/ai_assets/models/walsh_test\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       "\n",
       "\n",
       "# Model does not have dynamically generated code\n",
       ".define: &model_code_generator null\n",
       "\n",
       ".define: &model_code_writer null    \n",
       "\n",
       "\n",
       "\n",
       "# **Model Constructor**\n",
       "\n",
       ".define: &model !singleton:transformers:AutoModelForCausalLM.from_pretrained@model\n",
       "    args: [ \"/home/dinalt/ai_assets/models/walsh_test\" ]\n",
       "    kwargs:\n",
       "        trust_remote_code: True\n",
       "        <<: *model_constructor_args\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "# Name: Togethercomputer RedPajama 1T-Book\n",
       "# Define: \n",
       "# Source: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\n",
       "# Train Dataset: \"/home/dinalt/ai_assets/datasets/togethercomputer-RedPajama-Data-1T-book\" : \"train\"\n",
       "# Eval Dataset: \"/home/dinalt/ai_assets/datasets/red_pajamas_books_validation\" : \"validation\"\n",
       "\n",
       "# **Source Datasets**\n",
       "\n",
       ".define: &train_source_dataset !singleton:datasets:load_from_disk@train_source_dataset\n",
       "    - \"/home/dinalt/ai_assets/datasets/togethercomputer-RedPajama-Data-1T-book\"\n",
       "\n",
       ".define: &eval_source_dataset !singleton:datasets:load_from_disk@eval_source_dataset\n",
       "    - \"/home/dinalt/ai_assets/datasets/red_pajamas_books_validation\"\n",
       "\n",
       "# **Dataset Splits**\n",
       "\n",
       ".define: &train_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"train\"\n",
       "\n",
       ".define: &eval_dataset_split !singleton:operator:getitem\n",
       "    - *eval_source_dataset\n",
       "    - \"validation\"\n",
       "\n",
       "# **Preprocess Dataset Args**\n",
       "\n",
       ".define: &preprocess_args\n",
       "    block_size: 512\n",
       "    overflow: true\n",
       "    stride: 0\n",
       "\n",
       "# **Preprocessed Datasets**\n",
       "\n",
       ".define: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset\n",
       "    dataset: *train_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: null\n",
       "    desc: \"Tokenizing train\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "    # This dataset uses entire books as rows. Block_tokenize to split these \n",
       "    # into blocks of 'block_size' tokens.\n",
       "    map_fn: !lambda:forgather.ml.datasets:block_tokenize_fn\n",
       "\n",
       "    # We definitely do not want to tokenize several hundred GBs up-front.\n",
       "    # Convert to iterable to tokenize when loaded from the iterator.\n",
       "    to_iterable: true\n",
       "\n",
       "    # Break the dataset into this many shards for efficient shuffling.\n",
       "    num_shards: 1024\n",
       "\n",
       "    # The tokenizer will be running in the DataLoaders worker threads. Don't further parallelize\n",
       "    # tokenization within the workers!\n",
       "    parallel_tokenizer: false\n",
       "    map_kwargs:\n",
       "        # These records are huge! Don't process too many at a time.\n",
       "        batch_size: 2\n",
       "\n",
       "    # Shuffle, as this dataset is not very randomized 'as-is'\n",
       "    shuffle: true\n",
       "    seed: 42\n",
       "\n",
       ".define: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset\n",
       "    dataset: *eval_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: null\n",
       "    desc: \"Tokenizing validation split\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "    # Use the same pre-processor as the main dataset, but perform the pre-processing up-front.\n",
       "    map_fn: !lambda:forgather.ml.datasets:block_tokenize_fn\n",
       "\n",
       "    # Limit the number eval examples to this many 'books'\n",
       "    select_range: 20\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "# https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling\n",
       ".define: &data_collator !singleton:transformers:DataCollatorForLanguageModeling@data_collator\n",
       "    args:\n",
       "        - *tokenizer\n",
       "    kwargs:\n",
       "        mlm: False\n",
       "        return_tensors: pt\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"/home/dinalt/ai_assets/models/walsh_test/runs/project_default_2025-05-19T07-20-25\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       ".define: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-05-19T07:20:25\"\n",
       "    name: \"Finetune\"\n",
       "    description: \"Example configuration for finetuning a pre-trained model.\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.7.0', 'transformers': '4.51.3', 'accelerate': '1.7.0'}\n",
       "\n",
       ".define: &text_gen_callback_args\n",
       "    summary_writer: *summary_writer\n",
       "    prompts: *testprompts\n",
       "    generation_config: *generation_config\n",
       "    max_new_tokens: 50\n",
       "    generation_steps: 500\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       ".define: &trainer_callbacks !list:@trainer_callbacks\n",
       "    # Log all training output to JSON\n",
       "    - !singleton:forgather.ml.json_logger:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    - !singleton:forgather.ml.tb_logger:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "    - !singleton:forgather.ml.textgen_callback:TextgenCallback\n",
       "        <<: *text_gen_callback_args\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       ".define: &optimizer !lambda:torchao.optim:Adam8bit\n",
       "    kwargs:\n",
       "        lr: 5.0e-6\n",
       "        #bf16_stochastic_round: true\n",
       "############# LR Scheduler #############\n",
       "\n",
       ".define: &lr_scheduler ~\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: Custom forgather.ml.trainer.Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       ".define: &trainer_args\n",
       "    # Minimal Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    output_dir: \"/home/dinalt/ai_assets/models/walsh_test\"\n",
       "    logging_dir: \"/home/dinalt/ai_assets/models/walsh_test/runs/project_default_2025-05-19T07-20-25\"\n",
       "    logging_steps: 500\n",
       "    per_device_train_batch_size: 16\n",
       "    per_device_eval_batch_size: 32\n",
       "    learning_rate: 5.0e-5\n",
       "    num_train_epochs: 1\n",
       "    # Base Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    overwrite_output_dir: True\n",
       "    eval_steps: 100\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"no\"\n",
       "    logging_strategy: \"steps\"\n",
       "\n",
       "    # Project Overrides\n",
       "    learning_rate: 1.0e-5\n",
       "    per_device_train_batch_size: 4\n",
       "    per_device_eval_batch_size: 16\n",
       "    logging_steps: 50\n",
       "    eval_steps: 200\n",
       "    lr_scheduler_type: \"none\"\n",
       "    \n",
       "    max_steps: 2001\n",
       "\n",
       "    # Eval after the first step\n",
       "    eval_delay: -1\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       ".define: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    model: *model\n",
       "    args: !singleton:forgather.ml.trainer_types:TrainingArguments@trainer_args\n",
       "        <<: *trainer_args\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Finetune\"\n",
       "    config_description: \"Example configuration for finetuning a pre-trained model.\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/ai_assets/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "    models_dir: \"/home/dinalt/ai_assets/models\"\n",
       "    tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/ai_assets/datasets\"\n",
       "    output_dir: \"/home/dinalt/ai_assets/models/walsh_test\"\n",
       "    model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "    logging_dir: \"/home/dinalt/ai_assets/models/walsh_test/runs/project_default_2025-05-19T07-20-25\"\n",
       "    create_new_model: \"False\"\n",
       "    save_model: \"False\"\n",
       "    train: \"True\"\n",
       "    eval: \"False\"\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_save: False\n",
       "    do_train: True\n",
       "    do_eval: False\n",
       "    # Init distributed envrionment before initializing anyting which depends on it.\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "    pp_config: !var \"pp_config\"\n",
       "\n",
       "model_code_writer: *model_code_writer\n",
       "distributed_env: *distributed_env\n",
       "model: *model\n",
       "trainer: *trainer\n",
       "train_dataset: *train_dataset\n",
       "eval_dataset: *eval_dataset\n",
       "data_collator: *data_collator\n",
       "trainer_callbacks: *trainer_callbacks\n",
       "trainer_args: *trainer_args\n",
       "optimizer: *optimizer\n",
       "lr_scheduler: *lr_scheduler\n",
       "model_constructor_args: *model_constructor_args\n",
       "tokenizer: *tokenizer\n",
       "\n",
       "```\n",
       "\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'Example configuration for finetuning a pre-trained '\n",
       "                       'model.',\n",
       " 'config_name': 'Finetune',\n",
       " 'create_new_model': 'False',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/datasets',\n",
       " 'eval': 'False',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': '/home/dinalt/ai_assets/models/walsh_test/runs/project_default_2025-05-19T07-20-25',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': '/home/dinalt/ai_assets/models',\n",
       " 'output_dir': '/home/dinalt/ai_assets/models/walsh_test',\n",
       " 'project_dir': '.',\n",
       " 'save_model': 'False',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'train': 'True',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "## Output Targets\n",
       "- meta\n",
       "- main\n",
       "- model_code_writer\n",
       "- distributed_env\n",
       "- model\n",
       "- trainer\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- trainer_callbacks\n",
       "- trainer_args\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "nb.display_project_index(\n",
    "    config_template=\"\",\n",
    "    show_available_templates=True,\n",
    "    show_pp_config=True,\n",
    "    show_loaded_config=False,\n",
    "    show_generated_code=False,\n",
    "    materialize=False,\n",
    "    pp_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c0dab-e03d-4ba9-a3cb-86a0319a6f08",
   "metadata": {},
   "source": [
    "## Constuct Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf6c458-ec39-4c4b-a978-f54ed6a544d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "from forgather import Project\n",
    "from pprint import pp\n",
    "from forgather.ml.utils import count_parameters\n",
    "\n",
    "# Pass config name\n",
    "proj = Project(\"adafactor.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef885be2-63ab-46dc-9c05-45fa1a82bf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cc8693f5bb4b7f990605f371b44ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9735fc37779d46609c3956c34dcdcdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation split:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 07:20:57.860\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mforgather.ml.base_trainer\u001b[0m:\u001b[36m_validate_dirs\u001b[0m:\u001b[36m206\u001b[0m - \u001b[33m\u001b[1mModel exists in output dir '/home/dinalt/ai_assets/models/walsh_test' and model may be overwritten!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': '1742.57M', 'trainable': '1742.57M'}\n",
      "HFCausalModel(\n",
      "  (transformer_head): Transformer(\n",
      "    (embedding): Embedding(32000, 2048)\n",
      "    (positional_encoder): RSWalshPositionalEncoder()\n",
      "    (layer_stack): TransformerLayerStack(\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x DeepnetLayer(\n",
      "          (attention): CausalSelfAttention(\n",
      "            d_model=2048, num_heads=32, beta=0.25, attn_type=flash2, dropout=Dropout(p=0.1, inplace=False)\n",
      "            (in_proj): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "            (output_linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feedforward): FeedforwardLayer(\n",
      "            (activation): GELU(approximate='none')\n",
      "            (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "            (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=2048, out_features=32000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Note: Any combination of the \"output targets\" may be constructed.\n",
    "training_script, model = proj([\"main\", \"model\"]).values()\n",
    "\n",
    "print(count_parameters(model))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68e2a0-4aa7-479a-bec8-09d2e5399e04",
   "metadata": {},
   "source": [
    "## Train Model in Notebook\n",
    "This only works for a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f419ac-aec8-4edb-93ef-2850487ec34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Script Started *****\n",
      "config_name: Finetune w/ Adafactor\n",
      "config_description: Example configuration for finetuning a pre-trained model.\n",
      "output_dir: /home/dinalt/ai_assets/models/walsh_test\n",
      "logging_dir: /home/dinalt/ai_assets/models/walsh_test/runs/adafactor_2025-05-19T07-20-02\n",
      "not compiling model\n",
      "Calling optimizer factory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a6c8a663674e9e94eda91fc4715c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 400,000\n",
      "total_train_samples: 400,000\n",
      "per_device_train_batch_size: 4\n",
      "actual_per_device_batch_size: 4\n",
      "total_train_batch_size: 4\n",
      "max_steps: 2,001\n",
      "total_parameters: 1742.6M\n",
      "trainable_parameters: 1742.6M\n",
      "model:\n",
      "HFCausalModel(\n",
      "  (transformer_head): Transformer(\n",
      "    (embedding): Embedding(32000, 2048)\n",
      "    (positional_encoder): RSWalshPositionalEncoder()\n",
      "    (layer_stack): TransformerLayerStack(\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x DeepnetLayer(\n",
      "          (attention): CausalSelfAttention(\n",
      "            d_model=2048, num_heads=32, beta=0.25, attn_type=flash2, dropout=Dropout(p=0.1, inplace=False)\n",
      "            (in_proj): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "            (output_linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feedforward): FeedforwardLayer(\n",
      "            (activation): GELU(approximate='none')\n",
      "            (linear1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "            (linear2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=2048, out_features=32000, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56da056b648c40dda3d27878862653ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-19 07:22:09            1  0.0   eval-loss:  2.78446   \n",
      "2025-05-19 07:22:33           50  0.0   epoch: 0.0005 loss: 2.894 \n",
      "2025-05-19 07:22:50          100  0.0   epoch: 0.001 loss: 2.825 \n",
      "2025-05-19 07:23:07          150  0.0   epoch: 0.0015 loss: 2.588 \n",
      "2025-05-19 07:23:24          200  0.0   epoch: 0.002 loss: 2.901 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa1f349cf65417397d3a6da5e5871ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-19 07:24:29          201  0.0   eval-loss:  2.76587   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining_script\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:355\u001b[0m, in \u001b[0;36mrecord.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m error_handler\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m se:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# For run_path based entrypoints, SystemExit with code = 0 will never exit.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# Handling it here by returning a value:\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m se\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/ai_assets/forgather/src/forgather/ml/training_script.py:74\u001b[0m, in \u001b[0;36mTrainingScript.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpp_config)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_train:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# This is where the actual 'loop' is.\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_env\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     76\u001b[0m         distributed\u001b[38;5;241m.\u001b[39mbarrier()\n",
      "File \u001b[0;32m~/ai_assets/forgather/src/forgather/ml/base_trainer.py:115\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare(train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset, eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_assets/forgather/src/forgather/ml/trainer.py:227\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_step_begin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 227\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_loss(loss)\n\u001b[1;32m    229\u001b[0m     pstate\u001b[38;5;241m.\u001b[39mtotal_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/ai_assets/forgather/src/forgather/ml/trainer.py:289\u001b[0m, in \u001b[0;36mTrainer._train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    286\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward(loss)\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_optimizer_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/optimization.py:839\u001b[0m, in \u001b[0;36mAdafactor.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    837\u001b[0m     p_data_fp32\u001b[38;5;241m.\u001b[39madd_(p_data_fp32, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m lr))\n\u001b[0;32m--> 839\u001b[0m \u001b[43mp_data_fp32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m {torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16}:\n\u001b[1;32m    842\u001b[0m     p\u001b[38;5;241m.\u001b[39mcopy_(p_data_fp32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_script.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507440e9-ef48-464e-807d-d57314d28205",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "This simply performs a forward and backward pass on the model with a small sample of random input-ids. While hardly exhaustive, this can quickly expose bugs without having to run a real training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113582c9-e27a-46ea-93b8-2a127c7387f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kick_test(model, batch_size=2, seq_len=7, pad_probability=0.9, device=\"cpu\", dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df0d78-9884-4c26-9aa2-55feb6c8d2ec",
   "metadata": {},
   "source": [
    "## Train Model with Notebook Launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe9a7f-18a7-4bd2-9423-e89bc7f2e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "from forgather.ml.training_script import training_loop\n",
    "\n",
    "notebook_launcher(\n",
    "    training_loop,\n",
    "    args=(proj.project_dir, proj.config_name),\n",
    "    num_processes=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30fa78-b4e4-44f2-a12d-ce0138fd8c48",
   "metadata": {},
   "source": [
    "## Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0f013-5494-49c6-b6d4-432ea0555af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show command to run tensorboard; local_host should be false if tensorboard should run on all network interfaces.\n",
    "nb.display_tb_command(proj, local_host=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232926b8-13ba-4742-846a-08d07fbe0935",
   "metadata": {},
   "source": [
    "## Generate Trainingscript\n",
    "The preferred way of running training is via the command-line. This generates a simple bash script to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0188b-8ade-43e7-ba5d-11785e5b528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second arg specifies which GPUs may be used. For example, \"0,2\" only allows the first and third GPU.\n",
    "# Note that multi-GPU training requires a trainer implementation which supports this. e.g. \"accel_trainer\"\n",
    "nb.generate_trainingscript(proj, \"0,2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cabec2b-8ac6-4685-9cd5-e51f96a368aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810be7770ca64df19d21422b077e3de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "from forgather import Project\n",
    "from pprint import pp\n",
    "from forgather.ml.datasets import test_with_dataloader\n",
    "\n",
    "# Pass config name\n",
    "proj = Project(\"\")\n",
    "train_dataset, data_collator, tokenizer = proj([\"train_dataset\", \"data_collator\", \"tokenizer\"]).values()\n",
    "test_with_dataloader(train_dataset, tokenizer, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc7f906f-0dcb-4eaf-9e2b-cace92ce16e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f9abf-81e1-41af-aae2-de66c5c48500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
