-- extends 'models/control.yaml'

-- block model_meta_config
    == super()
    -- set model_def.name = "Pre-LN Transformer"
    -- set model_def.description = "Base + Pre-LN"
    -- set model_def.model_type = "dynmaic-models-preln"
    -- set model_def.model_path = joinpath(ns.output_dir, "pre_ln_model.py")
    -- set model_def.config_path = model_def.model_path
-- endblock model_meta_config


-- block layer_factory
# Pre-ln override; replaces post-LN module with pre-LN module.
# On Layer Normalization in the Transformer Architecture: https://arxiv.org/pdf/2002.04745
.define: &layer_factory !lambda:.pre_ln_layer:PreLNLayer
    feedforward: *feedforward_factory
    attention: *attention_factory
    norm1: *layer_norm_factory
    norm2: *layer_norm_factory
    dropout: !var "layer_dropout"
    residual_dropout: !var "residual_dropout"
-- endblock layer_factory


-- block layer_stack_factory

.define: &layer_stack_factory !factory:.layer_stack:LayerStack
    layer_factory: *layer_factory
    num_hidden_layers: !var "num_hidden_layers"
    # Pre-LN override; add a layer-norm to the end of the stack.
    post_norm: *layer_norm_factory
-- endblock layer_stack_factory