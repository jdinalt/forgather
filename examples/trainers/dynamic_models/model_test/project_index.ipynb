{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../../../notebooks/forgather.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Model Test Harness\n",
       "\n",
       "A front-end for assembling and debugging models.\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/templates](../templates)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/tiny_experiments](../../../../templates/tiny_experiments)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/modellib](../../../../templates/modellib)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/base](../../../../templates/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [default.yaml](templates/model_test/default.yaml)\n",
       "\n",
       "Default Configuration: default.yaml\n",
       "\n",
       "Active Configuration: default.yaml\n",
       "\n",
       "## Available Templates\n",
       "- [model_test/default.yaml](templates/model_test/default.yaml)\n",
       "- [project.yaml](../templates/project.yaml)\n",
       "- [models/pre_ln.yaml](../templates/models/pre_ln.yaml)\n",
       "- [models/walsh_pe.yaml](../templates/models/walsh_pe.yaml)\n",
       "- [models/relu-glu.yaml](../templates/models/relu-glu.yaml)\n",
       "- [models/swish.yaml](../templates/models/swish.yaml)\n",
       "- [models/swi-glu.yaml](../templates/models/swi-glu.yaml)\n",
       "- [models/control.yaml](../templates/models/control.yaml)\n",
       "- [experiments/pre_ln.yaml](../templates/experiments/pre_ln.yaml)\n",
       "- [experiments/walsh_pe.yaml](../templates/experiments/walsh_pe.yaml)\n",
       "- [experiments/relu-glu.yaml](../templates/experiments/relu-glu.yaml)\n",
       "- [experiments/swish.yaml](../templates/experiments/swish.yaml)\n",
       "- [experiments/swi-glu.yaml](../templates/experiments/swi-glu.yaml)\n",
       "- [experiments/control.yaml](../templates/experiments/control.yaml)\n",
       "- [projects/tiny_model.yaml](../../../../templates/tiny_experiments/projects/tiny_model.yaml)\n",
       "- [projects/tiny.yaml](../../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "- [datasets/tiny/tiny_stories.yaml](../../../../templates/tiny_experiments/datasets/tiny/tiny_stories.yaml)\n",
       "- [datasets/tiny/tiny_stories_abridged.yaml](../../../../templates/tiny_experiments/datasets/tiny/tiny_stories_abridged.yaml)\n",
       "- [models/tiny/tiny_causal.yaml](../../../../templates/tiny_experiments/models/tiny/tiny_causal.yaml)\n",
       "- [models/tiny/tiny_gpt2.yaml](../../../../templates/tiny_experiments/models/tiny/tiny_gpt2.yaml)\n",
       "- [models/tiny/tiny_llama.yaml](../../../../templates/tiny_experiments/models/tiny/tiny_llama.yaml)\n",
       "- [models/tiny/tiny_d128_l2.yaml](../../../../templates/tiny_experiments/models/tiny/tiny_d128_l2.yaml)\n",
       "- [prompts/tiny_stories.yaml](../../../../templates/tiny_experiments/prompts/tiny_stories.yaml)\n",
       "- [tokenizers/tiny_2k.yaml](../../../../templates/tiny_experiments/tokenizers/tiny_2k.yaml)\n",
       "- [tokenizers/tiny_8k.yaml](../../../../templates/tiny_experiments/tokenizers/tiny_8k.yaml)\n",
       "- [model_ctor/args.yaml](../../../../templates/modellib/model_ctor/args.yaml)\n",
       "- [models/dynamic_causal_transformer.yaml](../../../../templates/modellib/models/dynamic_causal_transformer.yaml)\n",
       "- [models/causal_transformer.yaml](../../../../templates/modellib/models/causal_transformer.yaml)\n",
       "- [models/gpt2.yaml](../../../../templates/modellib/models/gpt2.yaml)\n",
       "- [models/llama.yaml](../../../../templates/modellib/models/llama.yaml)\n",
       "- [trainers/accel_trainer.yaml](../../../../templates/base/trainers/accel_trainer.yaml)\n",
       "- [trainers/trainer.yaml](../../../../templates/base/trainers/trainer.yaml)\n",
       "- [trainers/hf_trainer.yaml](../../../../templates/base/trainers/hf_trainer.yaml)\n",
       "- [trainers/base_trainer.yaml](../../../../templates/base/trainers/base_trainer.yaml)\n",
       "- [datasets/abstract/pretokenized_dataset.yaml](../../../../templates/base/datasets/abstract/pretokenized_dataset.yaml)\n",
       "- [datasets/abstract/base_datasets.yaml](../../../../templates/base/datasets/abstract/base_datasets.yaml)\n",
       "- [models/abstract/causal_lm_from_config.yaml](../../../../templates/base/models/abstract/causal_lm_from_config.yaml)\n",
       "- [models/abstract/base_language_model.yaml](../../../../templates/base/models/abstract/base_language_model.yaml)\n",
       "- [models/abstract/custom_causal_lm.yaml](../../../../templates/base/models/abstract/custom_causal_lm.yaml)\n",
       "- [models/abstract/causal_lm_from_pretrained.yaml](../../../../templates/base/models/abstract/causal_lm_from_pretrained.yaml)\n",
       "- [models/abstract/dynamic_causal_lm.yaml](../../../../templates/base/models/abstract/dynamic_causal_lm.yaml)\n",
       "- [models/abstract/load_model.yaml](../../../../templates/base/models/abstract/load_model.yaml)\n",
       "- [callbacks/base_callbacks.yaml](../../../../templates/base/callbacks/base_callbacks.yaml)\n",
       "- [callbacks/loggers.yaml](../../../../templates/base/callbacks/loggers.yaml)\n",
       "- [types/meta_template.yaml](../../../../templates/base/types/meta_template.yaml)\n",
       "- [types/type.yaml](../../../../templates/base/types/type.yaml)\n",
       "- [types/tokenizer/tokenizer.yaml](../../../../templates/base/types/tokenizer/tokenizer.yaml)\n",
       "- [types/tokenizer/bpe/bpe.yaml](../../../../templates/base/types/tokenizer/bpe/bpe.yaml)\n",
       "- [types/model/model_type.yaml](../../../../templates/base/types/model/model_type.yaml)\n",
       "- [types/training_script/training_script.yaml](../../../../templates/base/types/training_script/training_script.yaml)\n",
       "- [types/training_script/causal_lm/causal_lm.yaml](../../../../templates/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "\n",
       "## Included Templates\n",
       "- [model_test/default.yaml](templates/model_test/default.yaml)\n",
       "    - [projects/tiny_model.yaml](../../../../templates/tiny_experiments/projects/tiny_model.yaml)\n",
       "        - [types/model/model_type.yaml](../../../../templates/base/types/model/model_type.yaml)\n",
       "            - [types/type.yaml](../../../../templates/base/types/type.yaml)\n",
       "                - [inc/formatting.jinja](../../../../templates/base/inc/formatting.jinja)\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#           Model Test Harness           \n",
       "#---------------------------------------\n",
       "# 2024-08-10T23:01:28\n",
       "# Description: Test and debug new models before training.\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: test_model\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/output_models/test_model\"\n",
       "\n",
       "################ Model #################\n",
       "\n",
       ".define: &model_constructor_args {}\n",
       "\n",
       "# Name: Tiny Causal\n",
       "# Description: A scaled-down version of the base Causal Transformer\n",
       "# model_def.cls = \"DynamicCasualLM\"\n",
       "# model_def.cfg_cls = \"DynamicCausalLMConfig\"\n",
       "# model_def.config_path = \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/output_models/test_model/dynamic_causal_transformer.py\"\n",
       "# model_def.model_path = \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/output_models/test_model/dynamic_causal_transformer.py\"\n",
       "# model_def.model_type = \"forgather-dynamic-causal-transformer\"\n",
       "# model_def.model_template_searchpath = \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# model_def.model_template_name = \"dynamic_causal_template.py\"\n",
       "# model_def.name_policy = \"named\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       "# Load custom tokenizer from sub-project definition\n",
       ".define: &tokenizer !singleton:forgather.ml.construct:load_from_config@tokenizer\n",
       "    project_dir: \"../../../../examples/tokenizers/tiny_stories_bpe\"\n",
       "    config_template: \"2k.yaml\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       ".define: &model_submodule_searchpath\n",
       "    - \"./../model_src\"\n",
       "    - \"../../../../model_src/bits\"\n",
       "    - \"./output_models/test_model\"\n",
       "\n",
       ".define: &loss_fn_factory !factory:.causal_loss:CausalLoss@loss_fn_factory []\n",
       "\n",
       ".define: &layer_norm_factory !factory:torch.nn:LayerNorm@layer_norm_factory\n",
       "    normalized_shape: !var \"hidden_size\"\n",
       "\n",
       ".define: &feedforward_factory !factory:.feedforward_layer:FeedforwardLayer@feedforward_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "    dropout: !var \"activation_dropout\"\n",
       "\n",
       ".define: &attention_factory !factory:.causal_multihead_attn:CausalMultiheadAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    num_heads: !var \"num_attention_heads\"\n",
       "    dropout: !var \"attention_dropout\"\n",
       "\n",
       ".define: &layer_factory !lambda:.post_ln_layer:PostLNLayer@layer_factory\n",
       "    feedforward: *feedforward_factory\n",
       "    attention: *attention_factory\n",
       "    norm1: *layer_norm_factory\n",
       "    norm2: *layer_norm_factory\n",
       "    dropout: !var \"layer_dropout\"\n",
       "    residual_dropout: !var \"residual_dropout\"\n",
       "\n",
       ".define: &layer_stack_factory !factory:.layer_stack:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    num_hidden_layers: !var \"num_hidden_layers\"\n",
       "\n",
       ".define: &output_decoder_factory !factory:torch.nn:Linear@output_decoder_factory\n",
       "    - !var \"hidden_size\"\n",
       "    - !var \"vocab_size\"\n",
       "\n",
       ".define: &positional_encoder_factory !factory:.sinusoidal_pe:SinusoidalPE@positional_encoder_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    max_sequence_length: !var \"max_sequence_length\"\n",
       "\n",
       ".define: &input_encoder_factory !factory:.input_encoder:InputEncoder@input_encoder_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    vocab_size: !var \"vocab_size\"\n",
       "    dropout: !var \"embedding_dropout\"\n",
       "    positional_encoder: *positional_encoder_factory\n",
       "\n",
       ".define: &init_weights_factory !factory:.init_weights:InitWeights@init_weights_factory\n",
       "    std: !var \"initializer_range\"\n",
       "\n",
       ".define: &model_factory !singleton:.causal_lm:CasualLM@model_factory\n",
       "    loss_fn: *loss_fn_factory\n",
       "    input_encoder: *input_encoder_factory\n",
       "    output_decoder: *output_decoder_factory\n",
       "    layer_stack: *layer_stack_factory\n",
       "    init_weights: *init_weights_factory\n",
       "\n",
       ".define: &model_code_writer !singleton:forgather.ml.construct:write_file@model_code_writer\n",
       "    data: &model_code_generator !meta:forgather.codegen:generate_code@model_code_generator\n",
       "        searchpath: \"../../../../model_src\"\n",
       "        template_name: \"dynamic_causal_template.py\"\n",
       "        name_policy: \"named\"\n",
       "        obj: *model_factory\n",
       "        # Template args\n",
       "        model_type: \"forgather-dynamic-causal-transformer\"\n",
       "    output_file: \"./output_models/test_model/dynamic_causal_transformer.py\"\n",
       "    return_value: \"Model constructor generated by Forgather 1.0\"\n",
       "\n",
       ".define: &model_config !singleton:./output_models/test_model/dynamic_causal_transformer.py:DynamicCausalLMConfig@model_config\n",
       "    submodule_searchpath: *model_submodule_searchpath\n",
       "    # Set auto-map for custom model; this ensures that the source code stays with the model.\n",
       "    auto_map:\n",
       "        AutoConfig: \"dynamic_causal_transformer.DynamicCausalLMConfig\"\n",
       "        AutoModel: \"dynamic_causal_transformer.DynamicCasualLM\"\n",
       "    # Get the vocab-size from the tokenizer definition.\n",
       "    vocab_size: !singleton:len [ *tokenizer ]\n",
       "    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]\n",
       "    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]\n",
       "    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]\n",
       "    # Add dependency on code generator\n",
       "    code_generator: *model_code_writer\n",
       "    hidden_size: 512\n",
       "    num_attention_heads: 8\n",
       "    num_hidden_layers: 6\n",
       "    max_sequence_length: !singleton:getattr\n",
       "        - *tokenizer\n",
       "        - \"model_max_length\"\n",
       "    dim_feedforward: 2048\n",
       "    initializer_range: 0.02\n",
       "    embedding_dropout: 0.10\n",
       "    layer_dropout: 0.10\n",
       "    residual_dropout: 0.0\n",
       "    attention_dropout: 0.0\n",
       "    activation_dropout: 0.0\n",
       "    \n",
       "    # Tiny Causal overrides\n",
       "    hidden_size: 256\n",
       "    dim_feedforward: 1024\n",
       "    num_attention_heads: 2\n",
       "    num_hidden_layers: 4\n",
       "    embedding_dropout: 0.0\n",
       "    layer_dropout: 0.0\n",
       "\n",
       "# **Model Constructor**\n",
       "\n",
       ".define: &pretrained_model !singleton:./output_models/test_model/dynamic_causal_transformer.py:DynamicCasualLM@pretrained_model\n",
       "    args:\n",
       "        - *model_config\n",
       "    kwargs:\n",
       "        submodule_searchpath: *model_submodule_searchpath\n",
       "        <<: *model_constructor_args\n",
       "\n",
       ".define: &model !singleton:forgather.ml.construct:dependency_list@model\n",
       "    - *pretrained_model\n",
       "    - !singleton:forgather.ml.construct:copy_package_files\n",
       "        - \"./output_models/test_model\"\n",
       "        - *model_config\n",
       "    - !singleton:forgather.ml.construct:copy_package_files\n",
       "        - \"./output_models/test_model\"\n",
       "        - *pretrained_model\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Model Test Harness\"\n",
       "    config_description: \"Test and debug new models before training.\"\n",
       "    project_dir: \".\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"../../../../tokenizers\"\n",
       "    datasets_dir: \"../../../../datasets\"\n",
       "    output_dir: \"./output_models/test_model\"\n",
       "    model_src_dir: \"../../../../model_src\"\n",
       "\n",
       "main:\n",
       "    model: *model\n",
       "    tokenizer: *tokenizer\n",
       "    model_config: *model_config\n",
       "    generated_code: *model_code_generator\n",
       "\n",
       "```\n",
       "\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_description': 'Test and debug new models before training.',\n",
       " 'config_name': 'Model Test Harness',\n",
       " 'datasets_dir': '../../../../datasets',\n",
       " 'model_src_dir': '../../../../model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'output_dir': './output_models/test_model',\n",
       " 'project_dir': '.',\n",
       " 'tokenizers_dir': '../../../../tokenizers'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "- [./output_models/test_model/dynamic_causal_transformer.py](output_models/test_model/dynamic_causal_transformer.py) : DynamicCasualLM\n",
       "    - [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/./output_models/test_model/dynamic_causal_transformer.py](output_models/test_model/dynamic_causal_transformer.py) : dynamic_causal_transformer\n",
       "        - [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/../../../../model_src/bits/causal_multihead_attn.py](../../../../model_src/bits/causal_multihead_attn.py) : dynamic_causal_transformer.causal_multihead_attn\n",
       "        - [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/../../../../model_src/bits/post_ln_layer.py](../../../../model_src/bits/post_ln_layer.py) : dynamic_causal_transformer.post_ln_layer\n",
       "        - [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/../../../../model_src/bits/layer_stack.py](../../../../model_src/bits/layer_stack.py) : dynamic_causal_transformer.layer_stack\n",
       "        - [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/../../../../model_src/bits/feedforward_layer.py](../../../../model_src/bits/feedforward_layer.py) : dynamic_causal_transformer.feedforward_layer\n",
       "        - [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/../../../../model_src/bits/sinusoidal_pe.py](../../../../model_src/bits/sinusoidal_pe.py) : dynamic_causal_transformer.sinusoidal_pe\n",
       "        - [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/../../../../model_src/bits/causal_lm.py](../../../../model_src/bits/causal_lm.py) : dynamic_causal_transformer.causal_lm\n",
       "        - [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/../../../../model_src/bits/init_weights.py](../../../../model_src/bits/init_weights.py) : dynamic_causal_transformer.init_weights\n",
       "        - [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/../../../../model_src/bits/causal_loss.py](../../../../model_src/bits/causal_loss.py) : dynamic_causal_transformer.causal_loss\n",
       "        - [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_test/../../../../model_src/bits/input_encoder.py](../../../../model_src/bits/input_encoder.py) : dynamic_causal_transformer.input_encoder\n",
       "- [./output_models/test_model/dynamic_causal_transformer.py](output_models/test_model/dynamic_causal_transformer.py) : DynamicCausalLMConfig\n",
       "## Loaded Configuration to YAML\n",
       "\n",
       "```yaml\n",
       ".define: &meta !singleton:named_dict@meta\n",
       "    config_name: 'Model Test Harness'\n",
       "    config_description: 'Test and debug new models before training.'\n",
       "    project_dir: '.'\n",
       "    models_dir: './output_models'\n",
       "    tokenizers_dir: '../../../../tokenizers'\n",
       "    datasets_dir: '../../../../datasets'\n",
       "    output_dir: './output_models/test_model'\n",
       "    model_src_dir: '../../../../model_src'\n",
       "\n",
       ".define: &tokenizer !singleton:forgather.ml.construct:load_from_config@tokenizer\n",
       "    project_dir: '../../../../examples/tokenizers/tiny_stories_bpe'\n",
       "    config_template: '2k.yaml'\n",
       "\n",
       ".define: &loss_fn_factory !factory:.causal_loss:CausalLoss@loss_fn_factory []\n",
       "\n",
       ".define: &positional_encoder_factory !factory:.sinusoidal_pe:SinusoidalPE@positional_encoder_factory\n",
       "    d_model: !var hidden_size\n",
       "    max_sequence_length: !var max_sequence_length\n",
       "\n",
       ".define: &input_encoder_factory !factory:.input_encoder:InputEncoder@input_encoder_factory\n",
       "    d_model: !var hidden_size\n",
       "    vocab_size: !var vocab_size\n",
       "    dropout: !var embedding_dropout\n",
       "    positional_encoder: *positional_encoder_factory\n",
       "\n",
       ".define: &output_decoder_factory !factory:torch.nn:Linear@output_decoder_factory\n",
       "    - !var hidden_size\n",
       "    - !var vocab_size\n",
       "\n",
       ".define: &feedforward_factory !factory:.feedforward_layer:FeedforwardLayer@feedforward_factory\n",
       "    d_model: !var hidden_size\n",
       "    d_feedforward: !var dim_feedforward\n",
       "    dropout: !var activation_dropout\n",
       "\n",
       ".define: &attention_factory !factory:.causal_multihead_attn:CausalMultiheadAttn@attention_factory\n",
       "    d_model: !var hidden_size\n",
       "    num_heads: !var num_attention_heads\n",
       "    dropout: !var attention_dropout\n",
       "\n",
       ".define: &layer_norm_factory !factory:torch.nn:LayerNorm@layer_norm_factory\n",
       "    normalized_shape: !var hidden_size\n",
       "\n",
       ".define: &layer_factory !lambda:.post_ln_layer:PostLNLayer@layer_factory\n",
       "    feedforward: *feedforward_factory\n",
       "    attention: *attention_factory\n",
       "    norm1: *layer_norm_factory\n",
       "    norm2: *layer_norm_factory\n",
       "    dropout: !var layer_dropout\n",
       "    residual_dropout: !var residual_dropout\n",
       "\n",
       ".define: &layer_stack_factory !factory:.layer_stack:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    num_hidden_layers: !var num_hidden_layers\n",
       "\n",
       ".define: &init_weights_factory !factory:.init_weights:InitWeights@init_weights_factory\n",
       "    std: !var initializer_range\n",
       "\n",
       ".define: &model_factory !singleton:.causal_lm:CasualLM@model_factory\n",
       "    loss_fn: *loss_fn_factory\n",
       "    input_encoder: *input_encoder_factory\n",
       "    output_decoder: *output_decoder_factory\n",
       "    layer_stack: *layer_stack_factory\n",
       "    init_weights: *init_weights_factory\n",
       "\n",
       ".define: &model_code_generator !meta:forgather.codegen:generate_code@model_code_generator\n",
       "    searchpath: '../../../../model_src'\n",
       "    template_name: 'dynamic_causal_template.py'\n",
       "    name_policy: 'named'\n",
       "    obj: *model_factory\n",
       "    model_type: 'forgather-dynamic-causal-transformer'\n",
       "\n",
       ".define: &model_code_writer !singleton:forgather.ml.construct:write_file@model_code_writer\n",
       "    data: *model_code_generator\n",
       "    output_file: './output_models/test_model/dynamic_causal_transformer.py'\n",
       "    return_value: 'Model constructor generated by Forgather 1.0'\n",
       "\n",
       ".define: &model_config !singleton:./output_models/test_model/dynamic_causal_transformer.py:DynamicCausalLMConfig@model_config\n",
       "    auto_map: \n",
       "        AutoConfig: 'dynamic_causal_transformer.DynamicCausalLMConfig'\n",
       "        AutoModel: 'dynamic_causal_transformer.DynamicCasualLM'\n",
       "    vocab_size: !singleton:len\n",
       "        - *tokenizer\n",
       "    pad_token_id: !singleton:getattr\n",
       "        - *tokenizer\n",
       "        - 'pad_token_id'\n",
       "    bos_token_id: !singleton:getattr\n",
       "        - *tokenizer\n",
       "        - 'bos_token_id'\n",
       "    eos_token_id: !singleton:getattr\n",
       "        - *tokenizer\n",
       "        - 'eos_token_id'\n",
       "    code_generator: *model_code_writer\n",
       "    hidden_size: 256\n",
       "    num_attention_heads: 2\n",
       "    num_hidden_layers: 4\n",
       "    max_sequence_length: !singleton:getattr\n",
       "        - *tokenizer\n",
       "        - 'model_max_length'\n",
       "    dim_feedforward: 1024\n",
       "    initializer_range: 0.02\n",
       "    embedding_dropout: 0.0\n",
       "    layer_dropout: 0.0\n",
       "    residual_dropout: 0.0\n",
       "    attention_dropout: 0.0\n",
       "    activation_dropout: 0.0\n",
       "\n",
       ".define: &pretrained_model !singleton:./output_models/test_model/dynamic_causal_transformer.py:DynamicCasualLM@pretrained_model\n",
       "    - *model_config\n",
       "\n",
       ".define: &model !singleton:forgather.ml.construct:dependency_list@model\n",
       "    - *pretrained_model\n",
       "    - !singleton:forgather.ml.construct:copy_package_files\n",
       "        - './output_models/test_model'\n",
       "        - *model_config\n",
       "    - !singleton:forgather.ml.construct:copy_package_files\n",
       "        - './output_models/test_model'\n",
       "        - *pretrained_model\n",
       "\n",
       "\n",
       "meta: *meta\n",
       "main: \n",
       "    model: *model\n",
       "    tokenizer: *tokenizer\n",
       "    model_config: *model_config\n",
       "    generated_code: *model_code_generator\n",
       "\n",
       "```\n",
       "\n",
       "### Generated Source Code\n",
       "\n",
       "```python\n",
       "from forgather.ml.construct import write_file\n",
       "from forgather.ml.construct import load_from_config\n",
       "from forgather.ml.construct import dependency_list\n",
       "from forgather.ml.construct import copy_package_files\n",
       "from importlib.util import spec_from_file_location, module_from_spec\n",
       "import os\n",
       "import sys\n",
       "\n",
       "# Import a dynamic module.\n",
       "def dynimport(module, name, searchpath):\n",
       "    module_path = module\n",
       "    module_name = os.path.basename(module).split(\".\")[0]\n",
       "    module_spec = spec_from_file_location(\n",
       "        module_name,\n",
       "        module_path,\n",
       "        submodule_search_locations=searchpath,\n",
       "    )\n",
       "    mod = module_from_spec(module_spec)\n",
       "    sys.modules[module_name] = mod\n",
       "    module_spec.loader.exec_module(mod)\n",
       "    for symbol in name.split(\".\"):\n",
       "        mod = getattr(mod, symbol)\n",
       "    return mod\n",
       "\n",
       "DynamicCausalLMConfig = lambda: dynimport(\"./output_models/test_model/dynamic_causal_transformer.py\", \"DynamicCausalLMConfig\", ['./../model_src', '../../../../model_src/bits', './output_models/test_model'])\n",
       "DynamicCasualLM = lambda: dynimport(\"./output_models/test_model/dynamic_causal_transformer.py\", \"DynamicCasualLM\", ['./../model_src', '../../../../model_src/bits', './output_models/test_model'])\n",
       "\n",
       "def construct(\n",
       "):\n",
       "    meta = {\n",
       "        'config_name': 'Model Test Harness',\n",
       "        'config_description': 'Test and debug new models before training.',\n",
       "        'project_dir': '.',\n",
       "        'models_dir': './output_models',\n",
       "        'tokenizers_dir': '../../../../tokenizers',\n",
       "        'datasets_dir': '../../../../datasets',\n",
       "        'output_dir': './output_models/test_model',\n",
       "        'model_src_dir': '../../../../model_src',\n",
       "    }\n",
       "\n",
       "    tokenizer = load_from_config(\n",
       "        project_dir='../../../../examples/tokenizers/tiny_stories_bpe',\n",
       "        config_template='2k.yaml',\n",
       "    )\n",
       "\n",
       "    model_code_writer = write_file(\n",
       "        data=(\n",
       "            '# See: https://huggingface.co/docs/transformers/custom_models\\n'\n",
       "            'from typing import Optional, Tuple, Callable\\n'\n",
       "            'from abc import abstractmethod\\n'\n",
       "            '\\n'\n",
       "            'from torch import nn, Tensor, LongTensor, FloatTensor\\n'\n",
       "            'from transformers.modeling_outputs import CausalLMOutput\\n'\n",
       "            'from transformers import (\\n'\n",
       "            '    PreTrainedModel,\\n'\n",
       "            '    PretrainedConfig,\\n'\n",
       "            '    AutoConfig,\\n'\n",
       "            '    AutoModelForCausalLM,\\n'\n",
       "            ')\\n'\n",
       "            '\\n'\n",
       "            'from torch.nn import Linear\\n'\n",
       "            'from .causal_multihead_attn import CausalMultiheadAttn\\n'\n",
       "            'from .post_ln_layer import PostLNLayer\\n'\n",
       "            'from .layer_stack import LayerStack\\n'\n",
       "            'from .feedforward_layer import FeedforwardLayer\\n'\n",
       "            'from torch.nn import LayerNorm\\n'\n",
       "            'from .sinusoidal_pe import SinusoidalPE\\n'\n",
       "            'from .causal_lm import CasualLM\\n'\n",
       "            'from .init_weights import InitWeights\\n'\n",
       "            'from .causal_loss import CausalLoss\\n'\n",
       "            'from .input_encoder import InputEncoder\\n'\n",
       "            '\\n'\n",
       "            'model_type = \"forgather-dynamic-causal-transformer\"\\n'\n",
       "            '\\n'\n",
       "            '\\n'\n",
       "            'class DynamicCausalLMConfig(PretrainedConfig):\\n'\n",
       "            '    model_type = model_type\\n'\n",
       "            '\\n'\n",
       "            '\\n'\n",
       "            'class DynamicCasualLM(PreTrainedModel):\\n'\n",
       "            '    config_class = DynamicCausalLMConfig\\n'\n",
       "            '    model_type = model_type\\n'\n",
       "            '\\n'\n",
       "            '    def __init__(self, config: PretrainedConfig):\\n'\n",
       "            '        super().__init__(config)\\n'\n",
       "            '        self.causal_lm = self.construct_model(**config.to_dict())\\n'\n",
       "            '\\n'\n",
       "            '    @staticmethod\\n'\n",
       "            '    def construct_model(\\n'\n",
       "            '        residual_dropout,\\n'\n",
       "            '        hidden_size,\\n'\n",
       "            '        vocab_size,\\n'\n",
       "            '        dim_feedforward,\\n'\n",
       "            '        max_sequence_length,\\n'\n",
       "            '        num_attention_heads,\\n'\n",
       "            '        attention_dropout,\\n'\n",
       "            '        num_hidden_layers,\\n'\n",
       "            '        activation_dropout,\\n'\n",
       "            '        embedding_dropout,\\n'\n",
       "            '        initializer_range,\\n'\n",
       "            '        layer_dropout,\\n'\n",
       "            '        **kwargs\\n'\n",
       "            '    ):\\n'\n",
       "            '        loss_fn_factory = lambda: CausalLoss()\\n'\n",
       "            '\\n'\n",
       "            '        positional_encoder_factory = lambda: SinusoidalPE(\\n'\n",
       "            '            d_model=hidden_size,\\n'\n",
       "            '            max_sequence_length=max_sequence_length,\\n'\n",
       "            '        )\\n'\n",
       "            '\\n'\n",
       "            '        input_encoder_factory = lambda: InputEncoder(\\n'\n",
       "            '            d_model=hidden_size,\\n'\n",
       "            '            vocab_size=vocab_size,\\n'\n",
       "            '            dropout=embedding_dropout,\\n'\n",
       "            '            positional_encoder=positional_encoder_factory(),\\n'\n",
       "            '        )\\n'\n",
       "            '\\n'\n",
       "            '        output_decoder_factory = lambda: Linear(\\n'\n",
       "            '            hidden_size,\\n'\n",
       "            '            vocab_size,\\n'\n",
       "            '        )\\n'\n",
       "            '\\n'\n",
       "            '        feedforward_factory = lambda: FeedforwardLayer(\\n'\n",
       "            '            d_model=hidden_size,\\n'\n",
       "            '            d_feedforward=dim_feedforward,\\n'\n",
       "            '            dropout=activation_dropout,\\n'\n",
       "            '        )\\n'\n",
       "            '\\n'\n",
       "            '        attention_factory = lambda: CausalMultiheadAttn(\\n'\n",
       "            '            d_model=hidden_size,\\n'\n",
       "            '            num_heads=num_attention_heads,\\n'\n",
       "            '            dropout=attention_dropout,\\n'\n",
       "            '        )\\n'\n",
       "            '\\n'\n",
       "            '        layer_norm_factory = lambda: LayerNorm(\\n'\n",
       "            '            normalized_shape=hidden_size,\\n'\n",
       "            '        )\\n'\n",
       "            '\\n'\n",
       "            '        layer_factory = lambda: PostLNLayer(\\n'\n",
       "            '            feedforward=feedforward_factory(),\\n'\n",
       "            '            attention=attention_factory(),\\n'\n",
       "            '            norm1=layer_norm_factory(),\\n'\n",
       "            '            norm2=layer_norm_factory(),\\n'\n",
       "            '            dropout=layer_dropout,\\n'\n",
       "            '            residual_dropout=residual_dropout,\\n'\n",
       "            '        )\\n'\n",
       "            '\\n'\n",
       "            '        layer_stack_factory = lambda: LayerStack(\\n'\n",
       "            '            layer_factory=layer_factory,\\n'\n",
       "            '            num_hidden_layers=num_hidden_layers,\\n'\n",
       "            '        )\\n'\n",
       "            '\\n'\n",
       "            '        init_weights_factory = lambda: InitWeights(\\n'\n",
       "            '            std=initializer_range,\\n'\n",
       "            '        )\\n'\n",
       "            '\\n'\n",
       "            '        model_factory = CasualLM(\\n'\n",
       "            '            loss_fn=loss_fn_factory(),\\n'\n",
       "            '            input_encoder=input_encoder_factory(),\\n'\n",
       "            '            output_decoder=output_decoder_factory(),\\n'\n",
       "            '            layer_stack=layer_stack_factory(),\\n'\n",
       "            '            init_weights=init_weights_factory(),\\n'\n",
       "            '        )\\n'\n",
       "            '        \\n'\n",
       "            '        return model_factory\\n'\n",
       "            '\\n'\n",
       "            '    def forward(\\n'\n",
       "            '        self,\\n'\n",
       "            '        input_ids: LongTensor,\\n'\n",
       "            '        labels: Optional[LongTensor] = None,\\n'\n",
       "            '        position_ids: Optional[LongTensor] = None,\\n'\n",
       "            '        attention_mask: Optional[FloatTensor] = None,\\n'\n",
       "            '        return_dict: bool = False,\\n'\n",
       "            '        **kwargs,\\n'\n",
       "            '    ) -> CausalLMOutput | Tuple[FloatTensor, dict[str, FloatTensor]] | FloatTensor:\\n'\n",
       "            '\\n'\n",
       "            '        outputs = self.causal_lm(\\n'\n",
       "            '            input_ids=input_ids,\\n'\n",
       "            '            labels=labels,\\n'\n",
       "            '            position_ids=position_ids,\\n'\n",
       "            '            attention_mask=attention_mask,\\n'\n",
       "            '            **kwargs,\\n'\n",
       "            '        )\\n'\n",
       "            '\\n'\n",
       "            '        # Return type depends on arguments.\\n'\n",
       "            '        if return_dict:\\n'\n",
       "            '            return CausalLMOutput(**outputs)\\n'\n",
       "            '        elif labels is not None:\\n'\n",
       "            '            return (outputs[\"loss\"], outputs[\"logits\"])\\n'\n",
       "            '        else:\\n'\n",
       "            '            return outputs[\"logits\"]\\n'\n",
       "            '\\n'\n",
       "            '    # Bare-minimum for HF text generation interface to work.\\n'\n",
       "            '    def prepare_inputs_for_generation(self, input_ids, **kwargs):\\n'\n",
       "            '        attention_mask = kwargs.get(\"attention_mask\", None)\\n'\n",
       "            '        model_inputs = {\\n'\n",
       "            '            \"input_ids\": input_ids,\\n'\n",
       "            '            \"attention_mask\": attention_mask,\\n'\n",
       "            '        }\\n'\n",
       "            '        return model_inputs\\n'\n",
       "            '\\n'\n",
       "            '\\n'\n",
       "            'AutoConfig.register(model_type, DynamicCausalLMConfig)\\n'\n",
       "            'AutoModelForCausalLM.register(DynamicCausalLMConfig, DynamicCasualLM)'\n",
       "        ),\n",
       "        output_file='./output_models/test_model/dynamic_causal_transformer.py',\n",
       "        return_value='Model constructor generated by Forgather 1.0',\n",
       "    )\n",
       "\n",
       "    model_config = DynamicCausalLMConfig()(\n",
       "        auto_map={\n",
       "            'AutoConfig': 'dynamic_causal_transformer.DynamicCausalLMConfig',\n",
       "            'AutoModel': 'dynamic_causal_transformer.DynamicCasualLM',\n",
       "        },\n",
       "        vocab_size=len(\n",
       "            tokenizer,\n",
       "        ),\n",
       "        pad_token_id=tokenizer.pad_token_id,\n",
       "        bos_token_id=tokenizer.bos_token_id,\n",
       "        eos_token_id=tokenizer.eos_token_id,\n",
       "        code_generator=model_code_writer,\n",
       "        hidden_size=256,\n",
       "        num_attention_heads=2,\n",
       "        num_hidden_layers=4,\n",
       "        max_sequence_length=tokenizer.model_max_length,\n",
       "        dim_feedforward=1024,\n",
       "        initializer_range=0.02,\n",
       "        embedding_dropout=0.0,\n",
       "        layer_dropout=0.0,\n",
       "        residual_dropout=0.0,\n",
       "        attention_dropout=0.0,\n",
       "        activation_dropout=0.0,\n",
       "    )\n",
       "\n",
       "    pretrained_model = DynamicCasualLM()(\n",
       "        model_config,\n",
       "    )\n",
       "\n",
       "    model = dependency_list(\n",
       "        pretrained_model,\n",
       "        copy_package_files(\n",
       "            './output_models/test_model',\n",
       "            model_config,\n",
       "        ),\n",
       "        copy_package_files(\n",
       "            './output_models/test_model',\n",
       "            pretrained_model,\n",
       "        ),\n",
       "    )\n",
       "    \n",
       "    return {\n",
       "        'meta': meta,\n",
       "        'main': {\n",
       "            'model': model,\n",
       "            'tokenizer': tokenizer,\n",
       "            'model_config': model_config,\n",
       "            'generated_code': (\n",
       "                '# See: https://huggingface.co/docs/transformers/custom_models\\n'\n",
       "                'from typing import Optional, Tuple, Callable\\n'\n",
       "                'from abc import abstractmethod\\n'\n",
       "                '\\n'\n",
       "                'from torch import nn, Tensor, LongTensor, FloatTensor\\n'\n",
       "                'from transformers.modeling_outputs import CausalLMOutput\\n'\n",
       "                'from transformers import (\\n'\n",
       "                '    PreTrainedModel,\\n'\n",
       "                '    PretrainedConfig,\\n'\n",
       "                '    AutoConfig,\\n'\n",
       "                '    AutoModelForCausalLM,\\n'\n",
       "                ')\\n'\n",
       "                '\\n'\n",
       "                'from torch.nn import Linear\\n'\n",
       "                'from .causal_multihead_attn import CausalMultiheadAttn\\n'\n",
       "                'from .post_ln_layer import PostLNLayer\\n'\n",
       "                'from .layer_stack import LayerStack\\n'\n",
       "                'from .feedforward_layer import FeedforwardLayer\\n'\n",
       "                'from torch.nn import LayerNorm\\n'\n",
       "                'from .sinusoidal_pe import SinusoidalPE\\n'\n",
       "                'from .causal_lm import CasualLM\\n'\n",
       "                'from .init_weights import InitWeights\\n'\n",
       "                'from .causal_loss import CausalLoss\\n'\n",
       "                'from .input_encoder import InputEncoder\\n'\n",
       "                '\\n'\n",
       "                'model_type = \"forgather-dynamic-causal-transformer\"\\n'\n",
       "                '\\n'\n",
       "                '\\n'\n",
       "                'class DynamicCausalLMConfig(PretrainedConfig):\\n'\n",
       "                '    model_type = model_type\\n'\n",
       "                '\\n'\n",
       "                '\\n'\n",
       "                'class DynamicCasualLM(PreTrainedModel):\\n'\n",
       "                '    config_class = DynamicCausalLMConfig\\n'\n",
       "                '    model_type = model_type\\n'\n",
       "                '\\n'\n",
       "                '    def __init__(self, config: PretrainedConfig):\\n'\n",
       "                '        super().__init__(config)\\n'\n",
       "                '        self.causal_lm = self.construct_model(**config.to_dict())\\n'\n",
       "                '\\n'\n",
       "                '    @staticmethod\\n'\n",
       "                '    def construct_model(\\n'\n",
       "                '        residual_dropout,\\n'\n",
       "                '        hidden_size,\\n'\n",
       "                '        vocab_size,\\n'\n",
       "                '        dim_feedforward,\\n'\n",
       "                '        max_sequence_length,\\n'\n",
       "                '        num_attention_heads,\\n'\n",
       "                '        attention_dropout,\\n'\n",
       "                '        num_hidden_layers,\\n'\n",
       "                '        activation_dropout,\\n'\n",
       "                '        embedding_dropout,\\n'\n",
       "                '        initializer_range,\\n'\n",
       "                '        layer_dropout,\\n'\n",
       "                '        **kwargs\\n'\n",
       "                '    ):\\n'\n",
       "                '        loss_fn_factory = lambda: CausalLoss()\\n'\n",
       "                '\\n'\n",
       "                '        positional_encoder_factory = lambda: SinusoidalPE(\\n'\n",
       "                '            d_model=hidden_size,\\n'\n",
       "                '            max_sequence_length=max_sequence_length,\\n'\n",
       "                '        )\\n'\n",
       "                '\\n'\n",
       "                '        input_encoder_factory = lambda: InputEncoder(\\n'\n",
       "                '            d_model=hidden_size,\\n'\n",
       "                '            vocab_size=vocab_size,\\n'\n",
       "                '            dropout=embedding_dropout,\\n'\n",
       "                '            positional_encoder=positional_encoder_factory(),\\n'\n",
       "                '        )\\n'\n",
       "                '\\n'\n",
       "                '        output_decoder_factory = lambda: Linear(\\n'\n",
       "                '            hidden_size,\\n'\n",
       "                '            vocab_size,\\n'\n",
       "                '        )\\n'\n",
       "                '\\n'\n",
       "                '        feedforward_factory = lambda: FeedforwardLayer(\\n'\n",
       "                '            d_model=hidden_size,\\n'\n",
       "                '            d_feedforward=dim_feedforward,\\n'\n",
       "                '            dropout=activation_dropout,\\n'\n",
       "                '        )\\n'\n",
       "                '\\n'\n",
       "                '        attention_factory = lambda: CausalMultiheadAttn(\\n'\n",
       "                '            d_model=hidden_size,\\n'\n",
       "                '            num_heads=num_attention_heads,\\n'\n",
       "                '            dropout=attention_dropout,\\n'\n",
       "                '        )\\n'\n",
       "                '\\n'\n",
       "                '        layer_norm_factory = lambda: LayerNorm(\\n'\n",
       "                '            normalized_shape=hidden_size,\\n'\n",
       "                '        )\\n'\n",
       "                '\\n'\n",
       "                '        layer_factory = lambda: PostLNLayer(\\n'\n",
       "                '            feedforward=feedforward_factory(),\\n'\n",
       "                '            attention=attention_factory(),\\n'\n",
       "                '            norm1=layer_norm_factory(),\\n'\n",
       "                '            norm2=layer_norm_factory(),\\n'\n",
       "                '            dropout=layer_dropout,\\n'\n",
       "                '            residual_dropout=residual_dropout,\\n'\n",
       "                '        )\\n'\n",
       "                '\\n'\n",
       "                '        layer_stack_factory = lambda: LayerStack(\\n'\n",
       "                '            layer_factory=layer_factory,\\n'\n",
       "                '            num_hidden_layers=num_hidden_layers,\\n'\n",
       "                '        )\\n'\n",
       "                '\\n'\n",
       "                '        init_weights_factory = lambda: InitWeights(\\n'\n",
       "                '            std=initializer_range,\\n'\n",
       "                '        )\\n'\n",
       "                '\\n'\n",
       "                '        model_factory = CasualLM(\\n'\n",
       "                '            loss_fn=loss_fn_factory(),\\n'\n",
       "                '            input_encoder=input_encoder_factory(),\\n'\n",
       "                '            output_decoder=output_decoder_factory(),\\n'\n",
       "                '            layer_stack=layer_stack_factory(),\\n'\n",
       "                '            init_weights=init_weights_factory(),\\n'\n",
       "                '        )\\n'\n",
       "                '        \\n'\n",
       "                '        return model_factory\\n'\n",
       "                '\\n'\n",
       "                '    def forward(\\n'\n",
       "                '        self,\\n'\n",
       "                '        input_ids: LongTensor,\\n'\n",
       "                '        labels: Optional[LongTensor] = None,\\n'\n",
       "                '        position_ids: Optional[LongTensor] = None,\\n'\n",
       "                '        attention_mask: Optional[FloatTensor] = None,\\n'\n",
       "                '        return_dict: bool = False,\\n'\n",
       "                '        **kwargs,\\n'\n",
       "                '    ) -> CausalLMOutput | Tuple[FloatTensor, dict[str, FloatTensor]] | FloatTensor:\\n'\n",
       "                '\\n'\n",
       "                '        outputs = self.causal_lm(\\n'\n",
       "                '            input_ids=input_ids,\\n'\n",
       "                '            labels=labels,\\n'\n",
       "                '            position_ids=position_ids,\\n'\n",
       "                '            attention_mask=attention_mask,\\n'\n",
       "                '            **kwargs,\\n'\n",
       "                '        )\\n'\n",
       "                '\\n'\n",
       "                '        # Return type depends on arguments.\\n'\n",
       "                '        if return_dict:\\n'\n",
       "                '            return CausalLMOutput(**outputs)\\n'\n",
       "                '        elif labels is not None:\\n'\n",
       "                '            return (outputs[\"loss\"], outputs[\"logits\"])\\n'\n",
       "                '        else:\\n'\n",
       "                '            return outputs[\"logits\"]\\n'\n",
       "                '\\n'\n",
       "                '    # Bare-minimum for HF text generation interface to work.\\n'\n",
       "                '    def prepare_inputs_for_generation(self, input_ids, **kwargs):\\n'\n",
       "                '        attention_mask = kwargs.get(\"attention_mask\", None)\\n'\n",
       "                '        model_inputs = {\\n'\n",
       "                '            \"input_ids\": input_ids,\\n'\n",
       "                '            \"attention_mask\": attention_mask,\\n'\n",
       "                '        }\\n'\n",
       "                '        return model_inputs\\n'\n",
       "                '\\n'\n",
       "                '\\n'\n",
       "                'AutoConfig.register(model_type, DynamicCausalLMConfig)\\n'\n",
       "                'AutoModelForCausalLM.register(DynamicCausalLMConfig, DynamicCasualLM)'\n",
       "            ),\n",
       "        },\n",
       "    }\n",
       "\n",
       "```\n",
       "\n",
       "## Constructed Project\n",
       "\n",
       "```python\n",
       "{'main': {'generated_code': '# See: '\n",
       "                            'https://huggingface.co/docs/transformers/custom_models\\n'\n",
       "                            'from typing import Optional, Tuple, Callable\\n'\n",
       "                            'from abc import abstractmethod\\n'\n",
       "                            '\\n'\n",
       "                            'from torch import nn, Tensor, LongTensor, '\n",
       "                            'FloatTensor\\n'\n",
       "                            'from transformers.modeling_outputs import '\n",
       "                            'CausalLMOutput\\n'\n",
       "                            'from transformers import (\\n'\n",
       "                            '    PreTrainedModel,\\n'\n",
       "                            '    PretrainedConfig,\\n'\n",
       "                            '    AutoConfig,\\n'\n",
       "                            '    AutoModelForCausalLM,\\n'\n",
       "                            ')\\n'\n",
       "                            '\\n'\n",
       "                            'from torch.nn import Linear\\n'\n",
       "                            'from .causal_multihead_attn import '\n",
       "                            'CausalMultiheadAttn\\n'\n",
       "                            'from .post_ln_layer import PostLNLayer\\n'\n",
       "                            'from .layer_stack import LayerStack\\n'\n",
       "                            'from .feedforward_layer import FeedforwardLayer\\n'\n",
       "                            'from torch.nn import LayerNorm\\n'\n",
       "                            'from .sinusoidal_pe import SinusoidalPE\\n'\n",
       "                            'from .causal_lm import CasualLM\\n'\n",
       "                            'from .init_weights import InitWeights\\n'\n",
       "                            'from .causal_loss import CausalLoss\\n'\n",
       "                            'from .input_encoder import InputEncoder\\n'\n",
       "                            '\\n'\n",
       "                            'model_type = '\n",
       "                            '\"forgather-dynamic-causal-transformer\"\\n'\n",
       "                            '\\n'\n",
       "                            '\\n'\n",
       "                            'class DynamicCausalLMConfig(PretrainedConfig):\\n'\n",
       "                            '    model_type = model_type\\n'\n",
       "                            '\\n'\n",
       "                            '\\n'\n",
       "                            'class DynamicCasualLM(PreTrainedModel):\\n'\n",
       "                            '    config_class = DynamicCausalLMConfig\\n'\n",
       "                            '    model_type = model_type\\n'\n",
       "                            '\\n'\n",
       "                            '    def __init__(self, config: '\n",
       "                            'PretrainedConfig):\\n'\n",
       "                            '        super().__init__(config)\\n'\n",
       "                            '        self.causal_lm = '\n",
       "                            'self.construct_model(**config.to_dict())\\n'\n",
       "                            '\\n'\n",
       "                            '    @staticmethod\\n'\n",
       "                            '    def construct_model(\\n'\n",
       "                            '        residual_dropout,\\n'\n",
       "                            '        hidden_size,\\n'\n",
       "                            '        vocab_size,\\n'\n",
       "                            '        dim_feedforward,\\n'\n",
       "                            '        max_sequence_length,\\n'\n",
       "                            '        num_attention_heads,\\n'\n",
       "                            '        attention_dropout,\\n'\n",
       "                            '        num_hidden_layers,\\n'\n",
       "                            '        activation_dropout,\\n'\n",
       "                            '        embedding_dropout,\\n'\n",
       "                            '        initializer_range,\\n'\n",
       "                            '        layer_dropout,\\n'\n",
       "                            '        **kwargs\\n'\n",
       "                            '    ):\\n'\n",
       "                            '        loss_fn_factory = lambda: CausalLoss()\\n'\n",
       "                            '\\n'\n",
       "                            '        positional_encoder_factory = lambda: '\n",
       "                            'SinusoidalPE(\\n'\n",
       "                            '            d_model=hidden_size,\\n'\n",
       "                            '            '\n",
       "                            'max_sequence_length=max_sequence_length,\\n'\n",
       "                            '        )\\n'\n",
       "                            '\\n'\n",
       "                            '        input_encoder_factory = lambda: '\n",
       "                            'InputEncoder(\\n'\n",
       "                            '            d_model=hidden_size,\\n'\n",
       "                            '            vocab_size=vocab_size,\\n'\n",
       "                            '            dropout=embedding_dropout,\\n'\n",
       "                            '            '\n",
       "                            'positional_encoder=positional_encoder_factory(),\\n'\n",
       "                            '        )\\n'\n",
       "                            '\\n'\n",
       "                            '        output_decoder_factory = lambda: Linear(\\n'\n",
       "                            '            hidden_size,\\n'\n",
       "                            '            vocab_size,\\n'\n",
       "                            '        )\\n'\n",
       "                            '\\n'\n",
       "                            '        feedforward_factory = lambda: '\n",
       "                            'FeedforwardLayer(\\n'\n",
       "                            '            d_model=hidden_size,\\n'\n",
       "                            '            d_feedforward=dim_feedforward,\\n'\n",
       "                            '            dropout=activation_dropout,\\n'\n",
       "                            '        )\\n'\n",
       "                            '\\n'\n",
       "                            '        attention_factory = lambda: '\n",
       "                            'CausalMultiheadAttn(\\n'\n",
       "                            '            d_model=hidden_size,\\n'\n",
       "                            '            num_heads=num_attention_heads,\\n'\n",
       "                            '            dropout=attention_dropout,\\n'\n",
       "                            '        )\\n'\n",
       "                            '\\n'\n",
       "                            '        layer_norm_factory = lambda: LayerNorm(\\n'\n",
       "                            '            normalized_shape=hidden_size,\\n'\n",
       "                            '        )\\n'\n",
       "                            '\\n'\n",
       "                            '        layer_factory = lambda: PostLNLayer(\\n'\n",
       "                            '            feedforward=feedforward_factory(),\\n'\n",
       "                            '            attention=attention_factory(),\\n'\n",
       "                            '            norm1=layer_norm_factory(),\\n'\n",
       "                            '            norm2=layer_norm_factory(),\\n'\n",
       "                            '            dropout=layer_dropout,\\n'\n",
       "                            '            residual_dropout=residual_dropout,\\n'\n",
       "                            '        )\\n'\n",
       "                            '\\n'\n",
       "                            '        layer_stack_factory = lambda: '\n",
       "                            'LayerStack(\\n'\n",
       "                            '            layer_factory=layer_factory,\\n'\n",
       "                            '            num_hidden_layers=num_hidden_layers,\\n'\n",
       "                            '        )\\n'\n",
       "                            '\\n'\n",
       "                            '        init_weights_factory = lambda: '\n",
       "                            'InitWeights(\\n'\n",
       "                            '            std=initializer_range,\\n'\n",
       "                            '        )\\n'\n",
       "                            '\\n'\n",
       "                            '        model_factory = CasualLM(\\n'\n",
       "                            '            loss_fn=loss_fn_factory(),\\n'\n",
       "                            '            '\n",
       "                            'input_encoder=input_encoder_factory(),\\n'\n",
       "                            '            '\n",
       "                            'output_decoder=output_decoder_factory(),\\n'\n",
       "                            '            layer_stack=layer_stack_factory(),\\n'\n",
       "                            '            init_weights=init_weights_factory(),\\n'\n",
       "                            '        )\\n'\n",
       "                            '        \\n'\n",
       "                            '        return model_factory\\n'\n",
       "                            '\\n'\n",
       "                            '    def forward(\\n'\n",
       "                            '        self,\\n'\n",
       "                            '        input_ids: LongTensor,\\n'\n",
       "                            '        labels: Optional[LongTensor] = None,\\n'\n",
       "                            '        position_ids: Optional[LongTensor] = '\n",
       "                            'None,\\n'\n",
       "                            '        attention_mask: Optional[FloatTensor] = '\n",
       "                            'None,\\n'\n",
       "                            '        return_dict: bool = False,\\n'\n",
       "                            '        **kwargs,\\n'\n",
       "                            '    ) -> CausalLMOutput | Tuple[FloatTensor, '\n",
       "                            'dict[str, FloatTensor]] | FloatTensor:\\n'\n",
       "                            '\\n'\n",
       "                            '        outputs = self.causal_lm(\\n'\n",
       "                            '            input_ids=input_ids,\\n'\n",
       "                            '            labels=labels,\\n'\n",
       "                            '            position_ids=position_ids,\\n'\n",
       "                            '            attention_mask=attention_mask,\\n'\n",
       "                            '            **kwargs,\\n'\n",
       "                            '        )\\n'\n",
       "                            '\\n'\n",
       "                            '        # Return type depends on arguments.\\n'\n",
       "                            '        if return_dict:\\n'\n",
       "                            '            return CausalLMOutput(**outputs)\\n'\n",
       "                            '        elif labels is not None:\\n'\n",
       "                            '            return (outputs[\"loss\"], '\n",
       "                            'outputs[\"logits\"])\\n'\n",
       "                            '        else:\\n'\n",
       "                            '            return outputs[\"logits\"]\\n'\n",
       "                            '\\n'\n",
       "                            '    # Bare-minimum for HF text generation '\n",
       "                            'interface to work.\\n'\n",
       "                            '    def prepare_inputs_for_generation(self, '\n",
       "                            'input_ids, **kwargs):\\n'\n",
       "                            '        attention_mask = '\n",
       "                            'kwargs.get(\"attention_mask\", None)\\n'\n",
       "                            '        model_inputs = {\\n'\n",
       "                            '            \"input_ids\": input_ids,\\n'\n",
       "                            '            \"attention_mask\": attention_mask,\\n'\n",
       "                            '        }\\n'\n",
       "                            '        return model_inputs\\n'\n",
       "                            '\\n'\n",
       "                            '\\n'\n",
       "                            'AutoConfig.register(model_type, '\n",
       "                            'DynamicCausalLMConfig)\\n'\n",
       "                            'AutoModelForCausalLM.register(DynamicCausalLMConfig, '\n",
       "                            'DynamicCasualLM)',\n",
       "          'model': DynamicCasualLM(\n",
       "  (causal_lm): CasualLM(\n",
       "    loss_fn=CausalLoss(), init_weights=InitWeights(std=0.02)\n",
       "    (input_encoder): InputEncoder(\n",
       "      d_model=256, vocab_size=2000, embedding_scale=16.0\n",
       "      (dropout): Identity()\n",
       "      (embedding): Embedding(2000, 256)\n",
       "      (positional_encoder): SinusoidalPE(d_model=256, max_sequence_length=2048)\n",
       "    )\n",
       "    (output_decoder): Linear(in_features=256, out_features=2000, bias=True)\n",
       "    (layer_stack): LayerStack(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x PostLNLayer(\n",
       "          (feedforward): FeedforwardLayer(\n",
       "            d_model=256, d_feedforward=1024\n",
       "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dropout): Identity()\n",
       "            (activation): ReLU()\n",
       "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (attention): CausalMultiheadAttn(\n",
       "            d_model=256, num_heads=2\n",
       "            (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Identity()\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Identity()\n",
       "          (residual_dropout): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "),\n",
       "          'model_config': DynamicCausalLMConfig {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"dynamic_causal_transformer.DynamicCausalLMConfig\",\n",
       "    \"AutoModel\": \"dynamic_causal_transformer.DynamicCasualLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 0,\n",
       "  \"code_generator\": \"Model constructor generated by Forgather 1.0\",\n",
       "  \"dim_feedforward\": 1024,\n",
       "  \"embedding_dropout\": 0.0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_dropout\": 0.0,\n",
       "  \"max_sequence_length\": 2048,\n",
       "  \"model_type\": \"forgather-dynamic-causal-transformer\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"residual_dropout\": 0.0,\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"vocab_size\": 2000\n",
       "}\n",
       ",\n",
       "          'tokenizer': PreTrainedTokenizerFast(name_or_path='../../../../tokenizers/tiny_stories_2k', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|PAD|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}},\n",
       " 'meta': {'config_description': 'Test and debug new models before training.',\n",
       "          'config_name': 'Model Test Harness',\n",
       "          'datasets_dir': '../../../../datasets',\n",
       "          'model_src_dir': '../../../../model_src',\n",
       "          'models_dir': './output_models',\n",
       "          'output_dir': './output_models/test_model',\n",
       "          'project_dir': '.',\n",
       "          'tokenizers_dir': '../../../../tokenizers'}}\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.ml.notebooks as nb\n",
    "\n",
    "# Set 'test_model' to a model template defined in the main project.\n",
    "nb.display_project_index(materialize=True, pp_first=True, test_model=\"models/control.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0a06f-f425-41ef-878b-017a2d2c2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e18857-ee80-4f33-b38f-c3088dbf51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forgather.ml.notebooks as nb\n",
    "\n",
    "# Set 'test_model' to a model template defined in the main project.\n",
    "nb.display_project_index(materialize=True, pp_first=True, test_model=\"models/walsh_pe.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef1879-e6bf-4413-a05e-fc1e7396072f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
