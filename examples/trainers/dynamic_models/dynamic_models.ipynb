{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../../notebooks/forgather.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Dynamic Models\n",
       "\n",
       "This is a demonstraction of how to perform model archetecture experiments by using the configuration system to dynamically change module types.\n",
       "\n",
       "As most of the examples, we use \"Tiny Causal\" as a baseline, then make various changes for comparison.\n",
       "\n",
       "### Common Configuration\n",
       "- Tokenizer: tokenizers/tiny_2k_bpe.yaml\n",
       "    - Vocabulary Size: 2000\n",
       "    - Maximum Model Sequence: 2048\n",
       "- Dataset: datasets/tiny/tiny_stories_abridged.yaml\n",
       "    - Dataset ID: roneneldan/TinyStories\n",
       "    - Reference: https://arxiv.org/abs/2305.07759\n",
       "    - Train Select Range: 10% \n",
       "- Model:\n",
       "    - Model Dimension: 256\n",
       "    - MLP Dimension: 1024\n",
       "    - Layers: 4\n",
       "    - Heads: 2\n",
       "    - All Dropout Probabilities: 0.0\n",
       "- Trainer:\n",
       "    - Class: aiws.trainer.Trainer\n",
       "    - Epochs: 1\n",
       "    - Initial Learning Rate: 1.0e-3\n",
       "    - Train Batch Size: 32\n",
       "    - LR Sheduler: Cosine\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "    - [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/forgather_workspace](../../../forgather_workspace)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/tiny_experiments](../../../templates/tiny_experiments)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/modellib](../../../templates/modellib)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/base](../../../templates/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [pre_ln.yaml](templates/experiments/pre_ln.yaml)\n",
       "- [walsh_pe.yaml](templates/experiments/walsh_pe.yaml)\n",
       "- [relu-glu.yaml](templates/experiments/relu-glu.yaml)\n",
       "- [swish.yaml](templates/experiments/swish.yaml)\n",
       "- [swi-glu.yaml](templates/experiments/swi-glu.yaml)\n",
       "- [control.yaml](templates/experiments/control.yaml)\n",
       "\n",
       "Default Configuration: control.yaml\n",
       "\n",
       "Active Configuration: control.yaml\n",
       "\n",
       "## Included Templates\n",
       "- [experiments/control.yaml](templates/experiments/control.yaml)\n",
       "    - [models/control.yaml](templates/models/control.yaml)\n",
       "        - [models/tiny/tiny_causal.yaml](../../../templates/tiny_experiments/models/tiny/tiny_causal.yaml)\n",
       "            - [tokenizers/tiny_2k.yaml](../../../templates/tiny_experiments/tokenizers/tiny_2k.yaml)\n",
       "            - [models/dynamic_causal_transformer.yaml](../../../templates/modellib/models/dynamic_causal_transformer.yaml)\n",
       "                - [models/abstract/dynamic_causal_lm.yaml](../../../templates/base/models/abstract/dynamic_causal_lm.yaml)\n",
       "                    - [models/abstract/custom_causal_lm.yaml](../../../templates/base/models/abstract/custom_causal_lm.yaml)\n",
       "                        - [models/abstract/base_language_model.yaml](../../../templates/base/models/abstract/base_language_model.yaml)\n",
       "                            - [inc/formatting.jinja](../../../templates/base/inc/formatting.jinja)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [projects/tiny.yaml](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "            - [prompts/tiny_stories.yaml](../../../templates/tiny_experiments/prompts/tiny_stories.yaml)\n",
       "            - [types/training_script/causal_lm/causal_lm.yaml](../../../templates/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "                - [trainers/trainer.yaml](../../../templates/base/trainers/trainer.yaml)\n",
       "                    - [trainers/base_trainer.yaml](../../../templates/base/trainers/base_trainer.yaml)\n",
       "                        - [trainers/minimal_trainer.yaml](../../../templates/base/trainers/minimal_trainer.yaml)\n",
       "                - [callbacks/loggers.yaml](../../../templates/base/callbacks/loggers.yaml)\n",
       "                    - [callbacks/base_callbacks.yaml](../../../templates/base/callbacks/base_callbacks.yaml)\n",
       "                - [models/abstract/load_model.yaml](../../../templates/base/models/abstract/load_model.yaml)\n",
       "                    - [models/abstract/causal_lm_from_pretrained.yaml](../../../templates/base/models/abstract/causal_lm_from_pretrained.yaml)\n",
       "                - [types/training_script/training_script.yaml](../../../templates/base/types/training_script/training_script.yaml)\n",
       "                    - [types/type.yaml](../../../templates/base/types/type.yaml)\n",
       "                        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "            - [tiny.callbacks](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "            - [tiny.model_config](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "            - [tiny.trainer_config](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "            - [tiny.dataset_config](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "                - [datasets/tiny/tiny_stories_abridged.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories_abridged.yaml)\n",
       "                    - [datasets/tiny/tiny_stories.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories.yaml)\n",
       "                        - [datasets/abstract/base_datasets.yaml](../../../templates/base/datasets/abstract/base_datasets.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'Tiny Causal; the baseline control',\n",
       " 'config_name': 'Control',\n",
       " 'create_new_model': 'True',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/forgather/datasets',\n",
       " 'eval': 'False',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': './output_models/tiny_causal/runs/control_2025-06-08T19-59-16',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'output_dir': './output_models/tiny_causal',\n",
       " 'project_dir': '.',\n",
       " 'save_model': 'False',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'train': 'True',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Output Targets\n",
       "- meta\n",
       "- main\n",
       "- model_code_writer\n",
       "- distributed_env\n",
       "- model\n",
       "- trainer\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- trainer_callbacks\n",
       "- trainer_args\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#                 Control                \n",
       "#---------------------------------------\n",
       "# 2025-06-08T19:59:16\n",
       "# Description: Tiny Causal; the baseline control\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: tiny_causal\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.7.0\n",
       "#     transformers: 4.51.3\n",
       "#     accelerate: 1.7.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"./output_models/tiny_causal\"\n",
       "# ns.logging_dir: \"./output_models/tiny_causal/runs/control_2025-06-08T19-59-16\"\n",
       "# ns.create_new_model: True\n",
       "# ns.save_model: False\n",
       "# ns.train: True\n",
       "# ns.eval: False\n",
       "# ns.trust_remote_code: True\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       ".define: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "# The model will be given the following prompts for text-gen at regular intervals.\n",
       ".define: &testprompts !list:@testprompts\n",
       "    # Test prompts from \"https://arxiv.org/abs/2305.07759\"\n",
       "    - \"Alice was so tired when she got back home so she went\"\n",
       "    - \"Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was\"\n",
       "    - \"Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \\\"Look, Lily. A rainbow has\"\n",
       "    - \"Jack wanted to read a book, so he went to\"\n",
       "    - \"\\\"Can cows fly?\\\" Alice asked her mother.\"\n",
       "    - \"\\\"What do birds like to eat?\\\" Tom asked his mother.\"\n",
       "    - \"\\\"What language do they speak in France?\\\" Tom asked his mother.\"\n",
       "    - \"If I throw a ball up in the air, eventually it will\"\n",
       "    - \"It was winter and cold outside so his mother told him, \\\"You should\"\n",
       "    - \"Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked\"\n",
       "    - \"Jack told Mary, \\\"If you give me your banana, I'll give you my apple.\\\" Mary gave Jack her Banana, so\"\n",
       "    - \"On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to\"\n",
       "    - \"Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that\"\n",
       "    - \"Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \\\"I want to go to the park\\\". Lily says\"\n",
       "    - \"Jack's mother was not home, and his father was at home. When Jack came home, he said hello to\"\n",
       "    - \"Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says\"\n",
       "    - \"Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They\"\n",
       "    - \"Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door,\"\n",
       "\n",
       "# Conservative text-generation parameters.\n",
       ".define: &generation_config !dict:@generation_config\n",
       "    identity: generation_config\n",
       "    do_sample: True\n",
       "    top_k: 20\n",
       "    top_p: 0.9\n",
       "    temperature: 0.7\n",
       "    repitition_penalty: 1.15\n",
       "\n",
       "################ Model #################\n",
       "\n",
       "# https://huggingface.co/docs/transformers/en/model_doc/auto\n",
       ".define: &model_constructor_args {}\n",
       "\n",
       "# Name: Tiny Causal\n",
       "# Description: A scaled-down version of the base Causal Transformer\n",
       "# model_def.cls = \"DynamicCasualLM\"\n",
       "# model_def.cfg_cls = \"DynamicCausalLMConfig\"\n",
       "# model_def.config_path = \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/output_models/tiny_causal/tiny_causal_transformer.py\"\n",
       "# model_def.model_path = \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/output_models/tiny_causal/tiny_causal_transformer.py\"\n",
       "# model_def.short_name = \"tiny_causal_transformer\"\n",
       "# model_def.model_type = \"forgather-dynamic-causal-tiny_causal_transformer\"\n",
       "# model_def.model_path = \"./output_models/tiny_causal/tiny_causal_transformer.py\"\n",
       "# model_def.model_template_searchpath = \"/home/dinalt/ai_assets/forgather/templates/dynamic_models\"\n",
       "# model_def.model_template_name = \"causal_lm.py\"\n",
       "# model_def.name_policy = \"named\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       "# Load custom tokenizer from sub-project definition\n",
       ".define: &tokenizer !singleton:forgather.ml.construct:load_from_config@tokenizer\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe\"\n",
       "    config_template: \"2k.yaml\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       ".define: &model_submodule_searchpath\n",
       "    - \"./model_src\"\n",
       "    - \"/home/dinalt/ai_assets/forgather/model_src/bits\"\n",
       "    - \"./output_models/tiny_causal\"\n",
       "\n",
       ".define: &loss_fn !singleton:.causal_loss:CausalLoss@loss_fn []\n",
       "\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    normalized_shape: !var \"hidden_size\"\n",
       "\n",
       ".define: &feedforward_factory !lambda:.feedforward_layer:FeedforwardLayer@feedforward_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "    dropout: !var \"activation_dropout\"\n",
       "\n",
       ".define: &attention_factory !lambda:.causal_multihead_attn:CausalMultiheadAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    num_heads: !var \"num_attention_heads\"\n",
       "    dropout: !var \"attention_dropout\"\n",
       "\n",
       ".define: &layer_factory !lambda:.post_ln_layer:PostLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "    dropout: !var \"layer_dropout\"\n",
       "    residual_dropout: !var \"residual_dropout\"\n",
       "\n",
       ".define: &layer_stack !singleton:.layer_stack:LayerStack@layer_stack\n",
       "    layer_factory: *layer_factory\n",
       "    num_hidden_layers: !var \"num_hidden_layers\"\n",
       "\n",
       ".define: &output_decoder !singleton:torch.nn:Linear@output_decoder\n",
       "    - !var \"hidden_size\"\n",
       "    - !var \"vocab_size\"\n",
       "\n",
       ".define: &positional_encoder !singleton:.sinusoidal_pe:SinusoidalPE@positional_encoder\n",
       "    d_model: !var \"hidden_size\"\n",
       "    max_sequence_length: !var \"max_sequence_length\"\n",
       "\n",
       ".define: &input_encoder !singleton:.input_encoder:InputEncoder@input_encoder\n",
       "    d_model: !var \"hidden_size\"\n",
       "    vocab_size: !var \"vocab_size\"\n",
       "    dropout: !var \"embedding_dropout\"\n",
       "    positional_encoder: *positional_encoder\n",
       "\n",
       ".define: &init_weights !lambda:.init_weights:simple_weight_init@init_weights []\n",
       "\n",
       ".define: &model_factory !singleton:.causal_lm:CasualLM@model_factory\n",
       "    loss_fn: *loss_fn\n",
       "    input_encoder: *input_encoder\n",
       "    output_decoder: *output_decoder\n",
       "    layer_stack: *layer_stack\n",
       "    init_weights: *init_weights\n",
       "\n",
       ".define: &model_code_generator !meta:forgather.codegen:generate_code@model_code_generator\n",
       "    searchpath: \"/home/dinalt/ai_assets/forgather/templates/dynamic_models\"\n",
       "    template_name: \"causal_lm.py\"\n",
       "    name_policy: \"named\"\n",
       "    obj: *model_factory\n",
       "    # Template args\n",
       "    model_type: \"forgather-dynamic-causal-tiny_causal_transformer\"\n",
       "\n",
       ".define: &model_code_writer !singleton:forgather.ml.construct:write_file@model_code_writer\n",
       "    data: *model_code_generator\n",
       "    output_file: \"./output_models/tiny_causal/tiny_causal_transformer.py\"\n",
       "    return_value: \"Model constructor generated by Forgather 1.0\"    \n",
       "\n",
       ".define: &model_config !singleton:./output_models/tiny_causal/tiny_causal_transformer.py:DynamicCausalLMConfig@model_config\n",
       "    submodule_searchpath: *model_submodule_searchpath\n",
       "    # Set auto-map for custom model; this ensures that the source code stays with the model.\n",
       "    auto_map:\n",
       "        AutoConfig: \"tiny_causal_transformer.DynamicCausalLMConfig\"\n",
       "        AutoModel: \"tiny_causal_transformer.DynamicCasualLM\"\n",
       "    # Get the vocab-size from the tokenizer definition.\n",
       "    vocab_size: !singleton:len [ *tokenizer ]\n",
       "    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]\n",
       "    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]\n",
       "    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]\n",
       "    # Add dependency on code generator\n",
       "    code_generator: *model_code_writer\n",
       "    hidden_size: 512\n",
       "    num_attention_heads: 8\n",
       "    num_hidden_layers: 6\n",
       "    max_sequence_length: !singleton:getattr\n",
       "        - *tokenizer\n",
       "        - \"model_max_length\"\n",
       "    dim_feedforward: 2048\n",
       "    embedding_dropout: 0.10\n",
       "    layer_dropout: 0.10\n",
       "    residual_dropout: 0.0\n",
       "    attention_dropout: 0.0\n",
       "    activation_dropout: 0.0\n",
       "    \n",
       "    # Tiny Causal overrides\n",
       "    hidden_size: 256\n",
       "    dim_feedforward: 1024\n",
       "    num_attention_heads: 2\n",
       "    num_hidden_layers: 4\n",
       "    embedding_dropout: 0.1\n",
       "    layer_dropout: 0.1\n",
       "\n",
       "# **Model Factory**\n",
       "\n",
       ".define: &pretrained_model !partial:./output_models/tiny_causal/tiny_causal_transformer.py:DynamicCasualLM@pretrained_model\n",
       "    args:\n",
       "        - *model_config\n",
       "    kwargs:\n",
       "        submodule_searchpath: *model_submodule_searchpath\n",
       "        <<: *model_constructor_args\n",
       "\n",
       ".define: &model !partial:forgather.ml.construct:dependency_list@model\n",
       "    - !factory:call [ *pretrained_model ]\n",
       "    - !singleton:forgather.ml.construct:copy_package_files\n",
       "        - \"./output_models/tiny_causal\"\n",
       "        - *model_config\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "# Name: TinyStories Abridged\n",
       "# Define: Abridged to 10% of original size; Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.\n",
       "# Source: https://arxiv.org/abs/2305.07759\n",
       "# Train Dataset: \"roneneldan/TinyStories\" : \"train\"\n",
       "# Eval Dataset: \"roneneldan/TinyStories\" : \"validation\"\n",
       "\n",
       "# **Source Datasets**\n",
       "\n",
       ".define: &train_source_dataset !singleton:datasets:load_dataset@train_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       ".define: &eval_source_dataset !singleton:datasets:load_dataset@eval_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "# **Dataset Splits**\n",
       "\n",
       ".define: &train_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"train\"\n",
       "\n",
       ".define: &eval_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"validation\"\n",
       "\n",
       "# **Preprocess Dataset Args**\n",
       "\n",
       ".define: &preprocess_args\n",
       "    truncation: True\n",
       "\n",
       "# **Preprocessed Datasets**\n",
       "\n",
       ".define: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset\n",
       "    dataset: *train_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 0.1\n",
       "    desc: \"Tokenizing train\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       ".define: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset\n",
       "    dataset: *eval_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 500\n",
       "    desc: \"Tokenizing validation split\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       ".define: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM\n",
       "    tokenizer: *tokenizer\n",
       "    return_tensors: pt\n",
       "\n",
       "    # Tiny Project\n",
       "    # Limit maximum sequence length 512 tokens, at the data-collator level.\n",
       "    truncate_to: 512\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/tiny_causal/runs/control_2025-06-08T19-59-16\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       ".define: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-06-08T19:59:16\"\n",
       "    name: \"Control\"\n",
       "    description: \"Tiny Causal; the baseline control\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.7.0', 'transformers': '4.51.3', 'accelerate': '1.7.0'}\n",
       "\n",
       ".define: &text_gen_callback_args\n",
       "    summary_writer: *summary_writer\n",
       "    prompts: *testprompts\n",
       "    generation_config: *generation_config\n",
       "    max_new_tokens: 40\n",
       "    generation_steps: 2000\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       ".define: &trainer_callbacks !list:@trainer_callbacks\n",
       "    # Log all training output to JSON\n",
       "    - !singleton:forgather.ml.json_logger:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    - !singleton:forgather.ml.tb_logger:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "    - !singleton:forgather.ml.textgen_callback:TextgenCallback\n",
       "        <<: *text_gen_callback_args\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       ".define: &optimizer !lambda:torch:optim.AdamW\n",
       "    lr: 1.0e-3\n",
       "\n",
       "############# LR Scheduler #############\n",
       "\n",
       "# https://arxiv.org/html/2503.02844v1\n",
       ".define: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler\n",
       "    warmup_steps: 5000\n",
       "    cooldown_steps: 50000\n",
       "    constant_lr: 1.0e-4\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: Custom forgather.ml.trainer.Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       ".define: &trainer_args\n",
       "    # Minimal Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    output_dir: \"./output_models/tiny_causal\"\n",
       "    logging_dir: \"./output_models/tiny_causal/runs/control_2025-06-08T19-59-16\"\n",
       "    logging_steps: 500\n",
       "    per_device_train_batch_size: 16\n",
       "    per_device_eval_batch_size: 32\n",
       "    learning_rate: 5.0e-5\n",
       "    num_train_epochs: 1\n",
       "    # Base Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    overwrite_output_dir: True\n",
       "    eval_steps: 100\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"no\"\n",
       "    logging_strategy: \"steps\"\n",
       "\n",
       "    # Tiny Project Overrides\n",
       "    seed: 42\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 64\n",
       "    logging_steps: 100\n",
       "    eval_steps: 500\n",
       "    learning_rate: 1.0e-3\n",
       "    num_train_epochs: 1\n",
       "    dataloader_num_workers: 1\n",
       "\n",
       "    # max_steps: 500\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       ".define: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    model_init: *model\n",
       "    args: !singleton:forgather.ml.trainer_types:TrainingArguments@trainer_args\n",
       "        <<: *trainer_args\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Control\"\n",
       "    config_description: \"Tiny Causal; the baseline control\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/ai_assets/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "    output_dir: \"./output_models/tiny_causal\"\n",
       "    model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "    logging_dir: \"./output_models/tiny_causal/runs/control_2025-06-08T19-59-16\"\n",
       "    create_new_model: \"True\"\n",
       "    save_model: \"False\"\n",
       "    train: \"True\"\n",
       "    eval: \"False\"\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_save: False\n",
       "    do_train: True\n",
       "    do_eval: False\n",
       "    # Init distributed envrionment before initializing anyting which depends on it.\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "    pp_config: !var \"pp_config\"\n",
       "\n",
       "model_code_writer: *model_code_writer\n",
       "distributed_env: *distributed_env\n",
       "model: *model\n",
       "trainer: *trainer\n",
       "train_dataset: *train_dataset\n",
       "eval_dataset: *eval_dataset\n",
       "data_collator: *data_collator\n",
       "trainer_callbacks: *trainer_callbacks\n",
       "trainer_args: *trainer_args\n",
       "optimizer: *optimizer\n",
       "lr_scheduler: *lr_scheduler\n",
       "model_constructor_args: *model_constructor_args\n",
       "tokenizer: *tokenizer\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "nb.display_project_index(config_template=\"\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42bf8639-f405-41a0-99b1-9cfbca25b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "# Load default baseline config\n",
    "proj = Project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd1b052-8595-4c46-b88a-5e8eae9b0113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Script Started *****\n",
      "config_name: Control\n",
      "config_description: Tiny Causal; the baseline control\n",
      "output_dir: ./output_models/tiny_causal\n",
      "logging_dir: ./output_models/tiny_causal/runs/control_2025-06-09T02-17-55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a28d25d029f45b7b3bf5512ddae6b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 212,000\n",
      "total_train_samples: 212,000\n",
      "per_device_train_batch_size: 32\n",
      "actual_per_device_batch_size: 32\n",
      "total_train_batch_size: 32\n",
      "max_steps: 6,625\n",
      "total_parameters: 4.2M\n",
      "trainable_parameters: 4.2M\n",
      "model:\n",
      "DynamicCasualLM(\n",
      "  (causal_lm): CasualLM(\n",
      "    loss_fn=CausalLoss()\n",
      "    (input_encoder): InputEncoder(\n",
      "      d_model=256, vocab_size=2000\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (embedding): Embedding(2000, 256)\n",
      "      (positional_encoder): SinusoidalPE(d_model=256, max_sequence_length=2048)\n",
      "    )\n",
      "    (output_decoder): Linear(in_features=256, out_features=2000, bias=True)\n",
      "    (layer_stack): LayerStack(\n",
      "      (layers): ModuleDict(\n",
      "        (0): PostLNLayer(\n",
      "          (feedforward): FeedforwardLayer(\n",
      "            d_model=256, d_feedforward=1024\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Identity()\n",
      "            (activation): ReLU()\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "          (attention): CausalMultiheadAttn(\n",
      "            d_model=256, num_heads=2\n",
      "            (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (dropout): Identity()\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (residual_dropout): Identity()\n",
      "        )\n",
      "        (1): PostLNLayer(\n",
      "          (feedforward): FeedforwardLayer(\n",
      "            d_model=256, d_feedforward=1024\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Identity()\n",
      "            (activation): ReLU()\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "          (attention): CausalMultiheadAttn(\n",
      "            d_model=256, num_heads=2\n",
      "            (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (dropout): Identity()\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (residual_dropout): Identity()\n",
      "        )\n",
      "        (2): PostLNLayer(\n",
      "          (feedforward): FeedforwardLayer(\n",
      "            d_model=256, d_feedforward=1024\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Identity()\n",
      "            (activation): ReLU()\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "          (attention): CausalMultiheadAttn(\n",
      "            d_model=256, num_heads=2\n",
      "            (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (dropout): Identity()\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (residual_dropout): Identity()\n",
      "        )\n",
      "        (3): PostLNLayer(\n",
      "          (feedforward): FeedforwardLayer(\n",
      "            d_model=256, d_feedforward=1024\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Identity()\n",
      "            (activation): ReLU()\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "          (attention): CausalMultiheadAttn(\n",
      "            d_model=256, num_heads=2\n",
      "            (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (dropout): Identity()\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (residual_dropout): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:17:59          100  0.02  train-loss: 7.40545   learning-rate: 2.00e-05\n",
      "2025-06-09 02:18:01          200  0.03  train-loss: 6.48599   learning-rate: 4.00e-05\n",
      "2025-06-09 02:18:03          300  0.05  train-loss: 5.8017    learning-rate: 6.00e-05\n",
      "2025-06-09 02:18:05          400  0.06  train-loss: 5.34018   learning-rate: 8.00e-05\n",
      "2025-06-09 02:18:06          500  0.08  train-loss: 4.8715    learning-rate: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093faccb05fc4deca136c38645ef1e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:18:07          500  0.08  eval-loss:  5.18221   \n",
      "2025-06-09 02:18:10          600  0.09  train-loss: 4.52426   learning-rate: 1.20e-04\n",
      "2025-06-09 02:18:12          700  0.11  train-loss: 4.25684   learning-rate: 1.40e-04\n",
      "2025-06-09 02:18:14          800  0.12  train-loss: 4.11578   learning-rate: 1.60e-04\n",
      "2025-06-09 02:18:15          900  0.14  train-loss: 3.9168    learning-rate: 1.80e-04\n",
      "2025-06-09 02:18:17        1,000  0.15  train-loss: 3.7224    learning-rate: 2.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437451be91444500a5ab7d98552a95d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:18:17        1,000  0.15  eval-loss:  4.03625   \n",
      "2025-06-09 02:18:19        1,100  0.17  train-loss: 3.68163   learning-rate: 2.20e-04\n",
      "2025-06-09 02:18:21        1,200  0.18  train-loss: 3.60762   learning-rate: 2.40e-04\n",
      "2025-06-09 02:18:23        1,300  0.2   train-loss: 3.52088   learning-rate: 2.60e-04\n",
      "2025-06-09 02:18:24        1,400  0.21  train-loss: 3.4514    learning-rate: 2.80e-04\n",
      "2025-06-09 02:18:26        1,500  0.23  train-loss: 3.39678   learning-rate: 3.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fc65ef2ca14b88a56d25fe79b52918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:18:26        1,500  0.23  eval-loss:  3.5571    \n",
      "2025-06-09 02:18:28        1,600  0.24  train-loss: 3.32648   learning-rate: 3.20e-04\n",
      "2025-06-09 02:18:29        1,700  0.26  train-loss: 3.25139   learning-rate: 3.40e-04\n",
      "2025-06-09 02:18:31        1,800  0.27  train-loss: 3.16096   learning-rate: 3.60e-04\n",
      "2025-06-09 02:18:33        1,900  0.29  train-loss: 3.11015   learning-rate: 3.80e-04\n",
      "2025-06-09 02:18:35        2,000  0.3   train-loss: 3.10194   learning-rate: 4.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6198a37f002644eb90e636a6ca49a778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:18:35        2,000  0.3   eval-loss:  3.18686   \n",
      "2025-06-09 02:18:39        2,100  0.32  train-loss: 3.0213    learning-rate: 4.20e-04\n",
      "2025-06-09 02:18:40        2,200  0.33  train-loss: 2.91586   learning-rate: 4.40e-04\n",
      "2025-06-09 02:18:42        2,300  0.35  train-loss: 2.84726   learning-rate: 4.60e-04\n",
      "2025-06-09 02:18:44        2,400  0.36  train-loss: 2.88745   learning-rate: 4.80e-04\n",
      "2025-06-09 02:18:45        2,500  0.38  train-loss: 2.80327   learning-rate: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df86784497c4d2486bde48a6cd03112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:18:45        2,500  0.38  eval-loss:  2.8713    \n",
      "2025-06-09 02:18:47        2,600  0.39  train-loss: 2.77034   learning-rate: 5.20e-04\n",
      "2025-06-09 02:18:49        2,700  0.41  train-loss: 2.69055   learning-rate: 5.40e-04\n",
      "2025-06-09 02:18:50        2,800  0.42  train-loss: 2.68562   learning-rate: 5.60e-04\n",
      "2025-06-09 02:18:52        2,900  0.44  train-loss: 2.58216   learning-rate: 5.80e-04\n",
      "2025-06-09 02:18:54        3,000  0.45  train-loss: 2.46821   learning-rate: 6.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d019846dff94e2e9bbcac1131e5803e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:18:54        3,000  0.45  eval-loss:  2.59739   \n",
      "2025-06-09 02:18:56        3,100  0.47  train-loss: 2.55538   learning-rate: 6.20e-04\n",
      "2025-06-09 02:18:57        3,200  0.48  train-loss: 2.58311   learning-rate: 6.40e-04\n",
      "2025-06-09 02:18:59        3,300  0.5   train-loss: 2.48109   learning-rate: 6.60e-04\n",
      "2025-06-09 02:19:01        3,400  0.51  train-loss: 2.40831   learning-rate: 6.80e-04\n",
      "2025-06-09 02:19:03        3,500  0.53  train-loss: 2.4158    learning-rate: 7.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fadd055ab534f4e802572f7cd248333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:19:03        3,500  0.53  eval-loss:  2.44907   \n",
      "2025-06-09 02:19:04        3,600  0.54  train-loss: 2.45379   learning-rate: 7.20e-04\n",
      "2025-06-09 02:19:06        3,700  0.56  train-loss: 2.35934   learning-rate: 7.40e-04\n",
      "2025-06-09 02:19:08        3,800  0.57  train-loss: 2.35905   learning-rate: 7.60e-04\n",
      "2025-06-09 02:19:10        3,900  0.59  train-loss: 2.41208   learning-rate: 7.80e-04\n",
      "2025-06-09 02:19:11        4,000  0.6   train-loss: 2.41811   learning-rate: 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff4b06ad8904156a96563f03cb462dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:19:12        4,000  0.6   eval-loss:  2.3598    \n",
      "2025-06-09 02:19:15        4,100  0.62  train-loss: 2.30591   learning-rate: 8.20e-04\n",
      "2025-06-09 02:19:17        4,200  0.63  train-loss: 2.28682   learning-rate: 8.40e-04\n",
      "2025-06-09 02:19:19        4,300  0.65  train-loss: 2.3223    learning-rate: 8.60e-04\n",
      "2025-06-09 02:19:20        4,400  0.66  train-loss: 2.35272   learning-rate: 8.80e-04\n",
      "2025-06-09 02:19:22        4,500  0.68  train-loss: 2.29876   learning-rate: 9.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa02f1302c86452b9c858f78399e2541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:19:22        4,500  0.68  eval-loss:  2.31837   \n",
      "2025-06-09 02:19:24        4,600  0.69  train-loss: 2.2197    learning-rate: 9.20e-04\n",
      "2025-06-09 02:19:26        4,700  0.71  train-loss: 2.21872   learning-rate: 9.40e-04\n",
      "2025-06-09 02:19:28        4,800  0.72  train-loss: 2.24251   learning-rate: 9.60e-04\n",
      "2025-06-09 02:19:29        4,900  0.74  train-loss: 2.24681   learning-rate: 9.80e-04\n",
      "2025-06-09 02:19:31        5,000  0.75  train-loss: 2.25256   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747fedcecfcf4140ac809a8fc6a01edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:19:31        5,000  0.75  eval-loss:  2.2818    \n",
      "2025-06-09 02:19:33        5,100  0.77  train-loss: 2.24558   learning-rate: 1.00e-03\n",
      "2025-06-09 02:19:35        5,200  0.78  train-loss: 2.15454   learning-rate: 1.00e-03\n",
      "2025-06-09 02:19:37        5,300  0.8   train-loss: 2.15086   learning-rate: 1.00e-03\n",
      "2025-06-09 02:19:39        5,400  0.82  train-loss: 2.17233   learning-rate: 1.00e-03\n",
      "2025-06-09 02:19:40        5,500  0.83  train-loss: 2.15159   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9c9ce4785c427e8c14d8a46b6b9ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:19:41        5,500  0.83  eval-loss:  2.20254   \n",
      "2025-06-09 02:19:42        5,600  0.85  train-loss: 2.1846    learning-rate: 1.00e-03\n",
      "2025-06-09 02:19:44        5,700  0.86  train-loss: 2.2152    learning-rate: 1.00e-03\n",
      "2025-06-09 02:19:46        5,800  0.88  train-loss: 2.15351   learning-rate: 9.99e-04\n",
      "2025-06-09 02:19:47        5,900  0.89  train-loss: 2.1599    learning-rate: 9.99e-04\n",
      "2025-06-09 02:19:49        6,000  0.91  train-loss: 2.08832   learning-rate: 9.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121b56e7643c4f86820b945605af010b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:19:49        6,000  0.91  eval-loss:  2.18569   \n",
      "2025-06-09 02:19:53        6,100  0.92  train-loss: 2.06596   learning-rate: 9.99e-04\n",
      "2025-06-09 02:19:55        6,200  0.94  train-loss: 2.12255   learning-rate: 9.99e-04\n",
      "2025-06-09 02:19:56        6,300  0.95  train-loss: 2.10061   learning-rate: 9.98e-04\n",
      "2025-06-09 02:19:58        6,400  0.97  train-loss: 2.08376   learning-rate: 9.98e-04\n",
      "2025-06-09 02:20:00        6,500  0.98  train-loss: 2.10188   learning-rate: 9.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinalt/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8622b7a2ecd349ce9f6c0e5f56f056d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 02:20:00        6,500  0.98  eval-loss:  2.109     \n",
      "2025-06-09 02:20:02        6,600  1.0   train-loss: 2.03015   learning-rate: 9.98e-04\n",
      "2025-06-09 02:20:02        6,625  1.0   train_runtime: 124.8 train_samples: 212,000 step: 6,625 train_samples_per_second: 1.699e+03 train_steps_per_second: 53.1 train_loss: 2.922 epoch: 1.0 \n",
      "**** Training Completed *****\n",
      "{'train_runtime': 124.75879192352295, 'train_samples': 212000, 'step': 6625, 'train_samples_per_second': 1699.279, 'train_steps_per_second': 53.102, 'train_loss': 2.9222500324249268, 'epoch': 1.0}\n",
      "Model saved to: ./output_models/tiny_causal\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "training_script = proj()\n",
    "training_script.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d420fb-9d5f-4067-a7d5-28cb8c34ed69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Generated Shell Script\n",
       "[control.sh](control.sh)\n",
       "```bash\n",
       "#!/bin/bash\n",
       "CUDA_VISIBLE_DEVICES='0' torchrun --standalone --nproc-per-node 'gpu' '/home/dinalt/ai_assets/forgather/scripts/train_script.py' -p '/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models' \"control.yaml\"\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.generate_trainingscript(proj, \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acbf5eef-6292-4026-adb4-bacb376414c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Tensorboard Command\n",
       "\n",
       "```bash\n",
       "tensorboard --bind_all --logdir \"/home/dinalt/ai_assets/forgather/examples/trainers/dynamic_models/output_models/tiny_causal\"\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_tb_command(proj, local_host=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e66aaa-7e3e-43fa-a4ef-74cef2ddc469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
