-- extends 'projects/tiny.yaml'

-- block experiment_metadata
    == super()
    -- set ns.save_model = True
-- endblock experiment_metadata


-- block base_directories
    == super()
    ## Save the output tokenizers in the project directory
    -- set ns.tokenizers_dir = path_join(ns.project_dir, 'tokenizers')
-- endblock base_directories


-- block datasets_definition
    -- include 'project.datasets'
-- endblock datasets_definition


-- block construct_new_model
    -- include 'project.model_config'
-- endblock construct_new_model


-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition

#-------------------- project.datasets --------------------
-- extends 'datasets/tiny/tiny_stories_abridged.yaml'

#-------------------- project.model_config --------------------
-- extends 'models/tiny/tiny.yaml'

## Replace default tokenizer with custom tokenizer
-- block model_tokenizer
    -- include "project.tokenizer"
<< endblock model_tokenizer

#-------------------- project.tokenizer --------------------
-- extends 'tokenizers/tiny_2k_bpe.yaml'

#-------------------- project.trainer_config --------------------
-- extends 'tiny.trainer_config'

-- block trainer_args
    == super()
    # Project Overrides
    # It seems that a CUDA OOM can occur with the smaller tokenizers; reducing batch size.
    per_device_train_batch_size: 32
-- endblock trainer_args