## Project level definitions and overrides go here.
-- extends "types/training_script/causal_lm/causal_lm.yaml"
-- from "model_ctor/args.yaml" import use_flash_attn2

## Set Project level defaults
-- block config_metadata
    == super()
    -- set ns.CONFIG_NAME = "GPT2 From Scratch"
    -- set ns.CONFIG_DESCRIPTION = "Train the base GPT2 model from scratch."
    -- set ns.SAVE_MODEL = True
-- endblock config_metadata


-- block base_directories
    -- set ASSETS_DIR = normpath(path_join(project_directory, '..', '..', '..'))
    -- include 'paths/default_paths.yaml'
-- endblock base_directories


## Enables flash-attention2
## Disable this block, if your hardware does not support this.
-- block model_constructor_args
.define: &model_constructor_args
    == use_flash_attn2
-- endblock model_constructor_args


-- block datasets_definition
    -- include 'datasets/tiny/tiny_stories.yaml'
-- endblock datasets_definition


# Override default trainer definition
-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


# Override default model defintion
-- block construct_new_model
    -- include 'project.model_config'
-- endblock construct_new_model

#-------------------- project.model_config --------------------
-- extends "models/gpt2.yaml"


#-------------------- project.trainer_config --------------------
## Select one-of:
## trainers/( trainer.yaml | accel_trainer.yaml | trainer.yaml )
-- extends 'trainers/accel_trainer.yaml'


-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
-- endblock trainer_meta_config


-- block trainer_args
    == super()
    # Project Overrides
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    logging_steps: 100
    eval_steps: 1000
    learning_rate: 1.0e-3
    lr_scheduler_type: "cosine"
-- endblock trainer_args