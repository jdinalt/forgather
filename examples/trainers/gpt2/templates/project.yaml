## Project level definitions and overrides go here.
-- extends "train_scripts/causal_language_model.yaml"
-- from "model_ctor/args.yaml" import use_flash_attn2

## Set Project level defaults
-- block experiment_metadata
    == super()
    -- set ns.EXPERIMENT_NAME = "GPT2 From Scratch"
    -- set ns.EXPERIMENT_DESCRIPTION = "Train the base GPT2 model from scratch."
    -- set ns.SAVE_MODEL = True
-- endblock experiment_metadata


-- block base_directories
    -- set ASSETS_DIR = normpath(path_join(project_directory, '..', '..', '..'))
    -- include 'paths/default_paths.yaml'
-- endblock base_directories


## Enables flash-attention2
## Disable this block, if your hardware does not support this.
-- block model_constructor_args
.define: &model_constructor_args
    == use_flash_attn2
-- endblock model_constructor_args


-- block datasets_definition
    -- include 'datasets/tiny_stories.yaml'
-- endblock datasets_definition


# Override default trainer definition
-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


# Override default model defintion
-- block construct_new_model
    -- include 'project.model_config'
-- endblock construct_new_model

#-------------------- project.model_config --------------------
-- extends "models/gpt2.yaml"


#-------------------- project.trainer_config --------------------
## Select one-of:
## trainers/( trainer.yaml | accel_trainer.yaml | trainer.yaml )
-- extends 'trainers/accel_trainer.yaml'


-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
-- endblock trainer_meta_config


-- block trainer_args
    == super()
    # Project Overrides
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    logging_steps: 100
    eval_steps: 1000
    learning_rate: 1.0e-3
    lr_scheduler_type: "cosine"
-- endblock trainer_args