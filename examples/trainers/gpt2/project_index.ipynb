{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../../notebooks/forgather.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da6cf6160de42929db714a06d1a29ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-10 19:41:50,516] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## GPT2\n",
       "\n",
       "Train the baseline GPT2 model on Tiny Stories dataset.\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/trainers/gpt2\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/trainers/gpt2/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/trainers/gpt2/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/tiny_experiments](../../../templates/tiny_experiments)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/modellib](../../../templates/modellib)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/base](../../../templates/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [gpt2.yaml](templates/configs/gpt2.yaml)\n",
       "\n",
       "Default Configuration: gpt2.yaml\n",
       "\n",
       "Active Configuration: gpt2.yaml\n",
       "\n",
       "## Available Templates\n",
       "- [project.yaml](templates/project.yaml)\n",
       "- [configs/gpt2.yaml](templates/configs/gpt2.yaml)\n",
       "- [projects/tiny.yaml](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "- [datasets/tiny/tiny_stories.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories.yaml)\n",
       "- [datasets/tiny/tiny_stories_abridged.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories_abridged.yaml)\n",
       "- [models/tiny/tiny_causal.yaml](../../../templates/tiny_experiments/models/tiny/tiny_causal.yaml)\n",
       "- [models/tiny/tiny_gpt2.yaml](../../../templates/tiny_experiments/models/tiny/tiny_gpt2.yaml)\n",
       "- [models/tiny/tiny_llama.yaml](../../../templates/tiny_experiments/models/tiny/tiny_llama.yaml)\n",
       "- [models/tiny/tiny_d128_l2.yaml](../../../templates/tiny_experiments/models/tiny/tiny_d128_l2.yaml)\n",
       "- [prompts/tiny_stories.yaml](../../../templates/tiny_experiments/prompts/tiny_stories.yaml)\n",
       "- [tokenizers/tiny_2k.yaml](../../../templates/tiny_experiments/tokenizers/tiny_2k.yaml)\n",
       "- [tokenizers/tiny_8k.yaml](../../../templates/tiny_experiments/tokenizers/tiny_8k.yaml)\n",
       "- [model_ctor/args.yaml](../../../templates/modellib/model_ctor/args.yaml)\n",
       "- [models/dynamic_causal_transformer.yaml](../../../templates/modellib/models/dynamic_causal_transformer.yaml)\n",
       "- [models/causal_transformer.yaml](../../../templates/modellib/models/causal_transformer.yaml)\n",
       "- [models/gpt2.yaml](../../../templates/modellib/models/gpt2.yaml)\n",
       "- [models/llama.yaml](../../../templates/modellib/models/llama.yaml)\n",
       "- [trainers/accel_trainer.yaml](../../../templates/base/trainers/accel_trainer.yaml)\n",
       "- [trainers/trainer.yaml](../../../templates/base/trainers/trainer.yaml)\n",
       "- [trainers/hf_trainer.yaml](../../../templates/base/trainers/hf_trainer.yaml)\n",
       "- [trainers/base_trainer.yaml](../../../templates/base/trainers/base_trainer.yaml)\n",
       "- [datasets/abstract/pretokenized_dataset.yaml](../../../templates/base/datasets/abstract/pretokenized_dataset.yaml)\n",
       "- [datasets/abstract/base_datasets.yaml](../../../templates/base/datasets/abstract/base_datasets.yaml)\n",
       "- [models/abstract/causal_lm_from_config.yaml](../../../templates/base/models/abstract/causal_lm_from_config.yaml)\n",
       "- [models/abstract/base_language_model.yaml](../../../templates/base/models/abstract/base_language_model.yaml)\n",
       "- [models/abstract/custom_causal_lm.yaml](../../../templates/base/models/abstract/custom_causal_lm.yaml)\n",
       "- [models/abstract/causal_lm_from_pretrained.yaml](../../../templates/base/models/abstract/causal_lm_from_pretrained.yaml)\n",
       "- [models/abstract/dynamic_causal_lm.yaml](../../../templates/base/models/abstract/dynamic_causal_lm.yaml)\n",
       "- [models/abstract/load_model.yaml](../../../templates/base/models/abstract/load_model.yaml)\n",
       "- [callbacks/base_callbacks.yaml](../../../templates/base/callbacks/base_callbacks.yaml)\n",
       "- [callbacks/loggers.yaml](../../../templates/base/callbacks/loggers.yaml)\n",
       "- [types/meta_template.yaml](../../../templates/base/types/meta_template.yaml)\n",
       "- [types/type.yaml](../../../templates/base/types/type.yaml)\n",
       "- [types/tokenizer/tokenizer.yaml](../../../templates/base/types/tokenizer/tokenizer.yaml)\n",
       "- [types/tokenizer/bpe/bpe.yaml](../../../templates/base/types/tokenizer/bpe/bpe.yaml)\n",
       "- [types/model/model_type.yaml](../../../templates/base/types/model/model_type.yaml)\n",
       "- [types/training_script/training_script.yaml](../../../templates/base/types/training_script/training_script.yaml)\n",
       "- [types/training_script/causal_lm/causal_lm.yaml](../../../templates/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "\n",
       "## Included Templates\n",
       "- [configs/gpt2.yaml](templates/configs/gpt2.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [datasets/tiny/tiny_stories_abridged.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories_abridged.yaml)\n",
       "            - [datasets/tiny/tiny_stories.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories.yaml)\n",
       "                - [datasets/abstract/base_datasets.yaml](../../../templates/base/datasets/abstract/base_datasets.yaml)\n",
       "                    - [inc/formatting.jinja](../../../templates/base/inc/formatting.jinja)\n",
       "        - [model_ctor/args.yaml](../../../templates/modellib/model_ctor/args.yaml)\n",
       "        - [types/training_script/causal_lm/causal_lm.yaml](../../../templates/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [trainers/trainer.yaml](../../../templates/base/trainers/trainer.yaml)\n",
       "                - [trainers/base_trainer.yaml](../../../templates/base/trainers/base_trainer.yaml)\n",
       "            - [callbacks/loggers.yaml](../../../templates/base/callbacks/loggers.yaml)\n",
       "                - [callbacks/base_callbacks.yaml](../../../templates/base/callbacks/base_callbacks.yaml)\n",
       "            - [models/abstract/load_model.yaml](../../../templates/base/models/abstract/load_model.yaml)\n",
       "                - [models/abstract/causal_lm_from_pretrained.yaml](../../../templates/base/models/abstract/causal_lm_from_pretrained.yaml)\n",
       "                    - [models/abstract/base_language_model.yaml](../../../templates/base/models/abstract/base_language_model.yaml)\n",
       "            - [types/training_script/training_script.yaml](../../../templates/base/types/training_script/training_script.yaml)\n",
       "                - [types/type.yaml](../../../templates/base/types/type.yaml)\n",
       "        - [project.model_config](templates/project.yaml)\n",
       "            - [models/gpt2.yaml](../../../templates/modellib/models/gpt2.yaml)\n",
       "                - [models/abstract/causal_lm_from_config.yaml](../../../templates/base/models/abstract/causal_lm_from_config.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "            - [trainers/accel_trainer.yaml](../../../templates/base/trainers/accel_trainer.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_description': 'Train the base GPT2 model from scratch.',\n",
       " 'config_name': 'GPT2 From Scratch',\n",
       " 'create_new_model': 'True',\n",
       " 'datasets_dir': '../../../datasets',\n",
       " 'eval': 'False',\n",
       " 'logging_dir': './output_models/default_model/runs/log_2024-08-10T19-41-38',\n",
       " 'model_src_dir': '../../../model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'output_dir': './output_models/default_model',\n",
       " 'project_dir': '.',\n",
       " 'save_model': 'True',\n",
       " 'tokenizers_dir': '../../../tokenizers',\n",
       " 'train': 'True'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#            GPT2 From Scratch           \n",
       "#---------------------------------------\n",
       "# 2024-08-10T19:41:38\n",
       "# Description: Train the base GPT2 model from scratch.\n",
       "# Project Dir: .\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/gpt2\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: default_model\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.3.1\n",
       "#     transformers: 4.41.2\n",
       "#     accelerate: 0.31.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"../../..\"\n",
       "# ns.models_dir: \"./output_models\"\n",
       "# ns.project_model_src_dir: \"./model_src\"\n",
       "# ns.tokenizers_dir: \"../../../tokenizers\"\n",
       "# ns.datasets_dir: \"../../../datasets\"\n",
       "# ns.model_src_dir: \"../../../model_src\"\n",
       "# ns.output_dir: \"./output_models/default_model\"\n",
       "# ns.logging_dir: \"./output_models/default_model/runs/log_2024-08-10T19-41-38\"\n",
       "# ns.create_new_model: True\n",
       "# ns.save_model: True\n",
       "# ns.train: True\n",
       "# ns.eval: False\n",
       "# ns.trust_remote_code: False\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       ".define: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "\n",
       "\n",
       "################ Model #################\n",
       "\n",
       ".define: &model_constructor_args\n",
       "    torch_dtype: !singleton:forgather.ml.construct:torch_dtype [ \"bfloat16\" ]\n",
       "    attn_implementation: \"flash_attention_2\"\n",
       "\n",
       "# Name: OpenAI GPT2\n",
       "# Description: \n",
       "\n",
       "# model_def.source = \"https://huggingface.co/docs/transformers/v4.42.0/en/model_doc/gpt2\"\n",
       "# model_def.model_config_cls = \"transformers:GPT2Config\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       ".define: &tokenizer !singleton:forgather.ml.construct:add_special_tokens@tokenizer\n",
       "    tokenizer:\n",
       "        !singleton:transformers:AutoTokenizer.from_pretrained\n",
       "        - \"openai-community/gpt2\"\n",
       "    token_map:\n",
       "        pad_token: \"[PAD]\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       "\n",
       "\n",
       "# Model does not have dynamically generated code\n",
       ".define: &model_code_generator null\n",
       "\n",
       "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/configuration_gpt2.py\n",
       ".define: &model_config !singleton:transformers:GPT2Config\n",
       "    vocab_size: !singleton:len [ *tokenizer ]\n",
       "    n_positions: !singleton:getattr [ *tokenizer, 'model_max_length' ]\n",
       "    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]\n",
       "    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]\n",
       "\n",
       "# **Model Constructor**\n",
       "\n",
       "# Custom transformer model; registers for AutoClass and will save code with weights.\n",
       ".define: &model !singleton:transformers:AutoModelForCausalLM.from_config@model\n",
       "    args:\n",
       "        - *model_config\n",
       "    kwargs:\n",
       "        <<: *model_constructor_args\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "# Name: TinyStories Abridged\n",
       "# Define: Abridged to 10% of original size; Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.\n",
       "# Source: https://arxiv.org/abs/2305.07759\n",
       "# Train Dataset: \"roneneldan/TinyStories\" : \"train\"\n",
       "# Eval Dataset: \"roneneldan/TinyStories\" : \"validation\"\n",
       "\n",
       "# **Source Datasets**\n",
       "\n",
       ".define: &train_source_dataset !singleton:datasets:load_dataset@train_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       ".define: &eval_source_dataset !singleton:datasets:load_dataset@eval_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "# **Dataset Splits**\n",
       "\n",
       ".define: &train_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"train\"\n",
       "\n",
       ".define: &eval_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"validation\"\n",
       "\n",
       "# **Tokenize Args**\n",
       "\n",
       ".define: &tokenize_args\n",
       "    truncation: True\n",
       "\n",
       "# **Tokenized Datasets**\n",
       "\n",
       ".define: &train_dataset !singleton:forgather.ml.datasets:tokenize_dataset@train_dataset\n",
       "    dataset: *train_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 0.1\n",
       "    desc: \"Tokenizing train\"\n",
       "    fn_kwargs:\n",
       "        <<: *tokenize_args\n",
       "\n",
       ".define: &eval_dataset !singleton:forgather.ml.datasets:tokenize_dataset@eval_dataset\n",
       "    dataset: *eval_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 500\n",
       "    desc: \"Tokenizing validation split\"\n",
       "    fn_kwargs:\n",
       "        <<: *tokenize_args\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "# https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling\n",
       ".define: &data_collator !singleton:transformers:DataCollatorForLanguageModeling@data_collator\n",
       "    args:\n",
       "        - *tokenizer\n",
       "    kwargs:\n",
       "        mlm: False\n",
       "        return_tensors: pt\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/default_model/runs/log_2024-08-10T19-41-38\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       ".define: &experiment_info !dict:@experiment_info\n",
       "    date: \"2024-08-10T19:41:38\"\n",
       "    name: \"GPT2 From Scratch\"\n",
       "    description: \"Train the base GPT2 model from scratch.\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.3.1', 'transformers': '4.41.2', 'accelerate': '0.31.0'}\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       ".define: &trainer_callbacks !list:@trainer_callbacks\n",
       "    # Log all training output to JSON\n",
       "    - !singleton:forgather.ml.json_logger:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    - !singleton:forgather.ml.tb_logger:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: Custom forgather.ml.accel_trainer:AccelTrainer\n",
       "# Description: A lightweight, extensible trainer w/ Accelerate support\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       ".define: &trainer_args\n",
       "    # Base Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    output_dir: \"./output_models/default_model\"\n",
       "    logging_dir: \"./output_models/default_model/runs/log_2024-08-10T19-41-38\"\n",
       "    overwrite_output_dir: True\n",
       "    per_device_train_batch_size: 16\n",
       "    per_device_eval_batch_size: 32\n",
       "    learning_rate: 1.0e-4\n",
       "    num_train_epochs: 1\n",
       "    eval_steps: 100\n",
       "    logging_steps: 500\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"no\"\n",
       "    logging_strategy: \"steps\"\n",
       "    lr_scheduler_type: \"cosine\"\n",
       "\n",
       "    \n",
       "    # Accel Trainer Defaults\n",
       "    accelerator_args:\n",
       "        device_placement: True\n",
       "        dataloader_config: !singleton:accelerate:DataLoaderConfiguration\n",
       "            dispatch_batches: False\n",
       "            split_batches: False\n",
       "    # Project Overrides\n",
       "    per_device_train_batch_size: 4\n",
       "    per_device_eval_batch_size: 4\n",
       "    logging_steps: 100\n",
       "    eval_steps: 1000\n",
       "    learning_rate: 1.0e-3\n",
       "    lr_scheduler_type: \"cosine\"\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       ".define: &trainer !singleton:forgather.ml.accel_trainer:AccelTrainer@trainer\n",
       "    model: *model\n",
       "    args: !singleton:forgather.ml.accel_trainer:AccelTrainingArguments\n",
       "        <<: *trainer_args\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    tokenizer: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"GPT2 From Scratch\"\n",
       "    config_description: \"Train the base GPT2 model from scratch.\"\n",
       "    project_dir: \".\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"../../../tokenizers\"\n",
       "    datasets_dir: \"../../../datasets\"\n",
       "    output_dir: \"./output_models/default_model\"\n",
       "    model_src_dir: \"../../../model_src\"\n",
       "    logging_dir: \"./output_models/default_model/runs/log_2024-08-10T19-41-38\"\n",
       "    create_new_model: \"True\"\n",
       "    save_model: \"True\"\n",
       "    train: \"True\"\n",
       "    eval: \"False\"\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_save: True\n",
       "    do_train: True\n",
       "    do_eval: False\n",
       "    # Init distributed envrionment before initializing anyting which depends on it.\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "\n",
       "```\n",
       "\n",
       "## Loaded Configuration to YAML\n",
       "\n",
       "```yaml\n",
       ".define: &meta !singleton:named_dict@meta\n",
       "    config_name: 'GPT2 From Scratch'\n",
       "    config_description: 'Train the base GPT2 model from scratch.'\n",
       "    project_dir: '.'\n",
       "    models_dir: './output_models'\n",
       "    tokenizers_dir: '../../../tokenizers'\n",
       "    datasets_dir: '../../../datasets'\n",
       "    output_dir: './output_models/default_model'\n",
       "    model_src_dir: '../../../model_src'\n",
       "    logging_dir: './output_models/default_model/runs/log_2024-08-10T19-41-38'\n",
       "    create_new_model: 'True'\n",
       "    save_model: 'True'\n",
       "    train: 'True'\n",
       "    eval: 'False'\n",
       "\n",
       ".define: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env []\n",
       "\n",
       ".define: &tokenizer !singleton:forgather.ml.construct:add_special_tokens@tokenizer\n",
       "    tokenizer: !singleton:transformers:AutoTokenizer.from_pretrained\n",
       "        - 'openai-community/gpt2'\n",
       "    token_map: \n",
       "        pad_token: '[PAD]'\n",
       "\n",
       ".define: &model !singleton:transformers:AutoModelForCausalLM.from_config@model\n",
       "    args:\n",
       "        - !singleton:transformers:GPT2Config\n",
       "            vocab_size: !singleton:len\n",
       "                - *tokenizer\n",
       "            n_positions: !singleton:getattr\n",
       "                - *tokenizer\n",
       "                - 'model_max_length'\n",
       "            bos_token_id: !singleton:getattr\n",
       "                - *tokenizer\n",
       "                - 'bos_token_id'\n",
       "            eos_token_id: !singleton:getattr\n",
       "                - *tokenizer\n",
       "                - 'eos_token_id'\n",
       "    kwargs:\n",
       "        torch_dtype: !singleton:forgather.ml.construct:torch_dtype\n",
       "            - 'bfloat16'\n",
       "        attn_implementation: 'flash_attention_2'\n",
       "\n",
       ".define: &data_collator !singleton:transformers:DataCollatorForLanguageModeling@data_collator\n",
       "    args:\n",
       "        - *tokenizer\n",
       "    kwargs:\n",
       "        mlm: False\n",
       "        return_tensors: 'pt'\n",
       "\n",
       ".define: &train_source_dataset !singleton:datasets:load_dataset@train_source_dataset\n",
       "    - 'roneneldan/TinyStories'\n",
       "\n",
       ".define: &train_dataset !singleton:forgather.ml.datasets:tokenize_dataset@train_dataset\n",
       "    dataset: !singleton:operator:getitem\n",
       "        - *train_source_dataset\n",
       "        - 'train'\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 0.1\n",
       "    desc: 'Tokenizing train'\n",
       "    fn_kwargs: \n",
       "        truncation: True\n",
       "\n",
       ".define: &eval_dataset !singleton:forgather.ml.datasets:tokenize_dataset@eval_dataset\n",
       "    dataset: !singleton:operator:getitem\n",
       "        - *train_source_dataset\n",
       "        - 'validation'\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 500\n",
       "    desc: 'Tokenizing validation split'\n",
       "    fn_kwargs: \n",
       "        truncation: True\n",
       "\n",
       ".define: &trainer_callbacks !singleton:named_list@trainer_callbacks\n",
       "    - !singleton:forgather.ml.json_logger:JsonLogger\n",
       "        date: '2024-08-10T19:41:38'\n",
       "        name: 'GPT2 From Scratch'\n",
       "        description: 'Train the base GPT2 model from scratch.'\n",
       "        config: !var pp_config\n",
       "        versions: \n",
       "            python: '3.10.13'\n",
       "            torch: '2.3.1'\n",
       "            transformers: '4.41.2'\n",
       "            accelerate: '0.31.0'\n",
       "    - !singleton:forgather.ml.tb_logger:TBLogger\n",
       "        args:\n",
       "            - !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "                - './output_models/default_model/runs/log_2024-08-10T19-41-38'\n",
       "        kwargs:\n",
       "            date: '2024-08-10T19:41:38'\n",
       "            name: 'GPT2 From Scratch'\n",
       "            description: 'Train the base GPT2 model from scratch.'\n",
       "            config: !var pp_config\n",
       "            versions: \n",
       "                python: '3.10.13'\n",
       "                torch: '2.3.1'\n",
       "                transformers: '4.41.2'\n",
       "                accelerate: '0.31.0'\n",
       "\n",
       ".define: &trainer !singleton:forgather.ml.accel_trainer:AccelTrainer@trainer\n",
       "    model: *model\n",
       "    args: !singleton:forgather.ml.accel_trainer:AccelTrainingArguments\n",
       "        output_dir: './output_models/default_model'\n",
       "        logging_dir: './output_models/default_model/runs/log_2024-08-10T19-41-38'\n",
       "        overwrite_output_dir: True\n",
       "        per_device_train_batch_size: 4\n",
       "        per_device_eval_batch_size: 4\n",
       "        learning_rate: 0.001\n",
       "        num_train_epochs: 1\n",
       "        eval_steps: 1000\n",
       "        logging_steps: 100\n",
       "        eval_strategy: 'steps'\n",
       "        save_strategy: 'no'\n",
       "        logging_strategy: 'steps'\n",
       "        lr_scheduler_type: 'cosine'\n",
       "        accelerator_args: \n",
       "            device_placement: True\n",
       "            dataloader_config: !singleton:accelerate:DataLoaderConfiguration\n",
       "                dispatch_batches: False\n",
       "                split_batches: False\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    tokenizer: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "\n",
       ".define: &training_script !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta\n",
       "    do_save: True\n",
       "    do_train: True\n",
       "    do_eval: False\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "\n",
       "\n",
       "meta: *meta\n",
       "main: *training_script\n",
       "\n",
       "```\n",
       "\n",
       "### Generated Source Code\n",
       "\n",
       "```python\n",
       "from forgather.ml.construct import torch_dtype\n",
       "from forgather.ml.datasets import tokenize_dataset\n",
       "from transformers import AutoModelForCausalLM.from_config\n",
       "from forgather.ml.accel_trainer import AccelTrainingArguments\n",
       "from forgather.ml.json_logger import JsonLogger\n",
       "from transformers import AutoTokenizer.from_pretrained\n",
       "from forgather.ml.construct import add_special_tokens\n",
       "from transformers import GPT2Config\n",
       "from datasets import load_dataset\n",
       "from torch.utils.tensorboard import SummaryWriter\n",
       "from forgather.ml.distributed import DistributedEnvironment\n",
       "from transformers import DataCollatorForLanguageModeling\n",
       "from forgather.ml.tb_logger import TBLogger\n",
       "from forgather.ml.accel_trainer import AccelTrainer\n",
       "from forgather.ml.training_script import TrainingScript\n",
       "from accelerate import DataLoaderConfiguration\n",
       "\n",
       "def construct(\n",
       "    pp_config,\n",
       "):\n",
       "    meta = {\n",
       "        'config_name': 'GPT2 From Scratch',\n",
       "        'config_description': 'Train the base GPT2 model from scratch.',\n",
       "        'project_dir': '.',\n",
       "        'models_dir': './output_models',\n",
       "        'tokenizers_dir': '../../../tokenizers',\n",
       "        'datasets_dir': '../../../datasets',\n",
       "        'output_dir': './output_models/default_model',\n",
       "        'model_src_dir': '../../../model_src',\n",
       "        'logging_dir': './output_models/default_model/runs/log_2024-08-10T19-41-38',\n",
       "        'create_new_model': 'True',\n",
       "        'save_model': 'True',\n",
       "        'train': 'True',\n",
       "        'eval': 'False',\n",
       "    }\n",
       "\n",
       "    distributed_env = DistributedEnvironment()\n",
       "\n",
       "    tokenizer = add_special_tokens(\n",
       "        tokenizer=AutoTokenizer.from_pretrained(\n",
       "            'openai-community/gpt2',\n",
       "        ),\n",
       "        token_map={\n",
       "            'pad_token': '[PAD]',\n",
       "        },\n",
       "    )\n",
       "\n",
       "    model = AutoModelForCausalLM.from_config(\n",
       "        GPT2Config(\n",
       "            vocab_size=len(\n",
       "                tokenizer,\n",
       "            ),\n",
       "            n_positions=tokenizer.model_max_length,\n",
       "            bos_token_id=tokenizer.bos_token_id,\n",
       "            eos_token_id=tokenizer.eos_token_id,\n",
       "        ),\n",
       "        torch_dtype=torch_dtype(\n",
       "            'bfloat16',\n",
       "        ),\n",
       "        attn_implementation='flash_attention_2',\n",
       "    )\n",
       "\n",
       "    data_collator = DataCollatorForLanguageModeling(\n",
       "        tokenizer,\n",
       "        mlm=False,\n",
       "        return_tensors='pt',\n",
       "    )\n",
       "\n",
       "    train_source_dataset = load_dataset(\n",
       "        'roneneldan/TinyStories',\n",
       "    )\n",
       "\n",
       "    train_dataset = tokenize_dataset(\n",
       "        dataset=train_source_dataset['train'],\n",
       "        tokenizer=tokenizer,\n",
       "        select_range=0.1,\n",
       "        desc='Tokenizing train',\n",
       "        fn_kwargs={\n",
       "            'truncation': True,\n",
       "        },\n",
       "    )\n",
       "\n",
       "    eval_dataset = tokenize_dataset(\n",
       "        dataset=train_source_dataset['validation'],\n",
       "        tokenizer=tokenizer,\n",
       "        select_range=500,\n",
       "        desc='Tokenizing validation split',\n",
       "        fn_kwargs={\n",
       "            'truncation': True,\n",
       "        },\n",
       "    )\n",
       "\n",
       "    trainer_callbacks = [\n",
       "        JsonLogger(\n",
       "            date='2024-08-10T19:41:38',\n",
       "            name='GPT2 From Scratch',\n",
       "            description='Train the base GPT2 model from scratch.',\n",
       "            config=pp_config,\n",
       "            versions={\n",
       "                'python': '3.10.13',\n",
       "                'torch': '2.3.1',\n",
       "                'transformers': '4.41.2',\n",
       "                'accelerate': '0.31.0',\n",
       "            },\n",
       "        ),\n",
       "        TBLogger(\n",
       "            SummaryWriter(\n",
       "                './output_models/default_model/runs/log_2024-08-10T19-41-38',\n",
       "            ),\n",
       "            date='2024-08-10T19:41:38',\n",
       "            name='GPT2 From Scratch',\n",
       "            description='Train the base GPT2 model from scratch.',\n",
       "            config=pp_config,\n",
       "            versions={\n",
       "                'python': '3.10.13',\n",
       "                'torch': '2.3.1',\n",
       "                'transformers': '4.41.2',\n",
       "                'accelerate': '0.31.0',\n",
       "            },\n",
       "        ),\n",
       "    ]\n",
       "\n",
       "    trainer = AccelTrainer(\n",
       "        model=model,\n",
       "        args=AccelTrainingArguments(\n",
       "            output_dir='./output_models/default_model',\n",
       "            logging_dir='./output_models/default_model/runs/log_2024-08-10T19-41-38',\n",
       "            overwrite_output_dir=True,\n",
       "            per_device_train_batch_size=4,\n",
       "            per_device_eval_batch_size=4,\n",
       "            learning_rate=0.001,\n",
       "            num_train_epochs=1,\n",
       "            eval_steps=1000,\n",
       "            logging_steps=100,\n",
       "            eval_strategy='steps',\n",
       "            save_strategy='no',\n",
       "            logging_strategy='steps',\n",
       "            lr_scheduler_type='cosine',\n",
       "            accelerator_args={\n",
       "                'device_placement': True,\n",
       "                'dataloader_config': DataLoaderConfiguration(\n",
       "                    dispatch_batches=False,\n",
       "                    split_batches=False,\n",
       "                ),\n",
       "            },\n",
       "        ),\n",
       "        data_collator=data_collator,\n",
       "        train_dataset=train_dataset,\n",
       "        eval_dataset=eval_dataset,\n",
       "        tokenizer=tokenizer,\n",
       "        callbacks=trainer_callbacks,\n",
       "    )\n",
       "\n",
       "    training_script = TrainingScript(\n",
       "        meta=meta,\n",
       "        do_save=True,\n",
       "        do_train=True,\n",
       "        do_eval=False,\n",
       "        distributed_env=distributed_env,\n",
       "        trainer=trainer,\n",
       "    )\n",
       "    \n",
       "    return {\n",
       "        'meta': meta,\n",
       "        'main': training_script,\n",
       "    }\n",
       "\n",
       "```\n",
       "\n",
       "## Constructed Project\n",
       "\n",
       "```python\n",
       "{'main': TrainingScript(meta={'config_description': 'Train the base GPT2 model '\n",
       "                                                    'from scratch.',\n",
       "                              'config_name': 'GPT2 From Scratch',\n",
       "                              'create_new_model': 'True',\n",
       "                              'datasets_dir': '../../../datasets',\n",
       "                              'eval': 'False',\n",
       "                              'logging_dir': './output_models/default_model/runs/log_2024-08-10T19-41-38',\n",
       "                              'model_src_dir': '../../../model_src',\n",
       "                              'models_dir': './output_models',\n",
       "                              'output_dir': './output_models/default_model',\n",
       "                              'project_dir': '.',\n",
       "                              'save_model': 'True',\n",
       "                              'tokenizers_dir': '../../../tokenizers',\n",
       "                              'train': 'True'},\n",
       "                        do_save=True,\n",
       "                        do_train=True,\n",
       "                        do_eval=False,\n",
       "                        distributed_env=DistributedEnvironment(rank=0, local_rank=0, world_size=1, local_world_size=1, master_addr=localhost, master_port=29501, backend=None),\n",
       "                        trainer=AccelTrainer(model=GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2FlashAttention2(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       "),args=AccelTrainingArguments(per_device_train_batch_size=4, output_dir='./output_models/default_model', overwrite_output_dir=True, per_device_eval_batch_size=4, max_steps=-1, logging_steps=100, eval_steps=1000, save_steps=500, learning_rate=0.001, num_train_epochs=1, seed=-1, lr_scheduler_type='cosine', warmup_steps=0, device=device(type='cuda', index=0), logging_dir='./output_models/default_model/runs/log_2024-08-10T19-41-38', dataloader_num_workers=0, dataloader_pin_memory=True, dataloader_persistent_workers=False, dataloader_prefetch_factor=None, dataloader_drop_last=False, eval_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_strategy=<IntervalStrategy.STEPS: 'steps'>, save_strategy=<IntervalStrategy.NO: 'no'>, logging_first_step=False, eval_delay=0, save_total_limit=2, use_cpu=False, torch_compile=False, torch_compile_backend='inductor', torch_compile_mode=None, accelerator_args={'device_placement': True, 'dataloader_config': DataLoaderConfiguration(split_batches=False, dispatch_batches=False, even_batches=True, use_seedable_sampler=False, non_blocking=False)}),data_collator=DataCollatorForLanguageModeling(tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50257: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt'),train_dataset=Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 211971\n",
       "}),eval_dataset=Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 500\n",
       "}),tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50257: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "},model_init=None,callbacks=[<forgather.ml.default_callbacks.ProgressCallback object at 0x7fae94adb670>, <forgather.ml.default_callbacks.InfoCallback object at 0x7fad9596b520>, <forgather.ml.json_logger.JsonLogger object at 0x7fad9468f9a0>, <forgather.ml.tb_logger.TBLogger object at 0x7fad9468f640>],)),\n",
       " 'meta': {'config_description': 'Train the base GPT2 model from scratch.',\n",
       "          'config_name': 'GPT2 From Scratch',\n",
       "          'create_new_model': 'True',\n",
       "          'datasets_dir': '../../../datasets',\n",
       "          'eval': 'False',\n",
       "          'logging_dir': './output_models/default_model/runs/log_2024-08-10T19-41-38',\n",
       "          'model_src_dir': '../../../model_src',\n",
       "          'models_dir': './output_models',\n",
       "          'output_dir': './output_models/default_model',\n",
       "          'project_dir': '.',\n",
       "          'save_model': 'True',\n",
       "          'tokenizers_dir': '../../../tokenizers',\n",
       "          'train': 'True'}}\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.ml.notebooks as nb\n",
    "\n",
    "nb.display_project_index(config_template=\"\", materialize=True, pp_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701993b-ed43-462c-87c7-3b9b88a2a8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
