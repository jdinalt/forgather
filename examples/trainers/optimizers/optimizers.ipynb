{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab1856-49cf-4bc0-8825-169c9f5f1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/pytorch/ao/tree/main/torchao/optim\n",
    "https://pytorch.org/blog/pytorch-native-architecture-optimization/\n",
    "\n",
    "https://arxiv.org/pdf/2502.10940"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../../notebooks/forgather.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "nb.display_project_index(config_template=\"my_sgd.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1b052-8595-4c46-b88a-5e8eae9b0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.ml.trainer import Trainer\n",
    "from forgather.ml.trainer_types import TrainingArguments\n",
    "from forgather.ml.training_script import TrainingScript\n",
    "from pprint import pp\n",
    "import torch\n",
    "from forgather.project import Project\n",
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "# Load default baseline config\n",
    "proj = Project(\"my_adafactor2.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113cc97-fb24-41db-9326-ed7e80405b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in outputs[\"model\"].named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c71de-0017-4758-830e-d3a008b5febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "outputs = proj([\n",
    "    \"meta\",\n",
    "    \"distributed_env\",\n",
    "    \"train_dataset\",\n",
    "    \"eval_dataset\",\n",
    "    \"trainer_args\",\n",
    "    \"model\",\n",
    "    \"data_collator\",\n",
    "    \"trainer_callbacks\",\n",
    "    \"tokenizer\",\n",
    "    \"optimizer\",\n",
    "    \"trainer\",\n",
    "    \"main\",\n",
    "])\n",
    "\n",
    "training_args = outputs[\"trainer_args\"] | dict(\n",
    "    #num_train_epochs = 1,\n",
    "    #learning_rate = 1.0e-3,\n",
    "    #lr_scheduler_type=None,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(**training_args)\n",
    "pp(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521962a-3bdb-451b-9fa2-8df2011c1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = outputs[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a492e1-c536-41fe-8942-efe1e1ed1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28073e4-6819-4e23-a8eb-ec1aa81598db",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[\"main\"].run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7591570f-a3f4-4f31-88ba-8ff7e566c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pickle import dump\n",
    "\n",
    "# tell CUDA to start recording memory allocations\n",
    "torch.cuda.memory._record_memory_history(enabled='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6691d6e-5116-43a9-88b2-ca1d0241e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "# save a snapshot of the memory allocations\n",
    "s = torch.cuda.memory._snapshot()\n",
    "with open(f\"snapshot.pickle\", \"wb\") as f:\n",
    "    dump(s, f)\n",
    "\n",
    "# tell CUDA to stop recording memory allocations now\n",
    "torch.cuda.memory._record_memory_history(enabled=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165e7bd-df76-43f1-a62d-7889a28685d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.generate_trainingscript(proj, \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ce13d-3db8-4329-a34c-5690f423befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.display_tb_command(proj, local_host=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32039171-0aa3-4de0-9d52-c030be9eb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.generate_trainingscript(proj, \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e3c29-f068-4bbf-aa5d-041975f52cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snap\n",
    "class ProjectorOptim(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements Adam algorithm with weight decay fix as introduced in [Decoupled Weight Decay\n",
    "    Regularization](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "    Parameters:\n",
    "        params (`Iterable[nn.parameter.Parameter]`):\n",
    "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
    "        lr (`float`, *optional*, defaults to 0.001):\n",
    "            The learning rate to use.\n",
    "        betas (`Tuple[float,float]`, *optional*, defaults to `(0.9, 0.999)`):\n",
    "            Adam's betas parameters (b1, b2).\n",
    "        eps (`float`, *optional*, defaults to 1e-06):\n",
    "            Adam's epsilon for numerical stability.\n",
    "        weight_decay (`float`, *optional*, defaults to 0.0):\n",
    "            Decoupled weight decay to apply.\n",
    "        correct_bias (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to correct bias in Adam (for instance, in Bert TF repository they use `False`).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[nn.parameter.Parameter],\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-6,\n",
    "        weight_decay: float = 0.0,\n",
    "        correct_bias: bool = True,\n",
    "    ):\n",
    "        require_version(\"torch>=1.5.0\")  # add_ with alpha\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
    "        defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay, \"correct_bias\": correct_bias}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Callable = None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if \"proj_args\" not in group:\n",
    "                    self.update_adamw(group, p)\n",
    "                else:\n",
    "                    self.update_projection(group, p)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_adamw(self, group, p):\n",
    "        grad = p.grad\n",
    "        if grad.is_sparse:\n",
    "            raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "        state = self.state[p]\n",
    "        \n",
    "        if \"step\" not in state:\n",
    "            state[\"step\"] = 0\n",
    "\n",
    "        # State initialization\n",
    "        if \"exp_avg\" not in state:\n",
    "            # Exponential moving average of gradient values\n",
    "            state[\"exp_avg\"] = torch.zeros_like(grad)\n",
    "            # Exponential moving average of squared gradient values\n",
    "            state[\"exp_avg_sq\"] = torch.zeros_like(grad)\n",
    "\n",
    "        exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "        beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "        state[\"step\"] += 1\n",
    "\n",
    "        # Decay the first and second moment running average coefficient\n",
    "        # In-place operations to update the averages at the same time\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
    "        denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "        step_size = group[\"lr\"]\n",
    "        if group[\"correct_bias\"]:  # No bias correction for Bert\n",
    "            bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "            bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "            step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "        # compute norm gradient\n",
    "        norm_grad = exp_avg / denom\n",
    "        \n",
    "        p.add_(norm_grad, alpha=-step_size)\n",
    "\n",
    "        # Just adding the square of the weights to the loss function is *not*\n",
    "        # the correct way of using L2 regularization/weight decay with Adam,\n",
    "        # since that will interact with the m and v parameters in strange ways.\n",
    "        #\n",
    "        # Instead we want to decay the weights in a manner that doesn't interact\n",
    "        # with the m/v parameters. This is equivalent to adding the square\n",
    "        # of the weights to the loss with plain (non-momentum) SGD.\n",
    "        # Add weight decay at the end (fixed version)\n",
    "        if group[\"weight_decay\"] > 0.0:\n",
    "            p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
    "\n",
    "    def update_projection(self, group, p):\n",
    "        grad = p.grad\n",
    "        state = self.state[p]\n",
    "        \n",
    "        if \"step\" not in state:\n",
    "            state[\"step\"] = 0\n",
    "\n",
    "        beta1, beta2 = group[\"betas\"]\n",
    "        \n",
    "        # Projection\n",
    "        if \"projector\" not in state:\n",
    "            projector = state[\"projector\"] = SubspaceProjector(\n",
    "                grad,\n",
    "                **group[\"proj_args\"],\n",
    "            )\n",
    "        else:\n",
    "            projector = state[\"projector\"]\n",
    "            projector.update(grad)\n",
    "        \n",
    "        grad = projector.down(grad)\n",
    "        match projector.proj_type:\n",
    "            case \"right\":\n",
    "                S = grad.square().mean(dim=1).sqrt().view(-1, 1)\n",
    "            case \"left\":\n",
    "                S = grad.square().mean(dim=0).sqrt()\n",
    "            case _:\n",
    "                raise Exception(\"Unknown projection type\")\n",
    "        \n",
    "        state[\"step\"] += 1\n",
    "\n",
    "        norm_grad = grad / (S + group[\"eps\"])\n",
    "        \n",
    "        # Project up\n",
    "        norm_grad = projector.up(norm_grad)\n",
    "        \n",
    "        p.add_(norm_grad, alpha=(-group[\"lr\"]))\n",
    "\n",
    "        if group[\"weight_decay\"] > 0.0:\n",
    "            p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1fdb9a-1e32-48d8-bf22-861b513080d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import Callable, Iterable, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "i = torch.randn(8).abs()\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907136a9-df77-4dba-832a-8587f6046b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 8\n",
    "eps = 1e-9\n",
    "\n",
    "r_ = torch.randn(rows)\n",
    "c_ = torch.randn(cols)\n",
    "\n",
    "# Rank-1 matrix\n",
    "M = torch.outer(r_, c_)\n",
    "M_square = M ** 2\n",
    "\n",
    "r_, c_, M, M_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa5a29-6126-4acc-b904-4fdc64bb4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = M_square.mean(dim=1)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ff4d2-0ad8-41ae-8d2a-c35691be26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = M_square.mean(dim=0)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506dd23-5011-4914-abd2-4290d50c9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_factor = 1 / torch.sqrt(r / r.mean(dim=-1, keepdim=True) + eps)\n",
    "r_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b38d7-193c-4a72-866d-012a343f0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_factor = 1 / torch.sqrt(c)\n",
    "c_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ed991-51a3-4a3b-8315-e8072f64d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_f = torch.outer(r_factor, c_factor)\n",
    "M_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c46bf-bbab-4c2d-a43b-f77b51229384",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_factor = (r / r.mean(dim=-1, keepdim=True)).rsqrt_().unsqueeze(-1)\n",
    "c_factor = c.unsqueeze(-2).rsqrt()\n",
    "torch.mul(r_factor, c_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d87a7-baf8-4a57-9ed7-6fa149f105ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_square = M ** 2\n",
    "r = M_square.mean(dim=1)\n",
    "c = M_square.mean(dim=0)\n",
    "r_factor = torch.rsqrt(r / r.mean())\n",
    "c_factor = torch.rsqrt(c)\n",
    "M_f = torch.outer(r_factor, c_factor)\n",
    "M_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08029921-18ae-4015-bab6-e90729087d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure low-rank factorization of M**2\n",
    "M_square = M ** 2\n",
    "r = M_square.mean(dim=1)\n",
    "r /= r.mean() + eps\n",
    "c = M_square.mean(dim=0)\n",
    "M_f = torch.outer(r, c)\n",
    "M_f\n",
    "#1 / (torch.sqrt(M_f) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51933f-d680-48f0-9ff8-0c28850569b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(M_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eda736-9268-4c73-9d79-a1a0aff3dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank-1 factorization of M\n",
    "r = M.mean(dim=1)\n",
    "r /= r.mean() + eps\n",
    "c = M.mean(dim=0)\n",
    "M_f = torch.outer(r, c)\n",
    "M_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bef897-a69a-47b4-9d0c-7d900de884f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
