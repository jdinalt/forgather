{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab1856-49cf-4bc0-8825-169c9f5f1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/pytorch/ao/tree/main/torchao/optim\n",
    "https://pytorch.org/blog/pytorch-native-architecture-optimization/\n",
    "\n",
    "https://arxiv.org/pdf/2502.10940"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index\n",
    "\n",
    "[Custom Model Notebook](../../../notebooks/custom_model.ipynb)  \n",
    "[Training Notebook](../../../notebooks/train.ipynb)  \n",
    "[Project Config Notebook](../../../notebooks/project_config.ipynb)  \n",
    "[Forgather Notebook](../../../notebooks/forgather.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Dynamic Models\n",
       "\n",
       "This is a demonstraction of how to perform model archetecture experiments by using the configuration system to dynamically change module types.\n",
       "\n",
       "As most of the examples, we use \"Tiny Causal\" as a baseline, then make various changes for comparison.\n",
       "\n",
       "### Common Configuration\n",
       "- Tokenizer: tokenizers/tiny_2k_bpe.yaml\n",
       "    - Vocabulary Size: 2000\n",
       "    - Maximum Model Sequence: 2048\n",
       "- Dataset: datasets/tiny/tiny_stories_abridged.yaml\n",
       "    - Dataset ID: roneneldan/TinyStories\n",
       "    - Reference: https://arxiv.org/abs/2305.07759\n",
       "    - Train Select Range: 10% \n",
       "- Model:\n",
       "    - Model Dimension: 256\n",
       "    - MLP Dimension: 1024\n",
       "    - Layers: 4\n",
       "    - Heads: 2\n",
       "    - All Dropout Probabilities: 0.0\n",
       "- Trainer:\n",
       "    - Class: aiws.trainer.Trainer\n",
       "    - Epochs: 1\n",
       "    - Initial Learning Rate: 1.0e-3\n",
       "    - Train Batch Size: 32\n",
       "    - LR Sheduler: Cosine\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/trainers/optimizers\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/trainers/optimizers/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "    - [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/trainers/optimizers/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/forgather_workspace](../../../forgather_workspace)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/tiny_experiments](../../../templates/tiny_experiments)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/modellib](../../../templates/modellib)\n",
       "- [/home/dinalt/ai_assets/forgather/templates/base](../../../templates/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [ngema_r128.yaml](templates/experiments/ngema_r128.yaml)\n",
       "- [hfadafactor.yaml](templates/experiments/hfadafactor.yaml)\n",
       "- [xnormgrad1-r128.yaml](templates/experiments/xnormgrad1-r128.yaml)\n",
       "- [adafactor.yaml](templates/experiments/adafactor.yaml)\n",
       "- [xnormgrad1-r1.yaml](templates/experiments/xnormgrad1-r1.yaml)\n",
       "- [norm_grad.yaml](templates/experiments/norm_grad.yaml)\n",
       "- [xng2_3.yaml](templates/experiments/xng2_3.yaml)\n",
       "- [ng_r128.yaml](templates/experiments/ng_r128.yaml)\n",
       "- [adafactor_momentum.yaml](templates/experiments/adafactor_momentum.yaml)\n",
       "- [xng2_col.yaml](templates/experiments/xng2_col.yaml)\n",
       "- [rmsprop.yaml](templates/experiments/rmsprop.yaml)\n",
       "- [xnormgrad1.yaml](templates/experiments/xnormgrad1.yaml)\n",
       "- [adafactor_r128.yaml](templates/experiments/adafactor_r128.yaml)\n",
       "- [sgd.yaml](templates/experiments/sgd.yaml)\n",
       "- [rmspropm.yaml](templates/experiments/rmspropm.yaml)\n",
       "- [xnormgrad3-r128.yaml](templates/experiments/xnormgrad3-r128.yaml)\n",
       "- [xng2.yaml](templates/experiments/xng2.yaml)\n",
       "- [rmspropc.yaml](templates/experiments/rmspropc.yaml)\n",
       "- [control.yaml](templates/experiments/control.yaml)\n",
       "\n",
       "Default Configuration: control.yaml\n",
       "\n",
       "Active Configuration: hfadafactor.yaml\n",
       "\n",
       "## Included Templates\n",
       "- [experiments/hfadafactor.yaml](templates/experiments/hfadafactor.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [models/bigger.yaml](templates/models/bigger.yaml)\n",
       "            - [tokenizers/tiny_8k.yaml](../../../templates/tiny_experiments/tokenizers/tiny_8k.yaml)\n",
       "            - [models/dynamic_causal_transformer.yaml](../../../templates/modellib/models/dynamic_causal_transformer.yaml)\n",
       "                - [models/abstract/dynamic_causal_lm.yaml](../../../templates/base/models/abstract/dynamic_causal_lm.yaml)\n",
       "                    - [models/abstract/custom_causal_lm.yaml](../../../templates/base/models/abstract/custom_causal_lm.yaml)\n",
       "                        - [models/abstract/base_language_model.yaml](../../../templates/base/models/abstract/base_language_model.yaml)\n",
       "                            - [inc/formatting.jinja](../../../templates/base/inc/formatting.jinja)\n",
       "        - [projects/tiny.yaml](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "            - [datasets/tiny/tiny_stories_abridged.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories_abridged.yaml)\n",
       "                - [datasets/tiny/tiny_stories.yaml](../../../templates/tiny_experiments/datasets/tiny/tiny_stories.yaml)\n",
       "                    - [datasets/abstract/base_datasets.yaml](../../../templates/base/datasets/abstract/base_datasets.yaml)\n",
       "            - [prompts/tiny_stories.yaml](../../../templates/tiny_experiments/prompts/tiny_stories.yaml)\n",
       "            - [types/training_script/causal_lm/causal_lm.yaml](../../../templates/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "                - [trainers/trainer.yaml](../../../templates/base/trainers/trainer.yaml)\n",
       "                    - [trainers/base_trainer.yaml](../../../templates/base/trainers/base_trainer.yaml)\n",
       "                        - [trainers/minimal_trainer.yaml](../../../templates/base/trainers/minimal_trainer.yaml)\n",
       "                - [callbacks/loggers.yaml](../../../templates/base/callbacks/loggers.yaml)\n",
       "                    - [callbacks/base_callbacks.yaml](../../../templates/base/callbacks/base_callbacks.yaml)\n",
       "                - [models/abstract/load_model.yaml](../../../templates/base/models/abstract/load_model.yaml)\n",
       "                    - [models/abstract/causal_lm_from_pretrained.yaml](../../../templates/base/models/abstract/causal_lm_from_pretrained.yaml)\n",
       "                - [types/training_script/training_script.yaml](../../../templates/base/types/training_script/training_script.yaml)\n",
       "                    - [types/type.yaml](../../../templates/base/types/type.yaml)\n",
       "                        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "            - [tiny.callbacks](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "            - [tiny.model_config](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "                - [models/tiny/tiny_causal.yaml](../../../templates/tiny_experiments/models/tiny/tiny_causal.yaml)\n",
       "                    - [tokenizers/tiny_2k.yaml](../../../templates/tiny_experiments/tokenizers/tiny_2k.yaml)\n",
       "            - [tiny.trainer_config](../../../templates/tiny_experiments/projects/tiny.yaml)\n",
       "        - [project.dataset_config](templates/project.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'HF Adafactor',\n",
       " 'config_name': 'HF Adafactor',\n",
       " 'create_new_model': 'True',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/forgather/datasets',\n",
       " 'eval': 'False',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': './output_models/bigger_cp/runs/hf_adafactor_2025-05-18T03-38-25',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'output_dir': './output_models/bigger_cp',\n",
       " 'project_dir': '.',\n",
       " 'save_model': 'False',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'train': 'True',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "- [./output_models/bigger_cp/dynamic_causal_transformer.py](output_models/bigger_cp/dynamic_causal_transformer.py) : DynamicCasualLM\n",
       "    - [/home/dinalt/ai_assets/forgather/examples/trainers/optimizers/./output_models/bigger_cp/dynamic_causal_transformer.py](output_models/bigger_cp/dynamic_causal_transformer.py) : dynamic_causal_transformer\n",
       "        - [/home/dinalt/ai_assets/forgather/model_src/bits/causal_alibi_attn.py](../../../model_src/bits/causal_alibi_attn.py) : dynamic_causal_transformer.causal_alibi_attn\n",
       "        - [/home/dinalt/ai_assets/forgather/model_src/bits/causal_lm.py](../../../model_src/bits/causal_lm.py) : dynamic_causal_transformer.causal_lm\n",
       "        - [/home/dinalt/ai_assets/forgather/model_src/bits/causal_loss.py](../../../model_src/bits/causal_loss.py) : dynamic_causal_transformer.causal_loss\n",
       "        - [/home/dinalt/ai_assets/forgather/model_src/bits/feedforward_layer.py](../../../model_src/bits/feedforward_layer.py) : dynamic_causal_transformer.feedforward_layer\n",
       "        - [/home/dinalt/ai_assets/forgather/model_src/bits/init_weights.py](../../../model_src/bits/init_weights.py) : dynamic_causal_transformer.init_weights\n",
       "        - [/home/dinalt/ai_assets/forgather/model_src/bits/input_encoder.py](../../../model_src/bits/input_encoder.py) : dynamic_causal_transformer.input_encoder\n",
       "        - [/home/dinalt/ai_assets/forgather/model_src/bits/layer_stack.py](../../../model_src/bits/layer_stack.py) : dynamic_causal_transformer.layer_stack\n",
       "        - [/home/dinalt/ai_assets/forgather/model_src/bits/null_pe.py](../../../model_src/bits/null_pe.py) : dynamic_causal_transformer.null_pe\n",
       "        - [/home/dinalt/ai_assets/forgather/model_src/bits/post_ln_layer.py](../../../model_src/bits/post_ln_layer.py) : dynamic_causal_transformer.post_ln_layer\n",
       "- [./output_models/bigger_cp/dynamic_causal_transformer.py](output_models/bigger_cp/dynamic_causal_transformer.py) : DynamicCausalLMConfig\n",
       "- [./src/adafactor.py](src/adafactor.py) : Adafactor\n",
       "    - [/home/dinalt/ai_assets/forgather/examples/trainers/optimizers/./src/adafactor.py](src/adafactor.py) : adafactor\n",
       "## Output Targets\n",
       "- meta\n",
       "- main\n",
       "- model_code_writer\n",
       "- distributed_env\n",
       "- model\n",
       "- trainer\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- trainer_callbacks\n",
       "- trainer_args\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#              HF Adafactor              \n",
       "#---------------------------------------\n",
       "# 2025-05-18T03:38:25\n",
       "# Description: HF Adafactor\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/trainers/optimizers\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/optimizers\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: bigger_cp\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.7.0\n",
       "#     transformers: 4.51.3\n",
       "#     accelerate: 1.7.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/optimizers/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/trainers/optimizers/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"./output_models/bigger_cp\"\n",
       "# ns.logging_dir: \"./output_models/bigger_cp/runs/hf_adafactor_2025-05-18T03-38-25\"\n",
       "# ns.create_new_model: True\n",
       "# ns.save_model: False\n",
       "# ns.train: True\n",
       "# ns.eval: False\n",
       "# ns.trust_remote_code: True\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       ".define: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "# The model will be given the following prompts for text-gen at regular intervals.\n",
       ".define: &testprompts !list:@testprompts\n",
       "    # Test prompts from \"https://arxiv.org/abs/2305.07759\"\n",
       "    - \"Alice was so tired when she got back home so she went\"\n",
       "    - \"Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was\"\n",
       "    - \"Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \\\"Look, Lily. A rainbow has\"\n",
       "    - \"Jack wanted to read a book, so he went to\"\n",
       "    - \"\\\"Can cows fly?\\\" Alice asked her mother.\"\n",
       "    - \"\\\"What do birds like to eat?\\\" Tom asked his mother.\"\n",
       "    - \"\\\"What language do they speak in France?\\\" Tom asked his mother.\"\n",
       "    - \"If I throw a ball up in the air, eventually it will\"\n",
       "    - \"It was winter and cold outside so his mother told him, \\\"You should\"\n",
       "    - \"Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked\"\n",
       "    - \"Jack told Mary, \\\"If you give me your banana, I'll give you my apple.\\\" Mary gave Jack her Banana, so\"\n",
       "    - \"On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to\"\n",
       "    - \"Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that\"\n",
       "    - \"Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \\\"I want to go to the park\\\". Lily says\"\n",
       "    - \"Jack's mother was not home, and his father was at home. When Jack came home, he said hello to\"\n",
       "    - \"Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says\"\n",
       "    - \"Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They\"\n",
       "    - \"Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door,\"\n",
       "\n",
       "# Conservative text-generation parameters.\n",
       ".define: &generation_config !dict:@generation_config\n",
       "    identity: generation_config\n",
       "    do_sample: True\n",
       "    top_k: 20\n",
       "    top_p: 0.9\n",
       "    temperature: 0.7\n",
       "    repitition_penalty: 1.15\n",
       "\n",
       "################ Model #################\n",
       "\n",
       "# https://huggingface.co/docs/transformers/en/model_doc/auto\n",
       ".define: &model_constructor_args {}\n",
       "\n",
       "#-- set model_def.name = \"Tiny Causal\"\n",
       "    #-- set model_def.description = \"A scaled-down version of the base Causal Transformer\"\n",
       "    #-- set model_def.short_name = \"tiny_causal_transformer\"\n",
       "\n",
       "\n",
       "\n",
       "# Name: Dynamic Causal Transformer\n",
       "# Description: A vanilla causal transformer model.\n",
       "# model_def.cls = \"DynamicCasualLM\"\n",
       "# model_def.cfg_cls = \"DynamicCausalLMConfig\"\n",
       "# model_def.config_path = \"/home/dinalt/ai_assets/forgather/examples/trainers/optimizers/output_models/bigger_cp/dynamic_causal_transformer.py\"\n",
       "# model_def.model_path = \"/home/dinalt/ai_assets/forgather/examples/trainers/optimizers/output_models/bigger_cp/dynamic_causal_transformer.py\"\n",
       "# model_def.short_name = \"dynamic_causal_transformer\"\n",
       "# model_def.model_type = \"forgather-dynamic-causal-dynamic_causal_transformer\"\n",
       "# model_def.model_path = \"./output_models/bigger_cp/dynamic_causal_transformer.py\"\n",
       "# model_def.model_template_searchpath = \"/home/dinalt/ai_assets/forgather/templates/dynamic_models\"\n",
       "# model_def.model_template_name = \"causal_lm.py\"\n",
       "# model_def.name_policy = \"named\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       "# Load custom tokenizer from sub-project definition\n",
       ".define: &tokenizer !singleton:forgather.ml.construct:load_from_config@tokenizer\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe\"\n",
       "    config_template: \"8k.yaml\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       ".define: &model_submodule_searchpath\n",
       "    - \"/home/dinalt/ai_assets/forgather/model_src/bits\"\n",
       "    - \"./output_models/bigger_cp\"\n",
       "\n",
       ".define: &loss_fn_factory !lambda:.causal_loss:CausalLoss@loss_fn_factory []\n",
       "\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    normalized_shape: !var \"hidden_size\"\n",
       "\n",
       ".define: &feedforward_factory !lambda:.feedforward_layer:FeedforwardLayer@feedforward_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "    dropout: !var \"activation_dropout\"\n",
       "\n",
       ".define: &attention_factory !lambda:.causal_alibi_attn:CausalAlibiAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    num_heads: !var \"num_attention_heads\"\n",
       "    dropout: !var \"attention_dropout\"\n",
       "    bias: false\n",
       "    trainable_alibi: false\n",
       "    alt_alibi_init: true\n",
       "\n",
       ".define: &layer_factory !lambda:.post_ln_layer:PostLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "    dropout: !var \"layer_dropout\"\n",
       "    residual_dropout: !var \"residual_dropout\"\n",
       "\n",
       ".define: &layer_stack_factory !lambda:.layer_stack:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    num_hidden_layers: !var \"num_hidden_layers\"\n",
       "\n",
       ".define: &output_decoder_factory !lambda:torch.nn:Linear@output_decoder_factory\n",
       "    - !var \"hidden_size\"\n",
       "    - !var \"vocab_size\"\n",
       "\n",
       ".define: &positional_encoder_factory !lambda:.null_pe:NullPE\n",
       "\n",
       "\n",
       ".define: &input_encoder_factory !lambda:.input_encoder:InputEncoder@input_encoder_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    vocab_size: !var \"vocab_size\"\n",
       "    dropout: !var \"embedding_dropout\"\n",
       "    positional_encoder_factory: *positional_encoder_factory\n",
       "\n",
       ".define: &init_weights_factory !lambda:.init_weights:InitWeights@init_weights_factory\n",
       "    std: !var \"initializer_range\"\n",
       "\n",
       ".define: &model_factory !singleton:.causal_lm:CasualLM@model_factory\n",
       "    loss_fn_factory: *loss_fn_factory\n",
       "    input_encoder_factory: *input_encoder_factory\n",
       "    output_decoder_factory: *output_decoder_factory\n",
       "    layer_stack_factory: *layer_stack_factory\n",
       "    init_weights_factory: *init_weights_factory\n",
       "\n",
       ".define: &model_code_generator !meta:forgather.codegen:generate_code@model_code_generator\n",
       "    searchpath: \"/home/dinalt/ai_assets/forgather/templates/dynamic_models\"\n",
       "    template_name: \"causal_lm.py\"\n",
       "    name_policy: \"named\"\n",
       "    obj: *model_factory\n",
       "    # Template args\n",
       "    model_type: \"forgather-dynamic-causal-dynamic_causal_transformer\"\n",
       "\n",
       ".define: &model_code_writer !singleton:forgather.ml.construct:write_file@model_code_writer\n",
       "    data: *model_code_generator\n",
       "    output_file: \"./output_models/bigger_cp/dynamic_causal_transformer.py\"\n",
       "    return_value: \"Model constructor generated by Forgather 1.0\"    \n",
       "\n",
       ".define: &model_config !singleton:./output_models/bigger_cp/dynamic_causal_transformer.py:DynamicCausalLMConfig@model_config\n",
       "    submodule_searchpath: *model_submodule_searchpath\n",
       "    # Set auto-map for custom model; this ensures that the source code stays with the model.\n",
       "    auto_map:\n",
       "        AutoConfig: \"dynamic_causal_transformer.DynamicCausalLMConfig\"\n",
       "        AutoModel: \"dynamic_causal_transformer.DynamicCasualLM\"\n",
       "    # Get the vocab-size from the tokenizer definition.\n",
       "    vocab_size: !singleton:len [ *tokenizer ]\n",
       "    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]\n",
       "    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]\n",
       "    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]\n",
       "    # Add dependency on code generator\n",
       "    code_generator: *model_code_writer\n",
       "    hidden_size: 512\n",
       "    num_attention_heads: 8\n",
       "    num_hidden_layers: 6\n",
       "    max_sequence_length: !singleton:getattr\n",
       "        - *tokenizer\n",
       "        - \"model_max_length\"\n",
       "    dim_feedforward: 2048\n",
       "    initializer_range: 0.02\n",
       "    embedding_dropout: 0.10\n",
       "    layer_dropout: 0.10\n",
       "    residual_dropout: 0.0\n",
       "    attention_dropout: 0.0\n",
       "    activation_dropout: 0.0\n",
       "    # Project Overrides\n",
       "    hidden_size: 512\n",
       "    num_attention_heads: 8\n",
       "    num_hidden_layers: 8\n",
       "    dim_feedforward: 2048\n",
       "    initializer_range: 0.02\n",
       "    embedding_dropout: 0.10\n",
       "    layer_dropout: 0.10\n",
       "\n",
       "# **Model Constructor**\n",
       "\n",
       ".define: &pretrained_model !singleton:./output_models/bigger_cp/dynamic_causal_transformer.py:DynamicCasualLM@pretrained_model\n",
       "    args:\n",
       "        - *model_config\n",
       "    kwargs:\n",
       "        submodule_searchpath: *model_submodule_searchpath\n",
       "        <<: *model_constructor_args\n",
       "\n",
       ".define: &model !singleton:forgather.ml.construct:dependency_list@model\n",
       "    - *pretrained_model\n",
       "    - !singleton:forgather.ml.construct:copy_package_files\n",
       "        - \"./output_models/bigger_cp\"\n",
       "        - *model_config\n",
       "    - !singleton:forgather.ml.construct:copy_package_files\n",
       "        - \"./output_models/bigger_cp\"\n",
       "        - *pretrained_model\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "# Name: TinyStories Abridged\n",
       "# Define: Abridged to 10% of original size; Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.\n",
       "# Source: https://arxiv.org/abs/2305.07759\n",
       "# Train Dataset: \"roneneldan/TinyStories\" : \"train\"\n",
       "# Eval Dataset: \"roneneldan/TinyStories\" : \"validation\"\n",
       "\n",
       "# **Source Datasets**\n",
       "\n",
       ".define: &train_source_dataset !singleton:datasets:load_dataset@train_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       ".define: &eval_source_dataset !singleton:datasets:load_dataset@eval_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "# **Dataset Splits**\n",
       "\n",
       ".define: &train_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"train\"\n",
       "\n",
       ".define: &eval_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"validation\"\n",
       "\n",
       "# **Tokenize Args**\n",
       "\n",
       ".define: &tokenize_args\n",
       "    truncation: True\n",
       "    # project overrides\n",
       "    max_length: 512\n",
       "\n",
       "# **Tokenized Datasets**\n",
       "\n",
       ".define: &train_dataset !singleton:forgather.ml.datasets:tokenize_dataset@train_dataset\n",
       "    dataset: *train_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 0.1\n",
       "    desc: \"Tokenizing train\"\n",
       "    fn_kwargs:\n",
       "        <<: *tokenize_args\n",
       "\n",
       ".define: &eval_dataset !singleton:forgather.ml.datasets:tokenize_dataset@eval_dataset\n",
       "    dataset: *eval_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 500\n",
       "    desc: \"Tokenizing validation split\"\n",
       "    fn_kwargs:\n",
       "        <<: *tokenize_args\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "# https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling\n",
       ".define: &data_collator !singleton:transformers:DataCollatorForLanguageModeling@data_collator\n",
       "    args:\n",
       "        - *tokenizer\n",
       "    kwargs:\n",
       "        mlm: False\n",
       "        return_tensors: pt\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/bigger_cp/runs/hf_adafactor_2025-05-18T03-38-25\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       ".define: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-05-18T03:38:25\"\n",
       "    name: \"HF Adafactor\"\n",
       "    description: \"HF Adafactor\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.7.0', 'transformers': '4.51.3', 'accelerate': '1.7.0'}\n",
       "\n",
       ".define: &text_gen_callback_args\n",
       "    summary_writer: *summary_writer\n",
       "    prompts: *testprompts\n",
       "    generation_config: *generation_config\n",
       "    max_new_tokens: 40\n",
       "    generation_steps: 2000\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       ".define: &trainer_callbacks !list:@trainer_callbacks\n",
       "    # Log all training output to JSON\n",
       "    - !singleton:forgather.ml.json_logger:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    - !singleton:forgather.ml.tb_logger:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "    - !singleton:forgather.ml.textgen_callback:TextgenCallback\n",
       "        <<: *text_gen_callback_args\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       ".define: &optimizer !lambda:./src/adafactor.py:Adafactor\n",
       "    kwargs:\n",
       "        lr: 0.0005\n",
       "        scale_parameter: False\n",
       "        relative_step: False\n",
       "############# LR Scheduler #############\n",
       "\n",
       ".define: &lr_scheduler ~\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: Custom forgather.ml.trainer.Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       ".define: &trainer_args\n",
       "    # Minimal Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    output_dir: \"./output_models/bigger_cp\"\n",
       "    logging_dir: \"./output_models/bigger_cp/runs/hf_adafactor_2025-05-18T03-38-25\"\n",
       "    logging_steps: 500\n",
       "    per_device_train_batch_size: 16\n",
       "    per_device_eval_batch_size: 32\n",
       "    learning_rate: 5.0e-5\n",
       "    num_train_epochs: 1\n",
       "    # Base Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    overwrite_output_dir: True\n",
       "    eval_steps: 100\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"no\"\n",
       "    logging_strategy: \"steps\"\n",
       "\n",
       "    # Tiny Project Overrides\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 64\n",
       "    logging_steps: 100\n",
       "    eval_steps: 500\n",
       "    learning_rate: 1.0e-3\n",
       "    num_train_epochs: 1\n",
       "\n",
       "    # project overrides\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 32\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       ".define: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    model: *model\n",
       "    args: !singleton:forgather.ml.trainer_types:TrainingArguments@trainer_args\n",
       "        <<: *trainer_args\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"HF Adafactor\"\n",
       "    config_description: \"HF Adafactor\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/ai_assets/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "    output_dir: \"./output_models/bigger_cp\"\n",
       "    model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "    logging_dir: \"./output_models/bigger_cp/runs/hf_adafactor_2025-05-18T03-38-25\"\n",
       "    create_new_model: \"True\"\n",
       "    save_model: \"False\"\n",
       "    train: \"True\"\n",
       "    eval: \"False\"\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_save: False\n",
       "    do_train: True\n",
       "    do_eval: False\n",
       "    # Init distributed envrionment before initializing anyting which depends on it.\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "    pp_config: !var \"pp_config\"\n",
       "\n",
       "model_code_writer: *model_code_writer\n",
       "distributed_env: *distributed_env\n",
       "model: *model\n",
       "trainer: *trainer\n",
       "train_dataset: *train_dataset\n",
       "eval_dataset: *eval_dataset\n",
       "data_collator: *data_collator\n",
       "trainer_callbacks: *trainer_callbacks\n",
       "trainer_args: *trainer_args\n",
       "optimizer: *optimizer\n",
       "lr_scheduler: *lr_scheduler\n",
       "model_constructor_args: *model_constructor_args\n",
       "tokenizer: *tokenizer\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "nb.display_project_index(config_template=\"hfadafactor.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3479f6-2572-42a4-adaf-012e91909f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchao\n",
    "torchao.optim.Adam8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd1b052-8595-4c46-b88a-5e8eae9b0113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(output_dir='./output_models/bigger_cp',\n",
      "                  logging_dir='./output_models/bigger_cp/runs/norm_grad_b32_lr1e-3_2025-05-16T04-03-59',\n",
      "                  logging_steps=100,\n",
      "                  per_device_eval_batch_size=32,\n",
      "                  per_device_train_batch_size=32,\n",
      "                  learning_rate=0.001,\n",
      "                  num_train_epochs=1,\n",
      "                  device=None,\n",
      "                  max_steps=-1,\n",
      "                  lr_scheduler_type=None,\n",
      "                  warmup_steps=0,\n",
      "                  eval_steps=500,\n",
      "                  save_steps=500,\n",
      "                  dataloader_num_workers=0,\n",
      "                  dataloader_pin_memory=True,\n",
      "                  dataloader_persistent_workers=False,\n",
      "                  dataloader_prefetch_factor=None,\n",
      "                  dataloader_drop_last=False,\n",
      "                  overwrite_output_dir=True,\n",
      "                  seed=-1,\n",
      "                  eval_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "                  logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
      "                  save_strategy=<IntervalStrategy.NO: 'no'>,\n",
      "                  logging_first_step=False,\n",
      "                  eval_delay=0,\n",
      "                  save_total_limit=2,\n",
      "                  use_cpu=False,\n",
      "                  torch_compile=False,\n",
      "                  torch_compile_backend='inductor',\n",
      "                  torch_compile_mode=None)\n"
     ]
    }
   ],
   "source": [
    "from forgather.ml.trainer import Trainer\n",
    "from forgather.ml.trainer_types import TrainingArguments\n",
    "from forgather.ml.training_script import TrainingScript\n",
    "from pprint import pp\n",
    "import torch\n",
    "from forgather.project import Project\n",
    "\n",
    "# Load default baseline config\n",
    "proj = Project(\"norm_grad.yaml\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "outputs = proj([\n",
    "    \"meta\",\n",
    "    \"distributed_env\",\n",
    "    \"train_dataset\",\n",
    "    \"eval_dataset\",\n",
    "    \"trainer_args\",\n",
    "    \"model\",\n",
    "    \"data_collator\",\n",
    "    \"trainer_callbacks\",\n",
    "    \"tokenizer\",\n",
    "    \"optimizer\",\n",
    "    \"trainer\",\n",
    "    \"main\",\n",
    "])\n",
    "\n",
    "training_args = outputs[\"trainer_args\"] | dict(\n",
    "    #num_train_epochs = 1,\n",
    "    #learning_rate = 1.0e-3,\n",
    "    #lr_scheduler_type=None,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(**training_args)\n",
    "pp(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28073e4-6819-4e23-a8eb-ec1aa81598db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Script Started *****\n",
      "config_name: Norm Grad\n",
      "config_description: Norm Grad\n",
      "output_dir: ./output_models/bigger_cp\n",
      "logging_dir: ./output_models/bigger_cp/runs/norm_grad_b32_lr1e-3_2025-05-16T04-03-59\n",
      "not compiling model\n",
      "Calling optimizer factory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690132a915404595b8939657936779a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 212,000\n",
      "total_train_samples: 212,000\n",
      "per_device_train_batch_size: 32\n",
      "actual_per_device_batch_size: 32\n",
      "total_train_batch_size: 32\n",
      "max_steps: 6,625\n",
      "total_parameters: 33.4M\n",
      "trainable_parameters: 33.4M\n",
      "model:\n",
      "DynamicCasualLM(\n",
      "  (causal_lm): CasualLM(\n",
      "    loss_fn=CausalLoss(), init_weights=InitWeights(std=0.02)\n",
      "    (input_encoder): InputEncoder(\n",
      "      d_model=512, vocab_size=8000, embedding_scale=22.627416997969522\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (embedding): Embedding(8000, 512)\n",
      "      (positional_encoder): NullPE()\n",
      "    )\n",
      "    (output_decoder): Linear(in_features=512, out_features=8000, bias=True)\n",
      "    (layer_stack): LayerStack(\n",
      "      (layers): ModuleList(\n",
      "        (0-7): 8 x PostLNLayer(\n",
      "          (feedforward): FeedforwardLayer(\n",
      "            d_model=512, d_feedforward=2048\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Identity()\n",
      "            (activation): ReLU()\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (attention): CausalAlibiAttn(\n",
      "            d_model=512, num_heads=8, trainable_alibi=False, alt_alibi_init=True\n",
      "            (query_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (key_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (value_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (output_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (dropout): Identity()\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (residual_dropout): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "2025-05-16 04:04:14          100  0.02  epoch: 0.01509 loss: 8.513 \n",
      "2025-05-16 04:04:23          200  0.03  epoch: 0.03019 loss: 6.743 \n",
      "2025-05-16 04:04:33          300  0.05  epoch: 0.04528 loss: 6.409 \n",
      "2025-05-16 04:04:44          400  0.06  epoch: 0.06038 loss: 6.33 \n",
      "2025-05-16 04:04:54          500  0.08  epoch: 0.07547 loss: 6.23 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7b3a3f17c247bd82a1719fa3aed13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 04:04:55          500  0.08  eval-loss:  6.57636   \n",
      "2025-05-16 04:05:08          600  0.09  epoch: 0.09057 loss: 6.198 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:355\u001b[0m, in \u001b[0;36mrecord.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m error_handler\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m se:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# For run_path based entrypoints, SystemExit with code = 0 will never exit.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# Handling it here by returning a value:\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m se\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/ai_assets/forgather/src/forgather/ml/training_script.py:74\u001b[0m, in \u001b[0;36mTrainingScript.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpp_config)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_train:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# This is where the actual 'loop' is.\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_env\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     76\u001b[0m         distributed\u001b[38;5;241m.\u001b[39mbarrier()\n",
      "File \u001b[0;32m~/ai_assets/forgather/src/forgather/ml/base_trainer.py:106\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare(train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset, eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_assets/forgather/src/forgather/ml/trainer.py:220\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_step_begin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 220\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_loss(loss)\n\u001b[1;32m    222\u001b[0m     state\u001b[38;5;241m.\u001b[39mtotal_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/ai_assets/forgather/src/forgather/ml/trainer.py:281\u001b[0m, in \u001b[0;36mTrainer._train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03mPerform a single training step\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03mReturns: mean loss (detached from graph)\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_batch(batch)\n\u001b[0;32m--> 281\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward(loss)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ai_assets/forgather/examples/trainers/optimizers/./output_models/bigger_cp/dynamic_causal_transformer.py:126\u001b[0m, in \u001b[0;36mDynamicCasualLM.forward\u001b[0;34m(self, input_ids, labels, position_ids, attention_mask, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    118\u001b[0m     input_ids: LongTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    124\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutput \u001b[38;5;241m|\u001b[39m Tuple[FloatTensor, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, FloatTensor]] \u001b[38;5;241m|\u001b[39m FloatTensor:\n\u001b[0;32m--> 126\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal_lm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Return type depends on arguments.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ai_assets/forgather/model_src/bits/causal_lm.py:41\u001b[0m, in \u001b[0;36mCasualLM.forward\u001b[0;34m(self, input_ids, labels, position_ids, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_encoder(input_ids, position_ids)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Pass the input through each of the layers.\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_stack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Convert embeddings to log-probabilities of next token-id\u001b[39;00m\n\u001b[1;32m     48\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_decoder(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ai_assets/forgather/model_src/bits/layer_stack.py:25\u001b[0m, in \u001b[0;36mLayerStack.forward\u001b[0;34m(self, hidden_states, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     21\u001b[0m     hidden_states: FloatTensor,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     23\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FloatTensor:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 25\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ai_assets/forgather/model_src/bits/post_ln_layer.py:35\u001b[0m, in \u001b[0;36mPostLNLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: FloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FloatTensor:\n\u001b[1;32m     34\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_dropout(x)\n\u001b[0;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(residual \u001b[38;5;241m+\u001b[39m x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ai_assets/forgather/model_src/bits/causal_alibi_attn.py:135\u001b[0m, in \u001b[0;36mCausalAlibiAttn.forward\u001b[0;34m(self, qkv)\u001b[0m\n\u001b[1;32m    128\u001b[0m attended_values \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    129\u001b[0m     attended_values\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, d_qkv)\n\u001b[1;32m    132\u001b[0m )\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Project the concatenated output through the output matrix.\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattended_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "outputs[\"main\"].run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7591570f-a3f4-4f31-88ba-8ff7e566c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pickle import dump\n",
    "\n",
    "# tell CUDA to start recording memory allocations\n",
    "torch.cuda.memory._record_memory_history(enabled='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6691d6e-5116-43a9-88b2-ca1d0241e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "# save a snapshot of the memory allocations\n",
    "s = torch.cuda.memory._snapshot()\n",
    "with open(f\"snapshot.pickle\", \"wb\") as f:\n",
    "    dump(s, f)\n",
    "\n",
    "# tell CUDA to stop recording memory allocations now\n",
    "torch.cuda.memory._record_memory_history(enabled=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e3c29-f068-4bbf-aa5d-041975f52cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snap\n",
    "class ProjectorOptim(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements Adam algorithm with weight decay fix as introduced in [Decoupled Weight Decay\n",
    "    Regularization](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "    Parameters:\n",
    "        params (`Iterable[nn.parameter.Parameter]`):\n",
    "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
    "        lr (`float`, *optional*, defaults to 0.001):\n",
    "            The learning rate to use.\n",
    "        betas (`Tuple[float,float]`, *optional*, defaults to `(0.9, 0.999)`):\n",
    "            Adam's betas parameters (b1, b2).\n",
    "        eps (`float`, *optional*, defaults to 1e-06):\n",
    "            Adam's epsilon for numerical stability.\n",
    "        weight_decay (`float`, *optional*, defaults to 0.0):\n",
    "            Decoupled weight decay to apply.\n",
    "        correct_bias (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to correct bias in Adam (for instance, in Bert TF repository they use `False`).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[nn.parameter.Parameter],\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-6,\n",
    "        weight_decay: float = 0.0,\n",
    "        correct_bias: bool = True,\n",
    "    ):\n",
    "        require_version(\"torch>=1.5.0\")  # add_ with alpha\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
    "        defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay, \"correct_bias\": correct_bias}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure: Callable = None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if \"proj_args\" not in group:\n",
    "                    self.update_adamw(group, p)\n",
    "                else:\n",
    "                    self.update_projection(group, p)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_adamw(self, group, p):\n",
    "        grad = p.grad\n",
    "        if grad.is_sparse:\n",
    "            raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "        state = self.state[p]\n",
    "        \n",
    "        if \"step\" not in state:\n",
    "            state[\"step\"] = 0\n",
    "\n",
    "        # State initialization\n",
    "        if \"exp_avg\" not in state:\n",
    "            # Exponential moving average of gradient values\n",
    "            state[\"exp_avg\"] = torch.zeros_like(grad)\n",
    "            # Exponential moving average of squared gradient values\n",
    "            state[\"exp_avg_sq\"] = torch.zeros_like(grad)\n",
    "\n",
    "        exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "        beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "        state[\"step\"] += 1\n",
    "\n",
    "        # Decay the first and second moment running average coefficient\n",
    "        # In-place operations to update the averages at the same time\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
    "        denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "        step_size = group[\"lr\"]\n",
    "        if group[\"correct_bias\"]:  # No bias correction for Bert\n",
    "            bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "            bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "            step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "        # compute norm gradient\n",
    "        norm_grad = exp_avg / denom\n",
    "        \n",
    "        p.add_(norm_grad, alpha=-step_size)\n",
    "\n",
    "        # Just adding the square of the weights to the loss function is *not*\n",
    "        # the correct way of using L2 regularization/weight decay with Adam,\n",
    "        # since that will interact with the m and v parameters in strange ways.\n",
    "        #\n",
    "        # Instead we want to decay the weights in a manner that doesn't interact\n",
    "        # with the m/v parameters. This is equivalent to adding the square\n",
    "        # of the weights to the loss with plain (non-momentum) SGD.\n",
    "        # Add weight decay at the end (fixed version)\n",
    "        if group[\"weight_decay\"] > 0.0:\n",
    "            p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
    "\n",
    "    def update_projection(self, group, p):\n",
    "        grad = p.grad\n",
    "        state = self.state[p]\n",
    "        \n",
    "        if \"step\" not in state:\n",
    "            state[\"step\"] = 0\n",
    "\n",
    "        beta1, beta2 = group[\"betas\"]\n",
    "        \n",
    "        # Projection\n",
    "        if \"projector\" not in state:\n",
    "            projector = state[\"projector\"] = SubspaceProjector(\n",
    "                grad,\n",
    "                **group[\"proj_args\"],\n",
    "            )\n",
    "        else:\n",
    "            projector = state[\"projector\"]\n",
    "            projector.update(grad)\n",
    "        \n",
    "        grad = projector.down(grad)\n",
    "        match projector.proj_type:\n",
    "            case \"right\":\n",
    "                S = grad.square().mean(dim=1).sqrt().view(-1, 1)\n",
    "            case \"left\":\n",
    "                S = grad.square().mean(dim=0).sqrt()\n",
    "            case _:\n",
    "                raise Exception(\"Unknown projection type\")\n",
    "        \n",
    "        state[\"step\"] += 1\n",
    "\n",
    "        norm_grad = grad / (S + group[\"eps\"])\n",
    "        \n",
    "        # Project up\n",
    "        norm_grad = projector.up(norm_grad)\n",
    "        \n",
    "        p.add_(norm_grad, alpha=(-group[\"lr\"]))\n",
    "\n",
    "        if group[\"weight_decay\"] > 0.0:\n",
    "            p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1fdb9a-1e32-48d8-bf22-861b513080d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import Callable, Iterable, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "i = torch.randn(8).abs()\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "907136a9-df77-4dba-832a-8587f6046b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5377, -0.8546,  0.4564,  0.2159]),\n",
       " tensor([ 3.0695, -1.0949, -0.6703, -0.0544,  0.4961,  0.5092, -0.5062,  0.7882]),\n",
       " tensor([[-1.6505,  0.5887,  0.3604,  0.0292, -0.2667, -0.2738,  0.2722, -0.4238],\n",
       "         [-2.6231,  0.9357,  0.5728,  0.0465, -0.4239, -0.4352,  0.4326, -0.6736],\n",
       "         [ 1.4010, -0.4997, -0.3059, -0.0248,  0.2264,  0.2324, -0.2310,  0.3598],\n",
       "         [ 0.6628, -0.2364, -0.1447, -0.0117,  0.1071,  0.1100, -0.1093,  0.1702]]),\n",
       " tensor([[2.7242e+00, 3.4662e-01, 1.2990e-01, 8.5508e-04, 7.1151e-02, 7.4975e-02,\n",
       "          7.4086e-02, 1.7965e-01],\n",
       "         [6.8809e+00, 8.7551e-01, 3.2810e-01, 2.1598e-03, 1.7972e-01, 1.8938e-01,\n",
       "          1.8713e-01, 4.5376e-01],\n",
       "         [1.9627e+00, 2.4974e-01, 9.3590e-02, 6.1608e-04, 5.1264e-02, 5.4019e-02,\n",
       "          5.3378e-02, 1.2943e-01],\n",
       "         [4.3928e-01, 5.5894e-02, 2.0947e-02, 1.3789e-04, 1.1473e-02, 1.2090e-02,\n",
       "          1.1947e-02, 2.8969e-02]]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = 4\n",
    "cols = 8\n",
    "eps = 1e-9\n",
    "\n",
    "r_ = torch.randn(rows)\n",
    "c_ = torch.randn(cols)\n",
    "\n",
    "# Rank-1 matrix\n",
    "M = torch.outer(r_, c_)\n",
    "M_square = M ** 2\n",
    "\n",
    "r_, c_, M, M_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5aa5a29-6126-4acc-b904-4fdc64bb4790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0338, 0.1305, 0.4477, 0.0801])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = M_square.mean(dim=1)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c2ff4d2-0ad8-41ae-8d2a-c35691be26f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1904, 0.0015, 0.0101, 0.0436, 0.3065, 0.1682, 0.6503, 0.0136])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = M_square.mean(dim=0)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7506dd23-5011-4914-abd2-4290d50c9449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2642, 1.1515, 0.6217, 1.4694])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_factor = 1 / torch.sqrt(r / r.mean(dim=-1, keepdim=True) + eps)\n",
    "r_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e7b38d7-193c-4a72-866d-012a343f0e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.2917, 26.0588,  9.9450,  4.7882,  1.8063,  2.4379,  1.2401,  8.5643])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_factor = 1 / torch.sqrt(c)\n",
    "c_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "153ed991-51a3-4a3b-8315-e8072f64d46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.1888, 59.0019, 22.5172, 10.8412,  4.0897,  5.5199,  2.8078, 19.3911],\n",
       "        [ 2.6389, 30.0073, 11.4518,  5.5137,  2.0799,  2.8073,  1.4280,  9.8619],\n",
       "        [ 1.4246, 16.1996,  6.1823,  2.9766,  1.1229,  1.5156,  0.7709,  5.3240],\n",
       "        [ 3.3674, 38.2905, 14.6130,  7.0357,  2.6541,  3.5823,  1.8222, 12.5842]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_f = torch.outer(r_factor, c_factor)\n",
    "M_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "149c46bf-bbab-4c2d-a43b-f77b51229384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.1888, 59.0019, 22.5172, 10.8412,  4.0897,  5.5199,  2.8078, 19.3911],\n",
       "        [ 2.6389, 30.0073, 11.4518,  5.5137,  2.0799,  2.8073,  1.4280,  9.8619],\n",
       "        [ 1.4246, 16.1996,  6.1823,  2.9766,  1.1229,  1.5156,  0.7709,  5.3240],\n",
       "        [ 3.3674, 38.2905, 14.6130,  7.0357,  2.6541,  3.5823,  1.8222, 12.5842]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_factor = (r / r.mean(dim=-1, keepdim=True)).rsqrt_().unsqueeze(-1)\n",
    "c_factor = c.unsqueeze(-2).rsqrt()\n",
    "torch.mul(r_factor, c_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a3d87a7-baf8-4a57-9ed7-6fa149f105ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.1888, 59.0019, 22.5172, 10.8412,  4.0897,  5.5199,  2.8078, 19.3911],\n",
       "        [ 2.6389, 30.0073, 11.4518,  5.5137,  2.0799,  2.8073,  1.4280,  9.8619],\n",
       "        [ 1.4246, 16.1996,  6.1823,  2.9766,  1.1229,  1.5156,  0.7709,  5.3240],\n",
       "        [ 3.3674, 38.2905, 14.6130,  7.0357,  2.6541,  3.5823,  1.8222, 12.5842]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_square = M ** 2\n",
    "r = M_square.mean(dim=1)\n",
    "c = M_square.mean(dim=0)\n",
    "r_factor = torch.rsqrt(r / r.mean())\n",
    "c_factor = torch.rsqrt(c)\n",
    "M_f = torch.outer(r_factor, c_factor)\n",
    "M_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08029921-18ae-4015-bab6-e90729087d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7242e+00, 3.4662e-01, 1.2990e-01, 8.5508e-04, 7.1151e-02, 7.4975e-02,\n",
       "         7.4086e-02, 1.7965e-01],\n",
       "        [6.8809e+00, 8.7551e-01, 3.2810e-01, 2.1598e-03, 1.7972e-01, 1.8938e-01,\n",
       "         1.8713e-01, 4.5376e-01],\n",
       "        [1.9627e+00, 2.4973e-01, 9.3590e-02, 6.1608e-04, 5.1264e-02, 5.4019e-02,\n",
       "         5.3378e-02, 1.2943e-01],\n",
       "        [4.3928e-01, 5.5894e-02, 2.0947e-02, 1.3789e-04, 1.1473e-02, 1.2090e-02,\n",
       "         1.1947e-02, 2.8969e-02]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pure low-rank factorization of M**2\n",
    "M_square = M ** 2\n",
    "r = M_square.mean(dim=1)\n",
    "r /= r.mean() + eps\n",
    "c = M_square.mean(dim=0)\n",
    "M_f = torch.outer(r, c)\n",
    "M_f\n",
    "#1 / (torch.sqrt(M_f) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6a51933f-d680-48f0-9ff8-0c28850569b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6505, 0.5887, 0.3604, 0.0292, 0.2667, 0.2738, 0.2722, 0.4238],\n",
       "        [2.6231, 0.9357, 0.5728, 0.0465, 0.4239, 0.4352, 0.4326, 0.6736],\n",
       "        [1.4010, 0.4997, 0.3059, 0.0248, 0.2264, 0.2324, 0.2310, 0.3598],\n",
       "        [0.6628, 0.2364, 0.1447, 0.0117, 0.1071, 0.1100, 0.1093, 0.1702]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(M_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "89eda736-9268-4c73-9d79-a1a0aff3dfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6505,  0.5887,  0.3604,  0.0292, -0.2667, -0.2738,  0.2722, -0.4238],\n",
       "        [-2.6231,  0.9357,  0.5728,  0.0465, -0.4239, -0.4352,  0.4326, -0.6736],\n",
       "        [ 1.4010, -0.4997, -0.3059, -0.0248,  0.2264,  0.2324, -0.2310,  0.3598],\n",
       "        [ 0.6628, -0.2364, -0.1447, -0.0117,  0.1071,  0.1100, -0.1093,  0.1702]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank-1 factorization of M\n",
    "r = M.mean(dim=1)\n",
    "r /= r.mean() + eps\n",
    "c = M.mean(dim=0)\n",
    "M_f = torch.outer(r, c)\n",
    "M_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bef897-a69a-47b4-9d0c-7d900de884f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
