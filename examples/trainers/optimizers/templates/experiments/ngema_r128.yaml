-- extends 'project.yaml'

-- block config_metadata
    == super()
    -- set ns.config_name = "Norm Grad EMA r128 all"
    -- set ns.config_description = "Norm Grad EMA Rank-128"
    -- set ns.model_name = "tiny_causal"
    -- set ns.log_name = "ngema_128_c0.999"
-- endblock config_metadata


-- block construct_new_model
    -- include 'models/control.yaml'
-- endblock construct_new_model

-- block optimizer
.define: &optimizer !lambda:./src/opt_utils.py:make_grouped_optimizer
    ## Note: Lambdas are translated to partial functions. The tl;dr is that you need to
    ## explicitly specify positional args via argN, whereas kvargs are handled automatically.
    args: 
        - !var "arg0"
    kwargs:
        opt_ctor: !lambda:./src/norm_grad_ema.py:NormGradEMA
            - !var "arg0"
        opt_kwargs:
            lr: 1.0e-3
        group_map:
            - [ "bias", "default" ]
            - [ "feedforward|attention|output_decoder", "low_rank" ]
            - [ ".*", "default" ]
        group_config:
            default: {}
            low_rank:
                proj_args:
                    rank: 128
                    orthag: none
                    proj_type: auto
                    update_steps: 10


<< endblock optimizer
    
#-- block datasets_definition
#    -- include 'datasets/tiny/tiny_stories.yaml'
#-- endblock datasets_definition