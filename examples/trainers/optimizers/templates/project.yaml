-- extends 'projects/tiny.yaml'

-- block config_metadata
    == super()
    -- set ns.create_new_model = False
    -- set ns.save_model = False
    -- set ns.model_name = "tiny_causal"
    -- set ns.model_dtype = "bfloat16"
-- endblock config_metadata

-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


-- block datasets_definition
    -- include 'project.dataset_config'
-- endblock datasets_definition


-- block trainer_callbacks
    -- include 'project.callbacks'
<< endblock trainer_callbacks

-- block lr_scheduler
    ##-- include 'lr_schedulers/cosine_annealing_with_warmup.yaml'
# https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 5000
    cooldown_steps: 50000
    constant_lr: 1.0e-4
<< endblock lr_scheduler

#-------------------- project.trainer_config --------------------
-- extends 'tiny.trainer_config'


-- block trainer_args
    == super()
    # project overrides
    ## max_steps: 3
-- endblock trainer_args


#-------------------- project.dataset_config --------------------
-- extends 'datasets/tiny/tiny_stories_abridged.yaml'
##-- extends 'datasets/tiny/tiny_stories.yaml'


#-------------------- project.callbacks --------------------
-- extends 'tiny.callbacks'
##-- extends 'callbacks/grad_logger.yaml'
