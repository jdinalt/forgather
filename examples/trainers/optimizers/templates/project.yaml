-- extends 'projects/tiny.yaml'

-- block config_metadata
    == super()
    -- set ns.create_new_model = True
    -- set ns.save_model = False
    -- set ns.model_name = "tiny_causal"
    ##-- set ns.model_name = "bigger_cp"
    ##-- set ns.model_name = "neoinit"
    -- set ns.lr = 1.0e-3
-- endblock config_metadata

-- block construct_new_model
    -- include 'models/control.yaml'
-- endblock construct_new_model


-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


-- block datasets_definition
    -- include 'project.dataset_config'
-- endblock datasets_definition

-- block model_constructor_args
.define: &model_constructor_args
    torch_dtype: !singleton:forgather.ml.construct:torch_dtype [ "bfloat16" ]
<< endblock model_constructor_args

#-------------------- project.trainer_config --------------------
-- extends 'tiny.trainer_config'


-- block trainer_args
    == super()
    # project overrides
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 64
    ## max_steps: 3
-- endblock trainer_args

#-------------------- project.dataset_config --------------------
-- extends 'datasets/tiny/tiny_stories_abridged.yaml'


-- block tokenize_args
## See: https://huggingface.co/docs/transformers/main_classes/tokenizer
    == super()
    # project overrides
    max_length: 512
<< endblock tokenize_args
