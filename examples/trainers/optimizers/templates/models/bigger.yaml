-- extends 'models/dynamic_causal_transformer.yaml'

-- block model_meta_config
    == super()
    #-- set model_def.name = "Tiny Causal"
    #-- set model_def.description = "A scaled-down version of the base Causal Transformer"
    #-- set model_def.short_name = "tiny_causal_transformer"
<< endblock model_meta_config


-- block model_tokenizer
    -- include 'tokenizers/tiny_8k.yaml'
<< endblock model_tokenizer


-- block model_config
    == super()
    # Project Overrides
    hidden_size: 512
    num_attention_heads: 8
    num_hidden_layers: 8
    dim_feedforward: 2048
    initializer_range: 0.02
    embedding_dropout: 0.10
    layer_dropout: 0.10
<< endblock model_config

# Null positional encoder
-- block positional_encoder_factory
.define: &positional_encoder_factory !lambda:.null_pe:NullPE
-- endblock positional_encoder_factory


-- block attention_factory
.define: &attention_factory !lambda:.causal_alibi_attn:CausalAlibiAttn@attention_factory
    d_model: !var "hidden_size"
    num_heads: !var "num_attention_heads"
    dropout: !var "attention_dropout"
    bias: false
    trainable_alibi: false
    alt_alibi_init: true
<< endblock attention_factory

