-- extends "configs/4M.yaml"

[config_metadata]
    == super()
    -- set ns.config_name = "Post Layer Norm Llama"
    -- set ns.config_description = "A tiny llama using Post Layer Norm"
    -- set ns.model_name = "post_layer_norm_llama"

[model_definition]
    -- include "config.tiny_pipeline_llama.model"

#------------- config.tiny_pipeline_llama.model --------------
-- extends "config.4M.model"

## As an experiment, we replace the PreLayerNorm layers with PostLayerNorm layers.

[layer_factory]
# Switch from PreLayerNorm to PostLayerNorm
layer_factory: &layer_factory !partial:.post_ln_layer:PostLNLayer@layer_factory
    feedforward_factory: *feedforward_factory
    attention_factory: *attention_factory
    norm_factory: *layer_norm_factory
    dropout: !var "layer_dropout"
    residual_dropout: !var "residual_dropout"

[layer_stack]
    == super()
    # Disable norm at last step
    post_norm_factory: null

[model_code_generator]
    == super()

    # Change this to PostLNLayer
    no_split_modules: ["PostLNLayer"]

    # Override PP plan by removing final layer-norm layer
    base_model_pp_plan:
        "causal_lm.input_encoder":
            - ["input_ids"]
            - ["hidden_states"]
        "causal_lm.layer_stack.layers":
            - ["hidden_states", "attention_mask"]
            - ["hidden_states"]