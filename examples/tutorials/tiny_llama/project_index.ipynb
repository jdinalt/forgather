{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Tiny LLama\n",
       "\n",
       "In this tutorial we will train a very small Llama model (about 5M parameters) on 10% of the Tiny Stories dataset. On a single RTX-4090, this takes about three minutes. Once training is complete, we will load the model an use it for text generation -- and the generation will be reasonably coherent for a three-minute-old model.\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "    - [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/forgather_workspace](../../../forgather_workspace)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/modellib](../../../templatelib/modellib)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/examples](../../../templatelib/examples)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/base](../../../templatelib/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [train_bigger_llama.yaml](templates/configs/train_bigger_llama.yaml)\n",
       "- [train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "- [train_bigger_llama_full.yaml](templates/configs/train_bigger_llama_full.yaml)\n",
       "- [prompts.yaml](templates/configs/prompts.yaml)\n",
       "\n",
       "Default Configuration: train_tiny_llama.yaml\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "nb.display_project_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4576985-d53c-4b74-b68b-31ec1e8fcdbd",
   "metadata": {},
   "source": [
    "---\n",
    "This example makes extensive use of the Forgather templates library. Take a look at the various files which go into the configuration and compare these to the pre-processed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d9e625-31e4-4694-b77c-cbcc32813b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Included Templates\n",
       "- [configs/train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "            - [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "                - [inc/formatting.jinja](../../../templatelib/base/inc/formatting.jinja)\n",
       "        - [datasets/tiny_stories_abridged.yaml](../../../templatelib/examples/datasets/tiny_stories_abridged.yaml)\n",
       "            - [datasets/tiny_stories.yaml](../../../templatelib/examples/datasets/tiny_stories.yaml)\n",
       "                - [datasets//base_datasets.yaml](../../../templatelib/base/datasets/base_datasets.yaml)\n",
       "        - [types/training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "                - [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "                    - [trainers/minimal_trainer.yaml](../../../templatelib/base/trainers/minimal_trainer.yaml)\n",
       "            - [models/causal_lm/load_model.yaml](../../../templatelib/base/models/causal_lm/load_model.yaml)\n",
       "                - [models/causal_lm/from_pretrained.yaml](../../../templatelib/base/models/causal_lm/from_pretrained.yaml)\n",
       "                    - [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "            - [types/training_script/training_script.yaml](../../../templatelib/base/types/training_script/training_script.yaml)\n",
       "                - [types/type.yaml](../../../templatelib/base/types/type.yaml)\n",
       "                    - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "        - [project.model_config](templates/project.yaml)\n",
       "            - [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "            - [models/llama.yaml](../../../templatelib/examples/models/llama.yaml)\n",
       "                - [models/causal_lm/from_config.yaml](../../../templatelib/base/models/causal_lm/from_config.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'A demo of training a tiny llama model from scratch.',\n",
       " 'config_name': 'Tiny Llama',\n",
       " 'create_new_model': 'True',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/forgather/datasets',\n",
       " 'eval': 'False',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': './output_models/default_model/runs/log_2025-06-22T03-46-07',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'output_dir': './output_models/default_model',\n",
       " 'project_dir': '.',\n",
       " 'save_model': 'True',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'train': 'True',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "## Output Targets\n",
       "- distributed_env\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "- model_code_generator\n",
       "- model_code_writer\n",
       "- model_config\n",
       "- model\n",
       "- train_source_dataset\n",
       "- eval_source_dataset\n",
       "- train_dataset_split\n",
       "- eval_dataset_split\n",
       "- preprocess_args\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- experiment_info\n",
       "- trainer_callbacks\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- trainer_args\n",
       "- model_preprocessor\n",
       "- trainer\n",
       "- meta\n",
       "- main\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#               Tiny Llama               \n",
       "#---------------------------------------\n",
       "# 2025-06-22T03:46:07\n",
       "# Description: A demo of training a tiny llama model from scratch.\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: default_model\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.7.1\n",
       "#     transformers: 4.51.3\n",
       "#     accelerate: 1.7.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"./output_models/default_model\"\n",
       "# ns.logging_dir: \"./output_models/default_model/runs/log_2025-06-22T03-46-07\"\n",
       "# ns.create_new_model: True\n",
       "# ns.save_model: True\n",
       "# ns.train: True\n",
       "# ns.eval: False\n",
       "# ns.trust_remote_code: False\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       "distributed_env: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "\n",
       "\n",
       "################ Model #################\n",
       "\n",
       "# https://huggingface.co/docs/transformers/en/model_doc/auto\n",
       "model_constructor_args: &model_constructor_args {}\n",
       "\n",
       "# Name: Llama\n",
       "# Description: Llama model\n",
       "\n",
       "# model_def.source = \"\"\n",
       "# model_def.model_config_cls = \"transformers:LlamaConfig\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       "# Load custom tokenizer from sub-project definition\n",
       "tokenizer: &tokenizer !singleton:forgather.ml.construct:load_from_config@tokenizer\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe\"\n",
       "    config_template: \"2k.yaml\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       "# Model config dependencies\n",
       "\n",
       "model_code_generator: &model_code_generator null\n",
       "\n",
       "model_code_writer: &model_code_writer null    \n",
       "\n",
       "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/configuration_llama.py\n",
       "model_config: &model_config !singleton:transformers:LlamaConfig\n",
       "    vocab_size: !singleton:len [ *tokenizer ]\n",
       "    max_position_embeddings: !singleton:getattr [ *tokenizer, 'model_max_length' ]\n",
       "    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]\n",
       "    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]\n",
       "    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]\n",
       "\n",
       "    # Tiny Llama overrides\n",
       "    hidden_size: 256\n",
       "    intermediate_size: 1024\n",
       "    num_attention_heads: 2\n",
       "    num_key_value_heads: 2\n",
       "    num_hidden_layers: 4\n",
       "\n",
       "# **Model Factory**\n",
       "\n",
       "model: &model !lambda:transformers:AutoModelForCausalLM.from_config@model\n",
       "    args:\n",
       "        - *model_config\n",
       "    kwargs:\n",
       "        <<: *model_constructor_args\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "# Name: TinyStories Abridged\n",
       "# Define: Abridged to 10% of original size; Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.\n",
       "# Source: https://arxiv.org/abs/2305.07759\n",
       "# Train Dataset: \"roneneldan/TinyStories\" : \"train\"\n",
       "# Eval Dataset: \"roneneldan/TinyStories\" : \"validation\"\n",
       "\n",
       "# **Source Datasets**\n",
       "\n",
       "train_source_dataset: &train_source_dataset !singleton:datasets:load_dataset@train_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "eval_source_dataset: &eval_source_dataset !singleton:datasets:load_dataset@eval_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "# **Dataset Splits**\n",
       "\n",
       "train_dataset_split: &train_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"train\"\n",
       "\n",
       "eval_dataset_split: &eval_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"validation\"\n",
       "\n",
       "# **Preprocess Dataset Args**\n",
       "\n",
       "preprocess_args: &preprocess_args\n",
       "    truncation: True\n",
       "\n",
       "# **Preprocessed Datasets**\n",
       "\n",
       "train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset\n",
       "    dataset: *train_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 0.1\n",
       "    desc: \"Tokenizing train\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset\n",
       "    dataset: *eval_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 500\n",
       "    desc: \"Tokenizing validation split\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM\n",
       "    tokenizer: *tokenizer\n",
       "    return_tensors: pt\n",
       "\n",
       "    # Tiny Llama\n",
       "    truncation: True\n",
       "    max_length: 512\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/default_model/runs/log_2025-06-22T03-46-07\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       "experiment_info: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-06-22T03:46:07\"\n",
       "    name: \"Tiny Llama\"\n",
       "    description: \"A demo of training a tiny llama model from scratch.\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.7.1', 'transformers': '4.51.3', 'accelerate': '1.7.0'}\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       "trainer_callbacks: &trainer_callbacks !list:@trainer_callbacks\n",
       "    # Log all training output to JSON\n",
       "    - !singleton:forgather.ml.json_logger:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    - !singleton:forgather.ml.tb_logger:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       "optimizer: &optimizer !lambda:torch:optim.AdamW\n",
       "    lr: 1.0e-3\n",
       "\n",
       "############# LR Scheduler #############\n",
       "\n",
       "lr_scheduler: &lr_scheduler ~\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: forgather.ml.trainer.Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       "trainer_args: &trainer_args\n",
       "    # Minimal Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    output_dir: \"./output_models/default_model\"\n",
       "    logging_dir: \"./output_models/default_model/runs/log_2025-06-22T03-46-07\"\n",
       "    logging_steps: 500\n",
       "    per_device_train_batch_size: 16\n",
       "    per_device_eval_batch_size: 32\n",
       "    num_train_epochs: 1\n",
       "    # Base Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    overwrite_output_dir: True\n",
       "    eval_steps: 100\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"no\"\n",
       "    logging_strategy: \"steps\"\n",
       "\n",
       "    # Tiny Llama Project Overrides\n",
       "    seed: 42\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 64\n",
       "    logging_steps: 100\n",
       "    eval_steps: 500\n",
       "    num_train_epochs: 1\n",
       "    dataloader_num_workers: 1\n",
       "\n",
       "\n",
       "model_preprocessor: &model_preprocessor !partial:call [ *model ]\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       "trainer: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    model_init: *model_preprocessor\n",
       "    args: !singleton:forgather.ml.trainer_types:TrainingArguments@trainer_args\n",
       "        <<: *trainer_args\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Tiny Llama\"\n",
       "    config_description: \"A demo of training a tiny llama model from scratch.\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/ai_assets/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "    output_dir: \"./output_models/default_model\"\n",
       "    model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "    logging_dir: \"./output_models/default_model/runs/log_2025-06-22T03-46-07\"\n",
       "    create_new_model: \"True\"\n",
       "    save_model: \"True\"\n",
       "    train: \"True\"\n",
       "    eval: \"False\"\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_save: True\n",
       "    do_train: True\n",
       "    do_eval: False\n",
       "    # Init distributed envrionment before initializing anyting which depends on it.\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "    pp_config: !var \"pp_config\"\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_config(config_template=\"\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bcb135-37fe-4f61-80e7-dc8d71c08294",
   "metadata": {},
   "source": [
    "## Load Project\n",
    "\n",
    "Load the default configuraiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1f9b45-db49-475f-8c73-b47d0e9ab08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "# Load the default project, which is \"train_tiny_llama.yaml\"\n",
    "proj = Project()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15abe5-957f-43ff-af18-0c8725fe961e",
   "metadata": {},
   "source": [
    "## Start Tensorboard\n",
    "\n",
    "This project has been configured to log training to Tensorboard (TB). To watch the model's training progress with TB, run the following command, which will generate a CLI command to start the TB server. Then run the command from a shell.\n",
    "\n",
    "When TB starts, it should provide the URL to access it. e.g.\n",
    "\n",
    "```\n",
    "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "We will get back to this after training starts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dbafacb-4168-411f-9f03-c2cfa85e466a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Tensorboard Command\n",
       "\n",
       "```bash\n",
       "tensorboard --logdir \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models/default_model\"\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show command to run tensorboard; local_host should be false if tensorboard should run on all network interfaces -- not running on the same computer as your browser.\n",
    "nb.display_tb_command(proj, local_host=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b3946-1faa-4224-afe8-58233a57e41c",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "You have a few options for training the mode.\n",
    "\n",
    "1. Run it directly from the notebook. This should work find with this example, although for projects using multiple GPUs, you will want to use one of the other options. To train from the notebook, just run the following cell.\n",
    "2. You can generate a training script and run it from the shell. To do so, run the cell with \"generate_trainingscript(),\" then run the generated shell script from a terminal.\n",
    "3. You can use the Forgather CLI.\n",
    "\n",
    "```bash\n",
    "# This assumes that you have added forgather's 'bin' directory to your path. If you have not, prefix the command with the path to the command (in the forgather \"bin\" directory).\n",
    "\n",
    "# Open a shell in thie project's directory, then run this command:\n",
    "fgcli.py train -d 0\n",
    "\n",
    "# See fgcli.py --help for more details.\n",
    "```\n",
    "\n",
    "Once training starts, switch to Tensorboard in your browser. One of the first things you will want to do is enable automatic refresh. To do so, click the gear in the upper-right corner and check \"Reload Data.\"\n",
    "\n",
    "Once training has started, take a look at the \"Text\" tab. You will see that we have automatically logged the preprocessed configuraiton as well as having dumped the primary training artifacts.\n",
    "\n",
    "Next, switch to the \"Scalars\" tab. You will see a plot of train and evaluation loss which will automatically update every 30 seconds. If you are not familiar with Tensorboard, now would be a good time to play with the UI elements to see how they work.\n",
    "\n",
    "When training completes, the model will be automatically saved to the output directory (\"./output_models/default_model\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d7f6eb9-33ec-489c-b66e-faf3e3f0bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Script Started *****\n",
      "config_name: Tiny Llama\n",
      "config_description: A demo of training a tiny llama model from scratch.\n",
      "output_dir: ./output_models/default_model\n",
      "logging_dir: ./output_models/default_model/runs/log_2025-06-22T03-46-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ffea1f1908422e946069b28b3fac69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 212,000\n",
      "total_train_samples: 212,000\n",
      "per_device_train_batch_size: 32\n",
      "actual_per_device_batch_size: 32\n",
      "total_train_batch_size: 32\n",
      "max_steps: 6,625\n",
      "total_parameters: 5.2M\n",
      "trainable_parameters: 5.2M\n",
      "model:\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(2000, 256, padding_idx=1)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (up_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (down_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((256,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((256,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((256,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=2000, bias=False)\n",
      ")\n",
      "\n",
      "\n",
      "2025-06-22 03:47:22          100  0.02  train-loss: 5.17478   learning-rate: 9.85e-04\n",
      "2025-06-22 03:47:24          200  0.03  train-loss: 3.83513   learning-rate: 9.70e-04\n",
      "2025-06-22 03:47:26          300  0.05  train-loss: 3.30633   learning-rate: 9.55e-04\n",
      "2025-06-22 03:47:28          400  0.06  train-loss: 3.08703   learning-rate: 9.40e-04\n",
      "2025-06-22 03:47:30          500  0.08  train-loss: 2.85455   learning-rate: 9.25e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a312e04064a43c38a82ae685b1edf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:47:30          500  0.08  eval-loss:  2.72421   \n",
      "2025-06-22 03:47:33          600  0.09  train-loss: 2.74511   learning-rate: 9.09e-04\n",
      "2025-06-22 03:47:35          700  0.11  train-loss: 2.63223   learning-rate: 8.94e-04\n",
      "2025-06-22 03:47:37          800  0.12  train-loss: 2.60686   learning-rate: 8.79e-04\n",
      "2025-06-22 03:47:39          900  0.14  train-loss: 2.50137   learning-rate: 8.64e-04\n",
      "2025-06-22 03:47:41        1,000  0.15  train-loss: 2.3653    learning-rate: 8.49e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a94390ebc94cd08a21ba575c60aff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:47:42        1,000  0.15  eval-loss:  2.26289   \n",
      "2025-06-22 03:47:44        1,100  0.17  train-loss: 2.38422   learning-rate: 8.34e-04\n",
      "2025-06-22 03:47:46        1,200  0.18  train-loss: 2.3678    learning-rate: 8.19e-04\n",
      "2025-06-22 03:47:48        1,300  0.2   train-loss: 2.32665   learning-rate: 8.04e-04\n",
      "2025-06-22 03:47:50        1,400  0.21  train-loss: 2.31223   learning-rate: 7.89e-04\n",
      "2025-06-22 03:47:52        1,500  0.23  train-loss: 2.28764   learning-rate: 7.74e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355885ad93ba4223b9360e86f41d205f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:47:53        1,500  0.23  eval-loss:  2.0599    \n",
      "2025-06-22 03:47:55        1,600  0.24  train-loss: 2.25563   learning-rate: 7.58e-04\n",
      "2025-06-22 03:47:57        1,700  0.26  train-loss: 2.20934   learning-rate: 7.43e-04\n",
      "2025-06-22 03:47:59        1,800  0.27  train-loss: 2.14287   learning-rate: 7.28e-04\n",
      "2025-06-22 03:48:01        1,900  0.29  train-loss: 2.13751   learning-rate: 7.13e-04\n",
      "2025-06-22 03:48:03        2,000  0.3   train-loss: 2.18676   learning-rate: 6.98e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6495db04e4da4b0684bb3a81c72e908b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:48:04        2,000  0.3   eval-loss:  1.9571    \n",
      "2025-06-22 03:48:06        2,100  0.32  train-loss: 2.13909   learning-rate: 6.83e-04\n",
      "2025-06-22 03:48:08        2,200  0.33  train-loss: 2.08504   learning-rate: 6.68e-04\n",
      "2025-06-22 03:48:10        2,300  0.35  train-loss: 2.04639   learning-rate: 6.53e-04\n",
      "2025-06-22 03:48:12        2,400  0.36  train-loss: 2.10779   learning-rate: 6.38e-04\n",
      "2025-06-22 03:48:14        2,500  0.38  train-loss: 2.07545   learning-rate: 6.23e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5357fcf970e24b1082ac371ac358da66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:48:14        2,500  0.38  eval-loss:  1.86501   \n",
      "2025-06-22 03:48:16        2,600  0.39  train-loss: 2.07933   learning-rate: 6.08e-04\n",
      "2025-06-22 03:48:19        2,700  0.41  train-loss: 2.01078   learning-rate: 5.92e-04\n",
      "2025-06-22 03:48:21        2,800  0.42  train-loss: 2.06126   learning-rate: 5.77e-04\n",
      "2025-06-22 03:48:23        2,900  0.44  train-loss: 1.97158   learning-rate: 5.62e-04\n",
      "2025-06-22 03:48:25        3,000  0.45  train-loss: 1.85841   learning-rate: 5.47e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7e263449464ef1af914ef006e78ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:48:25        3,000  0.45  eval-loss:  1.75771   \n",
      "2025-06-22 03:48:27        3,100  0.47  train-loss: 1.95764   learning-rate: 5.32e-04\n",
      "2025-06-22 03:48:29        3,200  0.48  train-loss: 2.04966   learning-rate: 5.17e-04\n",
      "2025-06-22 03:48:31        3,300  0.5   train-loss: 1.92616   learning-rate: 5.02e-04\n",
      "2025-06-22 03:48:34        3,400  0.51  train-loss: 1.8471    learning-rate: 4.87e-04\n",
      "2025-06-22 03:48:36        3,500  0.53  train-loss: 1.8766    learning-rate: 4.72e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13841ea9d81f438e848ce403a65ee209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:48:36        3,500  0.53  eval-loss:  1.72352   \n",
      "2025-06-22 03:48:38        3,600  0.54  train-loss: 1.95022   learning-rate: 4.57e-04\n",
      "2025-06-22 03:48:40        3,700  0.56  train-loss: 1.85785   learning-rate: 4.42e-04\n",
      "2025-06-22 03:48:43        3,800  0.57  train-loss: 1.86316   learning-rate: 4.26e-04\n",
      "2025-06-22 03:48:45        3,900  0.59  train-loss: 1.91145   learning-rate: 4.11e-04\n",
      "2025-06-22 03:48:47        4,000  0.6   train-loss: 1.94081   learning-rate: 3.96e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf62b0b249b45f8bacbcab486b78277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:48:47        4,000  0.6   eval-loss:  1.68185   \n",
      "2025-06-22 03:48:49        4,100  0.62  train-loss: 1.84015   learning-rate: 3.81e-04\n",
      "2025-06-22 03:48:52        4,200  0.63  train-loss: 1.80779   learning-rate: 3.66e-04\n",
      "2025-06-22 03:48:54        4,300  0.65  train-loss: 1.8393    learning-rate: 3.51e-04\n",
      "2025-06-22 03:48:56        4,400  0.66  train-loss: 1.89586   learning-rate: 3.36e-04\n",
      "2025-06-22 03:48:58        4,500  0.68  train-loss: 1.82264   learning-rate: 3.21e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2951a9e0e6304a67a79b272e54fc4377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:48:58        4,500  0.68  eval-loss:  1.6471    \n",
      "2025-06-22 03:49:01        4,600  0.69  train-loss: 1.75596   learning-rate: 3.06e-04\n",
      "2025-06-22 03:49:03        4,700  0.71  train-loss: 1.76089   learning-rate: 2.91e-04\n",
      "2025-06-22 03:49:05        4,800  0.72  train-loss: 1.79232   learning-rate: 2.75e-04\n",
      "2025-06-22 03:49:07        4,900  0.74  train-loss: 1.80353   learning-rate: 2.60e-04\n",
      "2025-06-22 03:49:10        5,000  0.75  train-loss: 1.80619   learning-rate: 2.45e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0faaef8382240cf8f98288bd9e92c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:49:10        5,000  0.75  eval-loss:  1.61131   \n",
      "2025-06-22 03:49:12        5,100  0.77  train-loss: 1.80021   learning-rate: 2.30e-04\n",
      "2025-06-22 03:49:14        5,200  0.78  train-loss: 1.70302   learning-rate: 2.15e-04\n",
      "2025-06-22 03:49:17        5,300  0.8   train-loss: 1.70104   learning-rate: 2.00e-04\n",
      "2025-06-22 03:49:19        5,400  0.82  train-loss: 1.73868   learning-rate: 1.85e-04\n",
      "2025-06-22 03:49:21        5,500  0.83  train-loss: 1.71429   learning-rate: 1.70e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e31a2ac4f0341dc8266851423fc9980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:49:21        5,500  0.83  eval-loss:  1.57267   \n",
      "2025-06-22 03:49:23        5,600  0.85  train-loss: 1.75668   learning-rate: 1.55e-04\n",
      "2025-06-22 03:49:25        5,700  0.86  train-loss: 1.78478   learning-rate: 1.40e-04\n",
      "2025-06-22 03:49:28        5,800  0.88  train-loss: 1.73766   learning-rate: 1.25e-04\n",
      "2025-06-22 03:49:30        5,900  0.89  train-loss: 1.75866   learning-rate: 1.09e-04\n",
      "2025-06-22 03:49:32        6,000  0.91  train-loss: 1.66765   learning-rate: 9.43e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b892e2f2add49a68430e935d2fbc88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:49:32        6,000  0.91  eval-loss:  1.54673   \n",
      "2025-06-22 03:49:34        6,100  0.92  train-loss: 1.63996   learning-rate: 7.92e-05\n",
      "2025-06-22 03:49:37        6,200  0.94  train-loss: 1.70862   learning-rate: 6.42e-05\n",
      "2025-06-22 03:49:39        6,300  0.95  train-loss: 1.67234   learning-rate: 4.91e-05\n",
      "2025-06-22 03:49:41        6,400  0.97  train-loss: 1.66689   learning-rate: 3.40e-05\n",
      "2025-06-22 03:49:43        6,500  0.98  train-loss: 1.69109   learning-rate: 1.89e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11a46e470f84ef78e2d43105e250b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:49:43        6,500  0.98  eval-loss:  1.52501   \n",
      "2025-06-22 03:49:46        6,600  1.0   train-loss: 1.64215   learning-rate: 3.77e-06\n",
      "2025-06-22 03:49:46        6,625  1.0   train_runtime: 146.7 train_samples: 212,000 step: 6,625 train_samples_per_second: 1.445e+03 train_steps_per_second: 45.16 epoch: 1.0 \n",
      "**** Training Completed *****\n",
      "{'train_runtime': 146.69839358329773, 'train_samples': 212000, 'step': 6625, 'train_samples_per_second': 1445.142, 'train_steps_per_second': 45.161, 'epoch': 1.0}\n",
      "Model saved to: ./output_models/default_model\n"
     ]
    }
   ],
   "source": [
    "# Train model in notebook.\n",
    "\n",
    "# Construct the default target, \"main,\" which is a training script.\n",
    "training_script = proj()\n",
    "\n",
    "# Start training the model.\n",
    "training_script.run()\n",
    "\n",
    "# Release resources\n",
    "training_script = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8f0679-604e-4d2b-80ae-0600e0f10148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Generated Shell Script\n",
       "[train_tiny_llama.sh](train_tiny_llama.sh)\n",
       "```bash\n",
       "#!/bin/bash\n",
       "CUDA_VISIBLE_DEVICES='0' torchrun --standalone --nproc-per-node 'gpu' '/home/dinalt/ai_assets/forgather/scripts/train_script.py' -p '/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama' \"train_tiny_llama.yaml\"\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate training script to run from shell.\n",
    "nb.generate_trainingscript(proj, \"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a33f36-246d-4627-aebf-5e8127f4b692",
   "metadata": {},
   "source": [
    "## Load Trained Model\n",
    "\n",
    "You can use the regular HF APIs to load the saved model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4846ff07-14ed-434e-aadd-b04063e6c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GenerationConfig, StoppingCriteria\n",
    "\n",
    "model_path = \"./output_models/default_model\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c9b82-0f42-4372-8751-c2d0c453970a",
   "metadata": {},
   "source": [
    "## Setup Generation Config\n",
    "\n",
    "Let's do something interesting with our newly trained model...\n",
    "\n",
    "To speed things up a little, there is a project configuraiton which just defines some prompts and generation parameters. The next cell will load this configuration and print it, but you can easily replace these with your own settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51b78c0-9505-4e57-b4a8-14ba98df0ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Alice was so tired when she got back home so she went',\n",
       "  'Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was',\n",
       "  'Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has',\n",
       "  'Jack wanted to read a book, so he went to',\n",
       "  '\"Can cows fly?\" Alice asked her mother.',\n",
       "  '\"What do birds like to eat?\" Tom asked his mother.',\n",
       "  '\"What language do they speak in France?\" Tom asked his mother.',\n",
       "  'If I throw a ball up in the air, eventually it will',\n",
       "  'It was winter and cold outside so his mother told him, \"You should',\n",
       "  'Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked',\n",
       "  'Jack told Mary, \"If you give me your banana, I\\'ll give you my apple.\" Mary gave Jack her Banana, so',\n",
       "  'On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to',\n",
       "  'Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that',\n",
       "  'Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \"I want to go to the park\". Lily says',\n",
       "  \"Jack's mother was not home, and his father was at home. When Jack came home, he said hello to\",\n",
       "  \"Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says\",\n",
       "  'Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They',\n",
       "  'Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door,'],\n",
       " {'identity': 'generation_config',\n",
       "  'do_sample': True,\n",
       "  'top_k': 20,\n",
       "  'top_p': 0.9,\n",
       "  'temperature': 0.7,\n",
       "  'repitition_penalty': 1.15})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts, generation_config_args = Project(\"prompts.yaml\")(\"testprompts\", \"generation_config\")\n",
    "prompts, generation_config_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16aaea54-3724-4863-ab1b-d02bcd1dca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct generation config object froma dictionary.\n",
    "gen_config = GenerationConfig(\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    **generation_config_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d72d1-1e8f-4433-abe0-a0d9aaa252c0",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "This loop will use the newly trained model to generate text, seeded with the above prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee6f8d5-c548-4b94-88d2-fc92f118746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was so tired when she got back home so she went [START] to sleep. She closed her eyes and dreamed of eating breakfast.\n",
      "\n",
      "The next day, Alice went to the park to play. She saw a big dog, a dog, and a dog. The dog was very cute and had big teeth.\n",
      "\n",
      "Alice had to go home. She was sad to see the dog go, but she was afraid to run away.\n",
      "\n",
      "Alice's mom saw her and said,\n",
      "----------------------------------------\n",
      "Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was [START] hard work. \n",
      "\n",
      "One night, Jack and Lily saw a big, shiny thing in the sky. It was a shiny thing that could make noises. Jack wanted to see what it could do. He picked it up and tried to pull it out. But the thing was not moving. \n",
      "\n",
      "Jack and Lily were scared. They did not know what to do. They tried to pull the button, but it was too heavy for them. They tried and tried, but it was too\n",
      "----------------------------------------\n",
      "Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has [START] rainbows!\" Lily pointed to the rainbow and said, \"Look, a rainbow! It's so beautiful!\" Jack nodded and said, \"Yes, it is. What is it?\" Lily said, \"It is so beautiful and it makes a wish.\" Jack smiled and said, \"Let's go and play in it!\" They ran around, laughing and having fun. When they got back, they hugged and said, \"It was so much fun!\" They couldn't wait\n",
      "----------------------------------------\n",
      "Jack wanted to read a book, so he went to [START] the library. He looked around and saw a book with a picture of a bird on it. He read the book and read the book. It was so colorful! Jack was very happy. He wanted to read the book and play with the bird. So he asked his mom if he could read the book. His mom said yes, but Jack didn't know what to read.\n",
      "\n",
      "Jack opened the book and read it. He read the word \"I will read the book.\"\n",
      "----------------------------------------\n",
      "\"Can cows fly?\" Alice asked her mother. [START] \n",
      "\"Yes, I do,\" her mom said. \"It's so pretty and fun. We can stay on the grass.\"\n",
      "\n",
      "Alice was so excited. She ran to the grass and started to explore. She saw a big pond with lots of flowers. She was so excited!\n",
      "\n",
      "The pond was so green and full of flowers. The flowers were so green and sparkly. Alice's mom said, \"Let's go inside and have some fun.\"\n",
      "\n",
      "----------------------------------------\n",
      "\"What do birds like to eat?\" Tom asked his mother. [START] \n",
      "\"I don't know,\" his mother said. \"Maybe we can find something special and I'll give you a hug.\"\n",
      "\n",
      "Tom was so excited! He asked his mother, \"What do you want to do with this?\"\n",
      "\n",
      "His mother smiled and said, \"You can use it to make the world a better place.\"\n",
      "\n",
      "Tom was so excited. He got to work. He worked very hard and soon he was able to make the world a better place.\n",
      "\n",
      "----------------------------------------\n",
      "\"What language do they speak in France?\" Tom asked his mother. [START] \n",
      "\"I don't know,\" his mother replied. \"It will be fun to be serious. I want to be a good boy and not be selfish.\"\n",
      "\n",
      "So they decided to go to the park. They ran and jumped and shouted. The boy felt sad and hurt. He wanted to play with his friends.\n",
      "\n",
      "Suddenly, a big dog came and saw the boy's face. He was scared and dropped the boy's head.\n",
      "\n",
      "\"A\n",
      "----------------------------------------\n",
      "If I throw a ball up in the air, eventually it will [START] be a fun day. If it rained, it was so good! I can't wait to go outside and play in the rain. But I'm too tired to go inside.\"\n",
      "\n",
      "A few days later, the rain stopped and the rain stopped. The rain was so bright that it made the water go away. The rain started to go away, and it made the little boy very sad. He wished he had stayed inside.\n",
      "\n",
      "The rain kept coming and the rain\n",
      "----------------------------------------\n",
      "It was winter and cold outside so his mother told him, \"You should [START] be careful.\" So his mother and dad went out into the house. It was time to go to bed.\n",
      "\n",
      "On the way, the boy saw something strange. It was a big, shiny machine. He was curious and wanted to touch it. He asked his mum, \"What is that?\"\n",
      "\n",
      "His mother smiled and said, \"It's a machine, sweetheart. It is very old and can be very strong. It can be strong and strong.\"\n",
      "----------------------------------------\n",
      "Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked [START] them to play with her dog. Lily did not want to share her dog. She liked dogs. She thought they were cool and wanted to play with them.\n",
      "\n",
      "But her mom did not know how. She said the dog was too small and it was too big for her. She said they had to be careful and not let her dogs play. Lily was sad and angry. She wanted to play with her dog. She asked her mom for some water and some water.\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Jack told Mary, \"If you give me your banana, I'll give you my apple.\" Mary gave Jack her Banana, so [START] he gave it to him. Jack was so happy, he thanked Jack.\n",
      "\n",
      "The next day, Jack was feeling very sad. He asked Jack, \"What's wrong?\"\n",
      "\n",
      "Jack said, \"I'm sorry for being so careless. I should have been able to get my banana back.\"\n",
      "\n",
      "The two friends started to argue and fight. Jack tried to fix the banana but he couldn't do it. He felt so sad and started\n",
      "----------------------------------------\n",
      "On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to [START] school, he noticed something strange. It was a big, beautiful palace. Jack couldn't believe his eyes. He had never seen anything like it before! He asked his grandmother what it was, and she said it was a special palace. Jack was so excited. He ran to the palace and asked her to come closer. The palace was so big that Jack had to take a closer look. He said that he was a very special pal\n",
      "----------------------------------------\n",
      "Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that [START] the ice cream is very important to the people who want it. Ben wanted to be good and share with his friends.\n",
      "\n",
      "Lily said, \"No, Ben, this is mine. You have your own money. You can't have it.\"\n",
      "\n",
      "Ben was sad and angry. He said, \"I want your own ice cream. I want your own ice cream.\"\n",
      "\n",
      "Lily said, \"No, Ben. I don't want my ice cream. It is not ours.\n",
      "----------------------------------------\n",
      "Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \"I want to go to the park\". Lily says [START]  \"No, I can do it. It is fun.\" Ben says, \"No, you can't. It is too dangerous. You are too stubborn.\"\n",
      "\n",
      "Lily is sad and angry. She says, \"I want to go to the park. I want to play.\" Ben says, \"Please, Ben, come with me. I don't want to go. I want to go to the park.\" He says, \"No, Lily. I want to go\n",
      "----------------------------------------\n",
      "Jack's mother was not home, and his father was at home. When Jack came home, he said hello to [START] his father and said he was a very special father.\n",
      "\n",
      "Mum was very pleased and said he was the only one he had. Jack was so happy that he hugged his father.\n",
      "\n",
      "Jack's mother gave him a big hug and said, \"I'm so glad you like the father, Jack. You have so many special memories.\"\n",
      "\n",
      "Jack smiled and hugged his father. He was very excited to show them the father and\n",
      "----------------------------------------\n",
      "Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says [START]  \"No, no, I don't want to get in trouble. I want to be a good friend.\"\n",
      "\n",
      "One day, Lily and her foolish friend went to the river to find the perfect spot. Lily saw a big, scary pool. She tried to climb it, but she was too small. She tried and tried, but she could not reach the pool.\n",
      "\n",
      "\"I can't do it, Lily. I want to be a good friend and\n",
      "----------------------------------------\n",
      "Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They [START] were very excited and started to play with the cake.\n",
      "\n",
      "Ben was excited to try the cake. He thought it would be fun to try it on. He put the cake on the table and smiled.\n",
      "\n",
      "Ben put the cake on the table and smiled. He was ready to try it. He grabbed the piece and threw it on the table. He threw it on the floor and shouted, \"Look, I made a cake!\"\n",
      "\n",
      "He laughed and shouted, \"That's great\n",
      "----------------------------------------\n",
      "Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door, [START] but Lily did not answer. She wanted to see the door.\n",
      "\n",
      "\"Give me your answer,\" Ben said. \"You need to be quiet. I don't want to go inside.\"\n",
      "\n",
      "\"But I want to see the door. I want to see what is inside. It is a big box. It has a door and a door. You have to stay in the house and wait for it to be. It is very big and scary.\"\n",
      "\n",
      "Ben did\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(model, tokenizer, prompts, gen_config, max_new_tokens, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for prompt in prompts:\n",
    "            tokenizer_outputs = tokenizer(\n",
    "                [prompt],\n",
    "                truncation=False,\n",
    "                return_length=True,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "        \n",
    "            input_ids = tokenizer_outputs[\"input_ids\"].to(device)\n",
    "            attention_mask = tokenizer_outputs[\"attention_mask\"].to(device)\n",
    "            use_cache = getattr(model, \"_supports_cache_class\", False)\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                generation_config=gen_config,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=use_cache,\n",
    "                past_key_values=None,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "    \n",
    "            output_text = tokenizer.decode(\n",
    "                outputs.sequences[0],\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "            yield prompt + \" [START] \" + output_text[len(prompt) + 1 :]\n",
    "\n",
    "for s in generate_text(model, tokenizer, prompts, gen_config, 100, \"cuda:0\"):\n",
    "    print(s)\n",
    "    print(f\"{'-' * 40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b87ddc-9fbf-4799-9d3d-c71e06859401",
   "metadata": {},
   "source": [
    "## Train Bigger Llama\n",
    "\n",
    "Next, let's try training a bigger version of the last model. We will double the dimensions.\n",
    "\n",
    "We will also extend the trainer callbacks by adding text generation every 1000 steps, which can be seen under \"text\" in Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b057e02-fe19-432c-840a-0dad3ec0d50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Included Templates\n",
       "- [configs/train_bigger_llama.yaml](templates/configs/train_bigger_llama.yaml)\n",
       "    - [bigger_llama_project.yaml](templates/bigger_llama_project.yaml)\n",
       "        - [project.yaml](templates/project.yaml)\n",
       "            - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "                - [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "                    - [inc/formatting.jinja](../../../templatelib/base/inc/formatting.jinja)\n",
       "            - [datasets/tiny_stories_abridged.yaml](../../../templatelib/examples/datasets/tiny_stories_abridged.yaml)\n",
       "                - [datasets/tiny_stories.yaml](../../../templatelib/examples/datasets/tiny_stories.yaml)\n",
       "                    - [datasets//base_datasets.yaml](../../../templatelib/base/datasets/base_datasets.yaml)\n",
       "            - [types/training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "                - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "                    - [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "                        - [trainers/minimal_trainer.yaml](../../../templatelib/base/trainers/minimal_trainer.yaml)\n",
       "                - [models/causal_lm/load_model.yaml](../../../templatelib/base/models/causal_lm/load_model.yaml)\n",
       "                    - [models/causal_lm/from_pretrained.yaml](../../../templatelib/base/models/causal_lm/from_pretrained.yaml)\n",
       "                        - [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "                - [types/training_script/training_script.yaml](../../../templatelib/base/types/training_script/training_script.yaml)\n",
       "                    - [types/type.yaml](../../../templatelib/base/types/type.yaml)\n",
       "                        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "            - [project.trainer_config](templates/project.yaml)\n",
       "            - [project.model_config](templates/project.yaml)\n",
       "                - [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "                - [models/llama.yaml](../../../templatelib/examples/models/llama.yaml)\n",
       "                    - [models/causal_lm/from_config.yaml](../../../templatelib/base/models/causal_lm/from_config.yaml)\n",
       "        - [biggerllama.logger_config](templates/bigger_llama_project.yaml)\n",
       "            - [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "        - [biggerllama.model_config](templates/bigger_llama_project.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'A bigger Llama',\n",
       " 'config_name': 'Bigger Llama',\n",
       " 'create_new_model': 'True',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/forgather/datasets',\n",
       " 'eval': 'False',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': './output_models/bigger_llama/runs/bigger_llama_2025-06-22T03-50-42',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'output_dir': './output_models/bigger_llama',\n",
       " 'project_dir': '.',\n",
       " 'save_model': 'True',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'train': 'True',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "## Output Targets\n",
       "- distributed_env\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "- model_code_generator\n",
       "- model_code_writer\n",
       "- model_config\n",
       "- model\n",
       "- train_source_dataset\n",
       "- eval_source_dataset\n",
       "- train_dataset_split\n",
       "- eval_dataset_split\n",
       "- preprocess_args\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- experiment_info\n",
       "- testprompts\n",
       "- generation_config\n",
       "- trainer_callbacks\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- trainer_args\n",
       "- model_preprocessor\n",
       "- trainer\n",
       "- meta\n",
       "- main\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#              Bigger Llama              \n",
       "#---------------------------------------\n",
       "# 2025-06-22T03:50:42\n",
       "# Description: A bigger Llama\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: bigger_llama\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.7.1\n",
       "#     transformers: 4.51.3\n",
       "#     accelerate: 1.7.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"./output_models/bigger_llama\"\n",
       "# ns.logging_dir: \"./output_models/bigger_llama/runs/bigger_llama_2025-06-22T03-50-42\"\n",
       "# ns.create_new_model: True\n",
       "# ns.save_model: True\n",
       "# ns.train: True\n",
       "# ns.eval: False\n",
       "# ns.trust_remote_code: False\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       "distributed_env: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "\n",
       "\n",
       "################ Model #################\n",
       "\n",
       "# https://huggingface.co/docs/transformers/en/model_doc/auto\n",
       "model_constructor_args: &model_constructor_args {}\n",
       "\n",
       "# Name: Llama\n",
       "# Description: Llama model\n",
       "\n",
       "# model_def.source = \"\"\n",
       "# model_def.model_config_cls = \"transformers:LlamaConfig\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       "# Load custom tokenizer from sub-project definition\n",
       "tokenizer: &tokenizer !singleton:forgather.ml.construct:load_from_config@tokenizer\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe\"\n",
       "    config_template: \"2k.yaml\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       "# Model config dependencies\n",
       "\n",
       "model_code_generator: &model_code_generator null\n",
       "\n",
       "model_code_writer: &model_code_writer null    \n",
       "\n",
       "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/configuration_llama.py\n",
       "model_config: &model_config !singleton:transformers:LlamaConfig\n",
       "    vocab_size: !singleton:len [ *tokenizer ]\n",
       "    max_position_embeddings: !singleton:getattr [ *tokenizer, 'model_max_length' ]\n",
       "    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]\n",
       "    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]\n",
       "    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]\n",
       "\n",
       "    # Tiny Llama overrides\n",
       "    hidden_size: 256\n",
       "    intermediate_size: 1024\n",
       "    num_attention_heads: 2\n",
       "    num_key_value_heads: 2\n",
       "    num_hidden_layers: 4\n",
       "\n",
       "    # Bigger Llama overrides\n",
       "    hidden_size: 512\n",
       "    intermediate_size: 2048\n",
       "    num_attention_heads: 4\n",
       "    num_key_value_heads: 4\n",
       "    num_hidden_layers: 8\n",
       "\n",
       "# **Model Factory**\n",
       "\n",
       "model: &model !lambda:transformers:AutoModelForCausalLM.from_config@model\n",
       "    args:\n",
       "        - *model_config\n",
       "    kwargs:\n",
       "        <<: *model_constructor_args\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "# Name: TinyStories Abridged\n",
       "# Define: Abridged to 10% of original size; Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.\n",
       "# Source: https://arxiv.org/abs/2305.07759\n",
       "# Train Dataset: \"roneneldan/TinyStories\" : \"train\"\n",
       "# Eval Dataset: \"roneneldan/TinyStories\" : \"validation\"\n",
       "\n",
       "# **Source Datasets**\n",
       "\n",
       "train_source_dataset: &train_source_dataset !singleton:datasets:load_dataset@train_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "eval_source_dataset: &eval_source_dataset !singleton:datasets:load_dataset@eval_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "# **Dataset Splits**\n",
       "\n",
       "train_dataset_split: &train_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"train\"\n",
       "\n",
       "eval_dataset_split: &eval_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"validation\"\n",
       "\n",
       "# **Preprocess Dataset Args**\n",
       "\n",
       "preprocess_args: &preprocess_args\n",
       "    truncation: True\n",
       "\n",
       "# **Preprocessed Datasets**\n",
       "\n",
       "train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset\n",
       "    dataset: *train_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 0.1\n",
       "    desc: \"Tokenizing train\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset\n",
       "    dataset: *eval_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 500\n",
       "    desc: \"Tokenizing validation split\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM\n",
       "    tokenizer: *tokenizer\n",
       "    return_tensors: pt\n",
       "\n",
       "    # Tiny Llama\n",
       "    truncation: True\n",
       "    max_length: 512\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/bigger_llama/runs/bigger_llama_2025-06-22T03-50-42\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       "experiment_info: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-06-22T03:50:42\"\n",
       "    name: \"Bigger Llama\"\n",
       "    description: \"A bigger Llama\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.7.1', 'transformers': '4.51.3', 'accelerate': '1.7.0'}\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       "# The model will be given the following prompts for text-gen at regular intervals.\n",
       "testprompts: &testprompts !list:@testprompts\n",
       "    # Test prompts from \"https://arxiv.org/abs/2305.07759\"\n",
       "    - \"Alice was so tired when she got back home so she went\"\n",
       "    - \"Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was\"\n",
       "    - \"Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \\\"Look, Lily. A rainbow has\"\n",
       "    - \"Jack wanted to read a book, so he went to\"\n",
       "    - \"\\\"Can cows fly?\\\" Alice asked her mother.\"\n",
       "    - \"\\\"What do birds like to eat?\\\" Tom asked his mother.\"\n",
       "    - \"\\\"What language do they speak in France?\\\" Tom asked his mother.\"\n",
       "    - \"If I throw a ball up in the air, eventually it will\"\n",
       "    - \"It was winter and cold outside so his mother told him, \\\"You should\"\n",
       "    - \"Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked\"\n",
       "    - \"Jack told Mary, \\\"If you give me your banana, I'll give you my apple.\\\" Mary gave Jack her Banana, so\"\n",
       "    - \"On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to\"\n",
       "    - \"Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that\"\n",
       "    - \"Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \\\"I want to go to the park\\\". Lily says\"\n",
       "    - \"Jack's mother was not home, and his father was at home. When Jack came home, he said hello to\"\n",
       "    - \"Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says\"\n",
       "    - \"Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They\"\n",
       "    - \"Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door,\"\n",
       "\n",
       "# Conservative text-generation parameters.\n",
       "generation_config: &generation_config !dict:@generation_config\n",
       "    identity: generation_config\n",
       "    do_sample: True\n",
       "    top_k: 20\n",
       "    top_p: 0.9\n",
       "    temperature: 0.7\n",
       "    repitition_penalty: 1.15\n",
       "\n",
       "trainer_callbacks: &trainer_callbacks !list:@trainer_callbacks\n",
       "    # Log all training output to JSON\n",
       "    - !singleton:forgather.ml.json_logger:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    - !singleton:forgather.ml.tb_logger:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "    - !singleton:forgather.ml.textgen_callback:TextgenCallback\n",
       "        summary_writer: *summary_writer\n",
       "        prompts: *testprompts\n",
       "        generation_config: *generation_config\n",
       "        max_new_tokens: 40\n",
       "        generation_steps: 1000\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       "optimizer: &optimizer !lambda:torch:optim.AdamW\n",
       "    lr: 1.0e-3\n",
       "\n",
       "############# LR Scheduler #############\n",
       "\n",
       "lr_scheduler: &lr_scheduler ~\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: forgather.ml.trainer.Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       "trainer_args: &trainer_args\n",
       "    # Minimal Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    output_dir: \"./output_models/bigger_llama\"\n",
       "    logging_dir: \"./output_models/bigger_llama/runs/bigger_llama_2025-06-22T03-50-42\"\n",
       "    logging_steps: 500\n",
       "    per_device_train_batch_size: 16\n",
       "    per_device_eval_batch_size: 32\n",
       "    num_train_epochs: 1\n",
       "    # Base Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    overwrite_output_dir: True\n",
       "    eval_steps: 100\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"no\"\n",
       "    logging_strategy: \"steps\"\n",
       "\n",
       "    # Tiny Llama Project Overrides\n",
       "    seed: 42\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 64\n",
       "    logging_steps: 100\n",
       "    eval_steps: 500\n",
       "    num_train_epochs: 1\n",
       "    dataloader_num_workers: 1\n",
       "\n",
       "\n",
       "model_preprocessor: &model_preprocessor !partial:call [ *model ]\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       "trainer: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    model_init: *model_preprocessor\n",
       "    args: !singleton:forgather.ml.trainer_types:TrainingArguments@trainer_args\n",
       "        <<: *trainer_args\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Bigger Llama\"\n",
       "    config_description: \"A bigger Llama\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/ai_assets/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "    output_dir: \"./output_models/bigger_llama\"\n",
       "    model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "    logging_dir: \"./output_models/bigger_llama/runs/bigger_llama_2025-06-22T03-50-42\"\n",
       "    create_new_model: \"True\"\n",
       "    save_model: \"True\"\n",
       "    train: \"True\"\n",
       "    eval: \"False\"\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_save: True\n",
       "    do_train: True\n",
       "    do_eval: False\n",
       "    # Init distributed envrionment before initializing anyting which depends on it.\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "    pp_config: !var \"pp_config\"\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_config(config_template=\"train_bigger_llama.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f477a8-c77e-429d-8a6e-46dc03145f71",
   "metadata": {},
   "source": [
    "## Start Tensorboard with CLI\n",
    "\n",
    "This time, start Tensorboard using the forgather CLI\n",
    "\n",
    "```bash\n",
    "fgcli.py tb --all\n",
    "\n",
    "# For bind all\n",
    "fgcli.py tb --all -- --bind_all\n",
    "```\n",
    "\n",
    "The \"--all\" argument will show all models in the output directory, rather than just the default project. As above, if this is not in your path, it's located in the bin directory.\n",
    "\n",
    "Remember to check for the generated sample text in Tensorboard, which will provide a more subjective measure of the model's progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72960f38-6af3-4de6-86df-d4b5e2bf1acf",
   "metadata": {},
   "source": [
    "## Train the Bigger Model\n",
    "\n",
    "Remember to shut-down the kernel in the notebook first, if the model is still loaded on the GPU.\n",
    "\n",
    "```bash\n",
    "fgcli.py -t train_bigger_llama.yaml train -d 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873413b3-362d-4436-b10f-7d048a7a49d7",
   "metadata": {},
   "source": [
    "### Extra Credit\n",
    "\n",
    "The learning-rate is likely not ideal for this larger model. Create new configurations, based upon \"train_bigger_model.yaml,\" to experiment with different learning rates.\n",
    "\n",
    "Hint: You will need to override the \"optimizer\" block.\n",
    "\n",
    "```yaml\n",
    "-- block optimizer\n",
    "    == super()\n",
    "    # Experiment overrides.\n",
    "    lr: new_lr_here\n",
    "-- endblock optimizer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e13d5-9597-43a4-956c-f438674b7f56",
   "metadata": {},
   "source": [
    "## Train on the Complete Dataset\n",
    "\n",
    "Finally, we have a configuration to train the bigger model on the full Tiny Stories dataset.\n",
    "\n",
    "In addtion to switch to the full dataset, we also add a custom learning rate scheduler. Take a look at the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e44fde-65d6-4dce-8ad3-c1196313d4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Included Templates\n",
       "- [configs/train_bigger_llama_full.yaml](templates/configs/train_bigger_llama_full.yaml)\n",
       "    - [datasets/tiny_stories.yaml](../../../templatelib/examples/datasets/tiny_stories.yaml)\n",
       "        - [datasets//base_datasets.yaml](../../../templatelib/base/datasets/base_datasets.yaml)\n",
       "            - [inc/formatting.jinja](../../../templatelib/base/inc/formatting.jinja)\n",
       "    - [bigger_llama_project.yaml](templates/bigger_llama_project.yaml)\n",
       "        - [project.yaml](templates/project.yaml)\n",
       "            - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "                - [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "            - [datasets/tiny_stories_abridged.yaml](../../../templatelib/examples/datasets/tiny_stories_abridged.yaml)\n",
       "            - [types/training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "                - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "                    - [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "                        - [trainers/minimal_trainer.yaml](../../../templatelib/base/trainers/minimal_trainer.yaml)\n",
       "                - [models/causal_lm/load_model.yaml](../../../templatelib/base/models/causal_lm/load_model.yaml)\n",
       "                    - [models/causal_lm/from_pretrained.yaml](../../../templatelib/base/models/causal_lm/from_pretrained.yaml)\n",
       "                        - [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "                - [types/training_script/training_script.yaml](../../../templatelib/base/types/training_script/training_script.yaml)\n",
       "                    - [types/type.yaml](../../../templatelib/base/types/type.yaml)\n",
       "                        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "            - [project.trainer_config](templates/project.yaml)\n",
       "            - [project.model_config](templates/project.yaml)\n",
       "                - [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "                - [models/llama.yaml](../../../templatelib/examples/models/llama.yaml)\n",
       "                    - [models/causal_lm/from_config.yaml](../../../templatelib/base/models/causal_lm/from_config.yaml)\n",
       "        - [biggerllama.logger_config](templates/bigger_llama_project.yaml)\n",
       "            - [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "        - [biggerllama.model_config](templates/bigger_llama_project.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'A bigger Llama with the full Tiny Stories Dataset',\n",
       " 'config_name': 'Bigger Llama Full',\n",
       " 'create_new_model': 'True',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/forgather/datasets',\n",
       " 'eval': 'False',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': './output_models/bigger_llama/runs/bigger_llama_full_2025-06-22T03-59-12',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'output_dir': './output_models/bigger_llama',\n",
       " 'project_dir': '.',\n",
       " 'save_model': 'True',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'train': 'True',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "## Output Targets\n",
       "- distributed_env\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "- model_code_generator\n",
       "- model_code_writer\n",
       "- model_config\n",
       "- model\n",
       "- train_source_dataset\n",
       "- eval_source_dataset\n",
       "- train_dataset_split\n",
       "- eval_dataset_split\n",
       "- preprocess_args\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- experiment_info\n",
       "- testprompts\n",
       "- generation_config\n",
       "- trainer_callbacks\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- trainer_args\n",
       "- model_preprocessor\n",
       "- trainer\n",
       "- meta\n",
       "- main\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_config(config_template=\"train_bigger_llama_full.yaml\", show_pp_config=False, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb6c21-28cd-47e0-ae75-b18e0d500ca6",
   "metadata": {},
   "source": [
    "## Show Preprocessed Config\n",
    "\n",
    "Rather than dumping the pp_config in the notebook, try it from the CLI.\n",
    "\n",
    "```bash\n",
    "fgcli.py -t train_bigger_llama_full.yaml pp | less\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032512d-af4f-457e-a4a5-8b1cc96147d3",
   "metadata": {},
   "source": [
    "## Train the Full Model\n",
    "\n",
    "This could take a bit of time...\n",
    "\n",
    "```bash\n",
    "fgcli.py -t train_bigger_llama_full.yaml train -d 0\n",
    "```\n",
    "\n",
    "When done training, you can load the model into the notebook for experimentation.\n",
    "\n",
    "Note that we saved it to a different path the the tiny model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927eb80-1c81-4d9f-96dd-304a1a9ff5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GenerationConfig, StoppingCriteria\n",
    "\n",
    "model_path = \"./output_models/bigger_llama\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
