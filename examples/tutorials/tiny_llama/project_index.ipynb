{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Tiny LLama\n",
       "\n",
       "In this tutorial we will train a very small Llama model (about 5M parameters) on 10% of the Tiny Stories dataset. On a single RTX-4090, this takes about three minutes. Once training is complete, we will load the model an use it for text generation -- and the generation will be reasonably coherent for a three-minute-old model.\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "    - [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/forgather_workspace](../../../forgather_workspace)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/modellib](../../../templatelib/modellib)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/examples](../../../templatelib/examples)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/base](../../../templatelib/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [train_hf_llama.yaml](templates/configs/train_hf_llama.yaml)\n",
       "- [train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "- [experimental_llama.yaml](templates/configs/experimental_llama.yaml)\n",
       "\n",
       "Default Configuration: train_tiny_llama.yaml\n",
       "\n",
       "## Available Templates\n",
       "- [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "- [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "- [datasets/llm_dataset_project.yaml](../../../templatelib/examples/datasets/llm_dataset_project.yaml)\n",
       "- [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "- [prompts/short_stories.yaml](../../../templatelib/examples/prompts/short_stories.yaml)\n",
       "- [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "- [tokenizers/tiny_8k.yaml](../../../templatelib/examples/tokenizers/tiny_8k.yaml)\n",
       "- [tokenizers/wikitext/32k.yaml](../../../templatelib/examples/tokenizers/wikitext/32k.yaml)\n",
       "- [tokenizers/wikitext/8k.yaml](../../../templatelib/examples/tokenizers/wikitext/8k.yaml)\n",
       "- [config_type.yaml](../../../templatelib/base/config_type.yaml)\n",
       "    - [datasets/dataset_type.yaml](../../../templatelib/base/datasets/dataset_type.yaml)\n",
       "        - [datasets/tokenized_dataset.yaml](../../../templatelib/base/datasets/tokenized_dataset.yaml)\n",
       "    - [models/model_type.yaml](../../../templatelib/base/models/model_type.yaml)\n",
       "    - [training_script/training_script_type.yaml](../../../templatelib/base/training_script/training_script_type.yaml)\n",
       "        - [training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [project.yaml](templates/project.yaml)\n",
       "                - [configs/train_hf_llama.yaml](templates/configs/train_hf_llama.yaml)\n",
       "                - [configs/train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "                - [configs/experimental_llama.yaml](templates/configs/experimental_llama.yaml)\n",
       "    - [tokenizers/tokenizer_type.yaml](../../../templatelib/base/tokenizers/tokenizer_type.yaml)\n",
       "        - [tokenizers/bpe/bpe.yaml](../../../templatelib/base/tokenizers/bpe/bpe.yaml)\n",
       "- [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "    - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "        - [trainers/accel_trainer.yaml](../../../templatelib/base/trainers/accel_trainer.yaml)\n",
       "        - [trainers/pipeline_trainer.yaml](../../../templatelib/base/trainers/pipeline_trainer.yaml)\n",
       "    - [trainers/hf_trainer.yaml](../../../templatelib/base/trainers/hf_trainer.yaml)\n",
       "- [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "    - [models/causal_lm/from_pretrained.yaml](../../../templatelib/base/models/causal_lm/from_pretrained.yaml)\n",
       "    - [models/causal_lm/from_pretrained_config.yaml](../../../templatelib/base/models/causal_lm/from_pretrained_config.yaml)\n",
       "    - [models/causal_lm/custom.yaml](../../../templatelib/base/models/causal_lm/custom.yaml)\n",
       "        - [models/causal_lm/custom_dynamic.yaml](../../../templatelib/base/models/causal_lm/custom_dynamic.yaml)\n",
       "            - [models/transformers/deepone.yaml](../../../templatelib/examples/models/transformers/deepone.yaml)\n",
       "            - [models/transformers/dynamic_causal_transformer.yaml](../../../templatelib/examples/models/transformers/dynamic_causal_transformer.yaml)\n",
       "            - [models/transformers/dynamic_llama.yaml](../../../templatelib/examples/models/transformers/dynamic_llama.yaml)\n",
       "                - [models/tiny_dynamic_llama.yaml](templates/models/tiny_dynamic_llama.yaml)\n",
       "                    - [project.model_config](templates/project.yaml)\n",
       "                    - [experiment.model_config](templates/configs/experimental_llama.yaml)\n",
       "    - [models/causal_lm/from_config.yaml](../../../templatelib/base/models/causal_lm/from_config.yaml)\n",
       "        - [models/transformers/gpt2.yaml](../../../templatelib/examples/models/transformers/gpt2.yaml)\n",
       "        - [models/transformers/llama.yaml](../../../templatelib/examples/models/transformers/llama.yaml)\n",
       "            - [models/tiny_hf_llama.yaml](templates/models/tiny_hf_llama.yaml)\n",
       "- [models/causal_lm/import_model_project.yaml](../../../templatelib/base/models/causal_lm/import_model_project.yaml)\n",
       "- [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "    - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "        - [project.logger_config](templates/project.yaml)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "nb.display_project_index(show_available_templates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4576985-d53c-4b74-b68b-31ec1e8fcdbd",
   "metadata": {},
   "source": [
    "---\n",
    "This example makes extensive use of the Forgather templates library. Take a look at the various files which go into the configuration and compare these to the pre-processed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d9e625-31e4-4694-b77c-cbcc32813b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Included Templates\n",
       "- [configs/train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [datasets/llm_dataset_project.yaml](../../../templatelib/examples/datasets/llm_dataset_project.yaml)\n",
       "        - [models/causal_lm/import_model_project.yaml](../../../templatelib/base/models/causal_lm/import_model_project.yaml)\n",
       "        - [project.logger_config](templates/project.yaml)\n",
       "            - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "                - [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "                    - [inc/formatting.jinja](../../../templatelib/base/inc/formatting.jinja)\n",
       "            - [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "            - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "                - [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "        - [training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [training_script/training_script_type.yaml](../../../templatelib/base/training_script/training_script_type.yaml)\n",
       "                - [config_type.yaml](../../../templatelib/base/config_type.yaml)\n",
       "                    - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'A demo of training a tiny llama model from scratch',\n",
       " 'config_name': 'Tiny Llama',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/forgather/datasets',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': './output_models/tiny_llama/runs/log_2025-09-19T10-05-42',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'nproc_per_node': 1,\n",
       " 'output_dir': './output_models/tiny_llama',\n",
       " 'project_dir': '.',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "## Output Targets\n",
       "- distributed_env\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "- model\n",
       "- tokenizer_args\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- experiment_info\n",
       "- testprompts\n",
       "- generation_config\n",
       "- trainer_callbacks\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- trainer_args\n",
       "- model_preprocessor\n",
       "- trainer\n",
       "- dynamic_args\n",
       "- meta\n",
       "- main\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#               Tiny Llama               \n",
       "#---------------------------------------\n",
       "# 2025-09-19T10:05:42\n",
       "# Description: A demo of training a tiny llama model from scratch\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: tiny_llama\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.8.0\n",
       "#     transformers: 4.56.1\n",
       "#     accelerate: 1.10.1\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"./output_models/tiny_llama\"\n",
       "# ns.logging_dir: \"./output_models/tiny_llama/runs/log_2025-09-19T10-05-42\"\n",
       "# ns.nproc_per_node: 1\n",
       "# ns.trust_remote_code: False\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       "distributed_env: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "\n",
       "\n",
       "################ Model #################\n",
       "\n",
       "# https://huggingface.co/docs/transformers/en/model_doc/auto\n",
       "model_constructor_args: &model_constructor_args {}\n",
       "\n",
       "# Import a model definition from another Forgather project\n",
       ".define: &model_dict !call:forgather:from_project\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/models/llama\"\n",
       "    config_template: \"4M.yaml\"\n",
       "    targets: [  \"pretrained_tokenizer\", \"pretrained_model_ctor\" ] \n",
       "    pp_kwargs:\n",
       "        output_dir: \"./output_models/tiny_llama\"\n",
       "    pp_debug: False\n",
       "    model_constructor_args: *model_constructor_args\n",
       "\n",
       "tokenizer: &tokenizer !call:getitem [ *model_dict, 'pretrained_tokenizer' ]\n",
       "model: &model !call:getitem [ *model_dict, 'pretrained_model_ctor' ]\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "tokenizer_args: &tokenizer_args !dict\n",
       "    truncation: True\n",
       "    max_length: 512    \n",
       "\n",
       "# Load dataset from sub-project\n",
       ".define: &dataset_dict !call:forgather:from_project\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/datasets/roneneldan\"\n",
       "    config_template: \"tinystories-abridged.yaml\"\n",
       "    targets: [  \"train_dataset\", \"eval_dataset\" ] \n",
       "    preprocess_args: *tokenizer_args\n",
       "    tokenizer: *tokenizer\n",
       "\n",
       "train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]\n",
       "eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM\n",
       "    tokenizer: *tokenizer\n",
       "    return_tensors: pt\n",
       "\n",
       "    # Tiny Llama\n",
       "    truncation: True\n",
       "    max_length: 512\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/tiny_llama/runs/log_2025-09-19T10-05-42\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       "experiment_info: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-09-19T10:05:42\"\n",
       "    name: \"Tiny Llama\"\n",
       "    description: \"A demo of training a tiny llama model from scratch\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.8.0', 'transformers': '4.56.1', 'accelerate': '1.10.1'}\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       "# The model will be given the following prompts for text-gen at regular intervals.\n",
       "testprompts: &testprompts !list:@testprompts\n",
       "    # Test prompts from \"https://arxiv.org/abs/2305.07759\"\n",
       "    - \"Alice was so tired when she got back home so she went\"\n",
       "    - \"Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was\"\n",
       "    - \"Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \\\"Look, Lily. A rainbow has\"\n",
       "    - \"Jack wanted to read a book, so he went to\"\n",
       "    - \"\\\"Can cows fly?\\\" Alice asked her mother.\"\n",
       "    - \"\\\"What do birds like to eat?\\\" Tom asked his mother.\"\n",
       "    - \"\\\"What language do they speak in France?\\\" Tom asked his mother.\"\n",
       "    - \"If I throw a ball up in the air, eventually it will\"\n",
       "    - \"It was winter and cold outside so his mother told him, \\\"You should\"\n",
       "    - \"Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked\"\n",
       "    - \"Jack told Mary, \\\"If you give me your banana, I'll give you my apple.\\\" Mary gave Jack her Banana, so\"\n",
       "    - \"On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to\"\n",
       "    - \"Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that\"\n",
       "    - \"Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \\\"I want to go to the park\\\". Lily says\"\n",
       "    - \"Jack's mother was not home, and his father was at home. When Jack came home, he said hello to\"\n",
       "    - \"Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says\"\n",
       "    - \"Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They\"\n",
       "    - \"Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door,\"\n",
       "\n",
       "# Conservative text-generation parameters.\n",
       "generation_config: &generation_config !dict:@generation_config\n",
       "    identity: generation_config\n",
       "    do_sample: True\n",
       "    top_k: 20\n",
       "    top_p: 0.9\n",
       "    temperature: 0.7\n",
       "    repitition_penalty: 1.15\n",
       "trainer_callbacks: &trainer_callbacks !dlist:@trainer_callbacks\n",
       "    null: ~\n",
       "    # Log all training output to JSON\n",
       "    json_logger: !singleton:forgather.ml.trainer.callbacks:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    tb_logger: !singleton:forgather.ml.trainer.callbacks:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "    text_gen_callback: !singleton:forgather.ml.trainer.callbacks:TextgenCallback\n",
       "        summary_writer: *summary_writer\n",
       "        prompts: *testprompts\n",
       "        generation_config: *generation_config\n",
       "        max_new_tokens: 40\n",
       "        generation_steps: 1000\n",
       "    \n",
       "    # Allow remote control of the training process\n",
       "    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       "optimizer: &optimizer !partial:torch:optim.AdamW\n",
       "    lr: 1.0e-3\n",
       "\n",
       "############# LR Scheduler #############\n",
       "\n",
       "# https://arxiv.org/html/2503.02844v1\n",
       "lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler\n",
       "    warmup_steps: 500\n",
       "    cooldown_steps: 50000\n",
       "    constant_lr: 1.0e-4\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: Forgather Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "# Trainer Config Class: forgather.ml.trainer:TrainingArguments\n",
       "# Trainer Class: forgather.ml.trainer:Trainer\n",
       "# nproc_per_node: 1\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       "\n",
       "\n",
       "trainer_args: &trainer_args !singleton:forgather.ml.trainer:TrainingArguments@trainer_args\n",
       "    save_strategy: \"no\"\n",
       "    max_steps: -1\n",
       "    output_dir: \"./output_models/tiny_llama\"\n",
       "    logging_dir: \"./output_models/tiny_llama/runs/log_2025-09-19T10-05-42\"\n",
       "    # Tiny Llama Project Overrides\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"steps\"\n",
       "    save_steps: 10000\n",
       "    # Safetensors can't handle tied parameters/buffers, so fallback to PyTorch format.\n",
       "    save_safetensors: False\n",
       "    seed: 42\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 64\n",
       "    logging_steps: 100\n",
       "    eval_steps: 500\n",
       "    num_train_epochs: 1\n",
       "    dataloader_num_workers: 1\n",
       "\n",
       "model_preprocessor: &model_preprocessor !partial:call\n",
       "    - *model\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       "trainer: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    args: *trainer_args\n",
       "    model_init: *model_preprocessor\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "\n",
       "    # **Trainer**\n",
       "    compute_loss_func: !singleton:forgather.ml.loss:CausalLoss\n",
       "    distributed_env: *distributed_env\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "# **Dynamic Args**\n",
       "dynamic_args: !dlist\n",
       "    null: ~\n",
       "    max_steps:\n",
       "        names: \"--max-steps\"\n",
       "        type: \"int\"\n",
       "        help: \"Set maximum training steps\"\n",
       "    save_strategy:\n",
       "        names: [ \"--save-strategy\", \"-S\" ]\n",
       "        choices: [ \"no\", \"steps\", \"epoch\" ]\n",
       "        type: \"str\"\n",
       "        help: \"When to save checkpoints\"\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Tiny Llama\"\n",
       "    config_description: \"A demo of training a tiny llama model from scratch\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/ai_assets/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "    output_dir: \"./output_models/tiny_llama\"\n",
       "    model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "    logging_dir: \"./output_models/tiny_llama/runs/log_2025-09-19T10-05-42\"\n",
       "    nproc_per_node: 1\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_train: True\n",
       "    do_save: False\n",
       "    do_eval: False\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_config(config_template=\"\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bcb135-37fe-4f61-80e7-dc8d71c08294",
   "metadata": {},
   "source": [
    "## Load Project\n",
    "\n",
    "Load the default configuraiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1f9b45-db49-475f-8c73-b47d0e9ab08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "# Load the default project, which is \"train_tiny_llama.yaml\"\n",
    "proj = Project()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15abe5-957f-43ff-af18-0c8725fe961e",
   "metadata": {},
   "source": [
    "## Start Tensorboard\n",
    "\n",
    "This project has been configured to log training to Tensorboard (TB). To watch the model's training progress with TB, run the following command, which will generate a CLI command to start the TB server. Then run the command from a shell.\n",
    "\n",
    "Tensorboard can be started from a terminal like this:\n",
    "\n",
    "```bash\n",
    "# By default, Tensorboard bind only to localhost. To bind to all interfaces, add --bind_all\n",
    "tensorboard --logdir \"/path/to/model/log/directory\" [--bind_all]\n",
    "```\n",
    "\n",
    "You can use the CLI to launch TB for you, where it will automatically determine the path to the log directory:\n",
    "\n",
    "```bash\n",
    "# --all : Watch all output model directories, otherwise just the one for the current configuration.\n",
    "# -- : Any arguments after '--' are passed directly to tensorboard, for example \"--bind_all\"\n",
    "cd PROJECT_DIR\n",
    "cfcli.py tb [--all] [-- <tensorboard-args>]\n",
    "```\n",
    "\n",
    "When TB starts, it should provide the URL to access it. e.g.\n",
    "\n",
    "```\n",
    "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b3946-1faa-4224-afe8-58233a57e41c",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "You have a few options for training the mode.\n",
    "\n",
    "1. Run it directly from the notebook. This should work find with this example, although for projects using multiple GPUs, you will want to use one of the other options. To train from the notebook, just run the following cell.\n",
    "2. You can generate a training script and run it from the shell. To do so, run the cell with \"generate_trainingscript(),\" then run the generated shell script from a terminal.\n",
    "3. You can use the Forgather CLI.\n",
    "\n",
    "```bash\n",
    "# Open a shell in thie project's directory, then run this command:\n",
    "cd PROJECT_DIR\n",
    "forgather train\n",
    "\n",
    "# See forgather --help for more details.\n",
    "```\n",
    "\n",
    "Once training starts, switch to Tensorboard in your browser. One of the first things you will want to do is enable automatic refresh. To do so, click the gear in the upper-right corner and check \"Reload Data.\"\n",
    "\n",
    "Once training has started, take a look at the \"Text\" tab. You will see that we have automatically logged the preprocessed configuraiton as well as having dumped the primary training artifacts.\n",
    "\n",
    "Next, switch to the \"Scalars\" tab. You will see a plot of train and evaluation loss which will automatically update every 30 seconds. If you are not familiar with Tensorboard, now would be a good time to play with the UI elements to see how they work.\n",
    "\n",
    "When training completes, the model will be automatically saved to the output directory (\"./output_models/default_model\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7f6eb9-33ec-489c-b66e-faf3e3f0bdee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b2393c85ea49a69e31d9da3bf723fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/211972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcff9b228144a8691237650551d9fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95100ed2a0b34703a91c846f84a19474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 212,000\n",
      "total_train_samples: 212,000\n",
      "per_device_train_batch_size: 32\n",
      "actual_per_device_batch_size: 32\n",
      "total_train_batch_size: 32\n",
      "max_steps: 6,625\n",
      "total_parameters: 4.2M\n",
      "trainable_parameters: 4.2M\n",
      "model:\n",
      "DynamicCasualLM(\n",
      "  (causal_lm): CasualLM(\n",
      "    loss_fn=CausalLoss()\n",
      "    (input_encoder): InputEncoder(\n",
      "      d_model=256, vocab_size=2000\n",
      "      (dropout): Identity()\n",
      "      (embedding): Embedding(2000, 256)\n",
      "    )\n",
      "    (output_decoder): Linear(in_features=256, out_features=2000, bias=False)\n",
      "    (layer_stack): LayerStack(\n",
      "      gradient_checkpointing=False, checkpoint_stride=1\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x PreLNLayer(\n",
      "          (feedforward): GLUFeedforwardLayer(\n",
      "            d_model=256, d_feedforward=676\n",
      "            (up_proj): Linear(in_features=256, out_features=676, bias=False)\n",
      "            (gate_proj): Linear(in_features=256, out_features=676, bias=False)\n",
      "            (down_proj): Linear(in_features=676, out_features=256, bias=False)\n",
      "            (activation): SiLU()\n",
      "            (dropout): Identity()\n",
      "          )\n",
      "          (attention): CausalRpeAttn(\n",
      "            d_model=256, num_heads=2, num_kv_heads=2\n",
      "            (pos_encoder): RealRotaryPE(d_head=128, max_sequence_length=2048, rope_theta=10000.0)\n",
      "            (query_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (key_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (value_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (output_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (norm1): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Identity()\n",
      "          (residual_dropout): Identity()\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "2025-09-19 10:06:51          100  0.02  train-loss: 7.03751   grad-norm: 0.68158   learning-rate: 2.00e-04\n",
      "2025-09-19 10:06:53          200  0.03  train-loss: 4.26383   grad-norm: 0.48972   learning-rate: 4.00e-04\n",
      "2025-09-19 10:06:55          300  0.05  train-loss: 3.38328   grad-norm: 0.53864   learning-rate: 6.00e-04\n",
      "2025-09-19 10:06:57          400  0.06  train-loss: 3.11105   grad-norm: 0.53277   learning-rate: 8.00e-04\n",
      "2025-09-19 10:06:59          500  0.08  train-loss: 2.83541   grad-norm: 0.46671   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b334db145c884f46b9d22a1bc782af07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:07:00          500  0.08  eval-loss:  2.69354   \n",
      "2025-09-19 10:07:04          600  0.09  train-loss: 2.66498   grad-norm: 0.45028   learning-rate: 1.00e-03\n",
      "2025-09-19 10:07:06          700  0.11  train-loss: 2.478     grad-norm: 0.44827   learning-rate: 1.00e-03\n",
      "2025-09-19 10:07:08          800  0.12  train-loss: 2.41238   grad-norm: 0.4311    learning-rate: 1.00e-03\n",
      "2025-09-19 10:07:11          900  0.14  train-loss: 2.27736   grad-norm: 0.4189    learning-rate: 1.00e-03\n",
      "2025-09-19 10:07:13        1,000  0.15  train-loss: 2.12937   grad-norm: 0.41794   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bfa722e399403792b4a1b8f97a2ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:07:13        1,000  0.15  eval-loss:  2.0445    \n",
      "2025-09-19 10:07:17        1,100  0.17  train-loss: 2.11906   grad-norm: 0.41671   learning-rate: 1.00e-03\n",
      "2025-09-19 10:07:20        1,200  0.18  train-loss: 2.08488   grad-norm: 0.4173    learning-rate: 1.00e-03\n",
      "2025-09-19 10:07:22        1,300  0.2   train-loss: 2.04039   grad-norm: 0.40087   learning-rate: 9.99e-04\n",
      "2025-09-19 10:07:24        1,400  0.21  train-loss: 2.00084   grad-norm: 0.39857   learning-rate: 9.99e-04\n",
      "2025-09-19 10:07:26        1,500  0.23  train-loss: 1.96937   grad-norm: 0.40235   learning-rate: 9.99e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fef5b2074f4339b1bdb3ae0ede0b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:07:26        1,500  0.23  eval-loss:  1.7836    \n",
      "2025-09-19 10:07:29        1,600  0.24  train-loss: 1.92732   grad-norm: 0.39787   learning-rate: 9.99e-04\n",
      "2025-09-19 10:07:31        1,700  0.26  train-loss: 1.88687   grad-norm: 0.3942    learning-rate: 9.99e-04\n",
      "2025-09-19 10:07:33        1,800  0.27  train-loss: 1.838     grad-norm: 0.4001    learning-rate: 9.98e-04\n",
      "2025-09-19 10:07:35        1,900  0.29  train-loss: 1.81779   grad-norm: 0.39304   learning-rate: 9.98e-04\n",
      "2025-09-19 10:07:37        2,000  0.3   train-loss: 1.85246   grad-norm: 0.38301   learning-rate: 9.98e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ee83ad90fe4d6fb3c9a738a3be33f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:07:38        2,000  0.3   eval-loss:  1.66765   \n",
      "2025-09-19 10:07:42        2,100  0.32  train-loss: 1.80197   grad-norm: 0.37383   learning-rate: 9.98e-04\n",
      "2025-09-19 10:07:44        2,200  0.33  train-loss: 1.75699   grad-norm: 0.38189   learning-rate: 9.97e-04\n",
      "2025-09-19 10:07:46        2,300  0.35  train-loss: 1.72324   grad-norm: 0.39111   learning-rate: 9.97e-04\n",
      "2025-09-19 10:07:49        2,400  0.36  train-loss: 1.7833    grad-norm: 0.39717   learning-rate: 9.97e-04\n",
      "2025-09-19 10:07:51        2,500  0.38  train-loss: 1.74988   grad-norm: 0.39627   learning-rate: 9.96e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3030744c59b94ef69f14e8a204e9d26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:07:51        2,500  0.38  eval-loss:  1.58031   \n",
      "2025-09-19 10:07:53        2,600  0.39  train-loss: 1.74971   grad-norm: 0.38706   learning-rate: 9.96e-04\n",
      "2025-09-19 10:07:55        2,700  0.41  train-loss: 1.69475   grad-norm: 0.37888   learning-rate: 9.96e-04\n",
      "2025-09-19 10:07:57        2,800  0.42  train-loss: 1.73729   grad-norm: 0.37353   learning-rate: 9.95e-04\n",
      "2025-09-19 10:08:00        2,900  0.44  train-loss: 1.65546   grad-norm: 0.3872    learning-rate: 9.95e-04\n",
      "2025-09-19 10:08:02        3,000  0.45  train-loss: 1.56566   grad-norm: 0.36852   learning-rate: 9.94e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec2e16e22924f02b805240c984692c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:08:02        3,000  0.45  eval-loss:  1.48591   \n",
      "2025-09-19 10:08:06        3,100  0.47  train-loss: 1.6476    grad-norm: 0.38164   learning-rate: 9.94e-04\n",
      "2025-09-19 10:08:08        3,200  0.48  train-loss: 1.7147    grad-norm: 0.36078   learning-rate: 9.94e-04\n",
      "2025-09-19 10:08:11        3,300  0.5   train-loss: 1.62638   grad-norm: 0.35737   learning-rate: 9.93e-04\n",
      "2025-09-19 10:08:13        3,400  0.51  train-loss: 1.56545   grad-norm: 0.368     learning-rate: 9.93e-04\n",
      "2025-09-19 10:08:15        3,500  0.53  train-loss: 1.58629   grad-norm: 0.35055   learning-rate: 9.92e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6372a28c3034a0fa32fd1726a3acff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:08:15        3,500  0.53  eval-loss:  1.4569    \n",
      "2025-09-19 10:08:17        3,600  0.54  train-loss: 1.63956   grad-norm: 0.36239   learning-rate: 9.91e-04\n",
      "2025-09-19 10:08:20        3,700  0.56  train-loss: 1.56118   grad-norm: 0.36122   learning-rate: 9.91e-04\n",
      "2025-09-19 10:08:22        3,800  0.57  train-loss: 1.56943   grad-norm: 0.36209   learning-rate: 9.90e-04\n",
      "2025-09-19 10:08:24        3,900  0.59  train-loss: 1.61105   grad-norm: 0.35517   learning-rate: 9.90e-04\n",
      "2025-09-19 10:08:26        4,000  0.6   train-loss: 1.64238   grad-norm: 0.35412   learning-rate: 9.89e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf74012d64d94132bbe2e614d0a050c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:08:26        4,000  0.6   eval-loss:  1.42438   \n",
      "2025-09-19 10:08:31        4,100  0.62  train-loss: 1.54302   grad-norm: 0.34402   learning-rate: 9.89e-04\n",
      "2025-09-19 10:08:33        4,200  0.63  train-loss: 1.53296   grad-norm: 0.35615   learning-rate: 9.88e-04\n",
      "2025-09-19 10:08:35        4,300  0.65  train-loss: 1.56578   grad-norm: 0.35344   learning-rate: 9.87e-04\n",
      "2025-09-19 10:08:38        4,400  0.66  train-loss: 1.61292   grad-norm: 0.34805   learning-rate: 9.87e-04\n",
      "2025-09-19 10:08:40        4,500  0.68  train-loss: 1.55118   grad-norm: 0.34782   learning-rate: 9.86e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971e6157db6a441ab58fdf31373615dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:08:40        4,500  0.68  eval-loss:  1.41177   \n",
      "2025-09-19 10:08:42        4,600  0.69  train-loss: 1.4922    grad-norm: 0.34156   learning-rate: 9.85e-04\n",
      "2025-09-19 10:08:44        4,700  0.71  train-loss: 1.49984   grad-norm: 0.3425    learning-rate: 9.84e-04\n",
      "2025-09-19 10:08:46        4,800  0.72  train-loss: 1.52592   grad-norm: 0.33863   learning-rate: 9.84e-04\n",
      "2025-09-19 10:08:49        4,900  0.74  train-loss: 1.52044   grad-norm: 0.32511   learning-rate: 9.83e-04\n",
      "2025-09-19 10:08:51        5,000  0.75  train-loss: 1.5379    grad-norm: 0.34537   learning-rate: 9.82e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d175d2e3af4e56aceaf7536a915be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:08:51        5,000  0.75  eval-loss:  1.3994    \n",
      "2025-09-19 10:08:56        5,100  0.77  train-loss: 1.53196   grad-norm: 0.34065   learning-rate: 9.81e-04\n",
      "2025-09-19 10:08:58        5,200  0.78  train-loss: 1.45921   grad-norm: 0.34047   learning-rate: 9.81e-04\n",
      "2025-09-19 10:09:00        5,300  0.8   train-loss: 1.45618   grad-norm: 0.33018   learning-rate: 9.80e-04\n",
      "2025-09-19 10:09:02        5,400  0.82  train-loss: 1.48482   grad-norm: 0.3375    learning-rate: 9.79e-04\n",
      "2025-09-19 10:09:05        5,500  0.83  train-loss: 1.46733   grad-norm: 0.32711   learning-rate: 9.78e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1982d8aca0bc4dc18c28c7d43ef595df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:09:05        5,500  0.83  eval-loss:  1.35102   \n",
      "2025-09-19 10:09:07        5,600  0.85  train-loss: 1.50472   grad-norm: 0.33294   learning-rate: 9.77e-04\n",
      "2025-09-19 10:09:09        5,700  0.86  train-loss: 1.52715   grad-norm: 0.33114   learning-rate: 9.76e-04\n",
      "2025-09-19 10:09:11        5,800  0.88  train-loss: 1.48738   grad-norm: 0.33355   learning-rate: 9.75e-04\n",
      "2025-09-19 10:09:14        5,900  0.89  train-loss: 1.49892   grad-norm: 0.31996   learning-rate: 9.74e-04\n",
      "2025-09-19 10:09:16        6,000  0.91  train-loss: 1.43931   grad-norm: 0.33945   learning-rate: 9.73e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44838feba2d42adb0478ffb8080751b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:09:16        6,000  0.91  eval-loss:  1.35941   \n",
      "2025-09-19 10:09:20        6,100  0.92  train-loss: 1.42192   grad-norm: 0.34209   learning-rate: 9.72e-04\n",
      "2025-09-19 10:09:23        6,200  0.94  train-loss: 1.46797   grad-norm: 0.3288    learning-rate: 9.71e-04\n",
      "2025-09-19 10:09:25        6,300  0.95  train-loss: 1.44657   grad-norm: 0.33299   learning-rate: 9.70e-04\n",
      "2025-09-19 10:09:27        6,400  0.97  train-loss: 1.43737   grad-norm: 0.34078   learning-rate: 9.69e-04\n",
      "2025-09-19 10:09:29        6,500  0.98  train-loss: 1.45573   grad-norm: 0.33638   learning-rate: 9.68e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da439c3f6db243cea6004ce98d67691e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:09:29        6,500  0.98  eval-loss:  1.32964   \n",
      "2025-09-19 10:09:32        6,600  1.0   train-loss: 1.39928   grad-norm: 0.31088   learning-rate: 9.67e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server thread error: Event loop stopped before Future completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 10:09:32        6,625  1.0   train_runtime: 163.9 train_samples: 212,000 step: 6,625 train_samples_per_second: 1.294e+03 train_steps_per_second: 40.43 epoch: 1.0 effective_batch_size: 32 \n"
     ]
    }
   ],
   "source": [
    "# Train model in notebook.\n",
    "\n",
    "# Construct the default target, \"main,\" which is a training script.\n",
    "training_script = proj()\n",
    "\n",
    "# Start training the model.\n",
    "training_script.run()\n",
    "\n",
    "# Release resources\n",
    "training_script = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a33f36-246d-4627-aebf-5e8127f4b692",
   "metadata": {},
   "source": [
    "## Load Trained Model\n",
    "\n",
    "You can use the regular HF APIs to load the saved model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4846ff07-14ed-434e-aadd-b04063e6c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GenerationConfig, StoppingCriteria\n",
    "from forgather.ml.sharded_checkpoint import create_pretrained_symlinks\n",
    "import torch\n",
    "\n",
    "model_path = \"./output_models/tiny_llama\"\n",
    "\n",
    "# Create symlinks to latest checkpoint model output directory\n",
    "# This is required for .from_pretrained() to find the latest checkpoint.\n",
    "create_pretrained_symlinks(model_path)\n",
    "\n",
    "# Set device to run inference on\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d72d1-1e8f-4433-abe0-a0d9aaa252c0",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "This loop will use the newly trained model to generate text, seeded with the above prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee6f8d5-c548-4b94-88d2-fc92f118746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was so tired when she got back home so she went [START] to sleep. She slept and slept, until she saw a big piece of paper on the table. She saw a piece of paper on the floor. The paper was so happy! She smiled and thanked the paper. She picked up the paper and put it on. She snuggled up and sle\n",
      "----------------------------------------\n",
      "Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was [START] different.\n",
      "\n",
      "One night, Jack saw a big balloon in the sky. He was very hungry and wanted to eat it. He tried to pick it, but it was too heavy. He felt sad.\n",
      "\n",
      "Luckily, he saw a little girl. She looked sad. She did not know what to do. She looked sad. She w\n",
      "----------------------------------------\n",
      "Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has [START] a big, red bow on it.\"\n",
      "\n",
      "Jack asked, \"What's inside, sweetie?\" He asked.\n",
      "\n",
      "\"It's a rainbow,\" Lily said. \"It's a rainbow. Can we touch it?\"\n",
      "\n",
      "Jack replied, \"Sure, we can touch it. Let's touch it\n",
      "----------------------------------------\n",
      "Jack wanted to read a book, so he went to [START] the book. He put on his a book and waited. He waited and waited. He saw so many books and pictures. \n",
      "\n",
      "When he came back, he saw that one of the pictures were red, and a big picture. He liked the pictures. He liked to read about the pictures and the lovely pictures. \n",
      "\n",
      "Jack had\n",
      "----------------------------------------\n",
      "\"Can cows fly?\" Alice asked her mother. [START] \n",
      "\"No, we have to stay safe,\" her mother said.\n",
      "\n",
      "\"OK, we have to wait until we're safe. Then we can go on a long adventure, and we can go on a tour of the beach.\"\n",
      "\n",
      "As they got on their journey, they saw a big boat. The boat was very big and had a big\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(model, tokenizer, prompts, gen_config, max_new_tokens, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for prompt in prompts:\n",
    "            tokenizer_outputs = tokenizer(\n",
    "                [prompt],\n",
    "                truncation=False,\n",
    "                return_length=True,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "        \n",
    "            input_ids = tokenizer_outputs[\"input_ids\"].to(device)\n",
    "            attention_mask = tokenizer_outputs[\"attention_mask\"].to(device)\n",
    "            use_cache = getattr(model, \"_supports_cache_class\", False)\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                generation_config=gen_config,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=use_cache,\n",
    "                past_key_values=None,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "    \n",
    "            output_text = tokenizer.decode(\n",
    "                outputs.sequences[0],\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "            yield prompt + \" [START] \" + output_text[len(prompt) + 1 :]\n",
    "\n",
    "prompts = [\n",
    "    'Alice was so tired when she got back home so she went',\n",
    "    'Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was',\n",
    "    'Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has',\n",
    "    'Jack wanted to read a book, so he went to',\n",
    "    '\"Can cows fly?\" Alice asked her mother.',\n",
    "]\n",
    "\n",
    "gen_config = GenerationConfig(\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    do_sample=True,\n",
    "    top_k=20,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    repitition_penalty=1.15,\n",
    ")\n",
    "\n",
    "for s in generate_text(model, tokenizer, prompts, gen_config, 100, \"cuda:0\"):\n",
    "    print(s)\n",
    "    print(f\"{'-' * 40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b87ddc-9fbf-4799-9d3d-c71e06859401",
   "metadata": {},
   "source": [
    "## Train Hugginface LLama Model\n",
    "\n",
    "Next, let's try training a Llama model using the Huggingface implementation.\n",
    "\n",
    "Train the model on the CLI\n",
    "\n",
    "```bash\n",
    "forgather -t train_hf_llama.yaml train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b057e02-fe19-432c-840a-0dad3ec0d50c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb.display_config(config_template=\"train_hf_llama.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e13d5-9597-43a4-956c-f438674b7f56",
   "metadata": {},
   "source": [
    "## Let's See What Happens...\n",
    "\n",
    "...if we replace the post-layer-norm implementation with a pre-layer-norm implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e44fde-65d6-4dce-8ad3-c1196313d4c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb.display_config(config_template=\"experimental_llama.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb6c21-28cd-47e0-ae75-b18e0d500ca6",
   "metadata": {},
   "source": [
    "```bash\n",
    "forgather -t experimental_llama.yaml train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15896a-6afa-47b5-bbf8-017277cc8d9c",
   "metadata": {},
   "source": [
    "## Test Model With the Inference Server\n",
    "\n",
    "There is a simple OpenAI compatible inference server implementation in \"tools/inference_server\"  \n",
    "\n",
    "To host your newly trained model on the inference server:\n",
    "\n",
    "```bash\n",
    "./server.py server_configs/tiny_llama.yaml\n",
    "```\n",
    "\n",
    "From another session, you can perform text completion like this:\n",
    "\n",
    "```bash\n",
    "./client.py client_configs/tiny_llama.yaml --stream --completion \"Once upon a time,\"\n",
    "```\n",
    "\n",
    "The Tiny Llama model, trained on Tiny Stories, will not be very good at interactive chat, but you cat test this with the following command:\n",
    "\n",
    "```bash\n",
    "./client.py client_configs/tiny_llama.yaml --stream --interactive\n",
    "```\n",
    "\n",
    "This server should work with other OpenAI compatible clients as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405bc11-9e44-4a24-8a21-91642c22cf50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Forgather",
   "language": "python",
   "name": "fg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
