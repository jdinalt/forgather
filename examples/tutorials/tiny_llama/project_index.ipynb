{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Tiny LLama\n",
       "\n",
       "In this tutorial we will train a very small Llama model (about 5M parameters) on 10% of the Tiny Stories dataset. On a single RTX-4090, this takes about three minutes. Once training is complete, we will load the model an use it for text generation -- and the generation will be reasonably coherent for a three-minute-old model.\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/rust/forgather/examples/tutorials/tiny_llama\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/rust/forgather/examples/tutorials/tiny_llama/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "    - [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/rust/forgather/examples/tutorials/tiny_llama/templates](templates)\n",
       "- [/home/dinalt/rust/forgather/forgather_workspace](../../../forgather_workspace)\n",
       "- [/home/dinalt/rust/forgather/templatelib/modellib](../../../templatelib/modellib)\n",
       "- [/home/dinalt/rust/forgather/templatelib/examples](../../../templatelib/examples)\n",
       "- [/home/dinalt/rust/forgather/templatelib/base](../../../templatelib/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [train_hf_llama.yaml](templates/configs/train_hf_llama.yaml)\n",
       "- [experimental_llama.yaml](templates/configs/experimental_llama.yaml)\n",
       "- [full_dataset.yaml](templates/configs/full_dataset.yaml)\n",
       "- [train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "\n",
       "Default Configuration: train_tiny_llama.yaml\n",
       "\n",
       "## Available Templates\n",
       "- [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "- [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "- [datasets/llm_dataset_project.yaml](../../../templatelib/examples/datasets/llm_dataset_project.yaml)\n",
       "- [tokenizers/tiny_8k.yaml](../../../templatelib/examples/tokenizers/tiny_8k.yaml)\n",
       "- [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "- [tokenizers/wikitext/32k.yaml](../../../templatelib/examples/tokenizers/wikitext/32k.yaml)\n",
       "- [tokenizers/wikitext/8k.yaml](../../../templatelib/examples/tokenizers/wikitext/8k.yaml)\n",
       "- [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "- [prompts/short_stories.yaml](../../../templatelib/examples/prompts/short_stories.yaml)\n",
       "- [attn_functions/default.yaml](../../../templatelib/examples/attn_functions/default.yaml)\n",
       "- [flex_kernel_options/default.yaml](../../../templatelib/examples/flex_kernel_options/default.yaml)\n",
       "- [config_type.yaml](../../../templatelib/base/config_type.yaml)\n",
       "    - [datasets/dataset_type.yaml](../../../templatelib/base/datasets/dataset_type.yaml)\n",
       "        - [datasets/tokenized_dataset.yaml](../../../templatelib/base/datasets/tokenized_dataset.yaml)\n",
       "    - [tokenizers/tokenizer_type.yaml](../../../templatelib/base/tokenizers/tokenizer_type.yaml)\n",
       "        - [tokenizers/bpe/bpe.yaml](../../../templatelib/base/tokenizers/bpe/bpe.yaml)\n",
       "    - [models/model_type.yaml](../../../templatelib/base/models/model_type.yaml)\n",
       "        - [projects/causal_lm_def.yaml](../../../templatelib/examples/projects/causal_lm_def.yaml)\n",
       "    - [training_script/training_script_type.yaml](../../../templatelib/base/training_script/training_script_type.yaml)\n",
       "        - [training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [project.yaml](templates/project.yaml)\n",
       "                - [configs/train_hf_llama.yaml](templates/configs/train_hf_llama.yaml)\n",
       "                - [configs/experimental_llama.yaml](templates/configs/experimental_llama.yaml)\n",
       "                - [configs/full_dataset.yaml](templates/configs/full_dataset.yaml)\n",
       "                - [configs/train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "- [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "    - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "        - [trainers/pipeline_trainer.yaml](../../../templatelib/base/trainers/pipeline_trainer.yaml)\n",
       "            - [trainers/auto_pipeline_trainer.yaml](../../../templatelib/base/trainers/auto_pipeline_trainer.yaml)\n",
       "        - [trainers/accel_trainer.yaml](../../../templatelib/base/trainers/accel_trainer.yaml)\n",
       "    - [trainers/hf_trainer.yaml](../../../templatelib/base/trainers/hf_trainer.yaml)\n",
       "- [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "    - [models/causal_lm/from_pretrained_model.yaml](../../../templatelib/base/models/causal_lm/from_pretrained_model.yaml)\n",
       "    - [models/causal_lm/from_pretrained_config.yaml](../../../templatelib/base/models/causal_lm/from_pretrained_config.yaml)\n",
       "    - [models/causal_lm/from_pretrained_class.yaml](../../../templatelib/base/models/causal_lm/from_pretrained_class.yaml)\n",
       "        - [models/transformers/llama.yaml](../../../templatelib/examples/models/transformers/llama.yaml)\n",
       "            - [config.model_config](templates/configs/train_hf_llama.yaml)\n",
       "        - [models/transformers/gpt2.yaml](../../../templatelib/examples/models/transformers/gpt2.yaml)\n",
       "    - [models/causal_lm/custom.yaml](../../../templatelib/base/models/causal_lm/custom.yaml)\n",
       "        - [models/causal_lm/custom_dynamic.yaml](../../../templatelib/base/models/causal_lm/custom_dynamic.yaml)\n",
       "            - [models/transformers/dynamic_causal_transformer.yaml](../../../templatelib/examples/models/transformers/dynamic_causal_transformer.yaml)\n",
       "            - [models/transformers/dynamic_llama.yaml](../../../templatelib/examples/models/transformers/dynamic_llama.yaml)\n",
       "            - [models/transformers/deepone.yaml](../../../templatelib/examples/models/transformers/deepone.yaml)\n",
       "- [models/causal_lm/import_model_project.yaml](../../../templatelib/base/models/causal_lm/import_model_project.yaml)\n",
       "- [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "    - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "        - [project.logger_config](templates/project.yaml)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "nb.display_project_index(show_available_templates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4576985-d53c-4b74-b68b-31ec1e8fcdbd",
   "metadata": {},
   "source": [
    "---\n",
    "This example makes extensive use of the Forgather templates library. Take a look at the various files which go into the configuration and compare these to the pre-processed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d9e625-31e4-4694-b77c-cbcc32813b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Included Templates\n",
       "- [configs/train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [datasets/llm_dataset_project.yaml](../../../templatelib/examples/datasets/llm_dataset_project.yaml)\n",
       "        - [models/causal_lm/import_model_project.yaml](../../../templatelib/base/models/causal_lm/import_model_project.yaml)\n",
       "        - [project.logger_config](templates/project.yaml)\n",
       "            - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "                - [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "                    - [inc/formatting.jinja](../../../templatelib/base/inc/formatting.jinja)\n",
       "            - [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "            - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "                - [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "        - [training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [training_script/training_script_type.yaml](../../../templatelib/base/training_script/training_script_type.yaml)\n",
       "                - [config_type.yaml](../../../templatelib/base/config_type.yaml)\n",
       "                    - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'A demo of training a tiny llama model from scratch',\n",
       " 'config_name': 'Tiny Llama',\n",
       " 'datasets_dir': '/home/dinalt/rust/forgather/datasets',\n",
       " 'forgather_dir': '/home/dinalt/rust/forgather',\n",
       " 'logging_dir': './output_models/tiny_llama/runs/log_2025-12-29T07-23-26',\n",
       " 'model_src_dir': '/home/dinalt/rust/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'nproc_per_node': 1,\n",
       " 'output_dir': './output_models/tiny_llama',\n",
       " 'project_dir': '.',\n",
       " 'tokenizers_dir': '/home/dinalt/rust/forgather/tokenizers',\n",
       " 'workspace_root': '/home/dinalt/rust/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "## Output Targets\n",
       "- distributed_env\n",
       "- tokenizer\n",
       "- model\n",
       "- tokenizer_args\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- experiment_info\n",
       "- testprompts\n",
       "- generation_config\n",
       "- trainer_callbacks\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- trainer_args\n",
       "- model_preprocessor\n",
       "- trainer\n",
       "- dynamic_args\n",
       "- meta\n",
       "- main\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#               Tiny Llama               \n",
       "#---------------------------------------\n",
       "# 2025-12-29T07:23:26\n",
       "# Description: A demo of training a tiny llama model from scratch\n",
       "# Project Dir: /home/dinalt/rust/forgather/examples/tutorials/tiny_llama\n",
       "# Current Working Dir: \"/home/dinalt/rust/forgather/examples/tutorials/tiny_llama\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: tiny_llama\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.12.3\n",
       "#     torch: 2.9.1+cu130\n",
       "#     transformers: 4.57.1\n",
       "#     accelerate: 1.12.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/rust/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/rust/forgather/examples/tutorials/tiny_llama/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/rust/forgather/examples/tutorials/tiny_llama/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/rust/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/rust/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/rust/forgather/model_src\"\n",
       "# ns.output_dir: \"./output_models/tiny_llama\"\n",
       "# ns.logging_dir: \"./output_models/tiny_llama/runs/log_2025-12-29T07-23-26\"\n",
       "# ns.nproc_per_node: 1\n",
       "# ns.trust_remote_code: False\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       "distributed_env: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "\n",
       "\n",
       "################ Model #################\n",
       "\n",
       "# https://huggingface.co/docs/transformers/en/model_doc/auto\n",
       ".define: &model_constructor_args\n",
       "    # See: https://huggingface.co/docs/transformers/en/attention_interface\n",
       "    attn_implementation: \"sdpa\"\n",
       "\n",
       "# Import a model definition from another Forgather project\n",
       ".define: &model_dict !call:forgather:from_project\n",
       "    project_dir: \"/home/dinalt/rust/forgather/examples/models/llama\"\n",
       "    config_template: \"4M.yaml\"\n",
       "    targets: [  \"pretrained_tokenizer\", \"model\" ] \n",
       "    pp_kwargs:\n",
       "        output_dir: \"./output_models/tiny_llama\"\n",
       "    pp_debug: False\n",
       "    model_constructor_args: *model_constructor_args\n",
       "\n",
       "tokenizer: &tokenizer !call:getitem [ *model_dict, 'pretrained_tokenizer' ]\n",
       "model: &model !call:getitem [ *model_dict, 'model' ]\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "tokenizer_args: &tokenizer_args !dict\n",
       "    truncation: True\n",
       "    max_length: 512    \n",
       "\n",
       "# Load dataset from sub-project\n",
       ".define: &dataset_dict !call:forgather:from_project\n",
       "    project_dir: \"/home/dinalt/rust/forgather/examples/datasets/roneneldan\"\n",
       "    config_template: \"tinystories-abridged.yaml\"\n",
       "    targets: [  \"train_dataset\", \"eval_dataset\" ] \n",
       "    preprocess_args: *tokenizer_args\n",
       "    tokenizer: *tokenizer\n",
       "\n",
       "train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]\n",
       "eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM\n",
       "    tokenizer: *tokenizer\n",
       "    return_tensors: pt\n",
       "\n",
       "    # Tiny Llama\n",
       "    truncation: True\n",
       "    max_length: 512\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/tiny_llama/runs/log_2025-12-29T07-23-26\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       "experiment_info: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-12-29T07:23:26\"\n",
       "    name: \"Tiny Llama\"\n",
       "    description: \"A demo of training a tiny llama model from scratch\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.12.3', 'torch': '2.9.1+cu130', 'transformers': '4.57.1', 'accelerate': '1.12.0'}\n",
       "\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       "# The model will be given the following prompts for text-gen at regular intervals.\n",
       "testprompts: &testprompts !list:@testprompts\n",
       "    # Test prompts from \"https://arxiv.org/abs/2305.07759\"\n",
       "    - \"Alice was so tired when she got back home so she went\"\n",
       "    - \"Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was\"\n",
       "    - \"Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \\\"Look, Lily. A rainbow has\"\n",
       "    - \"Jack wanted to read a book, so he went to\"\n",
       "    - \"\\\"Can cows fly?\\\" Alice asked her mother.\"\n",
       "    - \"\\\"What do birds like to eat?\\\" Tom asked his mother.\"\n",
       "    - \"\\\"What language do they speak in France?\\\" Tom asked his mother.\"\n",
       "    - \"If I throw a ball up in the air, eventually it will\"\n",
       "    - \"It was winter and cold outside so his mother told him, \\\"You should\"\n",
       "    - \"Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked\"\n",
       "    - \"Jack told Mary, \\\"If you give me your banana, I'll give you my apple.\\\" Mary gave Jack her Banana, so\"\n",
       "    - \"On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to\"\n",
       "    - \"Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that\"\n",
       "    - \"Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \\\"I want to go to the park\\\". Lily says\"\n",
       "    - \"Jack's mother was not home, and his father was at home. When Jack came home, he said hello to\"\n",
       "    - \"Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says\"\n",
       "    - \"Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They\"\n",
       "    - \"Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door,\"\n",
       "\n",
       "# Conservative text-generation parameters.\n",
       "generation_config: &generation_config !dict:@generation_config\n",
       "    identity: generation_config\n",
       "    do_sample: True\n",
       "    top_k: 20\n",
       "    top_p: 0.9\n",
       "    temperature: 0.7\n",
       "    repitition_penalty: 1.15\n",
       "trainer_callbacks: &trainer_callbacks !dlist:@trainer_callbacks\n",
       "    progress_callback: !singleton:forgather.ml.trainer.callbacks:ProgressCallback\n",
       "        use_tqdm: null # Optional[bool] : Use TQDM, Auto, if unspecified\n",
       "        output_stream: \"stdout\" #Literal[\"stderr\", \"stdout\"]\n",
       "    info_callback: !singleton:forgather.ml.trainer.callbacks:InfoCallback\n",
       "        verbose: False\n",
       "    # Log all training output to JSON\n",
       "    json_logger: !singleton:forgather.ml.trainer.callbacks:JsonLogger\n",
       "        <<: *experiment_info\n",
       "\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    tb_logger: !singleton:forgather.ml.trainer.callbacks:TBLogger\n",
       "        arg0: *summary_writer\n",
       "        <<: *experiment_info\n",
       "\n",
       "    text_gen_callback: !singleton:forgather.ml.trainer.callbacks:TextgenCallback\n",
       "        summary_writer: *summary_writer\n",
       "        prompts: *testprompts\n",
       "        generation_config: *generation_config\n",
       "        max_new_tokens: 40\n",
       "        generation_steps: 1000\n",
       "    # Allow remote control of the training process\n",
       "    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       "optimizer: &optimizer !partial:torch:optim.AdamW\n",
       "    lr: 1.0e-3\n",
       "\n",
       "############# LR Scheduler #############\n",
       "\n",
       "# https://arxiv.org/html/2503.02844v1\n",
       "lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler\n",
       "    warmup_steps: 500\n",
       "    cooldown_steps: 50000\n",
       "    constant_lr: 1.0e-6\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: Forgather Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "# Trainer Config Class: forgather.ml.trainer:TrainingArguments\n",
       "# Trainer Class: forgather.ml.trainer:Trainer\n",
       "# nproc_per_node: 1\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       "\n",
       "\n",
       "trainer_args: &trainer_args !singleton:forgather.ml.trainer:TrainingArguments@trainer_args\n",
       "    save_strategy: \"no\"\n",
       "    max_steps: -1\n",
       "    output_dir: \"./output_models/tiny_llama\"\n",
       "    logging_dir: \"./output_models/tiny_llama/runs/log_2025-12-29T07-23-26\"\n",
       "    # Tiny Llama Project Overrides\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"steps\"\n",
       "    save_steps: 10000\n",
       "    # Safetensors can't handle tied parameters/buffers, so fallback to PyTorch format.\n",
       "    save_safetensors: False\n",
       "    seed: 42\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 64\n",
       "    logging_steps: 100\n",
       "    eval_steps: 500\n",
       "    num_train_epochs: 1\n",
       "    dataloader_num_workers: 1\n",
       "\n",
       "model_preprocessor: &model_preprocessor !partial:call\n",
       "    - *model\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       "trainer: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    args: *trainer_args\n",
       "    model_init: *model_preprocessor\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "    # **Trainer**\n",
       "    compute_loss_func: !singleton:forgather.ml.loss:CausalLoss\n",
       "    distributed_env: *distributed_env\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "# **Dynamic Args**\n",
       "dynamic_args: !dlist\n",
       "    null: ~\n",
       "    max_steps:\n",
       "        names: \"--max-steps\"\n",
       "        type: \"int\"\n",
       "        help: \"Set maximum training steps\"\n",
       "    save_strategy:\n",
       "        names: [ \"--save-strategy\", \"-S\" ]\n",
       "        choices: [ \"no\", \"steps\", \"epoch\" ]\n",
       "        type: \"str\"\n",
       "        help: \"When to save checkpoints\"\n",
       "    attn_implementation:\n",
       "        names: \"--attn-implementation\"\n",
       "        type: \"str\"\n",
       "        choices: [ \"eager\", \"sdpa\", \"flash_attention_2\", \"flex_attention\" ]\n",
       "        help: \"Attention implementation\"\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Tiny Llama\"\n",
       "    config_description: \"A demo of training a tiny llama model from scratch\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/rust/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/rust/forgather\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"/home/dinalt/rust/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/rust/forgather/datasets\"\n",
       "    output_dir: \"./output_models/tiny_llama\"\n",
       "    model_src_dir: \"/home/dinalt/rust/forgather/model_src\"\n",
       "    logging_dir: \"./output_models/tiny_llama/runs/log_2025-12-29T07-23-26\"\n",
       "    nproc_per_node: 1\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_train: True\n",
       "    do_save: False\n",
       "    do_eval: False\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_config(config_template=\"\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bcb135-37fe-4f61-80e7-dc8d71c08294",
   "metadata": {},
   "source": [
    "## Load Project\n",
    "\n",
    "Load the default configuraiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1f9b45-db49-475f-8c73-b47d0e9ab08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "# Load the default project, which is \"train_tiny_llama.yaml\"\n",
    "proj = Project()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15abe5-957f-43ff-af18-0c8725fe961e",
   "metadata": {},
   "source": [
    "## Start Tensorboard\n",
    "\n",
    "This project has been configured to log training to Tensorboard (TB). To watch the model's training progress with TB, run the following command, which will generate a CLI command to start the TB server. Then run the command from a shell.\n",
    "\n",
    "Tensorboard can be started from a terminal like this:\n",
    "\n",
    "```bash\n",
    "# By default, Tensorboard bind only to localhost. To bind to all interfaces, add --bind_all\n",
    "tensorboard --logdir \"/path/to/model/log/directory\" [--bind_all]\n",
    "```\n",
    "\n",
    "You can use the CLI to launch TB for you, where it will automatically determine the path to the log directory:\n",
    "\n",
    "```bash\n",
    "# --all : Watch all output model directories, otherwise just the one for the current configuration.\n",
    "# -- : Any arguments after '--' are passed directly to tensorboard, for example \"--bind_all\"\n",
    "cd PROJECT_DIR\n",
    "forgather tb [--all] [-- <tensorboard-args>]\n",
    "```\n",
    "\n",
    "When TB starts, it should provide the URL to access it. e.g.\n",
    "\n",
    "```\n",
    "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b3946-1faa-4224-afe8-58233a57e41c",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "You have a few options for training the mode.\n",
    "\n",
    "1. Run it directly from the notebook. This should work find with this example, although for projects using multiple GPUs, you will want to use one of the other options. To train from the notebook, just run the following cell.\n",
    "2. You can generate a training script and run it from the shell. To do so, run the cell with \"generate_trainingscript(),\" then run the generated shell script from a terminal.\n",
    "3. You can use the Forgather CLI.\n",
    "\n",
    "```bash\n",
    "# Open a shell in thie project's directory, then run this command:\n",
    "cd PROJECT_DIR\n",
    "forgather train\n",
    "\n",
    "# See forgather --help for more details.\n",
    "```\n",
    "\n",
    "Once training starts, switch to Tensorboard in your browser. One of the first things you will want to do is enable automatic refresh. To do so, click the gear in the upper-right corner and check \"Reload Data.\"\n",
    "\n",
    "Once training has started, take a look at the \"Text\" tab. You will see that we have automatically logged the preprocessed configuraiton as well as having dumped the primary training artifacts.\n",
    "\n",
    "Next, switch to the \"Scalars\" tab. You will see a plot of train and evaluation loss which will automatically update every 30 seconds. If you are not familiar with Tensorboard, now would be a good time to play with the UI elements to see how they work.\n",
    "\n",
    "When training completes, the model will be automatically saved to the output directory (\"./output_models/default_model\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7f6eb9-33ec-489c-b66e-faf3e3f0bdee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.1+cu130 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "Fused loss factory not provided. Provide a fused-loss factory for enhanced performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2026b4e5e6e8421a8dd61d6034108502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|info_logger] \n",
      "total_examples: 212,000\n",
      "total_train_samples: 212,000\n",
      "per_device_train_batch_size: 32\n",
      "actual_per_device_batch_size: 32\n",
      "total_train_batch_size: 32\n",
      "max_steps: 6,625\n",
      "total_parameters: 4.4M\n",
      "trainable_parameters: 4.4M\n",
      "\n",
      "2025-12-29 07:23:47          100  0.02  train-loss: 6.88765   grad-norm: 0.65267   learning-rate: 2.00e-04\n",
      "2025-12-29 07:23:49          200  0.03  train-loss: 4.30503   grad-norm: 0.48353   learning-rate: 4.00e-04\n",
      "2025-12-29 07:23:51          300  0.05  train-loss: 3.36338   grad-norm: 0.50338   learning-rate: 6.00e-04\n",
      "2025-12-29 07:23:53          400  0.06  train-loss: 3.06899   grad-norm: 0.49006   learning-rate: 8.00e-04\n",
      "2025-12-29 07:23:55          500  0.08  train-loss: 2.77843   grad-norm: 0.42629   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b72bde65ff49a28b6b48722d75d93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:23:55          500  0.08  eval-loss:  2.62731   \n",
      "2025-12-29 07:23:59          600  0.09  train-loss: 2.59655   grad-norm: 0.42681   learning-rate: 1.00e-03\n",
      "2025-12-29 07:24:02          700  0.11  train-loss: 2.40455   grad-norm: 0.41618   learning-rate: 1.00e-03\n",
      "2025-12-29 07:24:04          800  0.12  train-loss: 2.34082   grad-norm: 0.40722   learning-rate: 1.00e-03\n",
      "2025-12-29 07:24:06          900  0.14  train-loss: 2.212     grad-norm: 0.39784   learning-rate: 1.00e-03\n",
      "2025-12-29 07:24:08        1,000  0.15  train-loss: 2.07026   grad-norm: 0.40033   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a3b6ac1afd4dc594019645f65add99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:24:08        1,000  0.15  eval-loss:  1.99104   \n",
      "2025-12-29 07:24:12        1,100  0.17  train-loss: 2.06937   grad-norm: 0.40356   learning-rate: 1.00e-03\n",
      "2025-12-29 07:24:14        1,200  0.18  train-loss: 2.03831   grad-norm: 0.39567   learning-rate: 1.00e-03\n",
      "2025-12-29 07:24:16        1,300  0.2   train-loss: 2.00233   grad-norm: 0.38852   learning-rate: 9.99e-04\n",
      "2025-12-29 07:24:18        1,400  0.21  train-loss: 1.97046   grad-norm: 0.37826   learning-rate: 9.99e-04\n",
      "2025-12-29 07:24:20        1,500  0.23  train-loss: 1.94784   grad-norm: 0.38883   learning-rate: 9.99e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626a9d830ff24a5ab9ce6dd02b4965ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:24:21        1,500  0.23  eval-loss:  1.74975   \n",
      "2025-12-29 07:24:23        1,600  0.24  train-loss: 1.90823   grad-norm: 0.38616   learning-rate: 9.99e-04\n",
      "2025-12-29 07:24:25        1,700  0.26  train-loss: 1.86886   grad-norm: 0.37719   learning-rate: 9.99e-04\n",
      "2025-12-29 07:24:27        1,800  0.27  train-loss: 1.82517   grad-norm: 0.38816   learning-rate: 9.98e-04\n",
      "2025-12-29 07:24:29        1,900  0.29  train-loss: 1.8071    grad-norm: 0.37966   learning-rate: 9.98e-04\n",
      "2025-12-29 07:24:31        2,000  0.3   train-loss: 1.84373   grad-norm: 0.37438   learning-rate: 9.98e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6080cba562c40959a077b2e6f1d537d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:24:31        2,000  0.3   eval-loss:  1.65692   \n",
      "2025-12-29 07:24:36        2,100  0.32  train-loss: 1.79691   grad-norm: 0.36641   learning-rate: 9.97e-04\n",
      "2025-12-29 07:24:38        2,200  0.33  train-loss: 1.74844   grad-norm: 0.36792   learning-rate: 9.97e-04\n",
      "2025-12-29 07:24:40        2,300  0.35  train-loss: 1.71255   grad-norm: 0.37593   learning-rate: 9.97e-04\n",
      "2025-12-29 07:24:42        2,400  0.36  train-loss: 1.77328   grad-norm: 0.37834   learning-rate: 9.96e-04\n",
      "2025-12-29 07:24:44        2,500  0.38  train-loss: 1.74461   grad-norm: 0.39035   learning-rate: 9.96e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7a16b73e2545b0b3441eb4f2974465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:24:44        2,500  0.38  eval-loss:  1.56892   \n",
      "2025-12-29 07:24:46        2,600  0.39  train-loss: 1.74146   grad-norm: 0.37168   learning-rate: 9.96e-04\n",
      "2025-12-29 07:24:48        2,700  0.41  train-loss: 1.68803   grad-norm: 0.37144   learning-rate: 9.95e-04\n",
      "2025-12-29 07:24:50        2,800  0.42  train-loss: 1.73194   grad-norm: 0.3629    learning-rate: 9.95e-04\n",
      "2025-12-29 07:24:52        2,900  0.44  train-loss: 1.64809   grad-norm: 0.37706   learning-rate: 9.94e-04\n",
      "2025-12-29 07:24:54        3,000  0.45  train-loss: 1.55665   grad-norm: 0.35236   learning-rate: 9.94e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232398fc88644529b3a5473eb78ba575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:24:54        3,000  0.45  eval-loss:  1.47587   \n",
      "2025-12-29 07:24:59        3,100  0.47  train-loss: 1.6416    grad-norm: 0.37025   learning-rate: 9.93e-04\n",
      "2025-12-29 07:25:01        3,200  0.48  train-loss: 1.70948   grad-norm: 0.34801   learning-rate: 9.93e-04\n",
      "2025-12-29 07:25:03        3,300  0.5   train-loss: 1.61597   grad-norm: 0.34541   learning-rate: 9.92e-04\n",
      "2025-12-29 07:25:05        3,400  0.51  train-loss: 1.55825   grad-norm: 0.36052   learning-rate: 9.92e-04\n",
      "2025-12-29 07:25:07        3,500  0.53  train-loss: 1.5827    grad-norm: 0.35073   learning-rate: 9.91e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e57400e87549bc9afd7ad8bf4aad0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:25:07        3,500  0.53  eval-loss:  1.45303   \n",
      "2025-12-29 07:25:09        3,600  0.54  train-loss: 1.63534   grad-norm: 0.3483    learning-rate: 9.91e-04\n",
      "2025-12-29 07:25:11        3,700  0.56  train-loss: 1.55805   grad-norm: 0.3547    learning-rate: 9.90e-04\n",
      "2025-12-29 07:25:13        3,800  0.57  train-loss: 1.56522   grad-norm: 0.35229   learning-rate: 9.89e-04\n",
      "2025-12-29 07:25:15        3,900  0.59  train-loss: 1.60674   grad-norm: 0.34669   learning-rate: 9.89e-04\n",
      "2025-12-29 07:25:17        4,000  0.6   train-loss: 1.63565   grad-norm: 0.34443   learning-rate: 9.88e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdaa275791a4c9d89acd0212e1facf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:25:17        4,000  0.6   eval-loss:  1.42217   \n",
      "2025-12-29 07:25:22        4,100  0.62  train-loss: 1.53881   grad-norm: 0.33184   learning-rate: 9.87e-04\n",
      "2025-12-29 07:25:24        4,200  0.63  train-loss: 1.52706   grad-norm: 0.34786   learning-rate: 9.87e-04\n",
      "2025-12-29 07:25:26        4,300  0.65  train-loss: 1.55825   grad-norm: 0.34447   learning-rate: 9.86e-04\n",
      "2025-12-29 07:25:28        4,400  0.66  train-loss: 1.60822   grad-norm: 0.34165   learning-rate: 9.85e-04\n",
      "2025-12-29 07:25:30        4,500  0.68  train-loss: 1.54526   grad-norm: 0.3382    learning-rate: 9.84e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2478d9e92941ecaae44c9d811c3bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:25:30        4,500  0.68  eval-loss:  1.40635   \n",
      "2025-12-29 07:25:32        4,600  0.69  train-loss: 1.48855   grad-norm: 0.33326   learning-rate: 9.84e-04\n",
      "2025-12-29 07:25:35        4,700  0.71  train-loss: 1.49441   grad-norm: 0.33088   learning-rate: 9.83e-04\n",
      "2025-12-29 07:25:37        4,800  0.72  train-loss: 1.51925   grad-norm: 0.32635   learning-rate: 9.82e-04\n",
      "2025-12-29 07:25:39        4,900  0.74  train-loss: 1.51519   grad-norm: 0.31614   learning-rate: 9.81e-04\n",
      "2025-12-29 07:25:41        5,000  0.75  train-loss: 1.53234   grad-norm: 0.33127   learning-rate: 9.80e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0be3acc21474cbda7ee2cab50ba0b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:25:41        5,000  0.75  eval-loss:  1.3907    \n",
      "2025-12-29 07:25:45        5,100  0.77  train-loss: 1.52618   grad-norm: 0.32921   learning-rate: 9.79e-04\n",
      "2025-12-29 07:25:47        5,200  0.78  train-loss: 1.45455   grad-norm: 0.33294   learning-rate: 9.78e-04\n",
      "2025-12-29 07:25:50        5,300  0.8   train-loss: 1.45012   grad-norm: 0.32411   learning-rate: 9.77e-04\n",
      "2025-12-29 07:25:52        5,400  0.82  train-loss: 1.48363   grad-norm: 0.33564   learning-rate: 9.77e-04\n",
      "2025-12-29 07:25:54        5,500  0.83  train-loss: 1.46404   grad-norm: 0.32302   learning-rate: 9.76e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333fed14b4994ee3b46a533b205364ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:25:54        5,500  0.83  eval-loss:  1.34799   \n",
      "2025-12-29 07:25:56        5,600  0.85  train-loss: 1.50253   grad-norm: 0.32968   learning-rate: 9.75e-04\n",
      "2025-12-29 07:25:58        5,700  0.86  train-loss: 1.52154   grad-norm: 0.32171   learning-rate: 9.74e-04\n",
      "2025-12-29 07:26:00        5,800  0.88  train-loss: 1.48362   grad-norm: 0.32428   learning-rate: 9.73e-04\n",
      "2025-12-29 07:26:02        5,900  0.89  train-loss: 1.49482   grad-norm: 0.31245   learning-rate: 9.72e-04\n",
      "2025-12-29 07:26:04        6,000  0.91  train-loss: 1.43278   grad-norm: 0.32528   learning-rate: 9.70e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24a9e8b8147446a8ad8340dc004480f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:26:05        6,000  0.91  eval-loss:  1.35024   \n",
      "2025-12-29 07:26:09        6,100  0.92  train-loss: 1.4147    grad-norm: 0.33273   learning-rate: 9.69e-04\n",
      "2025-12-29 07:26:11        6,200  0.94  train-loss: 1.46516   grad-norm: 0.33047   learning-rate: 9.68e-04\n",
      "2025-12-29 07:26:13        6,300  0.95  train-loss: 1.43909   grad-norm: 0.32528   learning-rate: 9.67e-04\n",
      "2025-12-29 07:26:15        6,400  0.97  train-loss: 1.43014   grad-norm: 0.32405   learning-rate: 9.66e-04\n",
      "2025-12-29 07:26:17        6,500  0.98  train-loss: 1.44907   grad-norm: 0.32634   learning-rate: 9.65e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5bcd3740674ab49bd6a01e75e06c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 12%|#################################################6                                                       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:26:17        6,500  0.98  eval-loss:  1.32157   \n",
      "2025-12-29 07:26:19        6,600  1.0   train-loss: 1.39438   grad-norm: 0.30354   learning-rate: 9.64e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server thread error: Event loop stopped before Future completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-29 07:26:20        6,625  1.0  train_runtime: 155.5\n",
      "train_samples: 211,968\n",
      "step: 6,624\n",
      "train_samples_per_second: 1.363e+03\n",
      "train_steps_per_second: 42.59\n",
      "epoch: 1.0\n",
      "effective_batch_size: 32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model in notebook.\n",
    "\n",
    "# Construct the default target, \"main,\" which is a training script.\n",
    "training_script = proj()\n",
    "\n",
    "# Start training the model.\n",
    "training_script.run()\n",
    "\n",
    "# Release resources\n",
    "training_script = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a33f36-246d-4627-aebf-5e8127f4b692",
   "metadata": {},
   "source": [
    "## Load Trained Model\n",
    "\n",
    "You can use the regular HF APIs to load the saved model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4846ff07-14ed-434e-aadd-b04063e6c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GenerationConfig, StoppingCriteria\n",
    "from forgather.ml.sharded_checkpoint import create_pretrained_symlinks\n",
    "import torch\n",
    "\n",
    "model_path = \"./output_models/tiny_llama\"\n",
    "\n",
    "# Create symlinks to latest checkpoint model output directory\n",
    "# This is required for .from_pretrained() to find the latest checkpoint.\n",
    "create_pretrained_symlinks(model_path)\n",
    "\n",
    "# Set device to run inference on\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d065c",
   "metadata": {},
   "source": [
    "The equivalent CLI for creating symbolic links to the latest checkpoint in the output directory is:\n",
    "```bash\n",
    "forgather checkpoint link\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d72d1-1e8f-4433-abe0-a0d9aaa252c0",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "This loop will use the newly trained model to generate text, seeded with the above prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee6f8d5-c548-4b94-88d2-fc92f118746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was so tired when she got back home so she went [START] to her mum and said, \"Mum, we need to rest.\"\n",
      "\n",
      "Mum looked at her and said, \"I have a surprise for you.\" She said, \"Okay, you can try.\"\n",
      "\n",
      "Alice hopped and jumped in the park. She felt a bit bitter, but she got a bit bit. She felt a little bit\n",
      "----------------------------------------\n",
      "Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was [START] cool.\n",
      "\n",
      "One day, Jack and Lily went to the moon. They saw a big box with a lock on it. They looked at it and saw a man with a big box. He had a lock on it.\n",
      "\n",
      "\"What do you want?\" he asked.\n",
      "\n",
      "\"I have a box,\" Lily said. \"Let's open the box.\n",
      "----------------------------------------\n",
      "Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has [START] a big sun!\"\n",
      "\n",
      "Lily liked the rainbow and wanted to play with it. She ran to the sun and opened it. Inside, she found a big, red sunflower. She smiled and said, \"Thank you, Lily!\"\n",
      "\n",
      "They played and had fun. They laughed and had lots of fun. They had so much fun\n",
      "----------------------------------------\n",
      "Jack wanted to read a book, so he went to [START] the book. He saw a big book with a tall structure. Jack wanted to read the book. He went to the book and opened the book. Inside the book, there were many books. Jack was very happy.\n",
      "\n",
      "Jack wanted to read the book. He looked for a book. He wanted to read the book. He opened the book. Inside the book, there were a book. Jack had\n",
      "----------------------------------------\n",
      "\"Can cows fly?\" Alice asked her mother. [START] \n",
      "\"Yes, we can, but be careful.\"\n",
      "\n",
      "Mum said. \"Anna.\"\n",
      "\n",
      "Andin, the morning, her mother gave her a hug. She said, \"It's just a game.\"\n",
      "\n",
      "Andin smiled and said, \"You can have a little bit.\"\n",
      "\n",
      "Andin happily agreed. She put her f\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(model, tokenizer, prompts, gen_config, max_new_tokens, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for prompt in prompts:\n",
    "            tokenizer_outputs = tokenizer(\n",
    "                [prompt],\n",
    "                truncation=False,\n",
    "                return_length=True,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "        \n",
    "            input_ids = tokenizer_outputs[\"input_ids\"].to(device)\n",
    "            attention_mask = tokenizer_outputs[\"attention_mask\"].to(device)\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                generation_config=gen_config,\n",
    "                return_dict_in_generate=True,\n",
    "                past_key_values=None,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "    \n",
    "            output_text = tokenizer.decode(\n",
    "                outputs.sequences[0],\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "            yield prompt + \" [START] \" + output_text[len(prompt) + 1 :]\n",
    "\n",
    "prompts = [\n",
    "    'Alice was so tired when she got back home so she went',\n",
    "    'Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was',\n",
    "    'Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has',\n",
    "    'Jack wanted to read a book, so he went to',\n",
    "    '\"Can cows fly?\" Alice asked her mother.',\n",
    "]\n",
    "\n",
    "gen_config = GenerationConfig(\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    do_sample=True,\n",
    "    top_k=20,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    repitition_penalty=1.15,\n",
    ")\n",
    "\n",
    "for s in generate_text(model, tokenizer, prompts, gen_config, 100, \"cuda:0\"):\n",
    "    print(s)\n",
    "    print(f\"{'-' * 40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b87ddc-9fbf-4799-9d3d-c71e06859401",
   "metadata": {},
   "source": [
    "## Train Hugginface LLama Model\n",
    "\n",
    "Next, let's try training a Llama model using the Huggingface implementation.\n",
    "\n",
    "Train the model on the CLI\n",
    "\n",
    "```bash\n",
    "forgather -t train_hf_llama.yaml train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b057e02-fe19-432c-840a-0dad3ec0d50c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb.display_config(config_template=\"train_hf_llama.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e13d5-9597-43a4-956c-f438674b7f56",
   "metadata": {},
   "source": [
    "## Let's See What Happens...\n",
    "\n",
    "...if we replace the post-layer-norm implementation with a pre-layer-norm implementation. This configuration uses a [custom model definition](./custom_models/llama/README.md) in the custom_models directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb6c21-28cd-47e0-ae75-b18e0d500ca6",
   "metadata": {},
   "source": [
    "```bash\n",
    "forgather -t experimental_llama.yaml train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15896a-6afa-47b5-bbf8-017277cc8d9c",
   "metadata": {},
   "source": [
    "## Test Model With the Inference Server\n",
    "\n",
    "There is a simple OpenAI compatible inference server implementation in \"tools/inference_server\"  \n",
    "\n",
    "To host your newly trained model on the inference server:\n",
    "\n",
    "```bash\n",
    "# Manual start\n",
    "forgather inf server -c -m ./output_models/tiny_llama/\n",
    "\n",
    "# Config with YAML file\n",
    "forgather inf server ./tiny_llama_server.yaml\n",
    "```\n",
    "\n",
    "From another session, you can perform text completion like this:\n",
    "\n",
    "```bash\n",
    "# Text completion request\n",
    "forgather inf client --completion \"Once upon a time\"\n",
    "\n",
    "# With manual generation settings\n",
    "forgather inf client --temperature 0.7 --no-repeat-ngram-size 2 --repetition-penalty 1.2 --top-k 40 --completion \"Once upon a time\" --max-tokens 512\n",
    "\n",
    "# From YAML config\n",
    "forgather inf client ./tiny_llama_client.yaml --completion \"Once upon a time\"\n",
    "```\n",
    "\n",
    "As the model has not been trained on a chat format, it will not be very good at it, but you can try with:\n",
    "\n",
    "```bash\n",
    "forgather inf client ./tiny_llama_client.yaml\n",
    "```\n",
    "\n",
    "This server should work with other OpenAI compatible clients as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a157e049",
   "metadata": {},
   "source": [
    "## Train on the Full Dataset\n",
    "\n",
    "The examples so far have been limited to training on only the first 10% of the dataset. You can train on the complete dataset with this configuration:\n",
    "```bash\n",
    "forgather -t full_dataset.yaml train\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
