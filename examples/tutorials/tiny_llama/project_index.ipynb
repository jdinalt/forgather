{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Tiny LLama\n",
       "\n",
       "In this tutorial we will train a very small Llama model (about 5M parameters) on 10% of the Tiny Stories dataset. On a single RTX-4090, this takes about three minutes. Once training is complete, we will load the model an use it for text generation -- and the generation will be reasonably coherent for a three-minute-old model.\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "    - [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/forgather_workspace](../../../forgather_workspace)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/modellib](../../../templatelib/modellib)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/examples](../../../templatelib/examples)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/base](../../../templatelib/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [train_hf_llama.yaml](templates/configs/train_hf_llama.yaml)\n",
       "- [train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "- [experimental_llama.yaml](templates/configs/experimental_llama.yaml)\n",
       "\n",
       "Default Configuration: train_tiny_llama.yaml\n",
       "\n",
       "## Available Templates\n",
       "- [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "- [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "- [datasets/llm_dataset_project.yaml](../../../templatelib/examples/datasets/llm_dataset_project.yaml)\n",
       "- [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "- [prompts/short_stories.yaml](../../../templatelib/examples/prompts/short_stories.yaml)\n",
       "- [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "- [tokenizers/tiny_8k.yaml](../../../templatelib/examples/tokenizers/tiny_8k.yaml)\n",
       "- [tokenizers/wikitext/32k.yaml](../../../templatelib/examples/tokenizers/wikitext/32k.yaml)\n",
       "- [tokenizers/wikitext/8k.yaml](../../../templatelib/examples/tokenizers/wikitext/8k.yaml)\n",
       "- [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "    - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "        - [trainers/accel_trainer.yaml](../../../templatelib/base/trainers/accel_trainer.yaml)\n",
       "        - [trainers/pipeline_trainer.yaml](../../../templatelib/base/trainers/pipeline_trainer.yaml)\n",
       "    - [trainers/hf_trainer.yaml](../../../templatelib/base/trainers/hf_trainer.yaml)\n",
       "- [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "    - [models/causal_lm/from_pretrained.yaml](../../../templatelib/base/models/causal_lm/from_pretrained.yaml)\n",
       "    - [models/causal_lm/from_pretrained_config.yaml](../../../templatelib/base/models/causal_lm/from_pretrained_config.yaml)\n",
       "    - [models/causal_lm/custom.yaml](../../../templatelib/base/models/causal_lm/custom.yaml)\n",
       "        - [models/causal_lm/custom_dynamic.yaml](../../../templatelib/base/models/causal_lm/custom_dynamic.yaml)\n",
       "            - [models/deepone.yaml](../../../templatelib/examples/models/deepone.yaml)\n",
       "            - [models/dynamic_causal_transformer.yaml](../../../templatelib/examples/models/dynamic_causal_transformer.yaml)\n",
       "            - [models/dynamic_llama.yaml](../../../templatelib/examples/models/dynamic_llama.yaml)\n",
       "                - [models/tiny_dynamic_llama.yaml](templates/models/tiny_dynamic_llama.yaml)\n",
       "                    - [project.model_config](templates/project.yaml)\n",
       "                        - [experiment.model_config](templates/configs/experimental_llama.yaml)\n",
       "    - [models/causal_lm/from_config.yaml](../../../templatelib/base/models/causal_lm/from_config.yaml)\n",
       "        - [models/gpt2.yaml](../../../templatelib/examples/models/gpt2.yaml)\n",
       "        - [models/llama.yaml](../../../templatelib/examples/models/llama.yaml)\n",
       "            - [models/tiny_hf_llama.yaml](templates/models/tiny_hf_llama.yaml)\n",
       "- [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "    - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "        - [project.logger_config](templates/project.yaml)\n",
       "- [types/type.yaml](../../../templatelib/base/types/type.yaml)\n",
       "    - [types/tokenizer/tokenizer.yaml](../../../templatelib/base/types/tokenizer/tokenizer.yaml)\n",
       "        - [types/tokenizer/bpe/bpe.yaml](../../../templatelib/base/types/tokenizer/bpe/bpe.yaml)\n",
       "    - [types/model/model_type.yaml](../../../templatelib/base/types/model/model_type.yaml)\n",
       "    - [types/training_script/training_script.yaml](../../../templatelib/base/types/training_script/training_script.yaml)\n",
       "        - [types/training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [project.yaml](templates/project.yaml)\n",
       "                - [configs/train_hf_llama.yaml](templates/configs/train_hf_llama.yaml)\n",
       "                - [configs/train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "                - [configs/experimental_llama.yaml](templates/configs/experimental_llama.yaml)\n",
       "    - [types/dataset/dataset_type.yaml](../../../templatelib/base/types/dataset/dataset_type.yaml)\n",
       "        - [datasets/tokenized_dataset.yaml](../../../templatelib/base/datasets/tokenized_dataset.yaml)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "nb.display_project_index(show_available_templates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4576985-d53c-4b74-b68b-31ec1e8fcdbd",
   "metadata": {},
   "source": [
    "---\n",
    "This example makes extensive use of the Forgather templates library. Take a look at the various files which go into the configuration and compare these to the pre-processed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d9e625-31e4-4694-b77c-cbcc32813b63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Included Templates\n",
       "- [configs/train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [datasets/llm_dataset_project.yaml](../../../templatelib/examples/datasets/llm_dataset_project.yaml)\n",
       "        - [types/training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "                - [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "            - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "                - [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "            - [types/training_script/training_script.yaml](../../../templatelib/base/types/training_script/training_script.yaml)\n",
       "                - [types/type.yaml](../../../templatelib/base/types/type.yaml)\n",
       "                    - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "            - [inc/formatting.jinja](../../../templatelib/base/inc/formatting.jinja)\n",
       "        - [project.logger_config](templates/project.yaml)\n",
       "            - [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "        - [project.model_config](templates/project.yaml)\n",
       "            - [models/tiny_dynamic_llama.yaml](templates/models/tiny_dynamic_llama.yaml)\n",
       "                - [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "                - [models/dynamic_llama.yaml](../../../templatelib/examples/models/dynamic_llama.yaml)\n",
       "                    - [models/causal_lm/custom_dynamic.yaml](../../../templatelib/base/models/causal_lm/custom_dynamic.yaml)\n",
       "                        - [models/causal_lm/custom.yaml](../../../templatelib/base/models/causal_lm/custom.yaml)\n",
       "                            - [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'A demo of training a tiny llama model from scratch',\n",
       " 'config_name': 'Tiny Llama',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/forgather/datasets',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': './output_models/tiny_llama/runs/log_2025-08-24T09-51-27',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'nproc_per_node': 1,\n",
       " 'output_dir': './output_models/tiny_llama',\n",
       " 'project_dir': '.',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "- [./output_models/tiny_llama/dynllama.py](output_models/tiny_llama/dynllama.py) : DynamicCausalLMConfig\n",
       "- [./output_models/tiny_llama/dynllama.py](output_models/tiny_llama/dynllama.py) : DynamicCasualLM\n",
       "## Output Targets\n",
       "- distributed_env\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "- model_submodule_searchpath\n",
       "- loss_fn\n",
       "- layer_norm_factory\n",
       "- feedforward_factory\n",
       "- relative_pe\n",
       "- attention_factory\n",
       "- layer_factory\n",
       "- layer_stack\n",
       "- output_decoder\n",
       "- absolute_pe\n",
       "- input_encoder\n",
       "- init_weights\n",
       "- model_factory\n",
       "- model_code_generator\n",
       "- model_code_writer\n",
       "- model_config\n",
       "- pretrained_model\n",
       "- model\n",
       "- tokenizer_args\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- experiment_info\n",
       "- testprompts\n",
       "- generation_config\n",
       "- trainer_callbacks\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- trainer_args\n",
       "- model_preprocessor\n",
       "- trainer\n",
       "- dynamic_args\n",
       "- meta\n",
       "- main\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#               Tiny Llama               \n",
       "#---------------------------------------\n",
       "# 2025-08-24T09:51:27\n",
       "# Description: A demo of training a tiny llama model from scratch\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: tiny_llama\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.7.1\n",
       "#     transformers: 4.51.3\n",
       "#     accelerate: 1.7.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"./output_models/tiny_llama\"\n",
       "# ns.logging_dir: \"./output_models/tiny_llama/runs/log_2025-08-24T09-51-27\"\n",
       "# ns.nproc_per_node: 1\n",
       "# ns.trust_remote_code: False\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       "distributed_env: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "\n",
       "\n",
       "################ Model #################\n",
       "\n",
       "# https://huggingface.co/docs/transformers/en/model_doc/auto\n",
       "model_constructor_args: &model_constructor_args {}\n",
       "\n",
       "# Name: Dynamic Llama\n",
       "# Description: A Llama compatible dynamic model.\n",
       "# model_def.cls = \"DynamicCasualLM\"\n",
       "# model_def.cfg_cls = \"DynamicCausalLMConfig\"\n",
       "# model_def.config_path = \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models/tiny_llama/dynllama.py\"\n",
       "# model_def.model_path = \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models/tiny_llama/dynllama.py\"\n",
       "# model_def.short_name = \"dynllama\"\n",
       "# model_def.model_type = \"forgather-dynamic-causal-dynllama\"\n",
       "# model_def.model_path = \"./output_models/tiny_llama/dynllama.py\"\n",
       "# model_def.model_template_searchpath = \"/home/dinalt/ai_assets/forgather/modelsrc/templates\"\n",
       "# model_def.model_template_name = \"hf_causal.py\"\n",
       "# model_def.name_policy = \"named\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       "# Load custom tokenizer from sub-project definition\n",
       "tokenizer: &tokenizer !singleton:forgather.ml.construct:load_from_config@tokenizer\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe\"\n",
       "    config_template: \"2k.yaml\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       "# Model config dependencies\n",
       "\n",
       "model_submodule_searchpath: &model_submodule_searchpath\n",
       "    - \"/home/dinalt/ai_assets/forgather/modelsrc/transformer\"\n",
       "    - \"./output_models/tiny_llama\"\n",
       "\n",
       "loss_fn: &loss_fn !singleton:.causal_loss:CausalLoss@loss_fn []\n",
       "\n",
       "layer_norm_factory: &layer_norm_factory !partial:torch.nn:RMSNorm@layer_norm_factory\n",
       "    normalized_shape: !var \"hidden_size\"\n",
       "    eps: !var \"rms_norm_eps\"\n",
       "\n",
       "feedforward_factory: &feedforward_factory !partial:.glu_feedforward:GLUFeedforwardLayer@feedforward_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "    activation_factory: !partial:torch.nn.SiLU []\n",
       "    dropout: !var \"activation_dropout\"\n",
       "\n",
       "relative_pe: &relative_pe !singleton:.real_rotary_embeddings:RealRotaryPE@relative_pe\n",
       "    d_head: !var \"d_head\"\n",
       "    max_sequence_length: !var \"max_sequence_length\"\n",
       "    rope_theta: !var \"rope_theta\"\n",
       "    \n",
       "attention_factory: &attention_factory !partial:.causal_rpe_attn:CausalRpeAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    num_heads: !var \"num_attention_heads\"\n",
       "    num_kv_heads: !var \"num_kv_heads\"\n",
       "    dropout: !var \"attention_dropout\"\n",
       "    bias: False\n",
       "    sdpa_function: !partial:torch.nn.functional:scaled_dot_product_attention []\n",
       "    pos_encoder: *relative_pe\n",
       "\n",
       "layer_factory: &layer_factory !partial:.pre_ln_layer:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "    dropout: !var \"layer_dropout\"\n",
       "    residual_dropout: !var \"residual_dropout\"\n",
       "\n",
       "layer_stack: &layer_stack !factory:.checkpoint_layer_stack:LayerStack@layer_stack\n",
       "    layer_factory: *layer_factory\n",
       "    num_hidden_layers: !var \"num_hidden_layers\"\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    enable_checkpoint: !var \"enable_activation_checkpoint\"\n",
       "    checkpoint_stride: !var \"checkpoint_stride\"\n",
       "\n",
       "output_decoder: &output_decoder !factory:torch.nn:Linear@output_decoder\n",
       "    in_features: !var \"hidden_size\"\n",
       "    out_features: !var \"vocab_size\"\n",
       "    bias: False\n",
       "\n",
       "absolute_pe: &absolute_pe null\n",
       "\n",
       "input_encoder: &input_encoder !factory:.input_encoder:InputEncoder@input_encoder\n",
       "    d_model: !var \"hidden_size\"\n",
       "    vocab_size: !var \"vocab_size\"\n",
       "    dropout: !var \"embedding_dropout\"\n",
       "    positional_encoder: *absolute_pe\n",
       "    scale_sqrt_d_model: False\n",
       "\n",
       "# Init method based upon https://github.com/pytorch/torchtitan/blob/main/torchtitan/models/llama3/model/model.py\n",
       "init_weights: &init_weights !partial:.init_weights:init_weights_by_regex@init_weights\n",
       "    # Note: Yaml treats single and double quotes differently WRT escapes. Use single\n",
       "    # quotes for regex expressions, wihc prevents Yaml from interpreting escapes.\n",
       "    # For a literal ' use ''\n",
       "    regex_list:\n",
       "        - [ 'norm', \"pass\" ]\n",
       "        - [ 'bias', \"zeros\" ]\n",
       "        - [ 'embedding\\.weight', \"init_embeddings\" ]\n",
       "        - [ 'up_proj|query_linear|key_linear|value_linear', \"trunc_normal_magic\" ]\n",
       "        - [ 'gate_proj|down_proj|output_linear', \"trunc_normal\" ]\n",
       "        - [ 'output_decoder', \"init_output_layer\" ]\n",
       "    init_f_map:\n",
       "        pass: !partial:.init_weights:init_pass\n",
       "        zeros: !partial:torch.nn.init:zeros_ []\n",
       "        init_embeddings: !partial:.llama_init:init_embeddings []\n",
       "        trunc_normal_magic: !partial:.llama_init:trunc_normal_magic []\n",
       "        trunc_normal: !partial:.llama_init:trunc_normal\n",
       "            std: !call:.llama_init:llama_std [ !var \"num_hidden_layers\" ]\n",
       "        init_output_layer: !partial:.llama_init:init_output_layer { d_model: !var \"hidden_size\" }\n",
       "    # Print how each param is being initialized.\n",
       "    debug: False\n",
       "\n",
       "model_factory: &model_factory !factory:.causal_lm:CasualLM@model_factory\n",
       "    loss_fn: *loss_fn\n",
       "    input_encoder: *input_encoder\n",
       "    output_decoder: *output_decoder\n",
       "    layer_stack: *layer_stack\n",
       "    init_weights: *init_weights\n",
       "\n",
       "model_code_generator: &model_code_generator !meta:forgather.codegen:generate_code@model_code_generator\n",
       "    searchpath: \"/home/dinalt/ai_assets/forgather/modelsrc/templates\"\n",
       "    template_name: \"hf_causal.py\"\n",
       "    name_policy: \"named\"\n",
       "    obj: *model_factory\n",
       "    # Template args\n",
       "    model_type: \"forgather-dynamic-causal-dynllama\"\n",
       "    # Dynamic Llama\n",
       "    supports_gradient_checkpointing: True\n",
       "    supports_sdpa: True\n",
       "\n",
       "model_code_writer: &model_code_writer !singleton:forgather.ml.construct:write_file@model_code_writer\n",
       "    data: *model_code_generator\n",
       "    output_file: \"./output_models/tiny_llama/dynllama.py\"\n",
       "    return_value: \"Model constructor generated by Forgather 1.0\"    \n",
       "\n",
       "model_config: &model_config !singleton:./output_models/tiny_llama/dynllama.py:DynamicCausalLMConfig@model_config\n",
       "    submodule_searchpath: *model_submodule_searchpath\n",
       "    # Set auto-map for custom model; this ensures that the source code stays with the model.\n",
       "    auto_map:\n",
       "        AutoConfig: \"dynllama.DynamicCausalLMConfig\"\n",
       "        AutoModel: \"dynllama.DynamicCasualLM\"\n",
       "    # Get the vocab-size from the tokenizer definition.\n",
       "    vocab_size: !singleton:len [ *tokenizer ]\n",
       "    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]\n",
       "    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]\n",
       "    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]\n",
       "    # Add dependency on code generator\n",
       "    code_generator: *model_code_writer\n",
       "    hidden_size: 4096\n",
       "    num_attention_heads: 32\n",
       "    # Default to MHA when null\n",
       "    num_kv_heads: null\n",
       "    d_head: 128 # Must be hidden_size // num_attention_heads\n",
       "    num_hidden_layers: 32\n",
       "    max_sequence_length: !singleton:getattr\n",
       "        - *tokenizer\n",
       "        - \"model_max_length\"\n",
       "    dim_feedforward: 11008\n",
       "    rope_theta: 10000.0\n",
       "    embedding_dropout: 0.0\n",
       "    rms_norm_eps: 1.0e-05\n",
       "    layer_dropout: 0.0\n",
       "    residual_dropout: 0.0\n",
       "    attention_dropout: 0.0\n",
       "    activation_dropout: 0.0\n",
       "    enable_activation_checkpoint: False\n",
       "    checkpoint_stride: 1\n",
       "    \n",
       "    # Tiny Llama overrides\n",
       "    hidden_size: 256\n",
       "    dim_feedforward: 1024\n",
       "    num_attention_heads: 2\n",
       "    num_hidden_layers: 4\n",
       "    d_head: 128 # Must be hidden_size // num_attention_heads\n",
       "\n",
       "# **Model Factory**\n",
       "\n",
       "pretrained_model: &pretrained_model !partial:./output_models/tiny_llama/dynllama.py:DynamicCasualLM@pretrained_model\n",
       "    args:\n",
       "        - *model_config\n",
       "    kwargs:\n",
       "        submodule_searchpath: *model_submodule_searchpath\n",
       "        <<: *model_constructor_args\n",
       "\n",
       "model: &model !partial:forgather.ml.construct:dependency_list@model\n",
       "    - !factory:call [ *pretrained_model ]\n",
       "    - !singleton:forgather.ml.construct:copy_package_files\n",
       "        - \"./output_models/tiny_llama\"\n",
       "        - *model_config\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "tokenizer_args: &tokenizer_args !dict\n",
       "    truncation: True\n",
       "    max_length: 512\n",
       "# Load dataset from sub-project\n",
       ".define: &dataset_dict !call:forgather:from_project\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/datasets/roneneldan\"\n",
       "    config_template: \"tinystories-abridged.yaml\"\n",
       "    targets: [  \"train_dataset\", \"eval_dataset\" ] \n",
       "    preprocess_args: *tokenizer_args\n",
       "    tokenizer: *tokenizer\n",
       "\n",
       "train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]\n",
       "eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM\n",
       "    tokenizer: *tokenizer\n",
       "    return_tensors: pt\n",
       "\n",
       "    # Tiny Llama\n",
       "    truncation: True\n",
       "    max_length: 512\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/tiny_llama/runs/log_2025-08-24T09-51-27\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       "experiment_info: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-08-24T09:51:27\"\n",
       "    name: \"Tiny Llama\"\n",
       "    description: \"A demo of training a tiny llama model from scratch\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.7.1', 'transformers': '4.51.3', 'accelerate': '1.7.0'}\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       "# The model will be given the following prompts for text-gen at regular intervals.\n",
       "testprompts: &testprompts !list:@testprompts\n",
       "    # Test prompts from \"https://arxiv.org/abs/2305.07759\"\n",
       "    - \"Alice was so tired when she got back home so she went\"\n",
       "    - \"Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was\"\n",
       "    - \"Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \\\"Look, Lily. A rainbow has\"\n",
       "    - \"Jack wanted to read a book, so he went to\"\n",
       "    - \"\\\"Can cows fly?\\\" Alice asked her mother.\"\n",
       "    - \"\\\"What do birds like to eat?\\\" Tom asked his mother.\"\n",
       "    - \"\\\"What language do they speak in France?\\\" Tom asked his mother.\"\n",
       "    - \"If I throw a ball up in the air, eventually it will\"\n",
       "    - \"It was winter and cold outside so his mother told him, \\\"You should\"\n",
       "    - \"Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked\"\n",
       "    - \"Jack told Mary, \\\"If you give me your banana, I'll give you my apple.\\\" Mary gave Jack her Banana, so\"\n",
       "    - \"On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to\"\n",
       "    - \"Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that\"\n",
       "    - \"Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \\\"I want to go to the park\\\". Lily says\"\n",
       "    - \"Jack's mother was not home, and his father was at home. When Jack came home, he said hello to\"\n",
       "    - \"Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says\"\n",
       "    - \"Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They\"\n",
       "    - \"Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door,\"\n",
       "\n",
       "# Conservative text-generation parameters.\n",
       "generation_config: &generation_config !dict:@generation_config\n",
       "    identity: generation_config\n",
       "    do_sample: True\n",
       "    top_k: 20\n",
       "    top_p: 0.9\n",
       "    temperature: 0.7\n",
       "    repitition_penalty: 1.15\n",
       "\n",
       "trainer_callbacks: &trainer_callbacks !list:@trainer_callbacks\n",
       "    # Log all training output to JSON\n",
       "    - !singleton:forgather.ml.trainer.callbacks:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    - !singleton:forgather.ml.trainer.callbacks:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "    - !singleton:forgather.ml.trainer.callbacks:TextgenCallback\n",
       "        summary_writer: *summary_writer\n",
       "        prompts: *testprompts\n",
       "        generation_config: *generation_config\n",
       "        max_new_tokens: 40\n",
       "        generation_steps: 1000\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       "optimizer: &optimizer !partial:torch:optim.AdamW\n",
       "    lr: 1.0e-3\n",
       "\n",
       "############# LR Scheduler #############\n",
       "\n",
       "# https://arxiv.org/html/2503.02844v1\n",
       "lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler\n",
       "    warmup_steps: 500\n",
       "    cooldown_steps: 50000\n",
       "    constant_lr: 1.0e-4\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: Forgather Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "# Trainer Config Class: forgather.ml.trainer:TrainingArguments\n",
       "# Trainer Class: forgather.ml.trainer:Trainer\n",
       "# nproc_per_node: 1\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       "\n",
       "\n",
       "trainer_args: &trainer_args !singleton:forgather.ml.trainer:TrainingArguments@trainer_args\n",
       "    save_strategy: \"no\"\n",
       "    max_steps: -1\n",
       "    output_dir: \"./output_models/tiny_llama\"\n",
       "    logging_dir: \"./output_models/tiny_llama/runs/log_2025-08-24T09-51-27\"\n",
       "    # Tiny Llama Project Overrides\n",
       "    save_strategy: \"steps\"\n",
       "    save_steps: 10000\n",
       "    # Safetensors can't handle tied parameters/buffers, so fallback to PyTorch format.\n",
       "    save_safetensors: False\n",
       "    seed: 42\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 64\n",
       "    logging_steps: 100\n",
       "    eval_steps: 500\n",
       "    num_train_epochs: 1\n",
       "    dataloader_num_workers: 1\n",
       "\n",
       "model_preprocessor: &model_preprocessor !partial:call\n",
       "    - *model\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       "trainer: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    args: *trainer_args\n",
       "    model_init: *model_preprocessor\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "    # Trainer Args\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "# **Dynamic Args**\n",
       "dynamic_args: !dlist\n",
       "    null: ~\n",
       "    max_steps:\n",
       "        names: \"--max-steps\"\n",
       "        type: \"int\"\n",
       "        help: \"Set maximum training steps\"\n",
       "    save_strategy:\n",
       "        names: \"--save-strategy\"\n",
       "        choices: [ \"no\", \"steps\", \"epoch\" ]\n",
       "        type: \"str\"\n",
       "        help: \"When to save checkpoints\"\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Tiny Llama\"\n",
       "    config_description: \"A demo of training a tiny llama model from scratch\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/ai_assets/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "    output_dir: \"./output_models/tiny_llama\"\n",
       "    model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "    logging_dir: \"./output_models/tiny_llama/runs/log_2025-08-24T09-51-27\"\n",
       "    nproc_per_node: 1\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_train: True\n",
       "    do_save: False\n",
       "    do_eval: False\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "    do_save: True\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_config(config_template=\"\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bcb135-37fe-4f61-80e7-dc8d71c08294",
   "metadata": {},
   "source": [
    "## Load Project\n",
    "\n",
    "Load the default configuraiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1f9b45-db49-475f-8c73-b47d0e9ab08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "# Load the default project, which is \"train_tiny_llama.yaml\"\n",
    "proj = Project()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15abe5-957f-43ff-af18-0c8725fe961e",
   "metadata": {},
   "source": [
    "## Start Tensorboard\n",
    "\n",
    "This project has been configured to log training to Tensorboard (TB). To watch the model's training progress with TB, run the following command, which will generate a CLI command to start the TB server. Then run the command from a shell.\n",
    "\n",
    "Tensorboard can be started from a terminal like this:\n",
    "\n",
    "```bash\n",
    "# By default, Tensorboard bind only to localhost. To bind to all interfaces, add --bind_all\n",
    "tensorboard --logdir \"/path/to/model/log/directory\" [--bind_all]\n",
    "```\n",
    "\n",
    "You can use the CLI to launch TB for you, where it will automatically determine the path to the log directory:\n",
    "\n",
    "```bash\n",
    "# --all : Watch all output model directories, otherwise just the one for the current configuration.\n",
    "# -- : Any arguments after '--' are passed directly to tensorboard, for example \"--bind_all\"\n",
    "cd PROJECT_DIR\n",
    "cfcli.py tb [--all] [-- <tensorboard-args>]\n",
    "```\n",
    "\n",
    "When TB starts, it should provide the URL to access it. e.g.\n",
    "\n",
    "```\n",
    "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b3946-1faa-4224-afe8-58233a57e41c",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "You have a few options for training the mode.\n",
    "\n",
    "1. Run it directly from the notebook. This should work find with this example, although for projects using multiple GPUs, you will want to use one of the other options. To train from the notebook, just run the following cell.\n",
    "2. You can generate a training script and run it from the shell. To do so, run the cell with \"generate_trainingscript(),\" then run the generated shell script from a terminal.\n",
    "3. You can use the Forgather CLI.\n",
    "\n",
    "```bash\n",
    "# Open a shell in thie project's directory, then run this command:\n",
    "cd PROJECT_DIR\n",
    "forgather train\n",
    "\n",
    "# See forgather --help for more details.\n",
    "```\n",
    "\n",
    "Once training starts, switch to Tensorboard in your browser. One of the first things you will want to do is enable automatic refresh. To do so, click the gear in the upper-right corner and check \"Reload Data.\"\n",
    "\n",
    "Once training has started, take a look at the \"Text\" tab. You will see that we have automatically logged the preprocessed configuraiton as well as having dumped the primary training artifacts.\n",
    "\n",
    "Next, switch to the \"Scalars\" tab. You will see a plot of train and evaluation loss which will automatically update every 30 seconds. If you are not familiar with Tensorboard, now would be a good time to play with the UI elements to see how they work.\n",
    "\n",
    "When training completes, the model will be automatically saved to the output directory (\"./output_models/default_model\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7f6eb9-33ec-489c-b66e-faf3e3f0bdee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:forgather.ml.training_script:**** Training Script Started *****\n",
      "INFO:forgather.ml.training_script:config_name: Tiny Llama\n",
      "INFO:forgather.ml.training_script:config_description: A demo of training a tiny llama model from scratch\n",
      "INFO:forgather.ml.training_script:output_dir: ./output_models/tiny_llama\n",
      "INFO:forgather.ml.training_script:logging_dir: ./output_models/tiny_llama/runs/log_2025-08-24T09-51-28\n",
      "INFO:forgather.ml.trainer.trainer:Constructing model on default device and moving to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs= {'model_type': 'forgather-dynamic-causal-dynllama', 'supports_gradient_checkpointing': True, 'supports_sdpa': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce611c4c1c94e11869d5414f3e20212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 212,000\n",
      "total_train_samples: 212,000\n",
      "per_device_train_batch_size: 32\n",
      "actual_per_device_batch_size: 32\n",
      "total_train_batch_size: 32\n",
      "max_steps: 6,625\n",
      "total_parameters: 5.2M\n",
      "trainable_parameters: 5.2M\n",
      "model:\n",
      "DynamicCasualLM(\n",
      "  (causal_lm): CasualLM(\n",
      "    loss_fn=CausalLoss()\n",
      "    (input_encoder): InputEncoder(\n",
      "      d_model=256, vocab_size=2000\n",
      "      (dropout): Identity()\n",
      "      (embedding): Embedding(2000, 256)\n",
      "    )\n",
      "    (output_decoder): Linear(in_features=256, out_features=2000, bias=False)\n",
      "    (layer_stack): LayerStack(\n",
      "      gradient_checkpointing=False, checkpoint_stride=1\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x PreLNLayer(\n",
      "          (feedforward): GLUFeedforwardLayer(\n",
      "            d_model=256, d_feedforward=1024\n",
      "            (up_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
      "            (gate_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
      "            (down_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "            (activation): SiLU()\n",
      "            (dropout): Identity()\n",
      "          )\n",
      "          (attention): CausalRpeAttn(\n",
      "            d_model=256, num_heads=2, num_kv_heads=2\n",
      "            (pos_encoder): RealRotaryPE(d_head=128, max_sequence_length=2048, rope_theta=10000.0)\n",
      "            (query_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (key_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (value_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (output_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (norm1): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Identity()\n",
      "          (residual_dropout): Identity()\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "2025-08-24 09:51:44          100  0.02  train-loss: 7.05188   learning-rate: 2.00e-04\n",
      "2025-08-24 09:51:46          200  0.03  train-loss: 4.97328   learning-rate: 4.00e-04\n",
      "2025-08-24 09:51:48          300  0.05  train-loss: 4.08712   learning-rate: 6.00e-04\n",
      "2025-08-24 09:51:50          400  0.06  train-loss: 3.71734   learning-rate: 8.00e-04\n",
      "2025-08-24 09:51:52          500  0.08  train-loss: 3.43777   learning-rate: 1.00e-03\n",
      "2025-08-24 09:51:54          600  0.09  train-loss: 3.29045   learning-rate: 1.00e-03\n",
      "2025-08-24 09:51:56          700  0.11  train-loss: 3.1282    learning-rate: 1.00e-03\n",
      "2025-08-24 09:51:58          800  0.12  train-loss: 3.05551   learning-rate: 1.00e-03\n",
      "2025-08-24 09:52:00          900  0.14  train-loss: 2.90666   learning-rate: 1.00e-03\n",
      "2025-08-24 09:52:02        1,000  0.15  train-loss: 2.72878   learning-rate: 1.00e-03\n",
      "2025-08-24 09:52:04        1,100  0.17  train-loss: 2.71315   learning-rate: 1.00e-03\n",
      "2025-08-24 09:52:06        1,200  0.18  train-loss: 2.65651   learning-rate: 1.00e-03\n",
      "2025-08-24 09:52:08        1,300  0.2   train-loss: 2.58488   learning-rate: 9.99e-04\n",
      "2025-08-24 09:52:10        1,400  0.21  train-loss: 2.53178   learning-rate: 9.99e-04\n",
      "2025-08-24 09:52:12        1,500  0.23  train-loss: 2.48868   learning-rate: 9.99e-04\n",
      "2025-08-24 09:52:14        1,600  0.24  train-loss: 2.42944   learning-rate: 9.99e-04\n",
      "2025-08-24 09:52:16        1,700  0.26  train-loss: 2.37178   learning-rate: 9.99e-04\n",
      "2025-08-24 09:52:18        1,800  0.27  train-loss: 2.29925   learning-rate: 9.98e-04\n",
      "2025-08-24 09:52:20        1,900  0.29  train-loss: 2.27722   learning-rate: 9.98e-04\n",
      "2025-08-24 09:52:22        2,000  0.3   train-loss: 2.31825   learning-rate: 9.98e-04\n",
      "2025-08-24 09:52:24        2,100  0.32  train-loss: 2.25999   learning-rate: 9.98e-04\n",
      "2025-08-24 09:52:26        2,200  0.33  train-loss: 2.20599   learning-rate: 9.97e-04\n",
      "2025-08-24 09:52:28        2,300  0.35  train-loss: 2.16787   learning-rate: 9.97e-04\n",
      "2025-08-24 09:52:30        2,400  0.36  train-loss: 2.22498   learning-rate: 9.97e-04\n",
      "2025-08-24 09:52:32        2,500  0.38  train-loss: 2.19161   learning-rate: 9.96e-04\n",
      "2025-08-24 09:52:34        2,600  0.39  train-loss: 2.18847   learning-rate: 9.96e-04\n",
      "2025-08-24 09:52:36        2,700  0.41  train-loss: 2.11783   learning-rate: 9.96e-04\n",
      "2025-08-24 09:52:38        2,800  0.42  train-loss: 2.16549   learning-rate: 9.95e-04\n",
      "2025-08-24 09:52:40        2,900  0.44  train-loss: 2.0817    learning-rate: 9.95e-04\n",
      "2025-08-24 09:52:42        3,000  0.45  train-loss: 1.96805   learning-rate: 9.94e-04\n",
      "2025-08-24 09:52:44        3,100  0.47  train-loss: 2.06347   learning-rate: 9.94e-04\n",
      "2025-08-24 09:52:46        3,200  0.48  train-loss: 2.1432    learning-rate: 9.94e-04\n",
      "2025-08-24 09:52:48        3,300  0.5   train-loss: 2.0335    learning-rate: 9.93e-04\n",
      "2025-08-24 09:52:50        3,400  0.51  train-loss: 1.9578    learning-rate: 9.93e-04\n",
      "2025-08-24 09:52:52        3,500  0.53  train-loss: 1.98457   learning-rate: 9.92e-04\n",
      "2025-08-24 09:52:54        3,600  0.54  train-loss: 2.05331   learning-rate: 9.91e-04\n",
      "2025-08-24 09:52:56        3,700  0.56  train-loss: 1.96816   learning-rate: 9.91e-04\n",
      "2025-08-24 09:52:58        3,800  0.57  train-loss: 1.96623   learning-rate: 9.90e-04\n",
      "2025-08-24 09:53:00        3,900  0.59  train-loss: 2.01913   learning-rate: 9.90e-04\n",
      "2025-08-24 09:53:02        4,000  0.6   train-loss: 2.04722   learning-rate: 9.89e-04\n",
      "2025-08-24 09:53:05        4,100  0.62  train-loss: 1.94409   learning-rate: 9.89e-04\n",
      "2025-08-24 09:53:07        4,200  0.63  train-loss: 1.91958   learning-rate: 9.88e-04\n",
      "2025-08-24 09:53:09        4,300  0.65  train-loss: 1.95794   learning-rate: 9.87e-04\n",
      "2025-08-24 09:53:11        4,400  0.66  train-loss: 2.00911   learning-rate: 9.87e-04\n",
      "2025-08-24 09:53:13        4,500  0.68  train-loss: 1.94185   learning-rate: 9.86e-04\n",
      "2025-08-24 09:53:15        4,600  0.69  train-loss: 1.8677    learning-rate: 9.85e-04\n",
      "2025-08-24 09:53:17        4,700  0.71  train-loss: 1.87549   learning-rate: 9.84e-04\n",
      "2025-08-24 09:53:19        4,800  0.72  train-loss: 1.90765   learning-rate: 9.84e-04\n",
      "2025-08-24 09:53:21        4,900  0.74  train-loss: 1.91331   learning-rate: 9.83e-04\n",
      "2025-08-24 09:53:23        5,000  0.75  train-loss: 1.91836   learning-rate: 9.82e-04\n",
      "2025-08-24 09:53:25        5,100  0.77  train-loss: 1.91795   learning-rate: 9.81e-04\n",
      "2025-08-24 09:53:28        5,200  0.78  train-loss: 1.82967   learning-rate: 9.81e-04\n",
      "2025-08-24 09:53:30        5,300  0.8   train-loss: 1.81627   learning-rate: 9.80e-04\n",
      "2025-08-24 09:53:32        5,400  0.82  train-loss: 1.8684    learning-rate: 9.79e-04\n",
      "2025-08-24 09:53:34        5,500  0.83  train-loss: 1.83597   learning-rate: 9.78e-04\n",
      "2025-08-24 09:53:36        5,600  0.85  train-loss: 1.8834    learning-rate: 9.77e-04\n",
      "2025-08-24 09:53:38        5,700  0.86  train-loss: 1.90774   learning-rate: 9.76e-04\n",
      "2025-08-24 09:53:40        5,800  0.88  train-loss: 1.86331   learning-rate: 9.75e-04\n",
      "2025-08-24 09:53:42        5,900  0.89  train-loss: 1.87977   learning-rate: 9.74e-04\n",
      "2025-08-24 09:53:44        6,000  0.91  train-loss: 1.79343   learning-rate: 9.73e-04\n",
      "2025-08-24 09:53:46        6,100  0.92  train-loss: 1.77333   learning-rate: 9.72e-04\n",
      "2025-08-24 09:53:48        6,200  0.94  train-loss: 1.83454   learning-rate: 9.71e-04\n",
      "2025-08-24 09:53:50        6,300  0.95  train-loss: 1.80618   learning-rate: 9.70e-04\n",
      "2025-08-24 09:53:52        6,400  0.97  train-loss: 1.79416   learning-rate: 9.69e-04\n",
      "2025-08-24 09:53:54        6,500  0.98  train-loss: 1.82039   learning-rate: 9.68e-04\n",
      "2025-08-24 09:53:56        6,600  1.0   train-loss: 1.75918   learning-rate: 9.67e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:forgather.ml.trainer.trainer:Saving final checkpoint at step 6625\n",
      "INFO:forgather.ml.trainer.base_trainer:Saving checkpoint at ./output_models/tiny_llama/checkpoints/checkpoint-6625\n",
      "INFO:forgather.ml.trainer.base_trainer:Saved training state to ./output_models/tiny_llama/checkpoints/checkpoint-6625/training_state.pt\n",
      "INFO:forgather.ml.training_script:**** Training Completed *****\n",
      "INFO:forgather.ml.training_script:{'train_runtime': 135.47482442855835, 'train_samples': 212000, 'step': 6625, 'train_samples_per_second': 1564.866, 'train_steps_per_second': 48.902, 'epoch': 1.0}\n",
      "INFO:forgather.ml.training_script:Model saved to: ./output_models/tiny_llama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-24 09:53:57        6,625  1.0   train_runtime: 135.5 train_samples: 212,000 step: 6,625 train_samples_per_second: 1.565e+03 train_steps_per_second: 48.9 epoch: 1.0 \n"
     ]
    }
   ],
   "source": [
    "# Train model in notebook.\n",
    "\n",
    "# Construct the default target, \"main,\" which is a training script.\n",
    "training_script = proj()\n",
    "\n",
    "# Start training the model.\n",
    "training_script.run()\n",
    "\n",
    "# Release resources\n",
    "training_script = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a33f36-246d-4627-aebf-5e8127f4b692",
   "metadata": {},
   "source": [
    "## Load Trained Model\n",
    "\n",
    "You can use the regular HF APIs to load the saved model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4846ff07-14ed-434e-aadd-b04063e6c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GenerationConfig, StoppingCriteria\n",
    "import torch\n",
    "\n",
    "model_path = \"./output_models/tiny_llama\"\n",
    "\n",
    "# Set device to run inference on\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d72d1-1e8f-4433-abe0-a0d9aaa252c0",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "This loop will use the newly trained model to generate text, seeded with the above prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee6f8d5-c548-4b94-88d2-fc92f118746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was so tired when she got back home so she went [START] to sleep. She closed her eyes and dreamt about her dream. Alice was always sleeping. Alice had a big dream about a brave little girl. In her dream, she saw a big, scary monster. The monster was very scary and scared. She had a big smile and a big smile.\n",
      "\n",
      "The monster said, \"I mustn't run away, little girl. The monster is not scary anymore.\" She was sad. She wanted to go away. The\n",
      "----------------------------------------\n",
      "Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was [START] moon.\n",
      "\n",
      "One night, they heard a loud noise outside. It was a big, scary monster! It was making a loud noise and a big monster appeared. The monster said, \"Be careful, the moon. It can hurt you.\" Lily said, \"Don't worry, monster. I will protect you.\"\n",
      "\n",
      "But the moon did not move. The moon did not move. The moon was in a dark. The moon was\n",
      "----------------------------------------\n",
      "Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has [START] a big flood on the ground. It's a flood, but it's a special flood. It's a special flood, and it's a special flood.\n",
      "\n",
      "Jack and Lily were excited and asked, \"What do you mean?\"\n",
      "\n",
      "Jack said, \"We're going to have a flood. It's a flood.\"\n",
      "\n",
      "They took a flood and started to spring the flood. They laughed and shouted, \"See, it\n",
      "----------------------------------------\n",
      "Jack wanted to read a book, so he went to [START] the kitchen and grabbed a big bowl. He put the pillows and the pillows on the table. Then he noticed a big pot of soup. Jack thought it was a great idea. He took a big pot and a bowl and a spoon. He put the pot and the soup and the bowl and the pot and the pot. Then, the soup started to blink and make a loud noise. The soup was so tasty! Jack's mom heard him and looked\n",
      "----------------------------------------\n",
      "\"Can cows fly?\" Alice asked her mother. [START] \n",
      "\"OK, sweetie,\" her mother said, \"We have to be careful and listen to our parents and dad's warnings.\"\n",
      "\n",
      "Alice was sad, but she knew that they should not go to the warnings. So they went to the warnings and looked forward to them.\n",
      "\n",
      "When they got home, Alice and her family went to the warnings. They were so happy. They had to stay in the warn\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(model, tokenizer, prompts, gen_config, max_new_tokens, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for prompt in prompts:\n",
    "            tokenizer_outputs = tokenizer(\n",
    "                [prompt],\n",
    "                truncation=False,\n",
    "                return_length=True,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "        \n",
    "            input_ids = tokenizer_outputs[\"input_ids\"].to(device)\n",
    "            attention_mask = tokenizer_outputs[\"attention_mask\"].to(device)\n",
    "            use_cache = getattr(model, \"_supports_cache_class\", False)\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                generation_config=gen_config,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=use_cache,\n",
    "                past_key_values=None,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "    \n",
    "            output_text = tokenizer.decode(\n",
    "                outputs.sequences[0],\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "            yield prompt + \" [START] \" + output_text[len(prompt) + 1 :]\n",
    "\n",
    "prompts = [\n",
    "    'Alice was so tired when she got back home so she went',\n",
    "    'Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was',\n",
    "    'Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has',\n",
    "    'Jack wanted to read a book, so he went to',\n",
    "    '\"Can cows fly?\" Alice asked her mother.',\n",
    "]\n",
    "\n",
    "gen_config = GenerationConfig(\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    do_sample=True,\n",
    "    top_k=20,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    repitition_penalty=1.15,\n",
    ")\n",
    "\n",
    "for s in generate_text(model, tokenizer, prompts, gen_config, 100, \"cuda:0\"):\n",
    "    print(s)\n",
    "    print(f\"{'-' * 40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b87ddc-9fbf-4799-9d3d-c71e06859401",
   "metadata": {},
   "source": [
    "## Train Hugginface LLama Model\n",
    "\n",
    "Next, let's try training a Llama model using the Huggingface implementation.\n",
    "\n",
    "Train the model on the CLI\n",
    "\n",
    "```bash\n",
    "forgather -t train_hf_llama.yaml train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b057e02-fe19-432c-840a-0dad3ec0d50c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb.display_config(config_template=\"train_hf_llama.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e13d5-9597-43a4-956c-f438674b7f56",
   "metadata": {},
   "source": [
    "## Let's See What Happens...\n",
    "\n",
    "...if we replace the post-layer-norm implementation with a pre-layer-norm implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e44fde-65d6-4dce-8ad3-c1196313d4c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb.display_config(config_template=\"experimental_llama.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb6c21-28cd-47e0-ae75-b18e0d500ca6",
   "metadata": {},
   "source": [
    "```bash\n",
    "forgather -t experimental_llama.yaml train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15896a-6afa-47b5-bbf8-017277cc8d9c",
   "metadata": {},
   "source": [
    "## Test Model With the Inference Server\n",
    "\n",
    "There is a simple OpenAI compatible inference server implementation in \"tools/inference_server\"  \n",
    "\n",
    "To host your newly trained model on the inference server:\n",
    "\n",
    "```bash\n",
    "./server.py server_configs/tiny_llama.yaml\n",
    "```\n",
    "\n",
    "From another session, you can perform text completion like this:\n",
    "\n",
    "```bash\n",
    "./client.py client_configs/tiny_llama.yaml --stream --completion \"Once upon a time,\"\n",
    "```\n",
    "\n",
    "The Tiny Llama model, trained on Tiny Stories, will not be very good at interactive chat, but you cat test this with the following command:\n",
    "\n",
    "```bash\n",
    "./client.py client_configs/tiny_llama.yaml --stream --interactive\n",
    "```\n",
    "\n",
    "This server should work with other OpenAI compatible clients as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405bc11-9e44-4a24-8a21-91642c22cf50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
