{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Tiny LLama\n",
       "\n",
       "In this tutorial we will train a very small Llama model (about 5M parameters) on 10% of the Tiny Stories dataset. On a single RTX-4090, this takes about three minutes. Once training is complete, we will load the model an use it for text generation -- and the generation will be reasonably coherent for a three-minute-old model.\n",
       "\n",
       "#### Project Directory: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "\n",
       "## Meta Config\n",
       "Meta Config: [/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/meta.yaml](meta.yaml)\n",
       "\n",
       "- [meta.yaml](meta.yaml)\n",
       "    - [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "        - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/templates](templates)\n",
       "- [/home/dinalt/ai_assets/forgather/forgather_workspace](../../../forgather_workspace)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/modellib](../../../templatelib/modellib)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/examples](../../../templatelib/examples)\n",
       "- [/home/dinalt/ai_assets/forgather/templatelib/base](../../../templatelib/base)\n",
       "\n",
       "## Available Configurations\n",
       "- [train_hf_llama.yaml](templates/configs/train_hf_llama.yaml)\n",
       "- [train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "- [experimental_llama.yaml](templates/configs/experimental_llama.yaml)\n",
       "\n",
       "Default Configuration: train_tiny_llama.yaml\n",
       "\n",
       "## Available Templates\n",
       "- [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "- [meta_defaults.yaml](../../../forgather_workspace/meta_defaults.yaml)\n",
       "- [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "- [prompts/short_stories.yaml](../../../templatelib/examples/prompts/short_stories.yaml)\n",
       "- [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "- [tokenizers/tiny_8k.yaml](../../../templatelib/examples/tokenizers/tiny_8k.yaml)\n",
       "- [trainers/minimal_trainer.yaml](../../../templatelib/base/trainers/minimal_trainer.yaml)\n",
       "    - [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "        - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "            - [project.trainer_config](templates/project.yaml)\n",
       "            - [trainers/accel_trainer.yaml](../../../templatelib/base/trainers/accel_trainer.yaml)\n",
       "            - [trainers/pipeline_trainer.yaml](../../../templatelib/base/trainers/pipeline_trainer.yaml)\n",
       "            - [trainers/hf_trainer.yaml](../../../templatelib/base/trainers/hf_trainer.yaml)\n",
       "- [datasets/pretokenized_dataset.yaml](../../../templatelib/base/datasets/pretokenized_dataset.yaml)\n",
       "- [datasets/base_datasets.yaml](../../../templatelib/base/datasets/base_datasets.yaml)\n",
       "    - [datasets/wikipedia.yaml](../../../templatelib/examples/datasets/wikipedia.yaml)\n",
       "    - [datasets/books.yaml](../../../templatelib/examples/datasets/books.yaml)\n",
       "    - [datasets/samantha.yaml](../../../templatelib/examples/datasets/samantha.yaml)\n",
       "    - [datasets/tinystories/tinystories.yaml](../../../templatelib/examples/datasets/tinystories/tinystories.yaml)\n",
       "        - [datasets/tinystories/tinystories_abridged.yaml](../../../templatelib/examples/datasets/tinystories/tinystories_abridged.yaml)\n",
       "            - [project.dataset](templates/project.yaml)\n",
       "- [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "    - [models/causal_lm/from_pretrained.yaml](../../../templatelib/base/models/causal_lm/from_pretrained.yaml)\n",
       "        - [models/causal_lm/load_model.yaml](../../../templatelib/base/models/causal_lm/load_model.yaml)\n",
       "    - [models/causal_lm/from_pretrained_config.yaml](../../../templatelib/base/models/causal_lm/from_pretrained_config.yaml)\n",
       "    - [models/causal_lm/custom.yaml](../../../templatelib/base/models/causal_lm/custom.yaml)\n",
       "        - [models/causal_lm/custom_dynamic.yaml](../../../templatelib/base/models/causal_lm/custom_dynamic.yaml)\n",
       "            - [models/deepone.yaml](../../../templatelib/examples/models/deepone.yaml)\n",
       "            - [models/dynamic_causal_transformer.yaml](../../../templatelib/examples/models/dynamic_causal_transformer.yaml)\n",
       "            - [models/dynamic_llama.yaml](../../../templatelib/examples/models/dynamic_llama.yaml)\n",
       "                - [models/tiny_dynamic_llama.yaml](templates/models/tiny_dynamic_llama.yaml)\n",
       "                    - [project.model_config](templates/project.yaml)\n",
       "                        - [experiment.model_config](templates/configs/experimental_llama.yaml)\n",
       "    - [models/causal_lm/from_config.yaml](../../../templatelib/base/models/causal_lm/from_config.yaml)\n",
       "        - [models/gpt2.yaml](../../../templatelib/examples/models/gpt2.yaml)\n",
       "        - [models/llama.yaml](../../../templatelib/examples/models/llama.yaml)\n",
       "            - [models/tiny_hf_llama.yaml](templates/models/tiny_hf_llama.yaml)\n",
       "- [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "    - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "        - [project.logger_config](templates/project.yaml)\n",
       "- [types/type.yaml](../../../templatelib/base/types/type.yaml)\n",
       "    - [types/tokenizer/tokenizer.yaml](../../../templatelib/base/types/tokenizer/tokenizer.yaml)\n",
       "        - [types/tokenizer/bpe/bpe.yaml](../../../templatelib/base/types/tokenizer/bpe/bpe.yaml)\n",
       "    - [types/model/model_type.yaml](../../../templatelib/base/types/model/model_type.yaml)\n",
       "    - [types/training_script/training_script.yaml](../../../templatelib/base/types/training_script/training_script.yaml)\n",
       "        - [types/training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [project.yaml](templates/project.yaml)\n",
       "                - [configs/train_hf_llama.yaml](templates/configs/train_hf_llama.yaml)\n",
       "                - [configs/train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "                - [configs/experimental_llama.yaml](templates/configs/experimental_llama.yaml)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "nb.display_project_index(show_available_templates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4576985-d53c-4b74-b68b-31ec1e8fcdbd",
   "metadata": {},
   "source": [
    "---\n",
    "This example makes extensive use of the Forgather templates library. Take a look at the various files which go into the configuration and compare these to the pre-processed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d9e625-31e4-4694-b77c-cbcc32813b63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Included Templates\n",
       "- [configs/train_tiny_llama.yaml](templates/configs/train_tiny_llama.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [types/training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "                - [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "                    - [trainers/minimal_trainer.yaml](../../../templatelib/base/trainers/minimal_trainer.yaml)\n",
       "            - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "                - [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "            - [models/causal_lm/load_model.yaml](../../../templatelib/base/models/causal_lm/load_model.yaml)\n",
       "                - [models/causal_lm/from_pretrained.yaml](../../../templatelib/base/models/causal_lm/from_pretrained.yaml)\n",
       "                    - [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "            - [types/training_script/training_script.yaml](../../../templatelib/base/types/training_script/training_script.yaml)\n",
       "                - [types/type.yaml](../../../templatelib/base/types/type.yaml)\n",
       "                    - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "            - [inc/formatting.jinja](../../../templatelib/base/inc/formatting.jinja)\n",
       "        - [project.logger_config](templates/project.yaml)\n",
       "            - [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "        - [project.model_config](templates/project.yaml)\n",
       "            - [models/tiny_dynamic_llama.yaml](templates/models/tiny_dynamic_llama.yaml)\n",
       "                - [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "                - [models/dynamic_llama.yaml](../../../templatelib/examples/models/dynamic_llama.yaml)\n",
       "                    - [models/causal_lm/custom_dynamic.yaml](../../../templatelib/base/models/causal_lm/custom_dynamic.yaml)\n",
       "                        - [models/causal_lm/custom.yaml](../../../templatelib/base/models/causal_lm/custom.yaml)\n",
       "        - [project.dataset](templates/project.yaml)\n",
       "            - [datasets/tinystories/tinystories_abridged.yaml](../../../templatelib/examples/datasets/tinystories/tinystories_abridged.yaml)\n",
       "                - [datasets/tinystories/tinystories.yaml](../../../templatelib/examples/datasets/tinystories/tinystories.yaml)\n",
       "                    - [datasets/base_datasets.yaml](../../../templatelib/base/datasets/base_datasets.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'A demo of training a tiny llama model from scratch',\n",
       " 'config_name': 'Tiny Llama',\n",
       " 'create_new_model': 'True',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/forgather/datasets',\n",
       " 'eval': 'False',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': './output_models/tiny_llama/runs/log_2025-08-06T09-01-34',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'nproc_per_node': 1,\n",
       " 'output_dir': './output_models/tiny_llama',\n",
       " 'project_dir': '.',\n",
       " 'save_model': 'True',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'train': 'True',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "- [./output_models/tiny_llama/dynllama.py](output_models/tiny_llama/dynllama.py) : DynamicCausalLMConfig\n",
       "- [./output_models/tiny_llama/dynllama.py](output_models/tiny_llama/dynllama.py) : DynamicCasualLM\n",
       "## Output Targets\n",
       "- distributed_env\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "- model_submodule_searchpath\n",
       "- loss_fn\n",
       "- layer_norm_factory\n",
       "- feedforward_factory\n",
       "- relative_pe\n",
       "- attention_factory\n",
       "- layer_factory\n",
       "- layer_stack\n",
       "- output_decoder\n",
       "- absolute_pe\n",
       "- input_encoder\n",
       "- init_weights\n",
       "- model_factory\n",
       "- model_code_generator\n",
       "- model_code_writer\n",
       "- model_config\n",
       "- pretrained_model\n",
       "- model\n",
       "- train_source_dataset\n",
       "- eval_source_dataset\n",
       "- train_dataset_split\n",
       "- eval_dataset_split\n",
       "- preprocess_args\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- experiment_info\n",
       "- testprompts\n",
       "- generation_config\n",
       "- trainer_callbacks\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- trainer_args\n",
       "- model_preprocessor\n",
       "- trainer\n",
       "- meta\n",
       "- main\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#               Tiny Llama               \n",
       "#---------------------------------------\n",
       "# 2025-08-06T09:01:34\n",
       "# Description: A demo of training a tiny llama model from scratch\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: tiny_llama\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.7.1\n",
       "#     transformers: 4.51.3\n",
       "#     accelerate: 1.7.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"./output_models/tiny_llama\"\n",
       "# ns.logging_dir: \"./output_models/tiny_llama/runs/log_2025-08-06T09-01-34\"\n",
       "# ns.create_new_model: True\n",
       "# ns.save_model: True\n",
       "# ns.train: True\n",
       "# ns.eval: False\n",
       "# ns.trust_remote_code: False\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       "distributed_env: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "\n",
       "\n",
       "################ Model #################\n",
       "\n",
       "# https://huggingface.co/docs/transformers/en/model_doc/auto\n",
       "model_constructor_args: &model_constructor_args {}\n",
       "\n",
       "# Name: Dynamic Llama\n",
       "# Description: A Llama compatible dynamic model.\n",
       "# model_def.cls = \"DynamicCasualLM\"\n",
       "# model_def.cfg_cls = \"DynamicCausalLMConfig\"\n",
       "# model_def.config_path = \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models/tiny_llama/dynllama.py\"\n",
       "# model_def.model_path = \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models/tiny_llama/dynllama.py\"\n",
       "# model_def.short_name = \"dynllama\"\n",
       "# model_def.model_type = \"forgather-dynamic-causal-dynllama\"\n",
       "# model_def.model_path = \"./output_models/tiny_llama/dynllama.py\"\n",
       "# model_def.model_template_searchpath = \"/home/dinalt/ai_assets/forgather/modelsrc/templates\"\n",
       "# model_def.model_template_name = \"hf_causal.py\"\n",
       "# model_def.name_policy = \"named\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       "# Load custom tokenizer from sub-project definition\n",
       "tokenizer: &tokenizer !singleton:forgather.ml.construct:load_from_config@tokenizer\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe\"\n",
       "    config_template: \"2k.yaml\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       "# Model config dependencies\n",
       "\n",
       "model_submodule_searchpath: &model_submodule_searchpath\n",
       "    - \"/home/dinalt/ai_assets/forgather/modelsrc/transformer\"\n",
       "    - \"./output_models/tiny_llama\"\n",
       "\n",
       "loss_fn: &loss_fn !singleton:.causal_loss:CausalLoss@loss_fn []\n",
       "\n",
       "layer_norm_factory: &layer_norm_factory !partial:torch.nn:RMSNorm@layer_norm_factory\n",
       "    normalized_shape: !var \"hidden_size\"\n",
       "    eps: !var \"rms_norm_eps\"\n",
       "\n",
       "feedforward_factory: &feedforward_factory !partial:.glu_feedforward:GLUFeedforwardLayer@feedforward_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "    activation_factory: !partial:torch.nn.SiLU []\n",
       "    dropout: !var \"activation_dropout\"\n",
       "\n",
       "relative_pe: &relative_pe !singleton:.real_rotary_embeddings:RealRotaryPE@relative_pe\n",
       "    d_head: !var \"d_head\"\n",
       "    max_sequence_length: !var \"max_sequence_length\"\n",
       "    rope_theta: !var \"rope_theta\"\n",
       "    \n",
       "attention_factory: &attention_factory !partial:.causal_rpe_attn:CausalRpeAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    num_heads: !var \"num_attention_heads\"\n",
       "    num_kv_heads: !var \"num_kv_heads\"\n",
       "    dropout: !var \"attention_dropout\"\n",
       "    bias: False\n",
       "    sdpa_function: !partial:torch.nn.functional:scaled_dot_product_attention []\n",
       "    pos_encoder: *relative_pe\n",
       "\n",
       "layer_factory: &layer_factory !partial:.pre_ln_layer:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "    dropout: !var \"layer_dropout\"\n",
       "    residual_dropout: !var \"residual_dropout\"\n",
       "\n",
       "layer_stack: &layer_stack !factory:.checkpoint_layer_stack:LayerStack@layer_stack\n",
       "    layer_factory: *layer_factory\n",
       "    num_hidden_layers: !var \"num_hidden_layers\"\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    enable_checkpoint: !var \"enable_activation_checkpoint\"\n",
       "    checkpoint_stride: !var \"checkpoint_stride\"\n",
       "\n",
       "output_decoder: &output_decoder !factory:torch.nn:Linear@output_decoder\n",
       "    in_features: !var \"hidden_size\"\n",
       "    out_features: !var \"vocab_size\"\n",
       "    bias: False\n",
       "\n",
       "absolute_pe: &absolute_pe null\n",
       "\n",
       "input_encoder: &input_encoder !factory:.input_encoder:InputEncoder@input_encoder\n",
       "    d_model: !var \"hidden_size\"\n",
       "    vocab_size: !var \"vocab_size\"\n",
       "    dropout: !var \"embedding_dropout\"\n",
       "    positional_encoder: *absolute_pe\n",
       "    scale_sqrt_d_model: False\n",
       "\n",
       "# Init method based upon https://github.com/pytorch/torchtitan/blob/main/torchtitan/models/llama3/model/model.py\n",
       "init_weights: &init_weights !partial:.init_weights:init_weights_by_regex@init_weights\n",
       "    # Note: Yaml treats single and double quotes differently WRT escapes. Use single\n",
       "    # quotes for regex expressions, wihc prevents Yaml from interpreting escapes.\n",
       "    # For a literal ' use ''\n",
       "    regex_list:\n",
       "        - [ 'norm', \"pass\" ]\n",
       "        - [ 'bias', \"zeros\" ]\n",
       "        - [ 'embedding\\.weight', \"init_embeddings\" ]\n",
       "        - [ 'up_proj|query_linear|key_linear|value_linear', \"trunc_normal_magic\" ]\n",
       "        - [ 'gate_proj|down_proj|output_linear', \"trunc_normal\" ]\n",
       "        - [ 'output_decoder', \"init_output_layer\" ]\n",
       "    init_f_map:\n",
       "        pass: !partial:.init_weights:init_pass\n",
       "        zeros: !partial:torch.nn.init:zeros_ []\n",
       "        init_embeddings: !partial:.llama_init:init_embeddings []\n",
       "        trunc_normal_magic: !partial:.llama_init:trunc_normal_magic []\n",
       "        trunc_normal: !partial:.llama_init:trunc_normal\n",
       "            std: !call:.llama_init:llama_std [ !var \"num_hidden_layers\" ]\n",
       "        init_output_layer: !partial:.llama_init:init_output_layer { d_model: !var \"hidden_size\" }\n",
       "    # Print how each param is being initialized.\n",
       "    debug: False\n",
       "\n",
       "model_factory: &model_factory !factory:.causal_lm:CasualLM@model_factory\n",
       "    loss_fn: *loss_fn\n",
       "    input_encoder: *input_encoder\n",
       "    output_decoder: *output_decoder\n",
       "    layer_stack: *layer_stack\n",
       "    init_weights: *init_weights\n",
       "\n",
       "model_code_generator: &model_code_generator !meta:forgather.codegen:generate_code@model_code_generator\n",
       "    searchpath: \"/home/dinalt/ai_assets/forgather/modelsrc/templates\"\n",
       "    template_name: \"hf_causal.py\"\n",
       "    name_policy: \"named\"\n",
       "    obj: *model_factory\n",
       "    # Template args\n",
       "    model_type: \"forgather-dynamic-causal-dynllama\"\n",
       "\n",
       "model_code_writer: &model_code_writer !singleton:forgather.ml.construct:write_file@model_code_writer\n",
       "    data: *model_code_generator\n",
       "    output_file: \"./output_models/tiny_llama/dynllama.py\"\n",
       "    return_value: \"Model constructor generated by Forgather 1.0\"    \n",
       "\n",
       "model_config: &model_config !singleton:./output_models/tiny_llama/dynllama.py:DynamicCausalLMConfig@model_config\n",
       "    submodule_searchpath: *model_submodule_searchpath\n",
       "    # Set auto-map for custom model; this ensures that the source code stays with the model.\n",
       "    auto_map:\n",
       "        AutoConfig: \"dynllama.DynamicCausalLMConfig\"\n",
       "        AutoModel: \"dynllama.DynamicCasualLM\"\n",
       "    # Get the vocab-size from the tokenizer definition.\n",
       "    vocab_size: !singleton:len [ *tokenizer ]\n",
       "    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]\n",
       "    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]\n",
       "    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]\n",
       "    # Add dependency on code generator\n",
       "    code_generator: *model_code_writer\n",
       "    hidden_size: 512\n",
       "    num_attention_heads: 8\n",
       "    # Default to MHA when null\n",
       "    num_kv_heads: null\n",
       "    d_head: 64 # Must be hidden_size // num_attention_heads\n",
       "    num_hidden_layers: 6\n",
       "    max_sequence_length: !singleton:getattr\n",
       "        - *tokenizer\n",
       "        - \"model_max_length\"\n",
       "    dim_feedforward: 2048\n",
       "    rope_theta: 10000.0\n",
       "    embedding_dropout: 0.0\n",
       "    rms_norm_eps: 1.0e-06\n",
       "    layer_dropout: 0.0\n",
       "    residual_dropout: 0.0\n",
       "    attention_dropout: 0.0\n",
       "    activation_dropout: 0.0\n",
       "    enable_activation_checkpoint: False\n",
       "    checkpoint_stride: 1\n",
       "    \n",
       "    # Tiny Llama overrides\n",
       "    hidden_size: 256\n",
       "    dim_feedforward: 1024\n",
       "    num_attention_heads: 2\n",
       "    num_hidden_layers: 4\n",
       "    d_head: 128 # Must be hidden_size // num_attention_heads\n",
       "\n",
       "# **Model Factory**\n",
       "\n",
       "pretrained_model: &pretrained_model !partial:./output_models/tiny_llama/dynllama.py:DynamicCasualLM@pretrained_model\n",
       "    args:\n",
       "        - *model_config\n",
       "    kwargs:\n",
       "        submodule_searchpath: *model_submodule_searchpath\n",
       "        <<: *model_constructor_args\n",
       "\n",
       "model: &model !partial:forgather.ml.construct:dependency_list@model\n",
       "    - !factory:call [ *pretrained_model ]\n",
       "    - !singleton:forgather.ml.construct:copy_package_files\n",
       "        - \"./output_models/tiny_llama\"\n",
       "        - *model_config\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "# Name: TinyStories Abridged\n",
       "# Define: Abridged to 10% of original size; Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.\n",
       "# Source: https://arxiv.org/abs/2305.07759\n",
       "# Train Dataset: \"roneneldan/TinyStories\" : \"train\"\n",
       "# Eval Dataset: \"roneneldan/TinyStories\" : \"validation\"\n",
       "\n",
       "# **Source Datasets**\n",
       "\n",
       "train_source_dataset: &train_source_dataset !singleton:datasets:load_dataset@train_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "eval_source_dataset: &eval_source_dataset !singleton:datasets:load_dataset@eval_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "# **Dataset Splits**\n",
       "\n",
       "train_dataset_split: &train_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"train\"\n",
       "\n",
       "eval_dataset_split: &eval_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"validation\"\n",
       "\n",
       "# **Preprocess Dataset Args**\n",
       "\n",
       "preprocess_args: &preprocess_args\n",
       "    truncation: True\n",
       "\n",
       "# **Preprocessed Datasets**\n",
       "\n",
       "train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset\n",
       "    dataset: *train_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 0.1\n",
       "    desc: \"Tokenizing train\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset\n",
       "    dataset: *eval_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 500\n",
       "    desc: \"Tokenizing validation split\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM\n",
       "    tokenizer: *tokenizer\n",
       "    return_tensors: pt\n",
       "\n",
       "    # Tiny Llama\n",
       "    truncation: True\n",
       "    max_length: 512\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/tiny_llama/runs/log_2025-08-06T09-01-34\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       "experiment_info: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-08-06T09:01:34\"\n",
       "    name: \"Tiny Llama\"\n",
       "    description: \"A demo of training a tiny llama model from scratch\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.7.1', 'transformers': '4.51.3', 'accelerate': '1.7.0'}\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       "# The model will be given the following prompts for text-gen at regular intervals.\n",
       "testprompts: &testprompts !list:@testprompts\n",
       "    # Test prompts from \"https://arxiv.org/abs/2305.07759\"\n",
       "    - \"Alice was so tired when she got back home so she went\"\n",
       "    - \"Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was\"\n",
       "    - \"Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \\\"Look, Lily. A rainbow has\"\n",
       "    - \"Jack wanted to read a book, so he went to\"\n",
       "    - \"\\\"Can cows fly?\\\" Alice asked her mother.\"\n",
       "    - \"\\\"What do birds like to eat?\\\" Tom asked his mother.\"\n",
       "    - \"\\\"What language do they speak in France?\\\" Tom asked his mother.\"\n",
       "    - \"If I throw a ball up in the air, eventually it will\"\n",
       "    - \"It was winter and cold outside so his mother told him, \\\"You should\"\n",
       "    - \"Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked\"\n",
       "    - \"Jack told Mary, \\\"If you give me your banana, I'll give you my apple.\\\" Mary gave Jack her Banana, so\"\n",
       "    - \"On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to\"\n",
       "    - \"Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that\"\n",
       "    - \"Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \\\"I want to go to the park\\\". Lily says\"\n",
       "    - \"Jack's mother was not home, and his father was at home. When Jack came home, he said hello to\"\n",
       "    - \"Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says\"\n",
       "    - \"Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They\"\n",
       "    - \"Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door,\"\n",
       "\n",
       "# Conservative text-generation parameters.\n",
       "generation_config: &generation_config !dict:@generation_config\n",
       "    identity: generation_config\n",
       "    do_sample: True\n",
       "    top_k: 20\n",
       "    top_p: 0.9\n",
       "    temperature: 0.7\n",
       "    repitition_penalty: 1.15\n",
       "\n",
       "trainer_callbacks: &trainer_callbacks !list:@trainer_callbacks\n",
       "    # Log all training output to JSON\n",
       "    - !singleton:forgather.ml.json_logger:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    - !singleton:forgather.ml.tb_logger:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "    - !singleton:forgather.ml.textgen_callback:TextgenCallback\n",
       "        summary_writer: *summary_writer\n",
       "        prompts: *testprompts\n",
       "        generation_config: *generation_config\n",
       "        max_new_tokens: 40\n",
       "        generation_steps: 1000\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       "optimizer: &optimizer !partial:torch:optim.AdamW\n",
       "    lr: 1.0e-3\n",
       "\n",
       "############# LR Scheduler #############\n",
       "\n",
       "# https://arxiv.org/html/2503.02844v1\n",
       "lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler\n",
       "    warmup_steps: 500\n",
       "    cooldown_steps: 50000\n",
       "    constant_lr: 1.0e-4\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: forgather.ml.trainer.Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       "trainer_args: &trainer_args\n",
       "    # Minimal Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    output_dir: \"./output_models/tiny_llama\"\n",
       "    logging_dir: \"./output_models/tiny_llama/runs/log_2025-08-06T09-01-34\"\n",
       "    logging_steps: 500\n",
       "    per_device_train_batch_size: 16\n",
       "    per_device_eval_batch_size: 32\n",
       "    num_train_epochs: 1\n",
       "    # Base Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    overwrite_output_dir: True\n",
       "    eval_steps: 100\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"no\"\n",
       "    logging_strategy: \"steps\"\n",
       "\n",
       "    # Tiny Llama Project Overrides\n",
       "    seed: 42\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 64\n",
       "    logging_steps: 100\n",
       "    eval_steps: 500\n",
       "    num_train_epochs: 1\n",
       "    dataloader_num_workers: 1\n",
       "    # RoPE embeddings are complex tensors and safetensors can't handle these.\n",
       "    save_safetensors: False\n",
       "\n",
       "\n",
       "model_preprocessor: &model_preprocessor !partial:call [ *model ]\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       "trainer: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    model_init: *model_preprocessor\n",
       "    args: !singleton:forgather.ml.trainer_types:TrainingArguments@trainer_args\n",
       "        <<: *trainer_args\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Tiny Llama\"\n",
       "    config_description: \"A demo of training a tiny llama model from scratch\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/ai_assets/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "    output_dir: \"./output_models/tiny_llama\"\n",
       "    model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "    logging_dir: \"./output_models/tiny_llama/runs/log_2025-08-06T09-01-34\"\n",
       "    create_new_model: \"True\"\n",
       "    save_model: \"True\"\n",
       "    train: \"True\"\n",
       "    eval: \"False\"\n",
       "    nproc_per_node: 1\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_save: True\n",
       "    do_train: True\n",
       "    do_eval: False\n",
       "    # Init distributed envrionment before initializing anyting which depends on it.\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "    pp_config: !var \"pp_config\"\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_config(config_template=\"\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bcb135-37fe-4f61-80e7-dc8d71c08294",
   "metadata": {},
   "source": [
    "## Load Project\n",
    "\n",
    "Load the default configuraiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1f9b45-db49-475f-8c73-b47d0e9ab08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "# Load the default project, which is \"train_tiny_llama.yaml\"\n",
    "proj = Project()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15abe5-957f-43ff-af18-0c8725fe961e",
   "metadata": {},
   "source": [
    "## Start Tensorboard\n",
    "\n",
    "This project has been configured to log training to Tensorboard (TB). To watch the model's training progress with TB, run the following command, which will generate a CLI command to start the TB server. Then run the command from a shell.\n",
    "\n",
    "Tensorboard can be started from a terminal like this:\n",
    "\n",
    "```bash\n",
    "# By default, Tensorboard bind only to localhost. To bind to all interfaces, add --bind_all\n",
    "tensorboard --logdir \"/path/to/model/log/directory\" [--bind_all]\n",
    "```\n",
    "\n",
    "You can use the CLI to launch TB for you, where it will automatically determine the path to the log directory:\n",
    "\n",
    "```bash\n",
    "# --all : Watch all output model directories, otherwise just the one for the current configuration.\n",
    "# -- : Any arguments after '--' are passed directly to tensorboard, for example \"--bind_all\"\n",
    "cd PROJECT_DIR\n",
    "cfcli.py tb [--all] [-- <tensorboard-args>]\n",
    "```\n",
    "\n",
    "When TB starts, it should provide the URL to access it. e.g.\n",
    "\n",
    "```\n",
    "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b3946-1faa-4224-afe8-58233a57e41c",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "You have a few options for training the mode.\n",
    "\n",
    "1. Run it directly from the notebook. This should work find with this example, although for projects using multiple GPUs, you will want to use one of the other options. To train from the notebook, just run the following cell.\n",
    "2. You can generate a training script and run it from the shell. To do so, run the cell with \"generate_trainingscript(),\" then run the generated shell script from a terminal.\n",
    "3. You can use the Forgather CLI.\n",
    "\n",
    "```bash\n",
    "# Open a shell in thie project's directory, then run this command:\n",
    "cd PROJECT_DIR\n",
    "forgather train\n",
    "\n",
    "# See forgather --help for more details.\n",
    "```\n",
    "\n",
    "Once training starts, switch to Tensorboard in your browser. One of the first things you will want to do is enable automatic refresh. To do so, click the gear in the upper-right corner and check \"Reload Data.\"\n",
    "\n",
    "Once training has started, take a look at the \"Text\" tab. You will see that we have automatically logged the preprocessed configuraiton as well as having dumped the primary training artifacts.\n",
    "\n",
    "Next, switch to the \"Scalars\" tab. You will see a plot of train and evaluation loss which will automatically update every 30 seconds. If you are not familiar with Tensorboard, now would be a good time to play with the UI elements to see how they work.\n",
    "\n",
    "When training completes, the model will be automatically saved to the output directory (\"./output_models/default_model\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7f6eb9-33ec-489c-b66e-faf3e3f0bdee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Script Started *****\n",
      "config_name: Tiny Llama\n",
      "config_description: A demo of training a tiny llama model from scratch\n",
      "output_dir: ./output_models/tiny_llama\n",
      "logging_dir: ./output_models/tiny_llama/runs/log_2025-06-28T02-03-41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43f7f489e1d4f5c9731d0957bec85b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 212,000\n",
      "total_train_samples: 212,000\n",
      "per_device_train_batch_size: 32\n",
      "actual_per_device_batch_size: 32\n",
      "total_train_batch_size: 32\n",
      "max_steps: 6,625\n",
      "total_parameters: 5.2M\n",
      "trainable_parameters: 5.2M\n",
      "model:\n",
      "DynamicCasualLM(\n",
      "  (causal_lm): CausalRpeLM(\n",
      "    loss_fn=CausalLoss()\n",
      "    (input_encoder): InputEncoder(\n",
      "      d_model=256, vocab_size=2000\n",
      "      (dropout): Identity()\n",
      "      (embedding): Embedding(2000, 256)\n",
      "    )\n",
      "    (output_decoder): Linear(in_features=256, out_features=2000, bias=False)\n",
      "    (layer_stack): LayerStack(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x PreLNLayer(\n",
      "          (feedforward): GLUFeedforwardLayer(\n",
      "            d_model=256, d_feedforward=1024\n",
      "            (up_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
      "            (gate_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
      "            (down_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "            (activation): SiLU()\n",
      "            (dropout): Identity()\n",
      "          )\n",
      "          (attention): CausalRpeAttn(\n",
      "            d_model=256, num_heads=2, num_kv_heads=2\n",
      "            (query_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (key_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (value_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (output_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "          )\n",
      "          (norm1): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Identity()\n",
      "          (residual_dropout): Identity()\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (relative_pe): RotaryPE()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "2025-06-28 02:03:50          100  0.02  train-loss: 6.62886   learning-rate: 2.00e-04\n",
      "2025-06-28 02:03:52          200  0.03  train-loss: 4.64612   learning-rate: 4.00e-04\n",
      "2025-06-28 02:03:54          300  0.05  train-loss: 3.77274   learning-rate: 6.00e-04\n",
      "2025-06-28 02:03:56          400  0.06  train-loss: 3.48798   learning-rate: 8.00e-04\n",
      "2025-06-28 02:03:58          500  0.08  train-loss: 3.20003   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eec45a2fb2b4f9c820d360c483289a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:03:58          500  0.08  eval-loss:  3.05297   \n",
      "2025-06-28 02:04:02          600  0.09  train-loss: 3.02968   learning-rate: 1.00e-03\n",
      "2025-06-28 02:04:04          700  0.11  train-loss: 2.85014   learning-rate: 1.00e-03\n",
      "2025-06-28 02:04:06          800  0.12  train-loss: 2.7901    learning-rate: 1.00e-03\n",
      "2025-06-28 02:04:08          900  0.14  train-loss: 2.63153   learning-rate: 1.00e-03\n",
      "2025-06-28 02:04:10        1,000  0.15  train-loss: 2.45764   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674da25abcf9413697ebfed4dde332a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:04:10        1,000  0.15  eval-loss:  2.35597   \n",
      "2025-06-28 02:04:14        1,100  0.17  train-loss: 2.44147   learning-rate: 1.00e-03\n",
      "2025-06-28 02:04:16        1,200  0.18  train-loss: 2.40005   learning-rate: 1.00e-03\n",
      "2025-06-28 02:04:18        1,300  0.2   train-loss: 2.35602   learning-rate: 9.99e-04\n",
      "2025-06-28 02:04:20        1,400  0.21  train-loss: 2.32656   learning-rate: 9.99e-04\n",
      "2025-06-28 02:04:22        1,500  0.23  train-loss: 2.2948    learning-rate: 9.99e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f12f83f76ac44c3b7517ade3d263ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:04:22        1,500  0.23  eval-loss:  2.07656   \n",
      "2025-06-28 02:04:24        1,600  0.24  train-loss: 2.26106   learning-rate: 9.99e-04\n",
      "2025-06-28 02:04:26        1,700  0.26  train-loss: 2.21194   learning-rate: 9.99e-04\n",
      "2025-06-28 02:04:28        1,800  0.27  train-loss: 2.15007   learning-rate: 9.98e-04\n",
      "2025-06-28 02:04:30        1,900  0.29  train-loss: 2.14023   learning-rate: 9.98e-04\n",
      "2025-06-28 02:04:32        2,000  0.3   train-loss: 2.18864   learning-rate: 9.98e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f153073090d4d53a081c210f8c8a989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:04:32        2,000  0.3   eval-loss:  1.97517   \n",
      "2025-06-28 02:04:36        2,100  0.32  train-loss: 2.14276   learning-rate: 9.98e-04\n",
      "2025-06-28 02:04:38        2,200  0.33  train-loss: 2.08952   learning-rate: 9.97e-04\n",
      "2025-06-28 02:04:40        2,300  0.35  train-loss: 2.05412   learning-rate: 9.97e-04\n",
      "2025-06-28 02:04:42        2,400  0.36  train-loss: 2.1148    learning-rate: 9.97e-04\n",
      "2025-06-28 02:04:44        2,500  0.38  train-loss: 2.08728   learning-rate: 9.96e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c75bbeaeeea420da416aac7fc75fad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:04:44        2,500  0.38  eval-loss:  1.88513   \n",
      "2025-06-28 02:04:46        2,600  0.39  train-loss: 2.08949   learning-rate: 9.96e-04\n",
      "2025-06-28 02:04:48        2,700  0.41  train-loss: 2.02368   learning-rate: 9.96e-04\n",
      "2025-06-28 02:04:50        2,800  0.42  train-loss: 2.07516   learning-rate: 9.95e-04\n",
      "2025-06-28 02:04:52        2,900  0.44  train-loss: 1.9916    learning-rate: 9.95e-04\n",
      "2025-06-28 02:04:53        3,000  0.45  train-loss: 1.88199   learning-rate: 9.94e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314f7af23fb84557aad1705b2f81fb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:04:54        3,000  0.45  eval-loss:  1.78102   \n",
      "2025-06-28 02:04:58        3,100  0.47  train-loss: 1.97969   learning-rate: 9.94e-04\n",
      "2025-06-28 02:04:59        3,200  0.48  train-loss: 2.06389   learning-rate: 9.94e-04\n",
      "2025-06-28 02:05:01        3,300  0.5   train-loss: 1.95108   learning-rate: 9.93e-04\n",
      "2025-06-28 02:05:03        3,400  0.51  train-loss: 1.87839   learning-rate: 9.93e-04\n",
      "2025-06-28 02:05:06        3,500  0.53  train-loss: 1.90196   learning-rate: 9.92e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ab6f24e4464e37ba2d57c25bda4f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:05:06        3,500  0.53  eval-loss:  1.75074   \n",
      "2025-06-28 02:05:08        3,600  0.54  train-loss: 1.9777    learning-rate: 9.91e-04\n",
      "2025-06-28 02:05:10        3,700  0.56  train-loss: 1.89099   learning-rate: 9.91e-04\n",
      "2025-06-28 02:05:11        3,800  0.57  train-loss: 1.89447   learning-rate: 9.90e-04\n",
      "2025-06-28 02:05:14        3,900  0.59  train-loss: 1.94667   learning-rate: 9.90e-04\n",
      "2025-06-28 02:05:15        4,000  0.6   train-loss: 1.9822    learning-rate: 9.89e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87f71cedc864a5097f8aaffb66bdad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:05:16        4,000  0.6   eval-loss:  1.72674   \n",
      "2025-06-28 02:05:20        4,100  0.62  train-loss: 1.87949   learning-rate: 9.89e-04\n",
      "2025-06-28 02:05:22        4,200  0.63  train-loss: 1.8505    learning-rate: 9.88e-04\n",
      "2025-06-28 02:05:24        4,300  0.65  train-loss: 1.88693   learning-rate: 9.87e-04\n",
      "2025-06-28 02:05:26        4,400  0.66  train-loss: 1.94372   learning-rate: 9.87e-04\n",
      "2025-06-28 02:05:28        4,500  0.68  train-loss: 1.8756    learning-rate: 9.86e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf4dfd9c6304999bc6e01c2aea00cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:05:28        4,500  0.68  eval-loss:  1.70758   \n",
      "2025-06-28 02:05:30        4,600  0.69  train-loss: 1.80415   learning-rate: 9.85e-04\n",
      "2025-06-28 02:05:32        4,700  0.71  train-loss: 1.81274   learning-rate: 9.84e-04\n",
      "2025-06-28 02:05:34        4,800  0.72  train-loss: 1.84652   learning-rate: 9.84e-04\n",
      "2025-06-28 02:05:36        4,900  0.74  train-loss: 1.84945   learning-rate: 9.83e-04\n",
      "2025-06-28 02:05:38        5,000  0.75  train-loss: 1.85878   learning-rate: 9.82e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c42ea2d178141f9865ffce53eda13a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:05:38        5,000  0.75  eval-loss:  1.69991   \n",
      "2025-06-28 02:05:42        5,100  0.77  train-loss: 1.85652   learning-rate: 9.81e-04\n",
      "2025-06-28 02:05:44        5,200  0.78  train-loss: 1.76746   learning-rate: 9.81e-04\n",
      "2025-06-28 02:05:46        5,300  0.8   train-loss: 1.75397   learning-rate: 9.80e-04\n",
      "2025-06-28 02:05:48        5,400  0.82  train-loss: 1.80928   learning-rate: 9.79e-04\n",
      "2025-06-28 02:05:50        5,500  0.83  train-loss: 1.78033   learning-rate: 9.78e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8c1859dfe94ab5bae0cb0201e81286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:05:50        5,500  0.83  eval-loss:  1.64513   \n",
      "2025-06-28 02:05:52        5,600  0.85  train-loss: 1.82898   learning-rate: 9.77e-04\n",
      "2025-06-28 02:05:54        5,700  0.86  train-loss: 1.84984   learning-rate: 9.76e-04\n",
      "2025-06-28 02:05:56        5,800  0.88  train-loss: 1.80894   learning-rate: 9.75e-04\n",
      "2025-06-28 02:05:58        5,900  0.89  train-loss: 1.82552   learning-rate: 9.74e-04\n",
      "2025-06-28 02:06:00        6,000  0.91  train-loss: 1.73571   learning-rate: 9.73e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e439bd376f4177830a60d5948bb306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:06:00        6,000  0.91  eval-loss:  1.65477   \n",
      "2025-06-28 02:06:04        6,100  0.92  train-loss: 1.71854   learning-rate: 9.72e-04\n",
      "2025-06-28 02:06:06        6,200  0.94  train-loss: 1.78193   learning-rate: 9.71e-04\n",
      "2025-06-28 02:06:08        6,300  0.95  train-loss: 1.75276   learning-rate: 9.70e-04\n",
      "2025-06-28 02:06:10        6,400  0.97  train-loss: 1.74012   learning-rate: 9.69e-04\n",
      "2025-06-28 02:06:12        6,500  0.98  train-loss: 1.76615   learning-rate: 9.68e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cfe85e6cf7472f8efbd745b54b433f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 02:06:12        6,500  0.98  eval-loss:  1.60841   \n",
      "2025-06-28 02:06:14        6,600  1.0   train-loss: 1.70679   learning-rate: 9.67e-04\n",
      "2025-06-28 02:06:15        6,625  1.0   train_runtime: 146.8 train_samples: 212,000 step: 6,625 train_samples_per_second: 1.444e+03 train_steps_per_second: 45.14 epoch: 1.0 \n",
      "**** Training Completed *****\n",
      "{'train_runtime': 146.76516938209534, 'train_samples': 212000, 'step': 6625, 'train_samples_per_second': 1444.484, 'train_steps_per_second': 45.14, 'epoch': 1.0}\n",
      "Model saved to: ./output_models/tiny_llama\n"
     ]
    }
   ],
   "source": [
    "# Train model in notebook.\n",
    "\n",
    "# Construct the default target, \"main,\" which is a training script.\n",
    "training_script = proj()\n",
    "\n",
    "# Start training the model.\n",
    "training_script.run()\n",
    "\n",
    "# Release resources\n",
    "training_script = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a33f36-246d-4627-aebf-5e8127f4b692",
   "metadata": {},
   "source": [
    "## Load Trained Model\n",
    "\n",
    "You can use the regular HF APIs to load the saved model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4846ff07-14ed-434e-aadd-b04063e6c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgather.project import Project\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GenerationConfig, StoppingCriteria\n",
    "import torch\n",
    "\n",
    "model_path = \"./output_models/tiny_llama\"\n",
    "\n",
    "# Set device to run inference on\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d72d1-1e8f-4433-abe0-a0d9aaa252c0",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "This loop will use the newly trained model to generate text, seeded with the above prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee6f8d5-c548-4b94-88d2-fc92f118746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was so tired when she got back home so she went [START] to her room and found her bedroom. She saw her mom and dad sitting on a chair. She had a big smile and a big smile.\n",
      "\n",
      "Alice's dad smiled and said, \"Let's go and play outside. Always remember to be safe and not leave when you're tired.\"\n",
      "\n",
      "So they went to the kitchen and picked up her toys. She picked up her toys and her toys. They started to put them away and put them away.\n",
      "\n",
      "----------------------------------------\n",
      "Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was [START] very happy.\n",
      "\n",
      "One night, Jack and Lily went to the park. They saw a big tree with a hole in it. Jack and Lily were curious. They wanted to see what was inside. They asked their mom if they could go inside and see it.\n",
      "\n",
      "Mom said yes, but they had to be careful. She said that they should never go inside because they might not have a good time. Jack and Lily learned that sometimes things that seem different things that can be dangerous\n",
      "----------------------------------------\n",
      "Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has [START] rainbows and blue eyes. Can we go outside and see it?\"\n",
      "\n",
      "They went outside to play and saw a big tree. They decided to climb the tree and play near the tree. They ran around and laughed. Suddenly, Jack's big brother came out of the tree. He saw Jack and Lily playing with a ball. She threw the ball too high and fell down. She hurt her knee and had to go to the hospital.\n",
      "\n",
      "Jack and Lily were\n",
      "----------------------------------------\n",
      "Jack wanted to read a book, so he went to [START] the kitchen. He saw a big, red cake and wanted to eat it. He asked his mom, \"Can I have some of the cake, please?\"\n",
      "\n",
      "His mom said, \"No, Jack. That is my favorite food. You have to eat first.\" Jack was sad, but he knew he could not.\n",
      "\n",
      "Then, his mom had an idea. She told Jack that he could have some cake. So Jack put his plate on the table and it looked very tast\n",
      "----------------------------------------\n",
      "\"Can cows fly?\" Alice asked her mother. [START] \n",
      "\"Yes, you want to go to the park. It will be fun!\" said her mother.\n",
      "\n",
      "Alice and her mother went to the park. They saw a big tree. They were excited. They saw a big tree with a hole in it.\n",
      "\n",
      "They decided to climb the tree. They climbed up the tree and looked down. They saw a big, scary tree. They heard a loud noise. Alice was a little scared.\n",
      "\n",
      "They ran\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(model, tokenizer, prompts, gen_config, max_new_tokens, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for prompt in prompts:\n",
    "            tokenizer_outputs = tokenizer(\n",
    "                [prompt],\n",
    "                truncation=False,\n",
    "                return_length=True,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "        \n",
    "            input_ids = tokenizer_outputs[\"input_ids\"].to(device)\n",
    "            attention_mask = tokenizer_outputs[\"attention_mask\"].to(device)\n",
    "            use_cache = getattr(model, \"_supports_cache_class\", False)\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                generation_config=gen_config,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=use_cache,\n",
    "                past_key_values=None,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "    \n",
    "            output_text = tokenizer.decode(\n",
    "                outputs.sequences[0],\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "            yield prompt + \" [START] \" + output_text[len(prompt) + 1 :]\n",
    "\n",
    "prompts = [\n",
    "    'Alice was so tired when she got back home so she went',\n",
    "    'Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was',\n",
    "    'Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \"Look, Lily. A rainbow has',\n",
    "    'Jack wanted to read a book, so he went to',\n",
    "    '\"Can cows fly?\" Alice asked her mother.',\n",
    "]\n",
    "\n",
    "gen_config = GenerationConfig(\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    do_sample=True,\n",
    "    top_k=20,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    repitition_penalty=1.15,\n",
    ")\n",
    "\n",
    "for s in generate_text(model, tokenizer, prompts, gen_config, 100, \"cuda:0\"):\n",
    "    print(s)\n",
    "    print(f\"{'-' * 40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b87ddc-9fbf-4799-9d3d-c71e06859401",
   "metadata": {},
   "source": [
    "## Train Hugginface LLama Model\n",
    "\n",
    "Next, let's try training a Llama model using the Huggingface implementation.\n",
    "\n",
    "Train the model on the CLI\n",
    "\n",
    "```bash\n",
    "forgather -t train_hf_llama.yaml train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b057e02-fe19-432c-840a-0dad3ec0d50c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Included Templates\n",
       "- [configs/train_hf_llama.yaml](templates/configs/train_hf_llama.yaml)\n",
       "    - [models/tiny_hf_llama.yaml](templates/models/tiny_hf_llama.yaml)\n",
       "        - [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "        - [models/llama.yaml](../../../templatelib/examples/models/llama.yaml)\n",
       "            - [models/causal_lm/from_config.yaml](../../../templatelib/base/models/causal_lm/from_config.yaml)\n",
       "                - [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "                    - [inc/formatting.jinja](../../../templatelib/base/inc/formatting.jinja)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [types/training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "                - [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "                    - [trainers/minimal_trainer.yaml](../../../templatelib/base/trainers/minimal_trainer.yaml)\n",
       "            - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "                - [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "            - [models/causal_lm/load_model.yaml](../../../templatelib/base/models/causal_lm/load_model.yaml)\n",
       "                - [models/causal_lm/from_pretrained.yaml](../../../templatelib/base/models/causal_lm/from_pretrained.yaml)\n",
       "            - [types/training_script/training_script.yaml](../../../templatelib/base/types/training_script/training_script.yaml)\n",
       "                - [types/type.yaml](../../../templatelib/base/types/type.yaml)\n",
       "                    - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "        - [project.logger_config](templates/project.yaml)\n",
       "            - [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "        - [project.model_config](templates/project.yaml)\n",
       "            - [models/tiny_dynamic_llama.yaml](templates/models/tiny_dynamic_llama.yaml)\n",
       "                - [models/dynamic_llama.yaml](../../../templatelib/examples/models/dynamic_llama.yaml)\n",
       "                    - [models/causal_lm/custom_dynamic.yaml](../../../templatelib/base/models/causal_lm/custom_dynamic.yaml)\n",
       "                        - [models/causal_lm/custom.yaml](../../../templatelib/base/models/causal_lm/custom.yaml)\n",
       "        - [project.dataset](templates/project.yaml)\n",
       "            - [datasets/tinystories/tinystories_abridged.yaml](../../../templatelib/examples/datasets/tinystories/tinystories_abridged.yaml)\n",
       "                - [datasets/tinystories/tinystories.yaml](../../../templatelib/examples/datasets/tinystories/tinystories.yaml)\n",
       "                    - [datasets/base_datasets.yaml](../../../templatelib/base/datasets/base_datasets.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'Train with HF LLama implementation for comparison',\n",
       " 'config_name': 'Hugginface Llama',\n",
       " 'create_new_model': 'True',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/forgather/datasets',\n",
       " 'eval': 'False',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': './output_models/hf_llama/runs/log_2025-06-28T02-09-52',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'nproc_per_node': 1,\n",
       " 'output_dir': './output_models/hf_llama',\n",
       " 'project_dir': '.',\n",
       " 'save_model': 'True',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'train': 'True',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "## Output Targets\n",
       "- distributed_env\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "- model_code_generator\n",
       "- model_code_writer\n",
       "- model_config\n",
       "- model\n",
       "- train_source_dataset\n",
       "- eval_source_dataset\n",
       "- train_dataset_split\n",
       "- eval_dataset_split\n",
       "- preprocess_args\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- experiment_info\n",
       "- testprompts\n",
       "- generation_config\n",
       "- trainer_callbacks\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- trainer_args\n",
       "- model_preprocessor\n",
       "- trainer\n",
       "- meta\n",
       "- main\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#            Hugginface Llama            \n",
       "#---------------------------------------\n",
       "# 2025-06-28T02:09:52\n",
       "# Description: Train with HF LLama implementation for comparison\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: hf_llama\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.7.1\n",
       "#     transformers: 4.51.3\n",
       "#     accelerate: 1.7.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"./output_models/hf_llama\"\n",
       "# ns.logging_dir: \"./output_models/hf_llama/runs/log_2025-06-28T02-09-52\"\n",
       "# ns.create_new_model: True\n",
       "# ns.save_model: True\n",
       "# ns.train: True\n",
       "# ns.eval: False\n",
       "# ns.trust_remote_code: False\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       "distributed_env: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "\n",
       "\n",
       "################ Model #################\n",
       "\n",
       "# https://huggingface.co/docs/transformers/en/model_doc/auto\n",
       "model_constructor_args: &model_constructor_args {}\n",
       "\n",
       "# Name: Llama\n",
       "# Description: Llama model\n",
       "\n",
       "# model_def.source = \"\"\n",
       "# model_def.model_config_cls = \"transformers:LlamaConfig\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       "# Load custom tokenizer from sub-project definition\n",
       "tokenizer: &tokenizer !singleton:forgather.ml.construct:load_from_config@tokenizer\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe\"\n",
       "    config_template: \"2k.yaml\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       "# Model config dependencies\n",
       "\n",
       "model_code_generator: &model_code_generator null\n",
       "\n",
       "model_code_writer: &model_code_writer null    \n",
       "\n",
       "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/configuration_llama.py\n",
       "model_config: &model_config !singleton:transformers:LlamaConfig\n",
       "    vocab_size: !singleton:len [ *tokenizer ]\n",
       "    max_position_embeddings: !singleton:getattr [ *tokenizer, 'model_max_length' ]\n",
       "    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]\n",
       "    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]\n",
       "    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]\n",
       "\n",
       "    # Tiny Llama overrides\n",
       "    hidden_size: 256\n",
       "    intermediate_size: 1024\n",
       "    num_attention_heads: 2\n",
       "    num_key_value_heads: 2\n",
       "    num_hidden_layers: 4\n",
       "\n",
       "# **Model Factory**\n",
       "\n",
       "model: &model !lambda:transformers:AutoModelForCausalLM.from_config@model\n",
       "    args:\n",
       "        - *model_config\n",
       "    kwargs:\n",
       "        <<: *model_constructor_args\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "# Name: TinyStories Abridged\n",
       "# Define: Abridged to 10% of original size; Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.\n",
       "# Source: https://arxiv.org/abs/2305.07759\n",
       "# Train Dataset: \"roneneldan/TinyStories\" : \"train\"\n",
       "# Eval Dataset: \"roneneldan/TinyStories\" : \"validation\"\n",
       "\n",
       "# **Source Datasets**\n",
       "\n",
       "train_source_dataset: &train_source_dataset !singleton:datasets:load_dataset@train_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "eval_source_dataset: &eval_source_dataset !singleton:datasets:load_dataset@eval_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "# **Dataset Splits**\n",
       "\n",
       "train_dataset_split: &train_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"train\"\n",
       "\n",
       "eval_dataset_split: &eval_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"validation\"\n",
       "\n",
       "# **Preprocess Dataset Args**\n",
       "\n",
       "preprocess_args: &preprocess_args\n",
       "    truncation: True\n",
       "\n",
       "# **Preprocessed Datasets**\n",
       "\n",
       "train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset\n",
       "    dataset: *train_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 0.1\n",
       "    desc: \"Tokenizing train\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset\n",
       "    dataset: *eval_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 500\n",
       "    desc: \"Tokenizing validation split\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM\n",
       "    tokenizer: *tokenizer\n",
       "    return_tensors: pt\n",
       "\n",
       "    # Tiny Llama\n",
       "    truncation: True\n",
       "    max_length: 512\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/hf_llama/runs/log_2025-06-28T02-09-52\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       "experiment_info: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-06-28T02:09:52\"\n",
       "    name: \"Hugginface Llama\"\n",
       "    description: \"Train with HF LLama implementation for comparison\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.7.1', 'transformers': '4.51.3', 'accelerate': '1.7.0'}\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       "# The model will be given the following prompts for text-gen at regular intervals.\n",
       "testprompts: &testprompts !list:@testprompts\n",
       "    # Test prompts from \"https://arxiv.org/abs/2305.07759\"\n",
       "    - \"Alice was so tired when she got back home so she went\"\n",
       "    - \"Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was\"\n",
       "    - \"Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \\\"Look, Lily. A rainbow has\"\n",
       "    - \"Jack wanted to read a book, so he went to\"\n",
       "    - \"\\\"Can cows fly?\\\" Alice asked her mother.\"\n",
       "    - \"\\\"What do birds like to eat?\\\" Tom asked his mother.\"\n",
       "    - \"\\\"What language do they speak in France?\\\" Tom asked his mother.\"\n",
       "    - \"If I throw a ball up in the air, eventually it will\"\n",
       "    - \"It was winter and cold outside so his mother told him, \\\"You should\"\n",
       "    - \"Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked\"\n",
       "    - \"Jack told Mary, \\\"If you give me your banana, I'll give you my apple.\\\" Mary gave Jack her Banana, so\"\n",
       "    - \"On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to\"\n",
       "    - \"Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that\"\n",
       "    - \"Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \\\"I want to go to the park\\\". Lily says\"\n",
       "    - \"Jack's mother was not home, and his father was at home. When Jack came home, he said hello to\"\n",
       "    - \"Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says\"\n",
       "    - \"Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They\"\n",
       "    - \"Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door,\"\n",
       "\n",
       "# Conservative text-generation parameters.\n",
       "generation_config: &generation_config !dict:@generation_config\n",
       "    identity: generation_config\n",
       "    do_sample: True\n",
       "    top_k: 20\n",
       "    top_p: 0.9\n",
       "    temperature: 0.7\n",
       "    repitition_penalty: 1.15\n",
       "\n",
       "trainer_callbacks: &trainer_callbacks !list:@trainer_callbacks\n",
       "    # Log all training output to JSON\n",
       "    - !singleton:forgather.ml.json_logger:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    - !singleton:forgather.ml.tb_logger:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "    - !singleton:forgather.ml.textgen_callback:TextgenCallback\n",
       "        summary_writer: *summary_writer\n",
       "        prompts: *testprompts\n",
       "        generation_config: *generation_config\n",
       "        max_new_tokens: 40\n",
       "        generation_steps: 1000\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       "optimizer: &optimizer !partial:torch:optim.AdamW\n",
       "    lr: 1.0e-3\n",
       "\n",
       "############# LR Scheduler #############\n",
       "\n",
       "# https://arxiv.org/html/2503.02844v1\n",
       "lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler\n",
       "    warmup_steps: 500\n",
       "    cooldown_steps: 50000\n",
       "    constant_lr: 1.0e-4\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: forgather.ml.trainer.Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       "trainer_args: &trainer_args\n",
       "    # Minimal Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    output_dir: \"./output_models/hf_llama\"\n",
       "    logging_dir: \"./output_models/hf_llama/runs/log_2025-06-28T02-09-52\"\n",
       "    logging_steps: 500\n",
       "    per_device_train_batch_size: 16\n",
       "    per_device_eval_batch_size: 32\n",
       "    num_train_epochs: 1\n",
       "    # Base Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    overwrite_output_dir: True\n",
       "    eval_steps: 100\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"no\"\n",
       "    logging_strategy: \"steps\"\n",
       "\n",
       "    # Tiny Llama Project Overrides\n",
       "    seed: 42\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 64\n",
       "    logging_steps: 100\n",
       "    eval_steps: 500\n",
       "    num_train_epochs: 1\n",
       "    dataloader_num_workers: 1\n",
       "    # RoPE embeddings are complex tensors and safetensors can't handle these.\n",
       "    save_safetensors: False\n",
       "\n",
       "\n",
       "model_preprocessor: &model_preprocessor !partial:call [ *model ]\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       "trainer: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    model_init: *model_preprocessor\n",
       "    args: !singleton:forgather.ml.trainer_types:TrainingArguments@trainer_args\n",
       "        <<: *trainer_args\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Hugginface Llama\"\n",
       "    config_description: \"Train with HF LLama implementation for comparison\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/ai_assets/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "    output_dir: \"./output_models/hf_llama\"\n",
       "    model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "    logging_dir: \"./output_models/hf_llama/runs/log_2025-06-28T02-09-52\"\n",
       "    create_new_model: \"True\"\n",
       "    save_model: \"True\"\n",
       "    train: \"True\"\n",
       "    eval: \"False\"\n",
       "    nproc_per_node: 1\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_save: True\n",
       "    do_train: True\n",
       "    do_eval: False\n",
       "    # Init distributed envrionment before initializing anyting which depends on it.\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "    pp_config: !var \"pp_config\"\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_config(config_template=\"train_hf_llama.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e13d5-9597-43a4-956c-f438674b7f56",
   "metadata": {},
   "source": [
    "## Let's See What Happens...\n",
    "\n",
    "...if we replace the post-layer-norm implementation with a pre-layer-norm implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e44fde-65d6-4dce-8ad3-c1196313d4c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Included Templates\n",
       "- [configs/experimental_llama.yaml](templates/configs/experimental_llama.yaml)\n",
       "    - [project.yaml](templates/project.yaml)\n",
       "        - [types/training_script/causal_lm/causal_lm.yaml](../../../templatelib/base/types/training_script/causal_lm/causal_lm.yaml)\n",
       "            - [trainers/trainer.yaml](../../../templatelib/base/trainers/trainer.yaml)\n",
       "                - [trainers/base_trainer.yaml](../../../templatelib/base/trainers/base_trainer.yaml)\n",
       "                    - [trainers/minimal_trainer.yaml](../../../templatelib/base/trainers/minimal_trainer.yaml)\n",
       "            - [callbacks/loggers.yaml](../../../templatelib/base/callbacks/loggers.yaml)\n",
       "                - [callbacks/base_callbacks.yaml](../../../templatelib/base/callbacks/base_callbacks.yaml)\n",
       "            - [models/causal_lm/load_model.yaml](../../../templatelib/base/models/causal_lm/load_model.yaml)\n",
       "                - [models/causal_lm/from_pretrained.yaml](../../../templatelib/base/models/causal_lm/from_pretrained.yaml)\n",
       "                    - [models/base_language_model.yaml](../../../templatelib/base/models/base_language_model.yaml)\n",
       "            - [types/training_script/training_script.yaml](../../../templatelib/base/types/training_script/training_script.yaml)\n",
       "                - [types/type.yaml](../../../templatelib/base/types/type.yaml)\n",
       "                    - [base_directories.yaml](../../../forgather_workspace/base_directories.yaml)\n",
       "            - [inc/formatting.jinja](../../../templatelib/base/inc/formatting.jinja)\n",
       "        - [project.logger_config](templates/project.yaml)\n",
       "            - [prompts/tiny_stories.yaml](../../../templatelib/examples/prompts/tiny_stories.yaml)\n",
       "        - [project.trainer_config](templates/project.yaml)\n",
       "        - [project.model_config](templates/project.yaml)\n",
       "            - [models/tiny_dynamic_llama.yaml](templates/models/tiny_dynamic_llama.yaml)\n",
       "                - [tokenizers/tiny_2k.yaml](../../../templatelib/examples/tokenizers/tiny_2k.yaml)\n",
       "                - [models/dynamic_llama.yaml](../../../templatelib/examples/models/dynamic_llama.yaml)\n",
       "                    - [models/causal_lm/custom_dynamic.yaml](../../../templatelib/base/models/causal_lm/custom_dynamic.yaml)\n",
       "                        - [models/causal_lm/custom.yaml](../../../templatelib/base/models/causal_lm/custom.yaml)\n",
       "        - [project.dataset](templates/project.yaml)\n",
       "            - [datasets/tinystories/tinystories_abridged.yaml](../../../templatelib/examples/datasets/tinystories/tinystories_abridged.yaml)\n",
       "                - [datasets/tinystories/tinystories.yaml](../../../templatelib/examples/datasets/tinystories/tinystories.yaml)\n",
       "                    - [datasets/base_datasets.yaml](../../../templatelib/base/datasets/base_datasets.yaml)\n",
       "    - [experiment.model_config](templates/configs/experimental_llama.yaml)\n",
       "### Config Metadata:\n",
       "\n",
       "```python\n",
       "{'config_class': 'type.training_script.causal_lm',\n",
       " 'config_description': 'Try a Post-Norm Llama model',\n",
       " 'config_name': 'Experimental Llama',\n",
       " 'create_new_model': 'True',\n",
       " 'datasets_dir': '/home/dinalt/ai_assets/forgather/datasets',\n",
       " 'eval': 'False',\n",
       " 'forgather_dir': '/home/dinalt/ai_assets/forgather',\n",
       " 'logging_dir': './output_models/exp_llama/runs/log_2025-06-28T02-10-02',\n",
       " 'model_src_dir': '/home/dinalt/ai_assets/forgather/model_src',\n",
       " 'models_dir': './output_models',\n",
       " 'nproc_per_node': 1,\n",
       " 'output_dir': './output_models/exp_llama',\n",
       " 'project_dir': '.',\n",
       " 'save_model': 'True',\n",
       " 'tokenizers_dir': '/home/dinalt/ai_assets/forgather/tokenizers',\n",
       " 'train': 'True',\n",
       " 'workspace_root': '/home/dinalt/ai_assets/forgather'}\n",
       "\n",
       "```\n",
       "\n",
       "## Modules\n",
       "- [./output_models/exp_llama/dynllama.py](output_models/exp_llama/dynllama.py) : DynamicCausalLMConfig\n",
       "- [./output_models/exp_llama/dynllama.py](output_models/exp_llama/dynllama.py) : DynamicCasualLM\n",
       "## Output Targets\n",
       "- distributed_env\n",
       "- model_constructor_args\n",
       "- tokenizer\n",
       "- model_submodule_searchpath\n",
       "- loss_fn\n",
       "- layer_norm_factory\n",
       "- feedforward_factory\n",
       "- attention_factory\n",
       "- layer_factory\n",
       "- layer_stack\n",
       "- output_decoder\n",
       "- absolute_pe\n",
       "- relative_pe\n",
       "- input_encoder\n",
       "- init_weights\n",
       "- model_factory\n",
       "- model_code_generator\n",
       "- model_code_writer\n",
       "- model_config\n",
       "- pretrained_model\n",
       "- model\n",
       "- train_source_dataset\n",
       "- eval_source_dataset\n",
       "- train_dataset_split\n",
       "- eval_dataset_split\n",
       "- preprocess_args\n",
       "- train_dataset\n",
       "- eval_dataset\n",
       "- data_collator\n",
       "- experiment_info\n",
       "- testprompts\n",
       "- generation_config\n",
       "- trainer_callbacks\n",
       "- optimizer\n",
       "- lr_scheduler\n",
       "- trainer_args\n",
       "- model_preprocessor\n",
       "- trainer\n",
       "- meta\n",
       "- main\n",
       "\n",
       "## Preprocessed Config\n",
       "\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#           Experimental Llama           \n",
       "#---------------------------------------\n",
       "# 2025-06-28T02:10:02\n",
       "# Description: Try a Post-Norm Llama model\n",
       "# Project Dir: /home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "# Model: exp_llama\n",
       "# Hostname: hal9000\n",
       "# Versions:\n",
       "#     python: 3.10.13\n",
       "#     torch: 2.7.1\n",
       "#     transformers: 4.51.3\n",
       "#     accelerate: 1.7.0\n",
       "\n",
       "############# Config Vars ##############\n",
       "\n",
       "# ns.forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "# ns.models_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models\"\n",
       "# ns.project_model_src_dir: \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/model_src\"\n",
       "# ns.tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "# ns.datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "# ns.model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "# ns.output_dir: \"./output_models/exp_llama\"\n",
       "# ns.logging_dir: \"./output_models/exp_llama/runs/log_2025-06-28T02-10-02\"\n",
       "# ns.create_new_model: True\n",
       "# ns.save_model: True\n",
       "# ns.train: True\n",
       "# ns.eval: False\n",
       "# ns.trust_remote_code: False\n",
       "\n",
       "####### Distributed Environment ########\n",
       "\n",
       "distributed_env: &distributed_env !singleton:forgather.ml.distributed:DistributedEnvironment@distributed_env\n",
       "\n",
       "############# Dependencies #############\n",
       "\n",
       "\n",
       "\n",
       "################ Model #################\n",
       "\n",
       "# https://huggingface.co/docs/transformers/en/model_doc/auto\n",
       "model_constructor_args: &model_constructor_args {}\n",
       "\n",
       "# Name: Dynamic Llama\n",
       "# Description: A Llama compatible dynamic model.\n",
       "# model_def.cls = \"DynamicCasualLM\"\n",
       "# model_def.cfg_cls = \"DynamicCausalLMConfig\"\n",
       "# model_def.config_path = \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models/exp_llama/dynllama.py\"\n",
       "# model_def.model_path = \"/home/dinalt/ai_assets/forgather/examples/tutorials/tiny_llama/output_models/exp_llama/dynllama.py\"\n",
       "# model_def.short_name = \"dynllama\"\n",
       "# model_def.model_type = \"forgather-dynamic-causal-dynllama\"\n",
       "# model_def.model_path = \"./output_models/exp_llama/dynllama.py\"\n",
       "# model_def.model_template_searchpath = \"/home/dinalt/ai_assets/forgather/modelsrc/templates\"\n",
       "# model_def.model_template_name = \"hf_causal.py\"\n",
       "# model_def.name_policy = \"named\"\n",
       "\n",
       "# **Tokenizer**\n",
       "\n",
       "# Load custom tokenizer from sub-project definition\n",
       "tokenizer: &tokenizer !singleton:forgather.ml.construct:load_from_config@tokenizer\n",
       "    project_dir: \"/home/dinalt/ai_assets/forgather/examples/tokenizers/tiny_stories_bpe\"\n",
       "    config_template: \"2k.yaml\"\n",
       "\n",
       "# **Model Config**\n",
       "\n",
       "# Model config dependencies\n",
       "\n",
       "model_submodule_searchpath: &model_submodule_searchpath\n",
       "    - \"/home/dinalt/ai_assets/forgather/modelsrc/transformer\"\n",
       "    - \"./output_models/exp_llama\"\n",
       "\n",
       "loss_fn: &loss_fn !singleton:.causal_loss:CausalLoss@loss_fn []\n",
       "\n",
       "layer_norm_factory: &layer_norm_factory !partial:torch.nn:RMSNorm@layer_norm_factory\n",
       "    normalized_shape: !var \"hidden_size\"\n",
       "    eps: !var \"rms_norm_eps\"\n",
       "\n",
       "feedforward_factory: &feedforward_factory !partial:.glu_feedforward:GLUFeedforwardLayer@feedforward_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "    activation_factory: !partial:torch.nn.SiLU []\n",
       "    dropout: !var \"activation_dropout\"\n",
       "\n",
       "attention_factory: &attention_factory !partial:.causal_rpe_attn:CausalRpeAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    num_heads: !var \"num_attention_heads\"\n",
       "    num_kv_heads: !var \"num_kv_heads\"\n",
       "    dropout: !var \"attention_dropout\"\n",
       "    bias: False\n",
       "    sdpa_function: !partial:torch.nn.functional:scaled_dot_product_attention []\n",
       "    apply_pos_emb: !partial:.rotary_embeddings:apply_rotary_emb []\n",
       "\n",
       "# Experiment: Switch from PreLayerNorm to PostLayerNorm\n",
       "layer_factory: &layer_factory !partial:.post_ln_layer:PostLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "    dropout: !var \"layer_dropout\"\n",
       "    residual_dropout: !var \"residual_dropout\"\n",
       "\n",
       "layer_stack: &layer_stack !factory:.layer_stack:LayerStack@layer_stack\n",
       "    layer_factory: *layer_factory\n",
       "    num_hidden_layers: !var \"num_hidden_layers\"\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    # Experiment; disable norm at last step\n",
       "    post_norm_factory: null\n",
       "\n",
       "output_decoder: &output_decoder !factory:torch.nn:Linear@output_decoder\n",
       "    in_features: !var \"hidden_size\"\n",
       "    out_features: !var \"vocab_size\"\n",
       "    bias: False\n",
       "\n",
       "absolute_pe: &absolute_pe null\n",
       "\n",
       "relative_pe: &relative_pe !partial:.rotary_embeddings:RotaryPE@relative_pe\n",
       "    d_head: !var \"d_head\"\n",
       "    max_sequence_length: !var \"max_sequence_length\"\n",
       "    rope_theta: !var \"rope_theta\"\n",
       "\n",
       "input_encoder: &input_encoder !factory:.input_encoder:InputEncoder@input_encoder\n",
       "    d_model: !var \"hidden_size\"\n",
       "    vocab_size: !var \"vocab_size\"\n",
       "    dropout: !var \"embedding_dropout\"\n",
       "    positional_encoder: *absolute_pe\n",
       "\n",
       "init_weights: &init_weights !partial:.init_weights:simple_weight_init@init_weights []\n",
       "\n",
       "model_factory: &model_factory !factory:.causal_rpe_lm:CausalRpeLM@model_factory\n",
       "    loss_fn: *loss_fn\n",
       "    input_encoder: *input_encoder\n",
       "    output_decoder: *output_decoder\n",
       "    layer_stack: *layer_stack\n",
       "    init_weights: *init_weights\n",
       "    relative_pe: *relative_pe\n",
       "\n",
       "model_code_generator: &model_code_generator !meta:forgather.codegen:generate_code@model_code_generator\n",
       "    searchpath: \"/home/dinalt/ai_assets/forgather/modelsrc/templates\"\n",
       "    template_name: \"hf_causal.py\"\n",
       "    name_policy: \"named\"\n",
       "    obj: *model_factory\n",
       "    # Template args\n",
       "    model_type: \"forgather-dynamic-causal-dynllama\"\n",
       "\n",
       "model_code_writer: &model_code_writer !singleton:forgather.ml.construct:write_file@model_code_writer\n",
       "    data: *model_code_generator\n",
       "    output_file: \"./output_models/exp_llama/dynllama.py\"\n",
       "    return_value: \"Model constructor generated by Forgather 1.0\"    \n",
       "\n",
       "model_config: &model_config !singleton:./output_models/exp_llama/dynllama.py:DynamicCausalLMConfig@model_config\n",
       "    submodule_searchpath: *model_submodule_searchpath\n",
       "    # Set auto-map for custom model; this ensures that the source code stays with the model.\n",
       "    auto_map:\n",
       "        AutoConfig: \"dynllama.DynamicCausalLMConfig\"\n",
       "        AutoModel: \"dynllama.DynamicCasualLM\"\n",
       "    # Get the vocab-size from the tokenizer definition.\n",
       "    vocab_size: !singleton:len [ *tokenizer ]\n",
       "    pad_token_id: !singleton:getattr [ *tokenizer, 'pad_token_id' ]\n",
       "    bos_token_id: !singleton:getattr [ *tokenizer, 'bos_token_id' ]\n",
       "    eos_token_id: !singleton:getattr [ *tokenizer, 'eos_token_id' ]\n",
       "    # Add dependency on code generator\n",
       "    code_generator: *model_code_writer\n",
       "    hidden_size: 512\n",
       "    num_attention_heads: 8\n",
       "    # Default to MHA when null\n",
       "    num_kv_heads: null\n",
       "    d_head: 64 # Must be hidden_size // num_attention_heads\n",
       "    num_hidden_layers: 6\n",
       "    max_sequence_length: !singleton:getattr\n",
       "        - *tokenizer\n",
       "        - \"model_max_length\"\n",
       "    dim_feedforward: 2048\n",
       "    rope_theta: 10000.0\n",
       "    embedding_dropout: 0.0\n",
       "    rms_norm_eps: 1.0e-05\n",
       "    layer_dropout: 0.0\n",
       "    residual_dropout: 0.0\n",
       "    attention_dropout: 0.0\n",
       "    activation_dropout: 0.0\n",
       "    \n",
       "    # Tiny Llama overrides\n",
       "    hidden_size: 256\n",
       "    dim_feedforward: 1024\n",
       "    num_attention_heads: 2\n",
       "    num_key_value_heads: 2\n",
       "    num_hidden_layers: 4\n",
       "    d_head: 128 # Must be hidden_size // num_attention_heads\n",
       "\n",
       "# **Model Factory**\n",
       "\n",
       "pretrained_model: &pretrained_model !partial:./output_models/exp_llama/dynllama.py:DynamicCasualLM@pretrained_model\n",
       "    args:\n",
       "        - *model_config\n",
       "    kwargs:\n",
       "        submodule_searchpath: *model_submodule_searchpath\n",
       "        <<: *model_constructor_args\n",
       "\n",
       "model: &model !partial:forgather.ml.construct:dependency_list@model\n",
       "    - !factory:call [ *pretrained_model ]\n",
       "    - !singleton:forgather.ml.construct:copy_package_files\n",
       "        - \"./output_models/exp_llama\"\n",
       "        - *model_config\n",
       "\n",
       "############### Datasets ###############\n",
       "\n",
       "# Name: TinyStories Abridged\n",
       "# Define: Abridged to 10% of original size; Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.\n",
       "# Source: https://arxiv.org/abs/2305.07759\n",
       "# Train Dataset: \"roneneldan/TinyStories\" : \"train\"\n",
       "# Eval Dataset: \"roneneldan/TinyStories\" : \"validation\"\n",
       "\n",
       "# **Source Datasets**\n",
       "\n",
       "train_source_dataset: &train_source_dataset !singleton:datasets:load_dataset@train_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "eval_source_dataset: &eval_source_dataset !singleton:datasets:load_dataset@eval_source_dataset\n",
       "    - \"roneneldan/TinyStories\"\n",
       "\n",
       "# **Dataset Splits**\n",
       "\n",
       "train_dataset_split: &train_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"train\"\n",
       "\n",
       "eval_dataset_split: &eval_dataset_split !singleton:operator:getitem\n",
       "    - *train_source_dataset\n",
       "    - \"validation\"\n",
       "\n",
       "# **Preprocess Dataset Args**\n",
       "\n",
       "preprocess_args: &preprocess_args\n",
       "    truncation: True\n",
       "\n",
       "# **Preprocessed Datasets**\n",
       "\n",
       "train_dataset: &train_dataset !singleton:forgather.ml.datasets:preprocess_dataset@train_dataset\n",
       "    dataset: *train_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 0.1\n",
       "    desc: \"Tokenizing train\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "eval_dataset: &eval_dataset !singleton:forgather.ml.datasets:preprocess_dataset@eval_dataset\n",
       "    dataset: *eval_dataset_split\n",
       "    tokenizer: *tokenizer\n",
       "    select_range: 500\n",
       "    desc: \"Tokenizing validation split\"\n",
       "    fn_kwargs:\n",
       "        <<: *preprocess_args\n",
       "\n",
       "############ Data Collator #############\n",
       "\n",
       "# Data collator for causal model\n",
       "# Batches are dynamically padded to longest sequence\n",
       "# labels are set to input_ids, with pad tokens set to -100\n",
       "data_collator: &data_collator !singleton:forgather.ml.data_collator:DataCollatorForCausalLM@DataCollatorForCausalLM\n",
       "    tokenizer: *tokenizer\n",
       "    return_tensors: pt\n",
       "\n",
       "    # Tiny Llama\n",
       "    truncation: True\n",
       "    max_length: 512\n",
       "\n",
       "########## Trainer Callbacks ###########\n",
       "\n",
       "# **Dependencies**\n",
       "\n",
       "# Experiment tracking: Tensorboard SummaryWriter\n",
       ".define: &summary_writer !singleton:torch.utils.tensorboard:SummaryWriter\n",
       "    - \"./output_models/exp_llama/runs/log_2025-06-28T02-10-02\"\n",
       "\n",
       "# Additional data to record to experiment loggers\n",
       "experiment_info: &experiment_info !dict:@experiment_info\n",
       "    date: \"2025-06-28T02:10:02\"\n",
       "    name: \"Experimental Llama\"\n",
       "    description: \"Try a Post-Norm Llama model\"\n",
       "    config: !var \"pp_config\"\n",
       "    versions: {'python': '3.10.13', 'torch': '2.7.1', 'transformers': '4.51.3', 'accelerate': '1.7.0'}\n",
       "\n",
       "# **Callback List**\n",
       "\n",
       "# The model will be given the following prompts for text-gen at regular intervals.\n",
       "testprompts: &testprompts !list:@testprompts\n",
       "    # Test prompts from \"https://arxiv.org/abs/2305.07759\"\n",
       "    - \"Alice was so tired when she got back home so she went\"\n",
       "    - \"Jack and Lily liked to watch the moon at night. They noticed that the moon changed its shape every night. Sometimes the moon was big and round, and sometimes it was\"\n",
       "    - \"Jack and Lily saw a rainbow after a rainy day.They were amazed by the colors. Jack said, \\\"Look, Lily. A rainbow has\"\n",
       "    - \"Jack wanted to read a book, so he went to\"\n",
       "    - \"\\\"Can cows fly?\\\" Alice asked her mother.\"\n",
       "    - \"\\\"What do birds like to eat?\\\" Tom asked his mother.\"\n",
       "    - \"\\\"What language do they speak in France?\\\" Tom asked his mother.\"\n",
       "    - \"If I throw a ball up in the air, eventually it will\"\n",
       "    - \"It was winter and cold outside so his mother told him, \\\"You should\"\n",
       "    - \"Lily likes cats and dogs. She asked her mom for a dog and her mom said no, so instead she asked\"\n",
       "    - \"Jack told Mary, \\\"If you give me your banana, I'll give you my apple.\\\" Mary gave Jack her Banana, so\"\n",
       "    - \"On weekends Jack went to visit his grandmother whereas on weekdays he would go to school. Last weekend, when Jack was on his way to\"\n",
       "    - \"Lily and Ben were having an argument. Ben said that cake is much better than ice cream and Lily said that\"\n",
       "    - \"Lily and Ben are having an argument. They are trying to decide between the park and the swimming pool. Ben says, \\\"I want to go to the park\\\". Lily says\"\n",
       "    - \"Jack's mother was not home, and his father was at home. When Jack came home, he said hello to\"\n",
       "    - \"Lily doesn't like swimming. When her father wants to take her to the swimming pool, she says\"\n",
       "    - \"Both Ben and Lily wanted cake. Father said that there was only one piece of cake left. They\"\n",
       "    - \"Ben went to visit Lily in her house, but she was not at home. Ben knocked on the door,\"\n",
       "\n",
       "# Conservative text-generation parameters.\n",
       "generation_config: &generation_config !dict:@generation_config\n",
       "    identity: generation_config\n",
       "    do_sample: True\n",
       "    top_k: 20\n",
       "    top_p: 0.9\n",
       "    temperature: 0.7\n",
       "    repitition_penalty: 1.15\n",
       "\n",
       "trainer_callbacks: &trainer_callbacks !list:@trainer_callbacks\n",
       "    # Log all training output to JSON\n",
       "    - !singleton:forgather.ml.json_logger:JsonLogger\n",
       "        <<: *experiment_info\n",
       "    # Log configuration and metrics to Tensorboard file\n",
       "    - !singleton:forgather.ml.tb_logger:TBLogger\n",
       "        args: [ *summary_writer ]\n",
       "        kwargs:\n",
       "            <<: *experiment_info\n",
       "    - !singleton:forgather.ml.textgen_callback:TextgenCallback\n",
       "        summary_writer: *summary_writer\n",
       "        prompts: *testprompts\n",
       "        generation_config: *generation_config\n",
       "        max_new_tokens: 40\n",
       "        generation_steps: 1000\n",
       "\n",
       "############## Optimizer ###############\n",
       "\n",
       "optimizer: &optimizer !partial:torch:optim.AdamW\n",
       "    lr: 1.0e-3\n",
       "\n",
       "############# LR Scheduler #############\n",
       "\n",
       "# https://arxiv.org/html/2503.02844v1\n",
       "lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler\n",
       "    warmup_steps: 500\n",
       "    cooldown_steps: 50000\n",
       "    constant_lr: 1.0e-4\n",
       "\n",
       "############### Trainer ################\n",
       "\n",
       "# Name: forgather.ml.trainer.Trainer\n",
       "# Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
       "\n",
       "# **Trainer Args**\n",
       "\n",
       "trainer_args: &trainer_args\n",
       "    # Minimal Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    output_dir: \"./output_models/exp_llama\"\n",
       "    logging_dir: \"./output_models/exp_llama/runs/log_2025-06-28T02-10-02\"\n",
       "    logging_steps: 500\n",
       "    per_device_train_batch_size: 16\n",
       "    per_device_eval_batch_size: 32\n",
       "    num_train_epochs: 1\n",
       "    # Base Trainer Defaults\n",
       "    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
       "    overwrite_output_dir: True\n",
       "    eval_steps: 100\n",
       "    eval_strategy: \"steps\"\n",
       "    save_strategy: \"no\"\n",
       "    logging_strategy: \"steps\"\n",
       "\n",
       "    # Tiny Llama Project Overrides\n",
       "    seed: 42\n",
       "    per_device_train_batch_size: 32\n",
       "    per_device_eval_batch_size: 64\n",
       "    logging_steps: 100\n",
       "    eval_steps: 500\n",
       "    num_train_epochs: 1\n",
       "    dataloader_num_workers: 1\n",
       "    # RoPE embeddings are complex tensors and safetensors can't handle these.\n",
       "    save_safetensors: False\n",
       "\n",
       "\n",
       "model_preprocessor: &model_preprocessor !partial:call [ *model ]\n",
       "\n",
       "# **Trainer Constructor**\n",
       "\n",
       "trainer: &trainer !singleton:forgather.ml.trainer:Trainer@trainer\n",
       "    model_init: *model_preprocessor\n",
       "    args: !singleton:forgather.ml.trainer_types:TrainingArguments@trainer_args\n",
       "        <<: *trainer_args\n",
       "    data_collator: *data_collator\n",
       "    train_dataset: *train_dataset\n",
       "    eval_dataset: *eval_dataset\n",
       "    processing_class: *tokenizer\n",
       "    callbacks: *trainer_callbacks\n",
       "    optimizer_factory: *optimizer\n",
       "    lr_scheduler_factory: *lr_scheduler\n",
       "\n",
       "#---------------------------------------\n",
       "#          Configuration Output          \n",
       "#---------------------------------------\n",
       "meta: &meta_output !dict:@meta\n",
       "    config_name: \"Experimental Llama\"\n",
       "    config_description: \"Try a Post-Norm Llama model\"\n",
       "    config_class: \"type.training_script.causal_lm\"\n",
       "    project_dir: \".\"\n",
       "    workspace_root: \"/home/dinalt/ai_assets/forgather\"\n",
       "    forgather_dir: \"/home/dinalt/ai_assets/forgather\"\n",
       "    models_dir: \"./output_models\"\n",
       "    tokenizers_dir: \"/home/dinalt/ai_assets/forgather/tokenizers\"\n",
       "    datasets_dir: \"/home/dinalt/ai_assets/forgather/datasets\"\n",
       "    output_dir: \"./output_models/exp_llama\"\n",
       "    model_src_dir: \"/home/dinalt/ai_assets/forgather/model_src\"\n",
       "    logging_dir: \"./output_models/exp_llama/runs/log_2025-06-28T02-10-02\"\n",
       "    create_new_model: \"True\"\n",
       "    save_model: \"True\"\n",
       "    train: \"True\"\n",
       "    eval: \"False\"\n",
       "    nproc_per_node: 1\n",
       "\n",
       "main: !singleton:forgather.ml.training_script:TrainingScript@training_script\n",
       "    meta: *meta_output\n",
       "    do_save: True\n",
       "    do_train: True\n",
       "    do_eval: False\n",
       "    # Init distributed envrionment before initializing anyting which depends on it.\n",
       "    distributed_env: *distributed_env\n",
       "    trainer: *trainer\n",
       "    pp_config: !var \"pp_config\"\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_config(config_template=\"experimental_llama.yaml\", show_pp_config=True, show_generated_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb6c21-28cd-47e0-ae75-b18e0d500ca6",
   "metadata": {},
   "source": [
    "```bash\n",
    "forgather -t experimental_llama.yaml train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15896a-6afa-47b5-bbf8-017277cc8d9c",
   "metadata": {},
   "source": [
    "## Test Model With the Inference Server\n",
    "\n",
    "There is a simple OpenAI compatible inference server implementation in \"tools/inference_server\"  \n",
    "\n",
    "To host your newly trained model on the inference server:\n",
    "\n",
    "```bash\n",
    "./server.py server_configs/tiny_llama.yaml\n",
    "```\n",
    "\n",
    "From another session, you can perform text completion like this:\n",
    "\n",
    "```bash\n",
    "./client.py client_configs/tiny_llama.yaml --stream --completion \"Once upon a time,\"\n",
    "```\n",
    "\n",
    "The Tiny Llama model, trained on Tiny Stories, will not be very good at interactive chat, but you cat test this with the following command:\n",
    "\n",
    "```bash\n",
    "./client.py client_configs/tiny_llama.yaml --stream --interactive\n",
    "```\n",
    "\n",
    "This server should work with other OpenAI compatible clients as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405bc11-9e44-4a24-8a21-91642c22cf50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
