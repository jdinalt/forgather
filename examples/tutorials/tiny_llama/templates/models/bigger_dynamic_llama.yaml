-- extends 'models/dynamic_llama.yaml'


-- block model_tokenizer
## Replace the default Llama tokenizer with the tiny_2k tokenizer.
    -- include 'tokenizers/tiny_2k.yaml'
<< endblock model_tokenizer


## Make the model much smaller.
-- block model_config
    == super()

    # Bigger Llama overrides
    hidden_size: 512
    intermediate_size: 2048
    num_attention_heads: 4
    d_head: 128 # Must be hidden_size // num_attention_heads
    num_key_value_heads: 4
    num_hidden_layers: 8
<< endblock model_config


