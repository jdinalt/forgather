-- extends "training_script/causal_lm/causal_lm.yaml"

[config_metadata]
    == super()
    ## Overrides
    -- set ns.config_name = "Tiny Llama"
    -- set ns.config_description = "A demo of training a tiny llama model from scratch"
    -- set ns.model_name = "tiny_llama"

    ## The dataset sub-project to use
    -- set ns.dataset_proj = joinpath(ns.forgather_dir, 'examples', 'datasets', 'roneneldan')

    ## The configuration in the dataset sub-project
    -- set ns.dataset_config = "tinystories-abridged.yaml"

    ## Set predefined model project to import
    -- set ns.model_project_dir = joinpath(ns.forgather_dir, "examples", 'models', "llama")
    
    ## Select 4M parameter Llama model
    -- set ns.model_project_config = "4M.yaml"

[datasets_preprocessor_args]
tokenizer_args: &tokenizer_args !dict
    ## Keep sequence length short for faster training
    truncation: True
    max_length: 512

## Pull dataset loader from template definition
[datasets_definition]
.define: &dataset_dict !call:forgather:from_project
    project_dir: "{{ ns.dataset_proj }}"
    config_template: "{{ ns.dataset_config }}"
    targets: [ "train_dataset", "eval_dataset" ] 
    preprocess_args: *tokenizer_args
    tokenizer: *tokenizer

train_dataset: &train_dataset !call:getitem [ *dataset_dict, 'train_dataset' ]
eval_dataset: &eval_dataset !call:getitem [ *dataset_dict, 'eval_dataset' ]

## Import a model from another Forgather project
[construct_new_model]
.define: &model_dict !call:forgather:from_project
    project_dir: "{{ ns.model_project_dir }}"
    config_template: "{{ ns.model_project_config }}"
    targets: [ "pretrained_tokenizer", "model" ] 
    pp_kwargs:
        output_dir: "{{ ns.output_dir }}"
    pp_debug: {{ ns.debug_model_project | default(False) }}
    model_constructor_args: *model_constructor_args

tokenizer: &tokenizer !call:getitem [ *model_dict, 'pretrained_tokenizer' ]
model: &model !call:getitem [ *model_dict, 'model' ]

[trainer_args]
    == super()
    # Tiny Llama Project Overrides
    eval_strategy: "steps"
    save_strategy: "{{ save_strategy | default('steps') }}"
    save_steps: 10000
    # Safetensors can't handle tied parameters/buffers, so fallback to PyTorch format.
    save_safetensors: False
    seed: 42
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 64
    logging_steps: 100
    eval_steps: 500
    num_train_epochs: 1
    dataloader_num_workers: 1

## This includes an inline template, defined below.
## The need to do this relates to Jinja2 disallowing a template from
## directly having more than one parent template. We can get around 
## this limitation by extending in 'project.model_config' and including
## the resulting template.
[trainer_callbacks]
    -- include 'project.logger_config'

## Explicitly set the optimizer.
[optimizer]
optimizer: &optimizer !partial:torch:optim.AdamW
    lr: 1.0e-3

## Override the default LR Scheduler
[lr_scheduler]
# https://arxiv.org/html/2503.02844v1
lr_scheduler: &lr_scheduler !lambda:forgather.ml.optim.infinite_lr_scheduler:InfiniteLRScheduler@lr_scheduler
    warmup_steps: 500
    cooldown_steps: 50000
    constant_lr: 1.0e-6

[datacollator]
    == super()
    # Tiny Llama
    ## Limit maximum sequence length 512 tokens, at the data-collator level.
    truncation: True
    max_length: 512

#-------------------- project.logger_config --------------------
-- extends 'callbacks/loggers.yaml'
## Add experiment loggers to the callbacks list.
## The parent creates a Tensor Board SummaryWriter, which we can use.

[callback_dependencies]
    == super()

    [generation_config]
generation_config: &generation_config !dict:@generation_config
    do_sample: True
    top_k: 20
    temperature: 0.7
    repetition_penalty: 1.15

    [text_gen_callback_args]
text_gen_callback_args: &text_gen_callback_args
    summary_writer: *summary_writer
    prompts: {{ abspath(joinpath(ns.forgather_dir, "prompts/tiny_stories.yaml")) }}
    generation_config: *generation_config
    max_new_tokens: 40
    generation_steps: 2000

[callback_list]
    == super()
    ## This adds a text-generationn sample every 'generation_steps'
    text_gen_callback: !singleton:forgather.ml.trainer.callbacks:TextgenCallback
        <<: *text_gen_callback_args
    # Allow remote control of the training process
    trainer_control: !singleton:forgather.ml.trainer.callbacks:TrainerControlCallback
