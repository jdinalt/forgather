from typing import Callable, Optional
from torch import nn, Tensor, FloatTensor
import torch

class DeepnetLayer(nn.Module):
    """
        DeepNet: Scaling Transformers to 1,000 Layers
        https://arxiv.org/pdf/2203.00555

        This is a post-norm type transformer layer which implements
        "deepnorm" in forward().
    """
    def __init__(
        self,
        *,
        feedforward_factory: Callable,
        attention_factory: Callable,
        norm_factory: Callable,
        dropout: Optional[float] = 0.1,
        residual_dropout: Optional[float] = 0.0,
        alpha=1.0, # See deepnet_alpha()
    ):
        super().__init__()
        self.feedforward = feedforward_factory()
        self.attention = attention_factory()
        self.norm1 = norm_factory()
        self.norm2 = norm_factory()
        if dropout == 0.0:
            self.dropout = nn.Identity()
        else:
            self.dropout = nn.Dropout(dropout)
        # Residual Dropout:A Simple Approach to Improve Transformerâ€™s Data Efficiency
        # https://aclanthology.org/2024.sigul-1.35.pdf
        if residual_dropout == 0.0:
            self.residual_dropout = nn.Identity()
        else:
            self.residual_dropout = nn.Dropout(residual_dropout)
        self.alpha = alpha

    def forward(self, x: FloatTensor) -> FloatTensor:
        residual = self.residual_dropout(x)
        x = self.attention(x)
        x = self.dropout(x)
        x = self.norm1(residual * self.alpha + x)
        residual = self.residual_dropout(x)
        x = self.feedforward(x)
        x = self.dropout(x)
        x = self.norm2(residual * self.alpha + x)
        return x
    
def deepnet_alpha(n_decoder_layers: int, n_encoder_layers: int, which: str=None):
    """
        Compute deeepnet "alpha", which is passed to DeepnetLayer.
        The residuals are scaled by this value.

        Note: Only tested with Decoder-Only config
    """
    # Alias for consistency with paper
    n = n_encoder_layers
    m = n_decoder_layers

    assert n >= 0 and m >=0
    
    # Decoder only (e.g. GPT)
    if n == 0:
        return (2 * m) ** (1/4)
    # Enocoder Only (e.g. BERT)
    elif m == 0:
        return (2 * n) ** (1/4)
    # Encoder / Decoder
    else:
        match which:
            case "encoder":
                return 0.81 * (n ** 4 * m) ** (1/16)
            case "decoder":
                return (3 * m) ** (1/4)
            case _:
                raise Exception("Which argument must be either encoder or decoder")

def deepnet_beta(n_decoder_layers: int, n_encoder_layers: int, which: str=None):
    """
        Compute deeepnet "beta", which passed as "std" or "gain" when
        initialiizing feedforard, value_projection, and out_projection weights.

        Note: Only tested with Decoder-Only config
    """
    # Alias for consistency with paper
    n = n_encoder_layers
    m = n_decoder_layers

    assert n >= 0 and m >=0
    
    # Decoder only (e.g. GPT)
    if n == 0:
        return (8 * m) ** (-1/4)
    # Enocoder Only (e.g. BERT)
    elif m == 0:
        return (8 * n) ** (-1/4)
    # Encoder / Decoder
    else:
        match which:
            case "encoder":
                return 0.87 * (n ** 4 * m) ** (-1/16)
            case "decoder":
                return (12 * m) ** (-1/4)
            case _:
                raise Exception("Which argument must be either encoder or decoder")