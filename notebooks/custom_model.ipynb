{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5862a8b9-ea0b-4fec-b150-8cbb3218e8bb",
   "metadata": {},
   "source": [
    "# Custom Model Notebook\n",
    "\n",
    "Debug configuration here: [Configuration Notebook](project_config.ipynb)\n",
    "\n",
    "Construct, test, and profile custom model architectures before pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490812e-67b0-4386-b7a4-b60056e339c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This should be a path to a \"model-project\" Loading other project type will not work!\n",
    "projects_directory =  \"/home/dinalt/ai_assets/projects/train/two_layer/model_test\"\n",
    "config_template = \"\"\n",
    "\n",
    "# The name of the model definition template to test (without the 'model' prefix)\n",
    "model_config_template = \"two_layer.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0bf98d-ea43-4043-9570-382b675e0a65",
   "metadata": {},
   "source": [
    "## Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49251715-0b19-4168-980d-44339b486a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "nb.display_model_project_index(projects_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3eb09-6801-4725-954e-9eae5478a40d",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b77923-7807-422d-bec1-e043825538dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..', 'src')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "\n",
    "from pprint import pp, pformat\n",
    "from IPython import display as ds\n",
    "from forgather import Project\n",
    "import forgather.nb.notebooks as nb\n",
    "from forgather.yaml_encoder import to_yaml\n",
    "from forgather.nb.notebooks import get_train_cmdline, make_train_script\n",
    "\n",
    "# Load the project\n",
    "proj = Project(config_template, projects_directory, test_model=\"models/\" + model_config_template)\n",
    "\n",
    "# Show project info\n",
    "md = \"\"\n",
    "md += nb.render_project_readme(proj.project_dir)\n",
    "md += nb.render_meta(proj.meta, \"### Meta Config\\n\")\n",
    "md += nb.render_template_list(proj.meta.find_templates(proj.meta.config_prefix), \"### Available Configurations\\n\")\n",
    "\n",
    "# Only construct the meta object\n",
    "config_meta = proj.config.meta()\n",
    "md += f\"### {config_meta['config_name']}:\\n\\n\"\n",
    "md += nb.render_codeblock(\"python\", pformat(config_meta))\n",
    "md += nb.render_codeblock(\"yaml\", proj.pp_config, \"### Preprocessed Configuration\\n\")\n",
    "md += nb.render_codeblock(\"yaml\", to_yaml(proj.config), \"### Loaded Configuration\\n\")\n",
    "\n",
    "# Show generated model code, if any.\n",
    "generated_code_node = proj.config['generated_model_code']\n",
    "\n",
    "if generated_code_node is not None:\n",
    "    generated_source = generated_code_node()\n",
    "    md += nb.render_codeblock(\"python\", generated_code_node(), \"### Generated Model Code\\n\")\n",
    "display(ds.Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e14a97-d7dd-45f9-bb4c-467b6646763a",
   "metadata": {},
   "source": [
    "## Instantiate New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5fd41-d25c-4fe0-9e20-ccbedad542ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_output = proj()\n",
    "model = main_output['model']\n",
    "tokenizer = main_output['tokenizer']\n",
    "\n",
    "def show_parameters(model):\n",
    "    total_parameters = sum(t.numel() for t in model.parameters())\n",
    "    trainable_parameters = sum(\n",
    "        t.numel() if t.requires_grad else 0 for t in model.parameters()\n",
    "    )\n",
    "    num_params = lambda x: f\"{x/1000000:.1f}M\"\n",
    "    print(\"Total Parameters: \", num_params(total_parameters))\n",
    "    print(\"Trainable Parameters: \", num_params(trainable_parameters))\n",
    "\n",
    "show_parameters(model)\n",
    "pp(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6d6ac-8cd9-42c6-98da-c088f737b858",
   "metadata": {},
   "source": [
    "## Test Forward Method\n",
    "This implements a simple forward and backward pass through the model as to provide a \"kick-test,\" to make sure it does not fall over. If it passes, it does not mean than it is correct, just that it can plausibly be used in a training loop without immediatly rasing an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5fd19d-d234-45df-aed1-959f6ea8b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Verify model does not fall over when given input.\n",
    "def test_model_forward(model, batch_size, seq_len, pad_probability, ignore_label=-100, device=\"cpu\", dtype=None):\n",
    "    if dtype is not None:\n",
    "        model = model.to(dtype)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    opt = torch.optim.AdamW(model.parameters())\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    #input_ids = torch.arange(1, batch_size * seq_len + 1, dtype=torch.long).view(batch_size, seq_len)\n",
    "    input_ids = torch.randint(1, model.config.vocab_size, (batch_size, seq_len), dtype=torch.long, device=device)\n",
    "\n",
    "    # Generate fake padding mask\n",
    "    pad_mask = torch.full(input_ids.shape, pad_probability, dtype=torch.float, device=device).bernoulli().to(dtype=torch.long, device=device)\n",
    "    pad_mask = pad_mask.sort(-1, descending=True)[0]\n",
    "    print(\"mask\\n\", pad_mask)\n",
    "\n",
    "    # Replace pad values with pad_id and ignore_label\n",
    "    labels = input_ids.masked_fill(~pad_mask.to(dtype=torch.bool), ignore_label)\n",
    "    input_ids = input_ids.masked_fill(~pad_mask.to(dtype=torch.bool), model.config.pad_token_id)\n",
    "    print(\"input_ids\\n\", input_ids)\n",
    "    print(\"labels\\n\", labels)\n",
    "\n",
    "    input_ids = input_ids\n",
    "    pad_mask = pad_mask\n",
    "    labels = labels\n",
    "    outputs = model(input_ids=input_ids, attention_mask=pad_mask, labels=labels, return_dict=True)\n",
    "    loss = outputs[\"loss\"]\n",
    "    logits = outputs[\"logits\"]\n",
    "    print(\"logits.shape:\", logits.shape)\n",
    "    print(\"loss:\", loss)\n",
    "    \n",
    "    # Make sure backward pass works.\n",
    "    print(\"Computing loss.backward()...\")\n",
    "    loss.backward()\n",
    "\n",
    "    print(\"Unused Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is None:\n",
    "            print(name)\n",
    "\n",
    "    print(\"Performing optimizertor step...\")\n",
    "    opt.step()\n",
    "    print(\"Done! Congratulations, your model passed the kick-test!\")\n",
    "\n",
    "test_model_forward(model, batch_size=2, seq_len=7, pad_probability=0.9, device=\"cpu\", dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f2567-ca9c-4d1a-ac02-09189a25b1df",
   "metadata": {},
   "source": [
    "## Torch Compile [optional]\n",
    "Apply torch-compile to the model.\n",
    "\n",
    "When used, the first pass through model forward will take much longer than normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13628bfc-80ca-419b-8daf-3670cef6d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f3d83-dc6a-4a8a-9ef6-cc18e84aeec1",
   "metadata": {},
   "source": [
    "## Code Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e257021-c14a-4327-bf46-3745732f2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "# https://pytorch.org/docs/stable/torch_cuda_memory.html\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "def print_prof_averages(prof, sort_by, row_limit):\n",
    "        for i in sort_by:\n",
    "            print(prof.key_averages().table(sort_by=i, row_limit=row_limit))\n",
    "\n",
    "def profile_train(model, batch_size, seq_len, use_cpu=True, dtype=None, row_limit=10):\n",
    "    if use_cpu:\n",
    "        device = \"cpu\"\n",
    "        prof_activity = ProfilerActivity.CPU\n",
    "        sort_by = [ \"cpu_time_total\" ]\n",
    "    else:\n",
    "        device = \"cuda\"\n",
    "        prof_activity = ProfilerActivity.CUDA\n",
    "        sort_by = [ \"cpu_time_total\", \"cuda_time_total\" ]\n",
    "\n",
    "    model = model.to(device=device, dtype=dtype)\n",
    "    model.train()\n",
    "    \n",
    "    input_ids = torch.randint(1, model.config.vocab_size, (batch_size, seq_len), dtype=torch.long, device=device)\n",
    "    labels = input_ids\n",
    "    print(\"Running Forward Pass\")\n",
    "    with profile(activities=[prof_activity], record_shapes=True, profile_memory=True) as prof:\n",
    "        with record_function(\"model_forward\"):\n",
    "            loss, logits = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "    print_prof_averages(prof, sort_by, row_limit)\n",
    "\n",
    "    print(\"Running Backward Pass\")\n",
    "    with profile(activities=[prof_activity], record_shapes=True) as prof:\n",
    "        with record_function(\"loss_backward\"):\n",
    "            loss.backward()\n",
    "    print_prof_averages(prof, sort_by, row_limit)\n",
    "\n",
    "def profile_inference(model, batch_size, seq_len, use_cpu=True, dtype=None, row_limit=10):\n",
    "    if use_cpu:\n",
    "        device = \"cpu\"\n",
    "        prof_activity = ProfilerActivity.CPU\n",
    "        sort_by = [ \"cpu_time_total\" ]\n",
    "    else:\n",
    "        device = \"cuda\"\n",
    "        prof_activity = ProfilerActivity.CUDA\n",
    "        sort_by = [ \"cpu_time_total\", \"cuda_time_total\" ]\n",
    "        torch.cuda.memory._record_memory_history()\n",
    "\n",
    "    model = model.to(device=device, dtype=dtype)\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = torch.randint(1, model.config.vocab_size, (batch_size, seq_len), dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with profile(activities=[prof_activity], record_shapes=True, profile_memory=True) as prof:\n",
    "            with record_function(\"model_forward\"):\n",
    "                loss = model(input_ids=input_ids)\n",
    "    \n",
    "    print_prof_averages(prof, sort_by, row_limit)\n",
    "    if not use_cpu:\n",
    "        torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9bd6a6-7d4a-42ab-acc0-f03d91d10fb9",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd920b-a4b1-400d-b6ef-84524355c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_train(model, 16, 512, use_cpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83812b-091d-4ed3-a020-edee5b928ae3",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1608d10-f566-4de3-827e-11bc153e6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_train(model, 16, 512, use_cpu=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
