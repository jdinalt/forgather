{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173c6fe4-0561-4294-9fca-10eb3ad26c2c",
   "metadata": {},
   "source": [
    "# Forgather\n",
    "\n",
    "A notebook for experimenting with Forgather's syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576f49ed-6e78-4a86-ae9f-494d2a41fa07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Forgather Syntax Reference\n",
       "\n",
       "Forgather defines a domain-specific language for the dynamic construciton of Python objects using a combination of Jinja2, YAML, and a few extensions.\n",
       "\n",
       "This guide will focus on the extensions to these languages. For details on YAML and Jinja2, see:\n",
       "\n",
       "- [Jinja2 Template Designer Documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/)\n",
       "- [YAML 1.1](https://yaml.org/spec/1.1/)\n",
       "- [PyYAML Documentation](https://pyyaml.org/wiki/PyYAMLDocumentation)\n",
       "\n",
       "## Jinja2 Extensions\n",
       "---\n",
       "### The Preprocessor\n",
       "\n",
       "There is a custom Jinja2 preprocessor which implemnts an extended version of Jinja2's [Line Statements](https://jinja.palletsprojects.com/en/3.1.x/templates/#line-statements). These are implemented via regex substition, where the match is converted to normal Jinja syntax.\n",
       "\n",
       "\n",
       "- \\#\\# : Line Comment\n",
       "- \\-\\- : Line Statement\n",
       "- << : Line Statement w/ left-trim\n",
       "- \\>> : Line Statement w/ right-trim\n",
       "- == : Print Command\n",
       "- '=>' : Print Command w/ right-trim\n",
       "\n",
       "Example Input:\n",
       "\n",
       "```jinja2\n",
       "## If 'do_loop' is True, then output a list of numbers.\n",
       "-- if do_loop:\n",
       "    -- for i in range(how_many): ## Loop 'how_many' times.\n",
       "        == '- ' + i|string\n",
       "    -- endfor\n",
       "<< endif\n",
       "```\n",
       "\n",
       "Is translated to:\n",
       "\n",
       "```jinja2\n",
       "{# If 'do_loop' is True, then output a list of numbers. #}\n",
       "{% if do_loop: %}\n",
       "{% for i in range(how_many): %}\n",
       "{{ '- ' + i|string }}\n",
       "{% endfor %}\n",
       "{%- endif %}\n",
       "```\n",
       "\n",
       "Output, when passed: do_loop=True, how_many=3\n",
       "```yaml\n",
       "- 0\n",
       "- 1\n",
       "- 2\n",
       "\n",
       "```\n",
       "\n",
       "\n",
       "Normal Jinja2 syntax works just fine too. I just find that the normal syntax is visually difficult to parse (without syntax-highlighting) and is awkward to type.\n",
       "\n",
       "More Formally\n",
       "\n",
       "```python\n",
       "line_comment = r'(.*)\\s+#{2,}.*'\n",
       "line_statement = r'\\s*(--|<<|>>|==|=>)\\s(.*)'\n",
       "\n",
       "Substitutions:\n",
       "{\n",
       "    '--': r\"{% \" + re_match[2] + r\" %}\n",
       "    '<<': r\"{%- \" + re_match[2] + r\" %}\"\n",
       "    '>>': r\"{% \" + re_match[2] + r\" -%}\"\n",
       "    '==': r\"{{ \" + re_match[2] + r\" }}\"\n",
       "    '=>': r\"{{ \" + re_match[2] + r\" -}}\"\n",
       "}\n",
       "```\n",
       "\n",
       "---\n",
       "### Jinja2 Globals\n",
       "\n",
       "A number of globals have been introduced to the Jinja2 environment to assist with pre-processing.\n",
       "\n",
       "- isotime() : Returns ISO formatted local-time, with 1-second resolution (\"%Y-%m-%dT%H:%M:%S\")\n",
       "- utcisotime() : As with isotime(), but UTC time.\n",
       "- filetime(): Generates a local-time string suitable to be concatenated with a file-name. (\"%Y-%m-%dT%H-%M-%S\")\n",
       "- utcfiletime() : As filetime(), but in UTC time.\n",
       "- now() : Get datetime.datetime.now()\n",
       "- utcnow() : Get datetime.datetime.utcnow()\n",
       "- joinpath(*names) : Join a list of file-path segments via os.path.join()\n",
       "- normpath(path) : Normalize a file path; os.path.normpath()\n",
       "- abspath(path) : Convert path to absolute path; os.path.abspath()\n",
       "- relpath(path) : Convert a path to a relative path; os.path.relpath()\n",
       "- repr(obj) : Get Python representation of object; repr()\n",
       "- modname_from_path(module_name) : Given a module file path, return the module name\n",
       "- user_home_dir() : Return absolute path of user's home directory  \n",
       "- getcwd() : Get the current working directory\n",
       "- forgather_config_dir() : Get the platform-specific config directory for Forgather.\n",
       "\n",
       "The following functions from https://pypi.org/project/platformdirs/\n",
       "- user_data_dir()\n",
       "- user_cache_dir()\n",
       "- user_config_dir()\n",
       "- site_data_dir()\n",
       "- site_config_dir()\n",
       "\n",
       "---\n",
       "### Custom File Loader\n",
       "\n",
       "A custom loader, derived from the FileSystemLoader, is defined. This loader has a syntax for splitting a single loaded template file into multiple sub-templates.\n",
       "\n",
       "The primary use-case for this syntax is [template inheritance](https://jinja.palletsprojects.com/en/3.1.x/templates/#template-inheritance), which disallows multiple-inheritance. If you inherit from a template and include a template which is derived from another, Jinja2 does not allow you to direclty override blocks from the included template. You can get around this by creating another template, which overrides the desired blocks, and is included by the top-level template.\n",
       "\n",
       "Normally, this would require creating another template file, but who needs that!? That's much more difficult to work with.\n",
       "\n",
       "```jinja2\n",
       "## This is the main template\n",
       "-- extends 'base_template.jinja'\n",
       "\n",
       "## Override block 'foo' from 'base_template.jinja'\n",
       "-- block foo\n",
       "    -- include 'foo.bar' ## Include the sub-template\n",
       "-- endblock\n",
       "\n",
       "\n",
       "##--------------------- foo.bar ---------------------\n",
       "## This is a sub-template named 'foo.bar'\n",
       "-- extends 'some_other_base_template.jinja'\n",
       "\n",
       "## Override block 'bar' from 'some_other_base_template.jinja'\n",
       "-- block bar\n",
       "    ## ... stuff\n",
       "-- endblock\n",
       "```\n",
       "\n",
       "More formally, the syntax for splitting a document is:\n",
       "\n",
       "```python\n",
       "split_on = r\"\\n#\\s*-{3,}\\s*([\\w./]+)\\s*-{3,}\\n\"\n",
       "```\n",
       "\n",
       "Note: You can't split a template defined via a Python string, as this bypasses the Loader; only file templates may be split like this.\n",
       "\n",
       "---\n",
       "## YAML\n",
       "\n",
       "### Dot-Name Elision\n",
       "YAML does not have a way of defining an object, without also constructing it. This can be inconvienient, as it may not be known ahead of time where the first use of an object will be and YAML requires that the defition occur at this point.\n",
       "\n",
       "To work around this, if the root-node is a mapping, we delete all keys containing strings starting with a dot. Once the object has been defined, YAML does not care if we delete the original definition/instance. My convention is to use \".define\", but any name, starting with a dot, will work.\n",
       "\n",
       "By convention, the primary output object of such a mapping is named \"main\"\n",
       "\n",
       "```yaml\n",
       "# Define points\n",
       ".define: &pt1 { x: 0, y: 0 }\n",
       ".define: &pt2 { x: 5, y: 0 }\n",
       ".define: &pt3 { x: 0, y: 5 }\n",
       "\n",
       "main:\n",
       "    # A list of lines, each defined by a pair of points.\n",
       "    - [ *pt1, *pt2 ]\n",
       "    - [ *pt2, *pt3 ]\n",
       "    - [ *pt3, *pt1 ]\n",
       "```\n",
       "\n",
       "Constructed graph...\n",
       "\n",
       "```python\n",
       "graph()\n",
       "\n",
       "{'main': [[{'x': 0, 'y': 0}, {'x': 5, 'y': 0}],\n",
       "          [{'x': 5, 'y': 0}, {'x': 0, 'y': 5}],\n",
       "          [{'x': 0, 'y': 5}, {'x': 0, 'y': 0}]]}\n",
       "```\n",
       "\n",
       "While not apparent from the representation, the points in the lines are not copies, they are all references to the original three points from the definition. There are only three point objects present in the graph!\n",
       "\n",
       "---\n",
       "### YAML Types\n",
       "\n",
       "Of the standard YAML 1.1 types, only those which can be implicilty (without specifying the tag) are supported\n",
       "\n",
       "YAML 1.1 Tag : Python Type / Examples\n",
       "- !!null : None\n",
       "    - null\n",
       "- !!bool : bool\n",
       "    - True\n",
       "    - False\n",
       "- !!int : int\n",
       "    - 2\n",
       "    - -6\n",
       "- !!float : float\n",
       "    - 2.0\n",
       "    - 1.2e-4\n",
       "- !!str : str\n",
       "    - \"Hello\"\n",
       "    -  world\n",
       "- !!seq : list\n",
       "    - \\[ 1, 2, 3 \\] \n",
       "- !!map : dict\n",
       "    - { x: 1, y: 12 }\n",
       "\n",
       "The following standard types are presently unsupported:\n",
       "- !!binary\n",
       "- !!timestamp\n",
       "- !!omap, !!pairs\n",
       "- !!set -- TODO: Implement me!\n",
       "\n",
       "---\n",
       "Complex types are instead supported through Forgather specific tags:\n",
       "\n",
       "#### !tuple : Named Tuple\n",
       "\n",
       "Syntax: !tuple\\[:@name\\] \\<sequence\\>\n",
       "\n",
       "Construct a named Python tuple from a YAML sequence\n",
       "\n",
       "```yaml\n",
       "!tuple:@my_tuple [ 1, 2, 3 ]\n",
       "```\n",
       "\n",
       "```python\n",
       "graph()\n",
       "(1, 2, 3)\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "#### !list : Named List\n",
       "\n",
       "Syntax: !list\\[:@name\\] \\<sequence\\>\n",
       "\n",
       "Construct a named Python list from a YAML sequence\n",
       "\n",
       "```yaml\n",
       "!list:@my_list [ 1, 2, 3 ]\n",
       "\n",
       "```\n",
       "\n",
       "```python\n",
       "graph()\n",
       "[1, 2, 3]\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "#### !dict : Named Dictionary\n",
       "\n",
       "Syntax: !dict\\[:@\\<name\\>\\] \\<mapping\\>\n",
       "\n",
       "Construct a named Python dict from a YAML mapping\n",
       "\n",
       "```yaml\n",
       "!dict:@my_dict\n",
       "    foo: 1\n",
       "    bar: 2\n",
       "    baz: 3\n",
       "```\n",
       "\n",
       "```python\n",
       "graph()\n",
       "{'foo': 1, 'bar': 2, 'baz': 3}\n",
       "```\n",
       "\n",
       "---\n",
       "#### !var\n",
       "\n",
       "Syntax: !var \"\\<var-name\\>\" | { name: \\<var-name\\>, default: \\<default-value\\> }\n",
       "\n",
       "This declares a global variable, which can be substituted anywhere in the graph.\n",
       "\n",
       "```yaml\n",
       "document = \"\"\"\n",
       "point: !dict\n",
       "    x: !var \"x\" # Define a variable named 'x'\n",
       "    y: !var # Define a variable named 'y' with a default value of 16\n",
       "        name: y\n",
       "        default: 16\n",
       "\"\"\"\n",
       "```\n",
       "\n",
       "The global context is passed in as the special 'context_vars' argument, a dictionary, when constructng the graph.\n",
       "\n",
       "```python\n",
       "graph.point(context_vars=dict(x=2.0))\n",
       "{'x': 2.0, 'y': 16}\n",
       "```\n",
       "\n",
       "---\n",
       "#### !call\n",
       "\n",
       "Alias: !singleton\n",
       "\n",
       "Synatx: !call:\\<import-spec\\>[@\\<name\\>\\] (\\<sequence\\> | \\<mapping\\> | ({ args: \\<sequence\\>, kwargs: \\<mapping\\> }))\n",
       "\n",
       "This is a callable object with only a single instance; any aliases refers to the same object instance.\n",
       "\n",
       "```yaml\n",
       "# Construct three random ints, all having the same value.\n",
       "- &random_int !call:random:randrange:@random_int [ 1000 ]\n",
       "- *random_int\n",
       "- *random_int\n",
       "```\n",
       "\n",
       "```python\n",
       "graph()\n",
       "\n",
       "[247, 247, 247]\n",
       "```\n",
       "\n",
       "The \"SingletonNode\" will generally be your 'go-to' for constructing objects, as the symantics mirror what is expected for YAML anchors and aliases.\n",
       "\n",
       "However, there are a few exceptions...\n",
       "\n",
       "---\n",
       "#### !factory\n",
       "\n",
       "Synatx: !factory:\\<import-spec\\>[@\\<name\\>\\] (\\<sequence\\> | \\<mapping\\> | ({ args: \\<sequence\\>, kwargs: \\<mapping\\> }))\n",
       "\n",
       "This is a callable object which instantiates a new instance everywhere it appears in the graph.\n",
       "\n",
       "```yaml\n",
       "# Construct three random ints, all (probably) having different values.\n",
       "- &random_int !factory:random:randrange [ 1000 ]\n",
       "- *random_int\n",
       "- *random_int\n",
       "```\n",
       "\n",
       "Constructed...\n",
       "```python\n",
       "graph()\n",
       "\n",
       "[99, 366, 116]\n",
       "```\n",
       "\n",
       "---\n",
       "#### !parial\n",
       "\n",
       "Alias (depricated): !lambda\n",
       "\n",
       "Synatx: !parial:\\<import-spec\\>[@\\<name\\>\\] (\\<sequence\\> | \\<mapping\\> | ({ args: \\<sequence\\>, kwargs: \\<mapping\\> }))\n",
       "\n",
       "This constructs a callable object with the same symantics of a Python partial function, where the provided positional and keyword arguments are passed \n",
       "to the function. If additional argmuents are given, the positional-args are appended and the keyword-args are merged.\n",
       "\n",
       "See: https://docs.python.org/3/library/functools.html\n",
       "\n",
       "```yaml\n",
       "!partial:pow [ 2 ]\n",
       "```\n",
       "\n",
       "```python\n",
       "graph(3)\n",
       "8\n",
       "\n",
       "# This is equivalent to:\n",
       "pow(2, 3)\n",
       "```\n",
       "\n",
       "```yaml\n",
       "\n",
       "```\n",
       "\n",
       "---\n",
       "### CallableNodes\n",
       "\n",
       "\n",
       "SingletonNode, FactoryNode, and FactoryNode are all instances of the abstract-base-class \"CallableNode.\" A CallableNode can call any Python function, including class constructors. As Python differentiates between positional args and kwargs, making use of both requires the following syntax:\n",
       "\n",
       "```yaml\n",
       "!singleton:random:sample\n",
       "    args:\n",
       "        - ['red', 'blue']\n",
       "        - 5\n",
       "    kwargs:\n",
       "        counts: [4, 2]\n",
       "```\n",
       "\n",
       "Generally speaking, you can omit the explict 'args' and 'kwargs' names, as long as the syntax is unambigous.\n",
       "\n",
       "```yaml\n",
       "- !singleton:torch:tensor\n",
       "    - 2\n",
       "    - 2\n",
       "- !singleton:random.binomialvariate { n: 1, p: 0.5 }\n",
       "```\n",
       "\n",
       "---\n",
       "#### CallableNode Tag Syntax\n",
       "\n",
       "The part of the YAML tag after the first ':' provides the information required to locate and import the requested Callable.\n",
       "\n",
       "In the simplest case, a [built-in](https://docs.python.org/3/library/functions.html) Python callable just needs to specify the name of the built-in.\n",
       "\n",
       "```yaml\n",
       "!singleton:tuple [ 1, 2, 3 ]\n",
       "```\n",
       "\n",
       "When the Callable is defined in a module, a second ':' is used to seperate the module name from the name within the module.\n",
       "\n",
       "```yaml\n",
       "# See: https://docs.python.org/3/library/operator.html\n",
       "!singleton:operator:mod [ 365, 7 ]\n",
       "```\n",
       "\n",
       "You can also dynamically import a name from a file.\n",
       "\n",
       "```yaml\n",
       "# See: https://docs.python.org/3/library/operator.html\n",
       "!singleton:/path/to/my/pymodule.py:MyClass [ \"foo\", \"bar\" ]\n",
       "```\n",
       "\n",
       "When using a file-import, which itself has relative imports, you will need to specify which directories to search for relative imports:\n",
       "\n",
       "```yaml\n",
       "# See: https://docs.python.org/3/library/operator.html\n",
       "!singleton:/path/to/my/pymodule.py:MyClass \n",
       "    args: [ \"foo\", \"bar\" ]\n",
       "    kwargs:\n",
       "        submodule_searchpath:\n",
       "            - \"/path/to/my/\"\n",
       "            - \"/path/to/shared/modules/\"\n",
       "```\n",
       "The key-word argument \"submodule_searchpath\" has a special meaning in this context and will not passed to the called object. \n",
       "The import system treats all of the directories in the list as a union, thus \"pymodule.py\" can perform a relative import from any of these directories.\n",
       "\n",
       "---\n",
       "#### Named Callable Nodes\n",
       "\n",
       "CallableNodes may be given an explcit name. The name servers the same purpose as the YAML anchor/alias, but PyYaml does not make this information available through the tag API. While feasible to hack PyYaml, doing so is risky. For now, there is a somewhat redundant interface for specitying node names.\n",
       "\n",
       "When a node has been assigned an explicit name, it will always be rendered as an explciit definition in the Python and Yaml code generators, as to improve readability. Doing so is entirely optional.\n",
       "\n",
       "A callable node's tag may end with '@\\<name\\>' which will assign a name to the node.\n",
       "\n",
       "```yaml\n",
       ".define: &foobar !singleton:dict@foobar\n",
       "    foo: 1\n",
       "    bar: 2\n",
       "    baz: |\n",
       "        She sells sea shells\n",
       "        by the sea shore\n",
       "main:\n",
       "    - *foobar\n",
       "```\n",
       "\n",
       "When rendered as Python:\n",
       "\n",
       "```python\n",
       "def construct(\n",
       "):\n",
       "    foobar = {\n",
       "        'foo': 1,\n",
       "        'bar': 2,\n",
       "        'baz': (\n",
       "                'She sells sea shells\\n'\n",
       "                'by the sea shore\\n'\n",
       "            ),\n",
       "    }\n",
       "    \n",
       "    return {\n",
       "        'main': [\n",
       "            foobar,\n",
       "        ],\n",
       "    }\n",
       "```\n",
       "\n",
       "And without the name, the object definition becomes anonymous:\n",
       "\n",
       "```yaml\n",
       ".define: &foobar !singleton:dict\n",
       "...\n",
       "```\n",
       "\n",
       "```python\n",
       "def construct(\n",
       "):\n",
       "    return {\n",
       "        'main': [\n",
       "            {\n",
       "                'foo': 1,\n",
       "                'bar': 2,\n",
       "                'baz': (\n",
       "                        'She sells sea shells\\n'\n",
       "                        'by the sea shore\\n'\n",
       "                    ),\n",
       "            },\n",
       "        ],\n",
       "    }\n",
       "```\n",
       "\n",
       "## Low Level API\n",
       "\n",
       "*Basic Usage*\n",
       "\n",
       "```python\n",
       "# Imports\n",
       "from forgather.config import ConfigEnvironment\n",
       "\n",
       "# Construct a configuration environment\n",
       "env = ConfigEnvironment()\n",
       "\n",
       "# Define a configuration\n",
       "document = \"\"\"\n",
       "!call:torch:randn [ 2, 2 ]\n",
       "\"\"\"\n",
       "\n",
       "# Convert the configuration to a graph\n",
       "graph = env.load_from_string(document).config\n",
       "\n",
       "# Construct the graph\n",
       "graph()\n",
       "tensor([[ 0.0090,  0.0064],\n",
       "        [-1.1638,  0.7066]])\n",
       "```\n",
       "\n",
       "### Create Config Environment\n",
       "\n",
       "A configuration environment is required to construct configurations from YAML/Jinja2 inputs; it conains the infromation needed to located Jina2 templates by name as well as defining the global variables available to templates.\n",
       "\n",
       "```python\n",
       "from forgather.config import ConfigEnvironment\n",
       "...\n",
       "ConfigEnvironment(\n",
       "    searchpath: Iterable[str | os.PathLike] | str | os.PathLike = tuple(\".\"),\n",
       "    pp_environment: Environment = None,\n",
       "    global_vars: Dict[str, Any] = None,\n",
       "):\n",
       "```\n",
       "\n",
       "- searchpath: A list of directories to search for templates in.\n",
       "- pp_environment: Override the default Jinja2 environment class with another implementation.\n",
       "- global_vars: Jinja2 global variables visible to all templates.\n",
       "\n",
       "```python\n",
       "env = ConfigEnvironment(\"./templates/\")\n",
       "```\n",
       "\n",
       "### Define Input\n",
       "\n",
       "A configuration document consists of a combination of YAML and Jinja2 syntax. Typically, a config template would be loaded from a file, but for testing we can create a template directly from a Python string.\n",
       "\n",
       "Both the Jinja2 template and the configuration may accept variables.\n",
       "\n",
       "### Convert Document to Graph\n",
       "\n",
       "```python\n",
       "class ConfigEnvironment:\n",
       "... \n",
       "    def load(\n",
       "        self,\n",
       "        config_path: os.PathLike | str,\n",
       "        /,\n",
       "        **kwargs,\n",
       "    ) -> Config:\n",
       "...\n",
       "    def load_from_string(\n",
       "        self,\n",
       "        config: str,\n",
       "        /,\n",
       "        **kwargs,\n",
       "    ) -> Config:\n",
       "```\n",
       "\n",
       "- load: Load a template from a path; all paths relative to 'searchpaths' are searched for the template.\n",
       "    - config_path: The relative (to searchpaths) template path.\n",
       "    - kwargs: These are passed into the context of the template.\n",
       "- load_from_string: As with load, but a Python string defines the template body; Note that this bypasses the template loader.\n",
       "    - config: A Python string with a Jinja2 template.\n",
       "    - kwargs: Passed to the template.\n",
       "\n",
       "### Materializing the Graph\n",
       "\n",
       "Construct the objects directly from the graph.\n",
       "\n",
       "```python\n",
       "from forgather.latent import Latent\n",
       "...\n",
       "def materialize(obj: Any, /, *args, context_vars: Dict=None, **kwargs):\n",
       "```\n",
       "\n",
       "Construct all object in the graph, returning the constructed root-node.\n",
       "\n",
       "context_vars: The global variables, which will be substitued by '!var' nodes.\n",
       "\n",
       "If the root node is a partial funciton, *args and **kwargs are forwarded to the function.\n",
       "\n",
       "Alternatively, if the root-node is not a dictionary, the following are equivalnt:\n",
       "\n",
       "```python\n",
       "Latent.materialize(graph)\n",
       "\n",
       "# Performs the same action, if the root-node is not a dictionary.\n",
       "graph()\n",
       "```\n",
       "\n",
       "If the root-node is a dictionary...\n",
       "\n",
       "```yaml\n",
       "main: !partial:math:sqrt []\n",
       "```\n",
       "\n",
       "The dictionary elements can be accessed using dot-notation and costructed individually.\n",
       "\n",
       "```python\n",
       "graph.main(16)\n",
       "4.0\n",
       "```\n",
       "\n",
       "### Convert Graph to YAML\n",
       "\n",
       "Convert the node-graph to a YAML representation. This may not be exactly the same as it was in the source template, but should be symantically equivalent.\n",
       "\n",
       "```python\n",
       "from forgather.yaml_encoder import to_yaml\n",
       "...\n",
       "def to_yaml(obj: Any):\n",
       "```\n",
       "\n",
       "## Convert Graph to Python\n",
       "\n",
       "This function takes the output from Latent.to_py(graph) and uses it to render Pyhon code using a Jinja2 template. If the template is unspecified, an implicit \"built-in\" template is used, which will generate appropriate import and dynamic import statements, where required.\n",
       "\n",
       "```python\n",
       "from forgather.codegen import generate_code\n",
       "...\n",
       "def generate_code(\n",
       "    obj,\n",
       "    template_name: Optional[str] = None,\n",
       "    template_str: Optional[str] = None,\n",
       "    searchpath: Optional[List[str | os.PathLike] | str | os.PathLike] = \".\",\n",
       "    env=None,  # jinja2 environment or compatible API\n",
       "    **kwargs,\n",
       ") -> Any:\n",
       "```\n",
       "\n",
       "The default template accepts the following additional kwargs:\n",
       "\n",
       "    factory_name: Optional[str]=\"construct\", ; The name of the generated factory function.\n",
       "    relaxed_kwargs: Optional[bool]=Undefined, ; if defined, **kwargs is added to the arg list\n",
       "    \n",
       "See 'help(generate_code)' for details."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..', 'src')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "\n",
    "from pprint import pp, pformat\n",
    "\n",
    "from IPython import display as ds\n",
    "\n",
    "from forgather.latent import Latent\n",
    "from forgather.config import ConfigEnvironment\n",
    "from forgather.preprocess import PPEnvironment\n",
    "from forgather.codegen import generate_code\n",
    "from forgather.yaml_encoder import to_yaml\n",
    "import forgather.nb.notebooks as nb\n",
    "\n",
    "# Show common syntax definition.\n",
    "with open(os.path.join('..', 'docs', 'syntax.md'), 'r') as f:\n",
    "    display(ds.Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162bace-b2fb-4041-8a75-c8ed1d613c1e",
   "metadata": {},
   "source": [
    "## Trivial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3ccc83-c952-4fc2-b78f-45d6258aa4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3099, -0.7097],\n",
       "        [ 1.0701,  0.2011]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from forgather.config import ConfigEnvironment\n",
    "\n",
    "# Construct a configuration environment\n",
    "env = ConfigEnvironment()\n",
    "\n",
    "# Define a configuration\n",
    "# Here, we construct a 2x2 random tensor.\n",
    "document = \"\"\"\n",
    "!call:torch:randn [ 2, 2 ]\n",
    "\"\"\"\n",
    "\n",
    "# Convert the configuration to a graph\n",
    "graph = env.load_from_string(document).config\n",
    "\n",
    "# Construct the graph\n",
    "graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd881bb-4b85-4f65-835d-31942dcf1a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a function which computes the square-root of its argument.\n",
    "graph = env.load_from_string(\"main: !partial:math:sqrt []\").config\n",
    "graph.main(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ca26e-8977-40a2-896f-2acb1cb83afb",
   "metadata": {},
   "source": [
    "## Complex Example\n",
    "\n",
    "The following template defines a simple (and somewhat incomplete) language model.\n",
    "\n",
    "[./examples/model_def.yaml](./examples/model_def.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c209c6a0-0ace-471a-ae84-25e295fbaf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Configuration Template\n",
       "```yaml\n",
       "-- set ns = namespace()\n",
       "-- from 'examples/formatting.jinja' import h1, h2, h3\n",
       "-- filter trim() ## This removes whitespace before the header.\n",
       "\n",
       "## Jina2 block definitions; we can override these in derived templates.\n",
       "-- block meta_config\n",
       "    -- set ns.model_src = '../model_src/bits/'\n",
       "    -- set ns.config_name = 'Control'\n",
       "    -- set ns.config_description = \"Baseline Control\"\n",
       "    ## Example of variable set by jinja2 template.\n",
       "    -- set ns.vocab_size = 1024\n",
       "<< endblock meta_config\n",
       "\n",
       "\n",
       "-- endfilter\n",
       "-- block header\n",
       "== h1(ns.config_name)\n",
       "# {{ utcisotime() }}\n",
       "# Description: {{ ns.config_description }}\n",
       "# model_src = {{ ns.model_src }}\n",
       "# Current Working Dir: \"{{ getcwd() }}\"\n",
       "# Forgather Config Dir: \"{{ abspath(forgather_config_dir()) }}\"\n",
       "<< endblock header\n",
       "\n",
       "\n",
       "== h2(\"Model Definition\")\n",
       "\n",
       "== h3(\"Layer Norm Factory\")\n",
       "\n",
       "-- block layer_norm_factory\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    - !var \"hidden_size\"\n",
       "<< endblock layer_norm_factory\n",
       "\n",
       "\n",
       "== h3(\"Activation Factory\")\n",
       "\n",
       "-- block activation_factory\n",
       ".define: &activation_factory !partial:torch.nn:ReLU@activation_factory []\n",
       "<< endblock activation_factory\n",
       "\n",
       "\n",
       "== h3(\"Feedforward Factory\")\n",
       "\n",
       "-- block feedforward_factory\n",
       ".define: &feedforward_factory !partial:{{ns.model_src}}feedforward_layer.py:FeedforwardLayer@feedforward_factory\n",
       "    activation_factory: *activation_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "<< endblock feedforward_factory\n",
       "\n",
       "\n",
       "== h3(\"Attention Factory\")\n",
       "\n",
       "-- block attention_factory\n",
       ".define: &attention_factory !partial:{{ns.model_src}}single_head_attn.py:SingleHeadAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "<< endblock attention_factory\n",
       "\n",
       "\n",
       "== h3(\"Layer Factory\")\n",
       "\n",
       "-- block layer_factory\n",
       ".define: &layer_factory !partial:{{ns.model_src}}pre_ln_layer.py:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "<< endblock layer_factory\n",
       "\n",
       "\n",
       "== h3(\"Layer Stack Factory\")\n",
       "\n",
       "-- block layer_stack_factory\n",
       ".define: &layer_stack_factory !factory:{{ns.model_src}}layer_stack.py:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    num_hidden_layers: !var \"n_layers\"\n",
       "<< endblock layer_stack_factory\n",
       "\n",
       "\n",
       "== h3(\"Model\")\n",
       "\n",
       "-- block model\n",
       "## This block is not nearly as factored-out as the others, using inline-definiions.\n",
       ".define: &model !call:{{ns.model_src}}causal_lm.py:CasualLM@model\n",
       "    loss_fn: !factory:{{ns.model_src}}causal_loss.py:CausalLoss\n",
       "    input_encoder: !factory:{{ns.model_src}}input_encoder.py:InputEncoder\n",
       "        d_model: !var \"hidden_size\"\n",
       "        vocab_size: {{ ns.vocab_size }}\n",
       "    output_decoder: !factory:torch.nn:Linear [ {{ ns.vocab_size }}, !var \"hidden_size\" ]\n",
       "    init_weights: !partial:{{ns.model_src}}init_weights.py:simple_weight_init\n",
       "    layer_stack: *layer_stack_factory\n",
       "<< endblock model\n",
       "\n",
       "\n",
       "## Main output\n",
       "main: *model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "template_path = os.path.join('examples', 'model_def.yaml')\n",
    "with open(template_path, 'r') as f:\n",
    "    nb.display_codeblock(\"yaml\", f.read(), \"### Configuration Template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df2615-dde6-4965-9849-1db05fa7051a",
   "metadata": {},
   "source": [
    "## Preprocess the Template\n",
    "\n",
    "This will only run the Jinja preprocessor.\n",
    "\n",
    "This more or less looks like the original template..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3aa9dd-688c-417e-80d7-a527d427d0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Pre Processed Template\n",
       "```yaml\n",
       "#---------------------------------------\n",
       "#                 Control                \n",
       "#---------------------------------------\n",
       "# 2025-06-07T04:25:44\n",
       "# Description: Baseline Control\n",
       "# model_src = ../model_src/bits/\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/notebooks\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "\n",
       "########### Model Definition ###########\n",
       "\n",
       "# **Layer Norm Factory**\n",
       "\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    - !var \"hidden_size\"\n",
       "\n",
       "# **Activation Factory**\n",
       "\n",
       ".define: &activation_factory !partial:torch.nn:ReLU@activation_factory []\n",
       "\n",
       "# **Feedforward Factory**\n",
       "\n",
       ".define: &feedforward_factory !partial:../model_src/bits/feedforward_layer.py:FeedforwardLayer@feedforward_factory\n",
       "    activation_factory: *activation_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "\n",
       "# **Attention Factory**\n",
       "\n",
       ".define: &attention_factory !partial:../model_src/bits/single_head_attn.py:SingleHeadAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "\n",
       "# **Layer Factory**\n",
       "\n",
       ".define: &layer_factory !partial:../model_src/bits/pre_ln_layer.py:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "\n",
       "# **Layer Stack Factory**\n",
       "\n",
       ".define: &layer_stack_factory !factory:../model_src/bits/layer_stack.py:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    num_hidden_layers: !var \"n_layers\"\n",
       "\n",
       "# **Model**\n",
       "\n",
       ".define: &model !call:../model_src/bits/causal_lm.py:CasualLM@model\n",
       "    loss_fn: !factory:../model_src/bits/causal_loss.py:CausalLoss\n",
       "    input_encoder: !factory:../model_src/bits/input_encoder.py:InputEncoder\n",
       "        d_model: !var \"hidden_size\"\n",
       "        vocab_size: 1024\n",
       "    output_decoder: !factory:torch.nn:Linear [ 1024, !var \"hidden_size\" ]\n",
       "    init_weights: !partial:../model_src/bits/init_weights.py:simple_weight_init\n",
       "    layer_stack: *layer_stack_factory\n",
       "\n",
       "main: *model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = ConfigEnvironment()\n",
    "\n",
    "pp_config = env.preprocess(template_path)\n",
    "nb.display_codeblock(\"yaml\", pp_config, \"### Pre Processed Template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427961e-dc0c-4cf0-bb27-7a4dee8743d2",
   "metadata": {},
   "source": [
    "## Construct Model Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6192bc1d-72dd-4b92-998a-35561ea993d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CasualLM(\n",
       "  loss_fn=CausalLoss()\n",
       "  (input_encoder): InputEncoder(\n",
       "    d_model=64, vocab_size=1024\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (embedding): Embedding(1024, 64)\n",
       "  )\n",
       "  (output_decoder): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  (layer_stack): LayerStack(\n",
       "    (layers): ModuleDict(\n",
       "      (0): PreLNLayer(\n",
       "        (feedforward): FeedforwardLayer(\n",
       "          d_model=64, d_feedforward=256\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Identity()\n",
       "          (activation): ReLU()\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "        (attention): SingleHeadAttn(\n",
       "          d_model=64, bias=True\n",
       "          (query_key_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (residual_dropout): Identity()\n",
       "      )\n",
       "      (1): PreLNLayer(\n",
       "        (feedforward): FeedforwardLayer(\n",
       "          d_model=64, d_feedforward=256\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Identity()\n",
       "          (activation): ReLU()\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "        (attention): SingleHeadAttn(\n",
       "          d_model=64, bias=True\n",
       "          (query_key_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (residual_dropout): Identity()\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = env.load(template_path).config\n",
    "\n",
    "model_config =dict(\n",
    "    hidden_size=64,\n",
    "    dim_feedforward=256,\n",
    "    n_layers=2,\n",
    ")\n",
    "\n",
    "graph.main(context_vars=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1fdb25-72c8-43be-b8d2-da3cd91c6b1e",
   "metadata": {},
   "source": [
    "### Override Something\n",
    "\n",
    "For our experiment, we will want to change just one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3f3930-a245-4472-801a-3ff683af52a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Pre Processed Experiment Config\n",
       "```yaml\n",
       "\n",
       "#---------------------------------------\n",
       "#                 No Bias                \n",
       "#---------------------------------------\n",
       "# 2025-06-07T04:26:00\n",
       "# Description: Disabled bias in attention. Does it matter?\n",
       "# model_src = ../model_src/bits/\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/notebooks\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "\n",
       "########### Model Definition ###########\n",
       "\n",
       "# **Layer Norm Factory**\n",
       "\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    - !var \"hidden_size\"\n",
       "\n",
       "# **Activation Factory**\n",
       "\n",
       ".define: &activation_factory !partial:torch.nn:ReLU@activation_factory []\n",
       "\n",
       "# **Feedforward Factory**\n",
       "\n",
       ".define: &feedforward_factory !partial:../model_src/bits/feedforward_layer.py:FeedforwardLayer@feedforward_factory\n",
       "    activation_factory: *activation_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "\n",
       "# **Attention Factory**\n",
       "\n",
       ".define: &attention_factory !partial:../model_src/bits/single_head_attn.py:SingleHeadAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "\n",
       "    # Experiment override.\n",
       "    bias: False\n",
       "\n",
       "# **Layer Factory**\n",
       "\n",
       ".define: &layer_factory !partial:../model_src/bits/pre_ln_layer.py:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "\n",
       "# **Layer Stack Factory**\n",
       "\n",
       ".define: &layer_stack_factory !factory:../model_src/bits/layer_stack.py:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    num_hidden_layers: !var \"n_layers\"\n",
       "\n",
       "# **Model**\n",
       "\n",
       ".define: &model !call:../model_src/bits/causal_lm.py:CasualLM@model\n",
       "    loss_fn: !factory:../model_src/bits/causal_loss.py:CausalLoss\n",
       "    input_encoder: !factory:../model_src/bits/input_encoder.py:InputEncoder\n",
       "        d_model: !var \"hidden_size\"\n",
       "        vocab_size: 1024\n",
       "    output_decoder: !factory:torch.nn:Linear [ 1024, !var \"hidden_size\" ]\n",
       "    init_weights: !partial:../model_src/bits/init_weights.py:simple_weight_init\n",
       "    layer_stack: *layer_stack_factory\n",
       "\n",
       "main: *model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_config = \"\"\"\n",
    "-- extends 'examples/model_def.yaml'\n",
    "\n",
    "-- block meta_config\n",
    "    ## This includes the definition from the parent.\n",
    "    == super()\n",
    "    -- set ns.config_name = \"No Bias\"\n",
    "    -- set ns.config_description = \"Disabled bias in attention. Does it matter?\"\n",
    "<< endblock meta_config\n",
    "\n",
    "-- block attention_factory\n",
    "    == super()\n",
    "\n",
    "    ## And add an override. We are essentially just appending more arguments to the definition.\n",
    "    # Experiment override.\n",
    "    bias: False\n",
    "<< endblock attention_factory\n",
    "\"\"\"\n",
    "\n",
    "output = env.load_from_string(experiment_config)\n",
    "nb.display_codeblock(\"yaml\", output.pp_config, \"#### Pre Processed Experiment Config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a6c5b-40d1-4abd-9420-091266812b07",
   "metadata": {},
   "source": [
    "## Construct Experiment Model\n",
    "\n",
    "This model now has been modified. The bias is now disabled on the attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2522af56-9f94-4692-85bd-d6d9e36a36d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CasualLM(\n",
       "  loss_fn=CausalLoss()\n",
       "  (input_encoder): InputEncoder(\n",
       "    d_model=64, vocab_size=1024\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (embedding): Embedding(1024, 64)\n",
       "  )\n",
       "  (output_decoder): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  (layer_stack): LayerStack(\n",
       "    (layers): ModuleDict(\n",
       "      (0): PreLNLayer(\n",
       "        (feedforward): FeedforwardLayer(\n",
       "          d_model=64, d_feedforward=256\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Identity()\n",
       "          (activation): ReLU()\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "        (attention): SingleHeadAttn(\n",
       "          d_model=64, bias=False\n",
       "          (query_key_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (residual_dropout): Identity()\n",
       "      )\n",
       "      (1): PreLNLayer(\n",
       "        (feedforward): FeedforwardLayer(\n",
       "          d_model=64, d_feedforward=256\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (dropout): Identity()\n",
       "          (activation): ReLU()\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "        (attention): SingleHeadAttn(\n",
       "          d_model=64, bias=False\n",
       "          (query_key_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (residual_dropout): Identity()\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = output.config\n",
    "graph.main(context_vars=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b5cbb-d506-445e-8199-82b3eb81727b",
   "metadata": {},
   "source": [
    "### Implementation Override\n",
    "\n",
    "Unlike most configuration systems, we can not only change numerical parameters, we can alter the implementatinon!\n",
    "\n",
    "Let's replace the simple single-head attention module with a multihead-attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62131420-1e08-4e9d-bf49-cf9c90654ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Pre Processed Experiment Config\n",
       "```yaml\n",
       "\n",
       "#---------------------------------------\n",
       "#           Multihead Attention          \n",
       "#---------------------------------------\n",
       "# 2025-06-07T04:26:03\n",
       "# Description: Swapped singlehead attention for multihead attention.\n",
       "# model_src = ../model_src/bits/\n",
       "# Current Working Dir: \"/home/dinalt/ai_assets/forgather/notebooks\"\n",
       "# Forgather Config Dir: \"/home/dinalt/.config/forgather\"\n",
       "\n",
       "########### Model Definition ###########\n",
       "\n",
       "# **Layer Norm Factory**\n",
       "\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    - !var \"hidden_size\"\n",
       "\n",
       "# **Activation Factory**\n",
       "\n",
       ".define: &activation_factory !partial:torch.nn:ReLU@activation_factory []\n",
       "\n",
       "# **Feedforward Factory**\n",
       "\n",
       ".define: &feedforward_factory !partial:../model_src/bits/feedforward_layer.py:FeedforwardLayer@feedforward_factory\n",
       "    activation_factory: *activation_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    d_feedforward: !var \"dim_feedforward\"\n",
       "\n",
       "# **Attention Factory**\n",
       "\n",
       "# Experiment Override.\n",
       ".define: &attention_factory !partial:../model_src/bits/causal_multihead_attn.py:CausalMultiheadAttn@attention_factory\n",
       "    d_model: !var \"hidden_size\"\n",
       "    num_heads: 2\n",
       "\n",
       "# **Layer Factory**\n",
       "\n",
       ".define: &layer_factory !partial:../model_src/bits/pre_ln_layer.py:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "\n",
       "# **Layer Stack Factory**\n",
       "\n",
       ".define: &layer_stack_factory !factory:../model_src/bits/layer_stack.py:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    num_hidden_layers: !var \"n_layers\"\n",
       "\n",
       "# **Model**\n",
       "\n",
       ".define: &model !call:../model_src/bits/causal_lm.py:CasualLM@model\n",
       "    loss_fn: !factory:../model_src/bits/causal_loss.py:CausalLoss\n",
       "    input_encoder: !factory:../model_src/bits/input_encoder.py:InputEncoder\n",
       "        d_model: !var \"hidden_size\"\n",
       "        vocab_size: 1024\n",
       "    output_decoder: !factory:torch.nn:Linear [ 1024, !var \"hidden_size\" ]\n",
       "    init_weights: !partial:../model_src/bits/init_weights.py:simple_weight_init\n",
       "    layer_stack: *layer_stack_factory\n",
       "\n",
       "main: *model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_config = \"\"\"\n",
    "-- extends 'examples/model_def.yaml'\n",
    "\n",
    "-- block meta_config\n",
    "    == super()\n",
    "    -- set ns.config_name = \"Multihead Attention\"\n",
    "    -- set ns.config_description = \"Swapped singlehead attention for multihead attention.\"\n",
    "    -- set ns.attention_heads = 2\n",
    "<< endblock meta_config\n",
    "\n",
    "\n",
    "-- block attention_factory\n",
    "# Experiment Override.\n",
    ".define: &attention_factory !partial:{{ns.model_src}}causal_multihead_attn.py:CausalMultiheadAttn@attention_factory\n",
    "    d_model: !var \"hidden_size\"\n",
    "    num_heads: {{ ns.attention_heads }}\n",
    "<< endblock attention_factory\n",
    "\"\"\"\n",
    "\n",
    "output = env.load_from_string(experiment_config)\n",
    "nb.display_codeblock(\"yaml\", output.pp_config, \"#### Pre Processed Experiment Config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df356bd-f0cd-4515-aee7-ed8693388517",
   "metadata": {},
   "source": [
    "### Examine the Graph\n",
    "\n",
    "Internally, the processed configuraiton is represented as an abstract node graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0956c00c-5950-45fd-b35c-932b2d141591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Node Graph\n",
       "```python\n",
       "{'main': SingletonNode('../model_src/bits/causal_lm.py:CasualLM', *(), identity='model', **{'loss_fn': FactoryNode('../model_src/bits/causal_loss.py:CausalLoss', *(), identity=140568966666448, **{}), 'input_encoder': FactoryNode('../model_src/bits/input_encoder.py:InputEncoder', *(), identity=140568966671056, **{'d_model': VarNode('hidden_size', identity=140568966666400, value=Undefined), 'vocab_size': 1024}), 'output_decoder': FactoryNode('torch.nn:Linear', *(1024, VarNode('hidden_size', identity=140568966661072, value=Undefined)), identity=140568966660784, **{}), 'init_weights': LambdaNode('../model_src/bits/init_weights.py:simple_weight_init', *(), identity=140568966672304, **{}), 'layer_stack': FactoryNode('../model_src/bits/layer_stack.py:LayerStack', *(), identity='layer_stack_factory', **{'layer_factory': LambdaNode('../model_src/bits/pre_ln_layer.py:PreLNLayer', *(), identity='layer_factory', **{'feedforward_factory': LambdaNode('../model_src/bits/feedforward_layer.py:FeedforwardLayer', *(), identity='feedforward_factory', **{'activation_factory': LambdaNode('torch.nn:ReLU', *(), identity='activation_factory', **{}), 'd_model': VarNode('hidden_size', identity=140568966674800, value=Undefined), 'd_feedforward': VarNode('dim_feedforward', identity=140568966661552, value=Undefined)}), 'attention_factory': LambdaNode('../model_src/bits/single_head_attn.py:SingleHeadAttn', *(), identity='attention_factory', **{'d_model': VarNode('hidden_size', identity=140568966660544, value=Undefined), 'bias': False}), 'norm_factory': LambdaNode('torch.nn:LayerNorm', *(VarNode('hidden_size', identity=140568966661600, value=Undefined),), identity='layer_norm_factory', **{})}), 'post_norm_factory': LambdaNode('torch.nn:LayerNorm', *(VarNode('hidden_size', identity=140568966661600, value=Undefined),), identity='layer_norm_factory', **{}), 'num_hidden_layers': VarNode('n_layers', identity=140568966671536, value=Undefined)})})}\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_codeblock(\"python\", pformat(graph), \"### Node Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c95e8-a4d0-4d8d-b1cb-407a7979e57b",
   "metadata": {},
   "source": [
    "### Convert Graph to YAML\n",
    "\n",
    "Convert the node-graph to a YAML representation. This may not be exactly the same as it was in the source template, but should be symantically equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f0d0c1b-abb3-47bc-aad8-f0d857f15e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       ".define: &activation_factory !lambda:torch.nn:ReLU@activation_factory []\n",
       "\n",
       ".define: &feedforward_factory !lambda:../model_src/bits/feedforward_layer.py:FeedforwardLayer@feedforward_factory\n",
       "    activation_factory: *activation_factory\n",
       "    d_model: !var 'hidden_size'\n",
       "    d_feedforward: !var 'dim_feedforward'\n",
       "\n",
       ".define: &attention_factory !lambda:../model_src/bits/single_head_attn.py:SingleHeadAttn@attention_factory\n",
       "    d_model: !var 'hidden_size'\n",
       "    bias: False\n",
       "\n",
       ".define: &layer_norm_factory !lambda:torch.nn:LayerNorm@layer_norm_factory\n",
       "    - !var 'hidden_size'\n",
       "\n",
       ".define: &layer_factory !lambda:../model_src/bits/pre_ln_layer.py:PreLNLayer@layer_factory\n",
       "    feedforward_factory: *feedforward_factory\n",
       "    attention_factory: *attention_factory\n",
       "    norm_factory: *layer_norm_factory\n",
       "\n",
       ".define: &layer_stack_factory !factory:../model_src/bits/layer_stack.py:LayerStack@layer_stack_factory\n",
       "    layer_factory: *layer_factory\n",
       "    post_norm_factory: *layer_norm_factory\n",
       "    num_hidden_layers: !var 'n_layers'\n",
       "\n",
       ".define: &model !singleton:../model_src/bits/causal_lm.py:CasualLM@model\n",
       "    loss_fn: !factory:../model_src/bits/causal_loss.py:CausalLoss []\n",
       "    input_encoder: !factory:../model_src/bits/input_encoder.py:InputEncoder\n",
       "        d_model: !var 'hidden_size'\n",
       "        vocab_size: 1024\n",
       "    output_decoder: !factory:torch.nn:Linear\n",
       "        - 1024\n",
       "        - !var 'hidden_size'\n",
       "    init_weights: !lambda:../model_src/bits/init_weights.py:simple_weight_init []\n",
       "    layer_stack: *layer_stack_factory\n",
       "\n",
       "\n",
       "main: *model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_codeblock(\"yaml\", to_yaml(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad4788-09fc-46ca-b6ce-9e604a3b3ea7",
   "metadata": {},
   "source": [
    "### Convert Graph to Python\n",
    "\n",
    "This function takes the output from Latent.to_py(graph) and uses it to render Pyhon code using a Jinja2 template. If the template is unspecified, an implicit \"built-in\" template is used, which will generate appropriate import and dynamic import statements, where required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec86456-4baa-4395-b005-6e5e768bff9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Generated Code\n",
       "```python\n",
       "from torch.nn import LayerNorm\n",
       "from torch.nn import Linear\n",
       "from torch.nn import ReLU\n",
       "from importlib.util import spec_from_file_location, module_from_spec\n",
       "import os\n",
       "import sys\n",
       "from functools import partial\n",
       "\n",
       "# Import a dynamic module.\n",
       "def dynimport(module, name, searchpath):\n",
       "    module_path = module\n",
       "    module_name = os.path.basename(module).split(\".\")[0]\n",
       "    module_spec = spec_from_file_location(\n",
       "        module_name,\n",
       "        module_path,\n",
       "        submodule_search_locations=searchpath,\n",
       "    )\n",
       "    mod = module_from_spec(module_spec)\n",
       "    sys.modules[module_name] = mod\n",
       "    module_spec.loader.exec_module(mod)\n",
       "    for symbol in name.split(\".\"):\n",
       "        mod = getattr(mod, symbol)\n",
       "    return mod\n",
       "\n",
       "CasualLM = lambda: dynimport(\"../model_src/bits/causal_lm.py\", \"CasualLM\", ())\n",
       "CausalLoss = lambda: dynimport(\"../model_src/bits/causal_loss.py\", \"CausalLoss\", ())\n",
       "FeedforwardLayer = lambda: dynimport(\"../model_src/bits/feedforward_layer.py\", \"FeedforwardLayer\", ())\n",
       "simple_weight_init = lambda: dynimport(\"../model_src/bits/init_weights.py\", \"simple_weight_init\", ())\n",
       "InputEncoder = lambda: dynimport(\"../model_src/bits/input_encoder.py\", \"InputEncoder\", ())\n",
       "LayerStack = lambda: dynimport(\"../model_src/bits/layer_stack.py\", \"LayerStack\", ())\n",
       "PreLNLayer = lambda: dynimport(\"../model_src/bits/pre_ln_layer.py\", \"PreLNLayer\", ())\n",
       "SingleHeadAttn = lambda: dynimport(\"../model_src/bits/single_head_attn.py\", \"SingleHeadAttn\", ())\n",
       "\n",
       "def construct(\n",
       "    dim_feedforward,\n",
       "    hidden_size,\n",
       "    n_layers,\n",
       "):\n",
       "    activation_factory = partial(ReLU, )\n",
       "\n",
       "    feedforward_factory = partial(FeedforwardLayer(), \n",
       "        activation_factory=activation_factory,\n",
       "        d_model=hidden_size,\n",
       "        d_feedforward=dim_feedforward,\n",
       "    )\n",
       "\n",
       "    attention_factory = partial(SingleHeadAttn(), \n",
       "        d_model=hidden_size,\n",
       "        bias=False,\n",
       "    )\n",
       "\n",
       "    layer_norm_factory = partial(LayerNorm, \n",
       "        hidden_size,\n",
       "    )\n",
       "\n",
       "    layer_factory = partial(PreLNLayer(), \n",
       "        feedforward_factory=feedforward_factory,\n",
       "        attention_factory=attention_factory,\n",
       "        norm_factory=layer_norm_factory,\n",
       "    )\n",
       "\n",
       "    layer_stack_factory = partial(LayerStack(), \n",
       "        layer_factory=layer_factory,\n",
       "        post_norm_factory=layer_norm_factory,\n",
       "        num_hidden_layers=n_layers,\n",
       "    )\n",
       "\n",
       "    model = CasualLM()(\n",
       "        loss_fn=CausalLoss()(),\n",
       "        input_encoder=InputEncoder()(\n",
       "            d_model=hidden_size,\n",
       "            vocab_size=1024,\n",
       "        ),\n",
       "        output_decoder=Linear(\n",
       "            1024,\n",
       "            hidden_size,\n",
       "        ),\n",
       "        init_weights=partial(simple_weight_init(), ),\n",
       "        layer_stack=layer_stack_factory(),\n",
       "    )\n",
       "    \n",
       "    return model\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from forgather.graph_encoder import NamePolicy # NamePolicy.REQUIRED | NamePolicy.ALL | NamePolicy.NAMED\n",
    "generated_code = generate_code(graph.main, name_policy=None)\n",
    "nb.display_codeblock(\"python\", generated_code, \"### Generated Code\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5eeef-511f-4bc2-ad87-d5d4b810befd",
   "metadata": {},
   "source": [
    "### Custom Code Template\n",
    "\n",
    "The above code is pretty generic. How about we wrap this class with a HF PreTrainedModel?  \n",
    "[./examples/causal_lm.py](./examples/causal_lm.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52e24273-ff22-4f46-8ec8-23720fb656d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Generated Code\n",
       "```python\n",
       "# See: https://huggingface.co/docs/transformers/custom_models\n",
       "# This is a template model, with the details filled-in by the code-generator.\n",
       "from typing import Optional, Tuple\n",
       "\n",
       "from functools import partial\n",
       "from torch import nn, Tensor, LongTensor, FloatTensor\n",
       "import torch\n",
       "from transformers.modeling_outputs import CausalLMOutput\n",
       "from transformers import (\n",
       "    PreTrainedModel,\n",
       "    PretrainedConfig,\n",
       "    AutoConfig,\n",
       "    AutoModelForCausalLM,\n",
       "    GenerationMixin,\n",
       ")\n",
       "\n",
       "from torch.nn import LayerNorm\n",
       "from torch.nn import Linear\n",
       "from torch.nn import ReLU\n",
       "\n",
       "from importlib.util import spec_from_file_location, module_from_spec\n",
       "import os\n",
       "import sys\n",
       "from functools import partial\n",
       "\n",
       "# Import a dynamic module.\n",
       "def dynimport(module, name, searchpath):\n",
       "    module_path = module\n",
       "    module_name = os.path.basename(module).split(\".\")[0]\n",
       "    module_spec = spec_from_file_location(\n",
       "        module_name,\n",
       "        module_path,\n",
       "        submodule_search_locations=searchpath,\n",
       "    )\n",
       "    mod = module_from_spec(module_spec)\n",
       "    sys.modules[module_name] = mod\n",
       "    module_spec.loader.exec_module(mod)\n",
       "    for symbol in name.split(\".\"):\n",
       "        mod = getattr(mod, symbol)\n",
       "    return mod\n",
       "\n",
       "CasualLM = lambda: dynimport(\"../model_src/bits/causal_lm.py\", \"CasualLM\", ())\n",
       "CausalLoss = lambda: dynimport(\"../model_src/bits/causal_loss.py\", \"CausalLoss\", ())\n",
       "FeedforwardLayer = lambda: dynimport(\"../model_src/bits/feedforward_layer.py\", \"FeedforwardLayer\", ())\n",
       "simple_weight_init = lambda: dynimport(\"../model_src/bits/init_weights.py\", \"simple_weight_init\", ())\n",
       "InputEncoder = lambda: dynimport(\"../model_src/bits/input_encoder.py\", \"InputEncoder\", ())\n",
       "LayerStack = lambda: dynimport(\"../model_src/bits/layer_stack.py\", \"LayerStack\", ())\n",
       "PreLNLayer = lambda: dynimport(\"../model_src/bits/pre_ln_layer.py\", \"PreLNLayer\", ())\n",
       "SingleHeadAttn = lambda: dynimport(\"../model_src/bits/single_head_attn.py\", \"SingleHeadAttn\", ())\n",
       "\n",
       "model_type = \"my_model\"\n",
       "\n",
       "\n",
       "class DynamicCausalLMConfig(PretrainedConfig):\n",
       "    model_type = model_type\n",
       "\n",
       "\n",
       "class DynamicCasualLM(PreTrainedModel, GenerationMixin):\n",
       "    config_class = DynamicCausalLMConfig\n",
       "    model_type = model_type\n",
       "\n",
       "    def __init__(self, config: PretrainedConfig):\n",
       "        super().__init__(config)\n",
       "        self.causal_lm = self.construct_model(**config.to_dict())\n",
       "        if \"torch_dtype\" in config:\n",
       "            self.to(config.torch_dtype)\n",
       "\n",
       "    @staticmethod\n",
       "    def construct_model(\n",
       "        dim_feedforward,\n",
       "        hidden_size,\n",
       "        n_layers,\n",
       "        **kwargs\n",
       "    ):\n",
       "        activation_factory = partial(ReLU, )\n",
       "\n",
       "        feedforward_factory = partial(FeedforwardLayer(), \n",
       "            activation_factory=activation_factory,\n",
       "            d_model=hidden_size,\n",
       "            d_feedforward=dim_feedforward,\n",
       "        )\n",
       "\n",
       "        attention_factory = partial(SingleHeadAttn(), \n",
       "            d_model=hidden_size,\n",
       "            bias=False,\n",
       "        )\n",
       "\n",
       "        layer_norm_factory = partial(LayerNorm, \n",
       "            hidden_size,\n",
       "        )\n",
       "\n",
       "        layer_factory = partial(PreLNLayer(), \n",
       "            feedforward_factory=feedforward_factory,\n",
       "            attention_factory=attention_factory,\n",
       "            norm_factory=layer_norm_factory,\n",
       "        )\n",
       "\n",
       "        layer_stack_factory = partial(LayerStack(), \n",
       "            layer_factory=layer_factory,\n",
       "            post_norm_factory=layer_norm_factory,\n",
       "            num_hidden_layers=n_layers,\n",
       "        )\n",
       "\n",
       "        model = CasualLM()(\n",
       "            loss_fn=CausalLoss()(),\n",
       "            input_encoder=InputEncoder()(\n",
       "                d_model=hidden_size,\n",
       "                vocab_size=1024,\n",
       "            ),\n",
       "            output_decoder=Linear(\n",
       "                1024,\n",
       "                hidden_size,\n",
       "            ),\n",
       "            init_weights=partial(simple_weight_init(), ),\n",
       "            layer_stack=layer_stack_factory(),\n",
       "        )\n",
       "        \n",
       "        return model\n",
       "\n",
       "    def forward(\n",
       "        self,\n",
       "        input_ids: LongTensor,\n",
       "        labels: Optional[LongTensor] = None,\n",
       "        position_ids: Optional[LongTensor] = None,\n",
       "        attention_mask: Optional[FloatTensor] = None,\n",
       "        return_dict: bool = False,\n",
       "        **kwargs,\n",
       "    ) -> CausalLMOutput | Tuple[FloatTensor, dict[str, FloatTensor]] | FloatTensor:\n",
       "\n",
       "        outputs = self.causal_lm(\n",
       "            input_ids=input_ids,\n",
       "            labels=labels,\n",
       "            position_ids=position_ids,\n",
       "            attention_mask=attention_mask,\n",
       "            **kwargs,\n",
       "        )\n",
       "\n",
       "        # Return type depends on arguments.\n",
       "        if return_dict:\n",
       "            return CausalLMOutput(**outputs)\n",
       "        elif labels is not None:\n",
       "            return (outputs[\"loss\"], outputs[\"logits\"])\n",
       "        else:\n",
       "            return outputs[\"logits\"]\n",
       "\n",
       "    # Bare-minimum for HF text generation interface to work.\n",
       "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
       "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
       "        model_inputs = {\n",
       "            \"input_ids\": input_ids,\n",
       "            \"attention_mask\": attention_mask,\n",
       "        }\n",
       "        return model_inputs\n",
       "\n",
       "\n",
       "AutoConfig.register(model_type, DynamicCausalLMConfig)\n",
       "AutoModelForCausalLM.register(DynamicCausalLMConfig, DynamicCasualLM)\n",
       "\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_code = generate_code(graph.main, template_name=\"examples/causal_lm.py\", model_type=\"my_model\")\n",
    "nb.display_codeblock(\"python\", generated_code, \"### Generated Code\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d4314-7137-4424-b711-38a80003a13f",
   "metadata": {},
   "source": [
    "## Execute Generated Code\n",
    "\n",
    "Execute the generated code, then call the generated 'construct' function to construct the objects.\n",
    "\n",
    "Note: Lambda nodes with args are not working at present (although Latent.materialize() works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44dc50cb-1111-4e1d-ab6e-ace13da13fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc48126-a989-4a76-a70e-90f241c3f378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicCasualLM(\n",
       "  (causal_lm): CasualLM(\n",
       "    loss_fn=CausalLoss()\n",
       "    (input_encoder): InputEncoder(\n",
       "      d_model=128, vocab_size=1024\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (embedding): Embedding(1024, 128)\n",
       "    )\n",
       "    (output_decoder): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (layer_stack): LayerStack(\n",
       "      (layers): ModuleDict(\n",
       "        (0): PreLNLayer(\n",
       "          (feedforward): FeedforwardLayer(\n",
       "            d_model=128, d_feedforward=512\n",
       "            (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dropout): Identity()\n",
       "            (activation): ReLU()\n",
       "            (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (attention): SingleHeadAttn(\n",
       "            d_model=128, bias=False\n",
       "            (query_key_linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (value_linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (residual_dropout): Identity()\n",
       "        )\n",
       "        (1): PreLNLayer(\n",
       "          (feedforward): FeedforwardLayer(\n",
       "            d_model=128, d_feedforward=512\n",
       "            (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dropout): Identity()\n",
       "            (activation): ReLU()\n",
       "            (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (attention): SingleHeadAttn(\n",
       "            d_model=128, bias=False\n",
       "            (query_key_linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (value_linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (residual_dropout): Identity()\n",
       "        )\n",
       "        (2): PreLNLayer(\n",
       "          (feedforward): FeedforwardLayer(\n",
       "            d_model=128, d_feedforward=512\n",
       "            (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dropout): Identity()\n",
       "            (activation): ReLU()\n",
       "            (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (attention): SingleHeadAttn(\n",
       "            d_model=128, bias=False\n",
       "            (query_key_linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (value_linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (residual_dropout): Identity()\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = DynamicCausalLMConfig(hidden_size=128, dim_feedforward=512, n_layers=3)\n",
    "model = DynamicCasualLM(model_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed1472-882a-43f0-869f-8f9b0cc26963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
