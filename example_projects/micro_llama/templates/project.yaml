## Project level definitions and overrides go here.
-- extends "train_scripts/causal_language_model.yaml"
-- from "model_ctor/args.yaml" import use_flash_attn2

## Set Project level defaults
-- block experiment_metadata
    == super()
    -- set ns.EXPERIMENT_NAME = "Micro Llama"
    -- set ns.EXPERIMENT_DESCRIPTION = "Train a tiny LLama model from scratch."
    -- set ns.SAVE_MODEL = True
-- endblock experiment_metadata


-- block base_directories
    -- set ASSETS_DIR = normpath(path_join(project_directory, '..', '..'))
    -- include 'paths/default_paths.yaml'
-- endblock base_directories


-- block model_constructor_args
# Enables flash-attention2
# Disable this block if your hardware does not support flash-attn2.
.define: &model_constructor_args
    == use_flash_attn2
-- endblock model_constructor_args


-- block datasets_definition
    -- include 'datasets/tiny_stories_pretokenized_2k.yaml'
-- endblock datasets_definition


# Override default trainer definition
-- block trainer_definition
    -- include 'project.trainer_config'
-- endblock trainer_definition


# Override default model defintion
-- block construct_new_model
    -- include 'project.model_config'
-- endblock construct_new_model

#-------------------- project.model_config --------------------
-- extends "models/micro_llama.yaml"


#-------------------- project.trainer_config --------------------
## Select one-of:
## trainers/( trainer.yaml | accel_trainer.yaml | trainer.yaml )
-- extends 'trainers/accel_trainer.yaml'


-- block trainer_meta_config
    == super()
    -- set trainer_def.name = "Custom " + trainer_def.name
-- endblock trainer_meta_config


-- block trainer_args
    == super()
    # Project Overrides
    per_device_train_batch_size: 64
    per_device_eval_batch_size: 64
    logging_steps: 100
    eval_steps: 500
    ##learning_rate: 1.0e-3
    ##lr_scheduler_type: "cosine"
    max_steps: 2000
-- endblock trainer_args