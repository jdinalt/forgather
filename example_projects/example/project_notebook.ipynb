{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e422f35-7a08-4a50-a075-51a68b5c3994",
   "metadata": {},
   "source": [
    "# Project\n",
    "\n",
    "### Project Resources\n",
    "**Experiment Template**  \n",
    "\n",
    "[templates/experiments/example/experiment.yaml](templates/experiments/example/experiment.yaml) \n",
    "    [templates/project.yaml](templates/project.yaml) \n",
    "    \n",
    "[templates/experiments/example/model_config.yaml](templates/experiments/example/model_config.yaml)  \n",
    "[templates/experiments/example/trainer_config.yaml](templates/experiments/example/trainer_config.yaml)  \n",
    "\n",
    "**Project Templates**  \n",
    "\n",
    "[meta_config.yaml](meta_config.yaml)  \n",
    "[templates/directories.yaml](templates/directories.yaml)  \n",
    " \n",
    "\n",
    "**Library Templates**  \n",
    "\n",
    "Config Templates  \n",
    "[../../templates/configs/default_train_script.yaml](../../templates/configs/default_train_script.yaml)  \n",
    "[../../templates/configs/base_train_config.yaml](../../templates/configs/base_train_config.yaml)  \n",
    "\n",
    "Model Templates  \n",
    "[../../templates/models/tiny_d128_l2.yaml](../../templates/models/tiny_d128_l2.yaml)  \n",
    "[../../templates/models/vanilla_transformer.yaml](../../templates/models/vanilla_transformer.yaml)  \n",
    "[../../templates/models/custom_model.yaml](../../templates/models/custom_model.yaml)  \n",
    "[../../templates/models/load_custom_model.yaml](../../templates/models/load_custom_model.yaml)  \n",
    "\n",
    "Dataset Templates  \n",
    "[../../templates/datasets/tiny_stories_pretokenized_2k.yaml](../../templates/datasets/tiny_stories_pretokenized_2k.yaml)  \n",
    "[../../templates/datasets/base_dataset_loader.yaml](../../templates/datasets/base_dataset_loader.yaml)  \n",
    "\n",
    "Trainer Templates  \n",
    "[../../templates/trainers/base_trainer.yaml](../../templates/trainers/base_trainer.yaml)  \n",
    "[../../templates/trainers/trainer.yaml](../../templates/trainers/trainer.yaml)  \n",
    "[../../templates/trainers/accel_trainer.yaml](../../templates/trainers/accel_trainer.yaml)  \n",
    "[../../templates/trainers/hf_trainer.yaml](../../templates/trainers/hf_trainer.yaml)  \n",
    "\n",
    "**Whitelists**  \n",
    "[templates/whitelist.yaml](templates/whitelist.yaml)  \n",
    "[../../templates/whitelists/global_whitelist.yaml](../../templates/whitelists/global_whitelist.yaml)  \n",
    "[../../templates/whitelists/model_zoo_whitelist.yaml](../../templates/whitelists/model_zoo_whitelist.yaml)  \n",
    "\n",
    "**Model Code**  \n",
    "\n",
    "[model_zoo/vanilla_transformer/vanilla_transformer.py](../../model_zoo/vanilla_transformer/vanilla_transformer.py)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5a90f-6386-4b8e-89ce-ec19195f7dff",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bffdcc-00ac-4adc-b3fd-974df44d09fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- meta_config.yaml -----------\n",
      "assets_dir: '../..'\n",
      "datasets_dir: '../../datasets'\n",
      "experiment_dir: './templates/experiments/'\n",
      "model_dir: './output_models'\n",
      "model_src_dir: '../../model_zoo'\n",
      "project_dir: '.'\n",
      "project_templates: './templates'\n",
      "scripts_dir: '../../scripts'\n",
      "search_paths:\n",
      "  - '../../templates'\n",
      "  - './templates'\n",
      "templates: '../../templates'\n",
      "tokenizer_dir: '../../tokenizers'\n",
      "train_script_path: '../../scripts/train_script.py'\n",
      "whitelist_path: './templates/whitelist.yaml'\n",
      "-------------- Experiment --------------\n",
      "./templates/experiments/example_experiment.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..', '..')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "import shutil\n",
    "\n",
    "from pprint import pformat, pp\n",
    "from transformers import set_seed\n",
    "\n",
    "from forgather.config import (\n",
    "    preprocess_config,\n",
    "    load_config,\n",
    "    load_whitelist_as_set,\n",
    "    materialize_config,\n",
    "    enumerate_whitelist_exceptions,\n",
    "    pconfig,\n",
    ")\n",
    "from forgather import Latent\n",
    "from aiws.dotdict import DotDict\n",
    "\n",
    "# Path to your project meta-config\n",
    "meta_config_path = 'meta_config.yaml'\n",
    "experiment_definition_file = 'example_experiment.yaml'\n",
    "\n",
    "metacfg = DotDict(load_config(meta_config_path).config)\n",
    "print(f\"{' '+meta_config_path+' ':-^40}\")\n",
    "pconfig(metacfg)\n",
    "\n",
    "# Get path to selected experiment\n",
    "experiment_path = os.path.join(metacfg.experiment_dir, experiment_definition_file)\n",
    "print(f\"{' Experiment ':-^40}\\n{experiment_path}\")\n",
    "\n",
    "def preprocess(preserve_line_numbers=False, pp_verbose=False):\n",
    "    return preprocess_config(\n",
    "        experiment_path,\n",
    "        search_path=metacfg.search_paths,\n",
    "        preserve_line_numbers=preserve_line_numbers,\n",
    "        pp_verbose=pp_verbose,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0835a75-8ac2-4e9a-9937-d9c260cdc211",
   "metadata": {},
   "source": [
    "### Preprocess Configuration\n",
    "\n",
    "#### preprocess_config() : Preprocess a configuration file.\n",
    "\n",
    "```python\n",
    "def preprocess_config(\n",
    "    config:  os.PathLike | str, *,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod = DEFAULT_LOAD_METHOD,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9c21cd-877d-4544-a16b-7aeb7606f577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1: #---------------------------------------\n",
      "     2: #          My ML Science Project         \n",
      "     3: #---------------------------------------\n",
      "     4: # 2024-07-12 20:02:08\n",
      "     5: # Description: It's not supid, it's advanced!\n",
      "     6: # Model: anonymous_model\n",
      "     7: # World Size: 1\n",
      "     8: # Hostname: hal9000\n",
      "     9: # Script Args: N/A\n",
      "    10: # Versions:\n",
      "    11: #     python: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n",
      "    12: #     torch: 2.3.1\n",
      "    13: #     transformers: 4.41.2\n",
      "    14: #     accelerate: 0.31.0\n",
      "    15: \n",
      "    16: ############# Config Vars ##############\n",
      "    17: \n",
      "    18: # ns.TOKENIZERS_DIR: \"../../tokenizers\"\n",
      "    19: # ns.MODELS_DIR: \"./output_models\"\n",
      "    20: # ns.DATASETS_DIR: \"../../datasets\"\n",
      "    21: # ns.SCRIPTS_DIR: \"../../scripts\"\n",
      "    22: # ns.MODEL_SOURCE_DIR: \"../../model_zoo\"\n",
      "    23: # ns.OUTPUT_DIR: \"./output_models/anonymous_model\"\n",
      "    24: # ns.LOGGING_DIR: path = \"./output_models/anonymous_model/runs/My ML Science Project_1720814528726868692\"\n",
      "    25: # ns.CREATE_NEW_MODEL: path = True\n",
      "    26: # ns.SAVE_MODEL: path = True\n",
      "    27: \n",
      "    28: ############# Dependencies #############\n",
      "    29: \n",
      "    30: # Experiment tracking: Tensorboard SummaryWriter\n",
      "    31: .define: &summary_writer !callable:torch.utils.tensorboard:SummaryWriter\n",
      "    32:     - \"./output_models/anonymous_model/runs/My ML Science Project_1720814528726868692\"\n",
      "    33: \n",
      "    34: ################ Model #################\n",
      "    35: \n",
      "    36: # Name: Custom Tiny-d128-2L\n",
      "    37: # Description: A Plain Vanilla Transformer with 2 layers, d=128, and a 2k tiny_stories tokenizer.\n",
      "    38: \n",
      "    39: # model_def.cls = \"VanillaTransformer\"\n",
      "    40: # model_def.cfg_cls = \"VanillaTransformerConfig\"\n",
      "    41: # model_def.tokenizer_path_or_id = \"../../tokenizers/tiny_stories_2k\"\n",
      "    42: # model_def.config_path = \"../../model_zoo/vanilla_transformer/vanilla_transformer.py\"\n",
      "    43: # model_def.model_path = \"../../model_zoo/vanilla_transformer/vanilla_transformer.py\"\n",
      "    44: \n",
      "    45: # **Tokenizer**\n",
      "    46: \n",
      "    47: # Load a tokenizer from a file or hub id\n",
      "    48: .define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained\n",
      "    49:     - \"../../tokenizers/tiny_stories_2k\"\n",
      "    50: \n",
      "    51: # **Model Config**\n",
      "    52: \n",
      "    53: .define: &model_config\n",
      "    54:     #  Vanilla Transformer Defaults\n",
      "    55:     vocab_size: !callable:forgather.construct:get_attr [ *tokenizer, vocab_size ]\n",
      "    56:     \n",
      "    57:     # Custom Tiny-d128-2L Defaults\n",
      "    58:     hidden_size: 128\n",
      "    59:     dim_feedforward: 512\n",
      "    60:     num_attention_heads: 1\n",
      "    61:     num_hidden_layers: 2\n",
      "    62:     \n",
      "    63:     # My ML Science Project Overrides\n",
      "    64:     hidden_size: 256\n",
      "    65: \n",
      "    66: # **Model Constructor**\n",
      "    67: \n",
      "    68: # Custom transformer model; registers for AutoClass and will save code with weights.\n",
      "    69: .define: &model !callable:aiws.construct:register_for_auto_class\n",
      "    70:     - !callable:../../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer\n",
      "    71:         - !callable:aiws.construct:register_for_auto_class\n",
      "    72:             - !callable:../../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig\n",
      "    73:                 <<: *model_config\n",
      "    74: \n",
      "    75: ############### Datasets ###############\n",
      "    76: \n",
      "    77: # Name: Tiny Stories 2K\n",
      "    78: # Description: Tiny Stories dataset, pre-tokenized with the tiny_2k tokenizer.\n",
      "    79: \n",
      "    80: # dataset_def.dataset_name_or_path = \"../../datasets/tiny_stories_tokenized\"\n",
      "    81: # dataset_def.dataset_train_split = \"train\"\n",
      "    82: # dataset_def.dataset_validation_split = \"validation\"\n",
      "    83: \n",
      "    84: # **Load Datasets**\n",
      "    85: \n",
      "    86: .define: &dataset !callable:datasets:load_from_disk [ \"../../datasets/tiny_stories_tokenized\" ]\n",
      "    87: \n",
      "    88: # **Get Splits**\n",
      "    89: \n",
      "    90: .define: &train_dataset !callable:forgather.construct:get_item [ *dataset, \"train\" ]\n",
      "    91: .define: &eval_dataset !callable:forgather.construct:get_item [ *dataset, \"validation\" ]\n",
      "    92: \n",
      "    93: ########## Trainer Callbacks ###########\n",
      "    94: \n",
      "    95: .define: &trainer_callbacks\n",
      "    96:     # Log training metrics to JSON fiie.\n",
      "    97:     - !callable:aiws.default_callbacks:JsonLogger []\n",
      "    98:     # Log configuration and metrics to Tensorboard file\n",
      "    99:     - !callable:aiws.tb_logger:TBLogger\n",
      "   100:         args: [ *summary_writer ]\n",
      "   101:         kwargs:\n",
      "   102:             date: \"2024-07-12 20:02:08\"\n",
      "   103:             name: \"My ML Science Project\"\n",
      "   104:             description: \"It's not supid, it's advanced!\"\n",
      "   105:             args: N/A\n",
      "   106:             world_size: 1\n",
      "   107:             config: !callable:pp_config\n",
      "   108:             versions: {'python': '3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]', 'torch': '2.3.1', 'transformers': '4.41.2', 'accelerate': '0.31.0'}\n",
      "   109: \n",
      "   110: ############ Data Collator #############\n",
      "   111: \n",
      "   112: .define: &data_collator !callable:transformers:DataCollatorForLanguageModeling\n",
      "   113:     args:\n",
      "   114:         - *tokenizer\n",
      "   115:     kwargs:\n",
      "   116:         mlm: False\n",
      "   117:         return_tensors: pt\n",
      "   118: \n",
      "   119: ############### Trainer ################\n",
      "   120: \n",
      "   121: # Name: Custom aiws.trainer.Trainer\n",
      "   122: # Description: A lightweight, extensible trainer; does not support multiple GPUs\n",
      "   123: \n",
      "   124: # **Trainer Args**\n",
      "   125: \n",
      "   126: .define: &trainer_args\n",
      "   127:     # Base Trainer Defaults\n",
      "   128:     output_dir: \"./output_models/anonymous_model\"\n",
      "   129:     logging_dir: \"./output_models/anonymous_model/runs/My ML Science Project_1720814528726868692\"\n",
      "   130:     overwrite_output_dir: True\n",
      "   131:     per_device_train_batch_size: 16\n",
      "   132:     per_device_eval_batch_size: 16\n",
      "   133:     learning_rate: 1.0e-3\n",
      "   134:     num_train_epochs: 1\n",
      "   135:     eval_steps: 500\n",
      "   136:     logging_steps: 500\n",
      "   137:     seed: 42\n",
      "   138:     eval_strategy: \"steps\"\n",
      "   139:     save_strategy: \"no\"\n",
      "   140:     logging_strategy: \"steps\"\n",
      "   141:     lr_scheduler_type: \"constant\"\n",
      "   142:     \n",
      "   143:     # Project Overrides\n",
      "   144:     per_device_train_batch_size: 64\n",
      "   145:     per_device_eval_batch_size: 64\n",
      "   146:     eval_steps: 100\n",
      "   147:     max_steps: 2000\n",
      "   148: \n",
      "   149: # **Trainer Constructor**\n",
      "   150: \n",
      "   151: .define: &trainer !callable:aiws.trainer:Trainer\n",
      "   152:     model: *model\n",
      "   153:     args: !callable:aiws.trainer_types:TrainingArguments\n",
      "   154:         <<: *trainer_args\n",
      "   155:     data_collator: *data_collator\n",
      "   156:     train_dataset: *train_dataset\n",
      "   157:     eval_dataset: *eval_dataset\n",
      "   158:     tokenizer: *tokenizer\n",
      "   159:     callbacks: *trainer_callbacks\n",
      "   160: \n",
      "   161: ############## Pre Config ##############\n",
      "   162: \n",
      "   163: # Undefined\n",
      "   164: \n",
      "   165: #---------------------------------------\n",
      "   166: #          Configuration Output          \n",
      "   167: #---------------------------------------\n",
      "   168: output_dir: \"./output_models/anonymous_model\"\n",
      "   169: logging_dir: \"./output_models/anonymous_model/runs/My ML Science Project_1720814528726868692\"\n",
      "   170: experiment_name: \"My ML Science Project\"\n",
      "   171: experiment_description: \"It's not supid, it's advanced!\"\n",
      "   172: trainer: *trainer\n",
      "   173: do_save: True\n",
      "   174: \n",
      "   175: ############# Post Config ##############\n",
      "   176: \n",
      "   177: # Undefined\n",
      "   178: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting 'preserve_line_numbers' to True preserves line numbers; without this, the reported lines in exceptions may be incorrect.\n",
    "# Setting 'pp_verbose' dumpts all pre-pre-processed templates for diagnostics.\n",
    "print(preprocess(preserve_line_numbers=False, pp_verbose=False).with_line_numbers())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438afc6-999c-4031-8193-ec230dfcdc9e",
   "metadata": {},
   "source": [
    "Run the cell and click the link to open the preprocessed file in the notebook.  \n",
    "[preprocessed_config.yaml](preprocessed_config.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "46bd1cab-66a4-4784-96ce-8514890291bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only preprocess the experiment template\n",
    "pp_config = preprocess()\n",
    "\n",
    "with open('preprocessed_config.yaml', 'w') as f:\n",
    "    f.write(pp_config.with_line_numbers(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6eba1-e054-4e8d-99e3-57030302a337",
   "metadata": {},
   "source": [
    "### Preprocess and Load the Whitelist\n",
    "```python\n",
    "def load_whitelist_as_set(\n",
    "    config: os.PathLike | str, *,\n",
    "    preprocess: bool = True,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod = DEFAULT_LOAD_METHOD\n",
    ") -> Set[str]:\n",
    "```\n",
    "Load a whitelist configuration from a file or string\n",
    "\n",
    "This is essentially just load_config, but it normalizes the paths in the whitelist and converts the list to a set, to improve search performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04018a-f7de-400b-91d3-8b07efec0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist_out = load_whitelist_as_set(metacfg.whitelist_path, search_path=metacfg.search_paths)\n",
    "pconfig(whitelist_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f72ba-670d-46fb-87c3-f9ce5f2c58b4",
   "metadata": {},
   "source": [
    "#### Check Whitelist Requirements\n",
    "\n",
    "If you would like to see which import-specs are used in a configuraiton (or which are missing), you can use enumerate_whitelist_exceptions().\n",
    "```python\n",
    "def enumerate_whitelist_exceptions(config: Any, whitelist: Container = set())\n",
    "```\n",
    "Print all import-specs not matching the whitelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "630c7929-da58-40db-b430-a8687e0a0541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "whitelist = whitelist_out.config\n",
    "#whitelist = set()\n",
    "enumerate_whitelist_exceptions(load_config(experiment_path, search_path=metacfg.search_paths).config, whitelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4848270-6d49-412f-a3a2-b72222461140",
   "metadata": {},
   "source": [
    "### Materialize the Configuration\n",
    "\n",
    "#### materialize_config() : Materialize the Latent objects in the configuration\n",
    "```python\n",
    "def materialize_config(\n",
    "    config: Any,\n",
    "    whitelist: Container | os.PathLike | str = None,\n",
    "    preprocess: bool = True,\n",
    "    search_path: str | List[str] = '.',\n",
    "    load_method: LoadMethod=DEFAULT_LOAD_METHOD,\n",
    "    pp_kwargs: Dict[str, Any] = {},\n",
    "    kwargs: Dict[str, Callable] = {},\n",
    ") -> MaterializedOutput:\n",
    "```\n",
    "- config: An instantiated, but Latent, configuration; a preprocessed configuration string; or a path to a configuraiton file.  \n",
    "- whitelist: A Container type, which means any object which supports 'str is in container'  \n",
    "- preprocess: Preprocess the string or file. Only applies if input is a path or string.  \n",
    "- load_method: One of \"from_file\", \"from_string\", \"from_file_search\"  \n",
    "- search_path: A str or List\\[str\\] paths to search for templates; also applies to \"from_file_search\" load method.  \n",
    "- pp_kwargs: Arguments to pass to the template, if preprocessing is to be performed.  \n",
    "- kwargs: A mapping str -> Callable to substitute when materializing the final config. This allows passing already instantiated objects into the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ce4a60c-ce9b-4738-9eb1-4deb39d559ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do_save: True\n",
      "experiment_description: 'It's not supid, it's advanced!'\n",
      "experiment_name: 'The latest example'\n",
      "logging_dir: './output_models/anonymous_model/runs/The latest example_1720807148953599303'\n",
      "output_dir: './output_models/anonymous_model'\n",
      "trainer:\n",
      "  Trainer(model=VanillaTransformer(\n",
      "    (embedding): Embedding(2000, 256)\n",
      "    (positional_encoder): PositionalEncoder()\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerLayer(\n",
      "        (attention): MultiheadAttention(\n",
      "          (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (feedforward): FeedforwardLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=256, out_features=2000, bias=True)\n",
      "  ),args=TrainingArguments(per_device_train_batch_size=16, output_dir='./output_models/anonymous_model', overwrite_output_dir=True, per_device_eval_batch_size=16, max_steps=2000, logging_steps=500, eval_steps=500, save_steps=500, learning_rate=0.001, num_train_epochs=1, seed=42, lr_scheduler_type='constant', warmup_steps=0, device=None, logging_dir='./output_models/anonymous_model/runs/The latest example_1720807148953599303', eval_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_strategy=<IntervalStrategy.STEPS: 'steps'>, save_strategy=<IntervalStrategy.NO: 'no'>, logging_first_step=False, eval_delay=0, save_total_limit=2),data_collator=DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='../../tokenizers/tiny_stories_2k', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|PAD|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "  \t0: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t1: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t2: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t3: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  }, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt'),train_dataset=Dataset({\n",
      "      features: ['input_ids'],\n",
      "      num_rows: 2119719\n",
      "  }),eval_dataset=Dataset({\n",
      "      features: ['input_ids'],\n",
      "      num_rows: 2199\n",
      "  }),tokenizer=PreTrainedTokenizerFast(name_or_path='../../tokenizers/tiny_stories_2k', vocab_size=2000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|BOS|>', 'eos_token': '<|EOS|>', 'unk_token': '<|UNK|>', 'pad_token': '<|PAD|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "  \t0: AddedToken(\"<|BOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t1: AddedToken(\"<|PAD|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t2: AddedToken(\"<|EOS|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  \t3: AddedToken(\"<|UNK|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "  },model_init=None,callbacks=[<aiws.default_callbacks.ProgressCallback object at 0x7f0311d2b9a0>, <aiws.default_callbacks.InfoCallback object at 0x7f0311d2beb0>, <aiws.default_callbacks.JsonLogger object at 0x7f0311d28430>, <aiws.tb_logger.TBLogger object at 0x7f0311d28580>],)\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "config_output = materialize_config(experiment_path, whitelist=metacfg.whitelist_path, search_path=metacfg.search_paths)\n",
    "config = DotDict(config_output.config)\n",
    "pconfig(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21d135-1575-4684-a639-ce1c38162063",
   "metadata": {},
   "source": [
    "### Run Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9ccce36-feda-4ba1-8fb9-31a5b3d7871c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d498667de54f482abcbb59796803b231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 2,119,712\n",
      "total_train_samples: 2,119,712\n",
      "per_device_train_batch_size: 16\n",
      "actual_per_device_batch_size: 16\n",
      "total_train_batch_size: 16\n",
      "max_steps: 2,000\n",
      "total_parameters: 1.9M\n",
      "trainable_parameters: 1.9M\n",
      "model:\n",
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 256)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=256, out_features=2000, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "2024-07-09 23:01:06          500  0.0   train-loss: 3.83067   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96272829bdab4c90b4968fc9a2304d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 23:01:06          500  0.0   eval-loss:  3.24676   \n",
      "2024-07-09 23:01:10        1,000  0.01  train-loss: 2.97355   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae99609af244c159f7f009644acb21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 23:01:11        1,000  0.01  eval-loss:  2.84039   \n",
      "2024-07-09 23:01:15        1,500  0.01  train-loss: 2.68934   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c424facf329481aaacb5f3aaf77fe74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 23:01:15        1,500  0.01  eval-loss:  2.63415   \n",
      "2024-07-09 23:01:19        2,000  0.02  train-loss: 2.51999   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27ab483a2904af6a78a1bffaed99692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 23:01:20        2,000  0.02  eval-loss:  2.52777   \n",
      "train_runtime: 18.08\n",
      "train_samples: 32,000\n",
      "step: 2,000\n",
      "train_samples_per_second: 1.77e+03\n",
      "train_steps_per_second: 110.6\n",
      "train_loss: 2.997\n",
      "epoch: 0.0151\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=2.519994020462036, metrics={'train_runtime': 18.078115224838257, 'train_samples': 32000, 'step': 2000, 'train_samples_per_second': 1770.096, 'train_steps_per_second': 110.631, 'train_loss': 2.997377872467041, 'epoch': 0.015096390453042677})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "config.trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a29692-aab9-44d6-b089-bf97e6f81d90",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2565208e-5df9-41f6-afc0-dd9d18d22d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "# This is the entry-point for the spawned procceses.\n",
    "def training_loop(meta_config, experiment_name):\n",
    "    set_seed(42)\n",
    "    metacfg = DotDict(load_config(meta_config).config)\n",
    "\n",
    "    # Get Torch Distributed parameters from environ.\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "    \n",
    "    config_output = materialize_config(\n",
    "        experiment_name,\n",
    "        metacfg.whitelist_path,\n",
    "        search_path=metacfg.search_paths,\n",
    "        pp_kwargs = dict(\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "            local_rank=local_rank,\n",
    "        )\n",
    "    )\n",
    "    config = DotDict(config_output.config)\n",
    "    is_main_process = config.trainer.accelerator if hasattr(config.trainer, \"accelerator\") else True\n",
    "    # If you don't want all processes to print to the console...\n",
    "    if is_main_process:\n",
    "        print(\"**** Training Started *****\")\n",
    "        print(f\"experiment_name: {config.experiment_name}\")\n",
    "        print(f\"experiment_description: {config.experiment_description}\")\n",
    "        print(f\"output_dir: {config.output_dir}\")\n",
    "        print(f\"logging_dir: {config.logging_dir}\")\n",
    "\n",
    "    # This is where the actual 'loop' is.\n",
    "    metrics = config.trainer.train().metrics\n",
    "    \n",
    "    if is_main_process:\n",
    "        print(\"**** Training Completed *****\")\n",
    "        print(metrics)\n",
    "\n",
    "    metrics = config.trainer.evaluate()\n",
    "\n",
    "    if is_main_process:\n",
    "        print(\"**** Evaluation Completed *****\")\n",
    "        print(metrics)\n",
    "    \n",
    "    if config.do_save:\n",
    "        config.trainer.save_model()\n",
    "        if is_main_process:\n",
    "            print(f\"Model saved to: {config.trainer.args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709fd93-5a27-487a-8aeb-166b8cd2274e",
   "metadata": {},
   "source": [
    "#### Run Training Loop Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdbf0acd-89c0-423c-bb6f-f2b98b6d82e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Training Started *****\n",
      "experiment_name: The latest example\n",
      "experiment_description: It's not supid, it's advanced!\n",
      "output_dir: ./output_models/anonymous_model\n",
      "logging_dir: ./output_models/anonymous_model/runs/The latest example_1720586319070237488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c155d6ad1b9140d4833f29597a21ad82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 2,119,712\n",
      "total_train_samples: 2,119,712\n",
      "per_device_train_batch_size: 16\n",
      "actual_per_device_batch_size: 16\n",
      "total_train_batch_size: 16\n",
      "max_steps: 2,000\n",
      "total_parameters: 1.9M\n",
      "trainable_parameters: 1.9M\n",
      "model:\n",
      "VanillaTransformer(\n",
      "  (embedding): Embedding(2000, 256)\n",
      "  (positional_encoder): PositionalEncoder()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (query_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (key_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (value_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (feedforward): FeedforwardLayer(\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (activation): ReLU()\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=256, out_features=2000, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "2024-07-10 04:38:43          500  0.0   train-loss: 3.83067   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bff7261b7a40dca6474e42ea1b659a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 04:38:43          500  0.0   eval-loss:  3.24676   \n",
      "2024-07-10 04:38:47        1,000  0.01  train-loss: 2.97355   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08be6291febc4810b6cd67097258bd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 04:38:48        1,000  0.01  eval-loss:  2.84039   \n",
      "2024-07-10 04:38:51        1,500  0.01  train-loss: 2.68934   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a137fe57dd3b47e9aa1e1a5b8c940a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 04:38:52        1,500  0.01  eval-loss:  2.63415   \n",
      "2024-07-10 04:38:56        2,000  0.02  train-loss: 2.51999   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0471611cfce74943ab3ffd1803f14428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 04:38:56        2,000  0.02  eval-loss:  2.52777   \n",
      "train_runtime: 17.29\n",
      "train_samples: 32,000\n",
      "step: 2,000\n",
      "train_samples_per_second: 1.851e+03\n",
      "train_steps_per_second: 115.7\n",
      "train_loss: 2.997\n",
      "epoch: 0.0151\n",
      "\n",
      "**** Training Completed *****\n",
      "{'train_runtime': 17.29047727584839, 'train_samples': 32000, 'step': 2000, 'train_samples_per_second': 1850.73, 'train_steps_per_second': 115.671, 'train_loss': 2.997377872467041, 'epoch': 0.015096390453042677}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f38b720fa14ba3a0aea42516df4b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 04:38:56        2,000  0.02  eval-loss:  2.52777   \n",
      "**** Evaluation Completed *****\n",
      "{'eval_loss': 2.527772903442383}\n",
      "[2024-07-10 04:38:57,086] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Model saved to: ./output_models/anonymous_model\n"
     ]
    }
   ],
   "source": [
    "training_loop(meta_config_path, experiment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fa3351-96d7-4769-8b45-99d103e1b941",
   "metadata": {},
   "source": [
    "#### Launch with Notebook Launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87183a-dbec-420e-ad2f-796384eb9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(\n",
    "    training_loop,\n",
    "    args=(meta_config_path, experiment_path,),\n",
    "    num_processes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece555d-aa15-4b5e-a80e-20cb9fa9db52",
   "metadata": {},
   "source": [
    "### Train from a Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a399e7a-6220-426f-affa-f9bfba4b9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stat\n",
    "# Output train-script command line as a string\n",
    "def train_cmdline(metacfg, nproc='gpu'):\n",
    "    includes = ''.join(f\"-I '{inc}' \" for inc in metacfg.search_paths)\n",
    "    return f\"torchrun --standalone --nproc-per-node {nproc} '{metacfg.train_script_path}' -w '{metacfg.whitelist_path}' {includes} -s '{metacfg.assets_dir}'\"\n",
    "\n",
    "# Output train-script as command line as a bash-script\n",
    "# ./train.sh [<other-sli-args] <experiment-config-file>\n",
    "def make_bash_script(metacfg, script_path='train.sh', nproc='gpu'):\n",
    "    with open(script_path, 'w') as f:\n",
    "        f.write('#!/bin/bash\\n' + train_cmdline(metacfg, nproc) + ' \"${@}\"\\n')\n",
    "        os.chmod(f.fileno(), stat.S_IREAD|stat.S_IRUSR|stat.S_IWUSR|stat.S_IXUSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae89f3-11cb-45d9-b4ec-6ac2ae35a5be",
   "metadata": {},
   "source": [
    "#### Generate Bash Script\n",
    "\n",
    "This will output a shell-script which will invoke the training script with the arguments for this project.\n",
    "\n",
    "```bash\n",
    "# Optional: Restrict the GPUs to use to a sub-set of those avialable.\n",
    "export CUDA_VISIBLE_DEVICES=\"0,1\"\n",
    "\n",
    "./train path_to_experiment.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61e3cdc6-925d-4b28-ae59-778c011dcd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "torchrun --standalone --nproc-per-node gpu '../../scripts/train_script.py' -w './templates/whitelist.yaml' -I './templates' -I '../../templates'  -s '../..' \"${@}\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_bash_script(metacfg, nproc='gpu')\n",
    "\n",
    "# Read back to verify\n",
    "with open('train.sh', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a3131-a09e-46de-8f60-6699f758630c",
   "metadata": {},
   "source": [
    "#### Run Training Script from Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a0cf4-1c93-47cf-8888-ffb806df684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, this will run on all available GPUs. To restrict it to a sub-set, you can use this envrionment variable.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "!{train_cmdline(metacfg)} 'forgather_demo/hf_trainer_experiment.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e49f3-d22c-4130-934b-460c0cf2e080",
   "metadata": {},
   "source": [
    "### View in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fafbb8c8-8f81-4eb2-a574-cd7999ec77ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.16.2 at http://hal9000:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --bind_all --logdir output_models/test_model/runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3c45a-d962-4316-8146-afd0a1024614",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Delete all of the output models produced by the demo and start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073be4ea-bebc-4f64-8248-3818d8717863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 'forgather_demo/output_models'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Removing '{metacfg.models_dir}'\")\n",
    "shutil.rmtree(metacfg.models_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed775b-a63b-4dd0-adbe-1c3d78de8898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
