# Multi-GPU
# 2024-07-08 18:17:21
# Description: Try training on multiple GPUs
# World Size: 1
# Hostname: hal9000
# Script Args:: N/A

# experiment.TOKENIZERS_DIR: "../../tokenizers"
# experiment.DATASETS_DIR: "../../datasets"
# experiment.MODEL_SRC_DIR: "../../model_zoo"
# experiment.MODELS_DIR: "output_models"

# experiment.DATASET: "tiny_stories_tokenized"
# experiment.EXPERIMENT_NAME: "Multi-GPU"
# experiment.EXPERIMENT_DESCRIPTION: "Try training on multiple GPUs"
# experiment.MODEL_NAME: "test_model"
# experiment.CREATE_NEW_MODEL: True
# experiment.SAVE_MODEL: True

# config.OUTPUT_DIR: path = "output_models/test_model"
# config.DATASET_PATH: path = "../../datasets/tiny_stories_tokenized"
# config.LOGGING_DIR: path = "output_models/test_model/runs/Multi-GPU_1720462641924163985"

# experiment.tokenizer
.define: &tokenizer !callable:transformers:AutoTokenizer.from_pretrained
    - "../../tokenizers/tiny_stories_2k"

.define: &model_config
    vocab_size: !callable:forgather.construct:get_attr [ *tokenizer, vocab_size ]
    hidden_size: 128
    dim_feedforward: 512
    num_attention_heads: 1
    num_hidden_layers: 2

    hidden_size: 128

# experiment.model
.define: &model !callable:aiws.construct:register_for_auto_class
    - !callable:../../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformer
        - !callable:aiws.construct:register_for_auto_class
            - !callable:../../model_zoo/vanilla_transformer/vanilla_transformer.py:VanillaTransformerConfig
                kwargs: *model_config


.define: &summary_writer !callable:torch.utils.tensorboard:SummaryWriter
    - "output_models/test_model/runs/Multi-GPU_1720462641924163985"
.define: &dataset !callable:datasets:load_from_disk [ "../../datasets/tiny_stories_tokenized" ]
.define: &train_dataset !callable:forgather.construct:get_item [ *dataset, "train" ]
.define: &eval_dataset !callable:forgather.construct:get_item [ *dataset, "validation" ]
.define: &data_collator !callable:transformers:DataCollatorForLanguageModeling
    args:
        - *tokenizer
    kwargs:
        mlm: False
        return_tensors: pt

.define: &training_args

# Training args
    overwrite_output_dir: False
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 16
    learning_rate: 1.0e-3
    num_train_epochs: 1
    eval_steps: 500
    logging_steps: 500
    seed: 42
    eval_strategy: "steps"
    save_strategy: "no"
    logging_strategy: "steps"
    lr_scheduler_type: "constant"

    overwrite_output_dir: True
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 32
    learning_rate: 1.0e-3
    num_train_epochs: 1
    eval_steps: 200
    logging_steps: 100
    max_steps: 2000

    output_dir: "output_models/test_model"
    logging_dir: "output_models/test_model/runs/Multi-GPU_1720462641924163985"

# Callbacks
.define: &callbacks
    - !callable:aiws.default_callbacks:JsonLogger []
    - !callable:aiws.tb_logger:TBLogger
        args: [ *summary_writer ]
        kwargs:
            date: "2024-07-08 18:17:21"
            name: "Multi-GPU"
            description: "Try training on multiple GPUs"
            args: N/A
            world_size: 1
            config: !callable:pp_config


# Accel trainer args
.define:  &accel_trainer_args
    device_placement: True
    dataloader_config: !callable:accelerate:DataLoaderConfiguration
        kwargs:
            dispatch_batches: False
            split_batches: False


# HF Trainer args
.define: &hf_trainer_args 
    accelerator_config:
        dispatch_batches: False
        split_batches: False
    ddp_find_unused_parameters: False
    report_to: "none"
    logging_nan_inf_filter: False


# Trainer
.define: &trainer !callable:aiws.accel_trainer:AccelTrainer
    kwargs:
        model: *model
        args: !callable:aiws.accel_trainer:AccelTrainingArguments
            kwargs:
                <<: *training_args
                accelerator_args: *accel_trainer_args
        data_collator: *data_collator
        train_dataset: *train_dataset
        eval_dataset: *eval_dataset
        tokenizer: *tokenizer
        callbacks: *callbacks



# Final Configuraiton
output_dir: "output_models/test_model"
logging_dir: "output_models/test_model/runs/Multi-GPU_1720462641924163985"
experiment_name: "Multi-GPU"
experiment_description: "Try training on multiple GPUs"
trainer: *trainer
do_save: True