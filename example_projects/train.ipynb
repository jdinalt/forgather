{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5862a8b9-ea0b-4fec-b150-8cbb3218e8bb",
   "metadata": {},
   "source": [
    "# Training Notebook\n",
    "\n",
    "Configuration details: [Configuration Notebook](project_config.ipynb)\n",
    "\n",
    "Run project training configurations and generate training scripts from project meta-data.\n",
    "\n",
    "https://huggingface.co/blog/codeparrot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7715292a-61c8-4be5-9cfd-72530c25e72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Micro Llama\n",
       "\n",
       "Train a tiny llama model.\n",
       "\n",
       "d=256, h=4, l=4, mlp=1024, tokenizer=TinyStories 2K"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Meta Config\n",
       "Project Directory: micro_llama\n",
       "\n",
       "Meta Config: [micro_llama/meta.yaml](micro_llama/meta.yaml)\n",
       "\n",
       "Template Search Paths:\n",
       "- [micro_llama/templates](micro_llama/templates)\n",
       "- [../templates](../templates)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Available Configurations\n",
       "- [micro_llama.yaml](micro_llama/templates/configs/micro_llama.yaml)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Active Configuration -------------------\n",
      "Project: micro_llama\n",
      "Configuration: configs/micro_llama.yaml\n",
      "Name: Micro Llama\n",
      "Description: Train a tiny LLama model from scratch.\n",
      "Output Directory: micro_llama/output_models/default_model\n",
      "Logging Directory: micro_llama/output_models/default_model/runs/Micro Llama_1721346357964679937\n",
      "Save Model: True\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "modules_path = os.path.join('..', 'src')\n",
    "if modules_path not in sys.path: sys.path.insert(0, modules_path)\n",
    "from IPython import display\n",
    "\n",
    "from forgather.config import load_config, ConfigEnvironment\n",
    "from aiws.notebooks import get_train_cmdline, make_train_script\n",
    "from aiws.config import base_preprocessor_globals, MetaConfig\n",
    "from aiws.training_loop import TrainingScriptConfig\n",
    "import aiws.notebooks as nb\n",
    "\n",
    "# Set project:\n",
    "project_directory = \"micro_llama\"\n",
    "\n",
    "# Set configuration:\n",
    "config_template = \"micro_llama.yaml\"\n",
    "\n",
    "\n",
    "nb.show_project_readme(project_directory)\n",
    "meta = MetaConfig(project_directory)\n",
    "nb.display_meta(meta, \"### Meta Config\\n\")\n",
    "nb.list_templates(meta.find_templates(meta.config_prefix), \"### Available Configurations\\n\")\n",
    "config_template_path = os.path.join(meta.config_prefix, config_template)\n",
    "environment = ConfigEnvironment(\n",
    "    searchpath=meta.searchpath,\n",
    "    globals = base_preprocessor_globals() | dict(project_directory=project_directory)\n",
    ")\n",
    "\n",
    "config = environment.load(config_template_path).config\n",
    "print(f\"{' Active Configuration ':-^60}\")\n",
    "print(f\"Project: {project_directory}\")\n",
    "print(f\"Configuration: {config_template_path}\")\n",
    "print(f\"Name: {config.experiment_name}\")\n",
    "print(f\"Description: {config.experiment_description}\")\n",
    "print(f\"Output Directory: {config.output_dir}\")\n",
    "print(f\"Logging Directory: {config.logging_dir}\")\n",
    "print(f\"Save Model: {config.do_save}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62d15b-9479-4ee0-a5d0-e148e45572e9",
   "metadata": {},
   "source": [
    "### Launch Notebook Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff957464-78cc-48c0-a6e3-7a68cc9df5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n",
      "Creating directory: micro_llama/output_models/default_model/runs/Micro Llama_1721345624489813743\n",
      "**** Training Started *****\n",
      "experiment_name: Micro Llama\n",
      "experiment_description: Train a tiny LLama model from scratch.\n",
      "output_dir: micro_llama/output_models/default_model\n",
      "logging_dir: micro_llama/output_models/default_model/runs/Micro Llama_1721345624489813743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61158acf774c436c8401417a31c2175a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_examples: 2,119,680\n",
      "total_train_samples: 2,119,680\n",
      "per_device_train_batch_size: 64\n",
      "actual_per_device_batch_size: 64\n",
      "total_train_batch_size: 64\n",
      "max_steps: 2,000\n",
      "total_parameters: 5.2M\n",
      "trainable_parameters: 5.2M\n",
      "model:\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(2000, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (up_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (down_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=2000, bias=False)\n",
      ")\n",
      "\n",
      "\n",
      "2024-07-18 23:33:49          100  0.0   train-loss: 5.21237   learning-rate: 1.00e-03\n",
      "2024-07-18 23:33:54          200  0.01  train-loss: 3.80752   learning-rate: 1.00e-03\n",
      "2024-07-18 23:33:58          300  0.01  train-loss: 3.2541    learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:02          400  0.01  train-loss: 2.95774   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:06          500  0.02  train-loss: 2.70131   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:11          600  0.02  train-loss: 2.59723   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:14          700  0.02  train-loss: 2.52392   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:18          800  0.02  train-loss: 2.4506    learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:23          900  0.03  train-loss: 2.34248   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:27        1,000  0.03  train-loss: 2.30927   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94269bca85b74ba5bbdd54f232360452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-18 23:34:27        1,000  0.03  eval-loss:  2.31061   \n",
      "2024-07-18 23:34:31        1,100  0.03  train-loss: 2.25196   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:35        1,200  0.04  train-loss: 2.21356   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:39        1,300  0.04  train-loss: 2.19952   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:43        1,400  0.04  train-loss: 2.15759   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:47        1,500  0.05  train-loss: 2.03841   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:50        1,600  0.05  train-loss: 2.11876   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:55        1,700  0.05  train-loss: 2.00525   learning-rate: 1.00e-03\n",
      "2024-07-18 23:34:59        1,800  0.05  train-loss: 2.02464   learning-rate: 1.00e-03\n",
      "2024-07-18 23:35:03        1,900  0.06  train-loss: 1.96943   learning-rate: 1.00e-03\n",
      "2024-07-18 23:35:08        2,000  0.06  train-loss: 2.03572   learning-rate: 1.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bfae2b7de2464b9ff227b249c1c65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-18 23:35:08        2,000  0.06  eval-loss:  2.01092   \n",
      "train_runtime: 83.04\n",
      "train_samples: 128,000\n",
      "step: 2,000\n",
      "train_samples_per_second: 1.541e+03\n",
      "train_steps_per_second: 24.09\n",
      "train_loss: 2.533\n",
      "epoch: 0.06039\n",
      "\n",
      "**** Training Completed *****\n",
      "{'train_runtime': 83.0407645702362, 'train_samples': 128000, 'step': 2000, 'train_samples_per_second': 1541.412, 'train_steps_per_second': 24.085, 'train_loss': 2.5329842567443848, 'epoch': 0.06038647342995169}\n",
      "[2024-07-18 23:35:08,777] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Model saved to: micro_llama/output_models/default_model\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "from aiws.training_loop import training_loop\n",
    "\n",
    "notebook_launcher(\n",
    "    training_loop,\n",
    "    args=(project_directory, config_template,),\n",
    "    num_processes=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba86cc3-734d-46b4-8813-d8494cbc41b1",
   "metadata": {},
   "source": [
    "### Generate Training Script\n",
    "\n",
    "```python\n",
    "def make_train_script(\n",
    "    project_directory,\n",
    "    config_template=None,\n",
    "    script_name='train.sh',\n",
    "    nproc='gpu',\n",
    "    cuda_devices=None\n",
    "):\n",
    "```\n",
    "Generate a bash training script from a project meta-config\n",
    "\n",
    "The generated script will be written to 'project_directory' and all paths will be\n",
    "relative to this location.\n",
    "\n",
    "- project_directory: The project directory. Assumes meta-config is 'meta_config.yaml'\n",
    "- script_name: The name of the output script. If none, the script can be specified on the command-line.\n",
    "- nproc: Number of processes; 'gpu' is number of available GPUs\n",
    "- cuda_devices: List of CUDA devices to limit training to.  \n",
    "\n",
    "i.e. If you wish to only CUDA 0 and 1, then \"0,1\"\n",
    "\n",
    "```\n",
    ".../my_project$ ./train.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbec75bc-20e7-49d5-a7c2-8a3af0c8f4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Generated Shell Script\n",
       "[train.sh](train.sh)\n",
       "```bash\n",
       "#!/bin/bash\n",
       "CUDA_VISIBLE_DEVICES='0,1' torchrun --standalone --nproc-per-node 'gpu' '../../scripts/train_script.py' -p '.' -s '../../src' \"base_config.yaml\"\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select name of generaed script.\n",
    "script_name = 'base_2gpu.sh'\n",
    "\n",
    "make_train_script(\n",
    "    project_directory=project_directory,\n",
    "    config_template=config_template,\n",
    "    script_name=script_name,\n",
    "    cuda_devices=\"0,1\")\n",
    "\n",
    "# Read back to verify\n",
    "with open(os.path.join(project_directory, script_name), 'r') as f:\n",
    "    md = (\n",
    "        f\"#### Generated Shell Script\\n\"\n",
    "        f\"[train.sh](train.sh)\\n\"\n",
    "        f\"```bash\\n{f.read()}\\n```\"\n",
    "    )\n",
    "    display.display(display.Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94085b9-a00a-4bda-914c-9de37c7c92a4",
   "metadata": {},
   "source": [
    "### Run Script from Notebook\n",
    "Lauch the training script from the notebook.\n",
    "\n",
    "Note: The terminal emulation of the notebook is lacking, thus rendering of progress bars may be broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba880a-18c2-4e5e-b845-985d29d95972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{get_train_cmdline(meta, cuda_devices='0')} '{config_template}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c0e8f-aa96-4547-8239-bb4404556b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{get_train_cmdline(meta, cuda_devices=\"0\")} '{config_template}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855f14c-92d4-4adc-804a-f3ff17eedeb8",
   "metadata": {},
   "source": [
    "### View in Tensorboard\n",
    "Note: If the notebook is running on the same machine as the trainer, remove \"--bind_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba51dd-1914-4f24-a785-ff31e56f07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --bind_all --logdir \"{config.output_dir}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a75244-f9f6-44d9-9eea-81055b1f7f68",
   "metadata": {},
   "source": [
    "#### Generate Bash Script\n",
    "\n",
    "This will output a shell-script which will invoke the training script with the arguments for this project.\n",
    "\n",
    "```bash\n",
    "./train.sh path/to/experiment.yaml\n",
    "```\n",
    "\n",
    "If 'cuda_devices' is not None, this can restrict execution to a sub-set of available GPUs.\n",
    "```python\n",
    "# Restrict training to GPU's 0 and 1\n",
    "make_bash_script(metacfg, cuda_devices=\"0,1\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c0efd5-d7be-4afd-9a10-87d51edf2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bash_script(metacfg, cuda_devices=\"0\")\n",
    "\n",
    "# Read back to verify\n",
    "with open('train.sh', 'r') as f:\n",
    "    md = (\n",
    "        f\"#### Generated Shell Script\\n\"\n",
    "        f\"[train.sh](train.sh)\\n\"\n",
    "        f\"```bash\\n{f.read()}\\n```\"\n",
    "    )\n",
    "    display.display(display.Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a8337a-08a2-411f-8173-4946347bcc4b",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Note: These will show the target directory and ask for confirmation before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f81bf-d882-42fe-915c-f52336a4e774",
   "metadata": {},
   "source": [
    "#### Delete All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96252b4b-1b5e-4538-85e3-c4616a7c32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.delete_dir(metacfg.model_dir, \"Delete all models in project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7e0b2-b8c3-4d77-88d7-fe17f80541b4",
   "metadata": {},
   "source": [
    "#### Delete Configuration Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980be21d-b95d-4ef5-baf4-2378e5971a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.delete_dir(config.output_dir, \"Delete output directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
