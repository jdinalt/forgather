-- extends 'models/abstract/custom_causal_lm.yaml'

-- block model_meta_config
    == super()
    -- set model_def.name = "Causal Transformer"
    -- set model_def.description = "A causal transformer model, based upon 'Attention is All You Need'"
    -- set model_def.cls = 'CausalTransformer'
    -- set model_def.cfg_cls = 'CausalTransformerConfig'
    -- set model_def.config_path = joinpath(ns.model_src_dir, 'causal_transformer.py')
    -- set model_def.model_path = model_def.config_path
-- endblock model_meta_config


-- block model_tokenizer
    -- include 'tokenizers/tiny_8k.yaml'
<< endblock model_tokenizer


-- block model_config
    == super()
    hidden_size: 512
    num_attention_heads: 8
    num_hidden_layers: 6
    max_sequence_length: !singleton:getattr
        - *tokenizer
        - "model_max_length"
    dim_feedforward: 2048
    initializer_range: 0.02
    embedding_dropout: 0.10
    layer_dropout: 0.10
    residual_dropout: 0.0
    attention_dropout: 0.0
    activation_dropout: 0.0
<< endblock model_config