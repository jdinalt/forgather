-- extends 'datasets/abstract/base_datasets.yaml'

-- block datasets_meta_config
    == super()
    -- set datasets_ns.name = "Tiny Stories"
    -- set datasets_ns.description = "Tiny Stories, as a on-disk dataset."
    -- set datasets_ns.train_dataset_split = "train"
    -- set datasets_ns.eval_dataset_split = "validation"
    -- set datasets_ns.train_dataset_id = joinpath(ns.datasets_dir, "roneneldan-TinyStories")
    -- set datasets_ns.eval_dataset_id = datasets_ns.train_dataset_id
    -- set datasets_ns.train_load_method = "datasets:load_from_disk"
    -- set datasets_ns.eval_load_method = datasets_ns.train_load_method

    # Sliced to 10% -- the abridged version.
    -- set datasets_ns.train_select_range = 0.1
    -- set datasets_ns.eval_select_range = 500
-- endblock datasets_meta_config

-- block preprocess_args
## See: https://huggingface.co/docs/transformers/main_classes/tokenizer
tokenize_args: &tokenize_args
    truncation: True
    max_length: 512
<< endblock preprocess_args