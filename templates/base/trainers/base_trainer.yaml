-- set trainer_def = namespace()
-- from 'inc/formatting.jinja' import h2, h3, sep
-- filter trim()
-- block trainer_meta_config
    -- set trainer_def.name = "Base Trainer"
    -- set trainer_def.description = "A ML model trainer"
    -- block trainer_vars
        ## Define vars like this
        ## -- set trainer_def.foo = '<bar>'
        ##
    << endblock trainer_vars
<< endblock trainer_meta_config

-- block trainer_header
# Name: {{ trainer_def.name }}
# Description: {{ trainer_def.description }}
<< endblock trainer_header
-- endfilter ## trim()


== h3('Trainer Args')

-- filter trim()
-- block trainer_args
.define: &trainer_args
    # Base Trainer Defaults
    # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments
    output_dir: "{{ ns.output_dir }}"
    logging_dir: "{{ ns.logging_dir }}"
    overwrite_output_dir: True
    per_device_train_batch_size: 16 ## Decrease on OOM errors!
    per_device_eval_batch_size: 32
    learning_rate: 1.0e-4 ## Relatively safe, but not idea for most models.
    num_train_epochs: 1
    eval_steps: 100
    logging_steps: 500
    eval_strategy: "steps"
    save_strategy: "no"
    logging_strategy: "steps"
    lr_scheduler_type: "cosine"
    ## Additional common params
    ## use_cpu: True ; Set in config for debugging
    ## max_steps: -1
    ## seed: -1
    ## device: null
    ## dataloader_num_workers: 0
    ## dataloader_pin_memory: True
    ## dataloader_persistent_workers: False
    ## dataloader_prefetch_factor: null ; defaults to 2, if num_workers
    ## dataloader_drop_last: False
    ## logging_first_step: False
    ## eval_delay: 0
    ## save_total_limit: 2
    ## torch_compile: False
    ## torch_compile_backend: null
    ## torch_compile_mode: null
    
<< endblock trainer_args
-- endfilter


== h3('Trainer Constructor')

-- filter trim()
-- block trainer_constructor required
## .define: &trainer
<< endblock trainer_constructor
-- endfilter
