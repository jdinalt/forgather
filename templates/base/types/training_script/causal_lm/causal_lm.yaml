-- extends "types/training_script/training_script.yaml"
-- from 'inc/formatting.jinja' import h2, sep

-- block config_metadata
    == super()
    ## This needs to be set to True for a custom model.
    -- set ns.trust_remote_code = False
-- endblock config_metadata


-- block variable_listing
    == super()
# ns.trust_remote_code: {{ ns.trust_remote_code }}
<< endblock variable_listing


-- block model_constructor_args
## These are-runtime model construction arguments which
## typically control things like Flash Attention or what
## data-type to convert the model to.
# https://huggingface.co/docs/transformers/en/model_doc/auto
.define: &model_constructor_args {}
-- endblock model_constructor_args


-- block model_definition
## Depending upon the value 'create_new_model,' either load an existing
## model or construct a new one.
## The details for constructing a new model need to be filled in by
## a child template, while an existing model should work 'as-is'
-- filter trim()
    -- if ns.create_new_model
        -- block construct_new_model
# Undefined model constructor
        << endblock construct_new_model
    -- else
        -- block load_model
            -- include 'models/abstract/load_model.yaml'
        << endblock load_model
    -- endif
-- endfilter
-- endblock model_definition


-- block trainer_callbacks
    -- include 'callbacks/loggers.yaml'
<< endblock trainer_callbacks


-- block datacollator
# Data collator for causal model
# Batches are dynamically padded to longest sequence
# labels are set to input_ids, with pad tokens set to -100
# https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling
.define: &data_collator !singleton:transformers:DataCollatorForLanguageModeling@data_collator
    args:
        - *tokenizer
    kwargs:
        mlm: False
        return_tensors: pt
-- endblock datacollator


-- block trainer_definition
    ## Set defulat trainer
    -- include 'trainers/trainer.yaml'
-- endblock trainer_definition