-- set use_flash_attn2
    torch_dtype: !callable:aiws.construct:torch_dtype [ "bfloat16" ]
    attn_implementation: "flash_attention_2"
-- endset