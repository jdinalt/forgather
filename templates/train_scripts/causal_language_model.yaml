-- extends "abstract_base_types/training_script.yaml"
-- from 'inc/formatting.jinja' import h2, sep


-- block model_constructor_args
# https://huggingface.co/docs/transformers/en/model_doc/auto
.define: &model_constructor_args {}
-- endblock model_constructor_args


-- block model_definition
-- filter trim()
    -- if ns.CREATE_NEW_MODEL
        -- block construct_new_model
# Undefined model constructor
        << endblock construct_new_model
    -- else
        -- block load_model
            -- include 'models/abstract/load_model.yaml'
        << endblock load_model
    -- endif
-- endfilter
-- endblock model_definition


-- block trainer_callbacks
    -- include 'callbacks/loggers.yaml'
<< endblock trainer_callbacks


-- block datacollator
# Data collator for causal model
# Batches are dynamically padded to longest sequence
# labels are set to input_ids, with pad tokens set to -100
# https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling
.define: &data_collator !callable:transformers:DataCollatorForLanguageModeling
    args:
        - *tokenizer
    kwargs:
        mlm: False
        return_tensors: pt
-- endblock datacollator


-- block trainer_definition
    -- include 'trainers/trainer.yaml'
-- endblock trainer_definition